<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

  <meta name="description" content="Holden Lee's Research Notebook">
  <meta name="author" content="Holden Lee">
    
  <title>DL generalization</title>

  <!-- Bootstrap core CSS -->
  <link href="../../../../bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

  <!-- Custom styles for this template -->
  <link href="../../../../css/blog.css" rel="stylesheet">
  <link href="../../../../css/default.css" rel="stylesheet">

  <!-- Extension : Footnotes -->
  <link href="../../../../footnotes/css/footnotes.css" rel="stylesheet">

  <!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->
  

  <!-- Extension : Collapsible lists @ http://code.stephenmorley.org/javascript/collapsible-lists/-->
  <link href="../../../../collapsible_lists/css/collapsible.css" rel="stylesheet">
  <script type="text/javascript" src="../../../../collapsible_lists/js/CollapsibleLists.js"></script>

</head>

<body>

<!-- Navigation bar. navbar-inverse is black, navbar-default is white.-->
<!-- To make a button active (pressed), use <li class="active"> -->
<div id="header">
  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../../">Notebook</a>
      </div>
      <div id="navbar" class="collapse navbar-collapse">
        <ul class="nav navbar-nav">
          <li><a href="../../../../">Home</a></li>
          <li><a href="../../../../sitemap.html">Sitemap</a></li>
<!-- TODO: Distinguish between PAPERS, RESEARCH QUESTIONS, BOOKS -->
<!-- TODO: make this part a for loop over main pages -->
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </nav>
</div>
<!-- Content -->
<!--div id="content">
  <h1>Mental Wilderness</h1>-->



<div class="container">
  <div id="content">
    <div class="page header">
      <h1>DL generalization</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-09-19 
          , Modified: 2016-09-19 
	</p>
      
       <p>Tags: <a href="../../../../tags/dictionary%20learning.html">dictionary learning</a>, <a href="../../../../tags/sparse%20coding.html">sparse coding</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#group-sparsity">Group sparsity</a><ul>
 <li><a href="#from-subspaces-to-vectors">From subspaces to vectors</a><ul>
 <li><a href="#try-1">Try 1</a></li>
 <li><a href="#try-2">Try 2</a></li>
 </ul></li>
 </ul></li>
 <li><a href="#big-picture">Big picture</a><ul>
 <li><a href="#dl-experiments">DL experiments</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="group-sparsity">Group sparsity</h2>
<p>Suppose that we have group sparsity with partition <span class="math inline">\(\bigsqcup S_i\)</span>.</p>
<p>In AGMM15, we get instead <span class="math inline">\(\ve{E_1+E_2+E_3}\le O^*\pa{\fc{\mu k^2 \ln^2 n}{m\sqrt n}\max |S_i|^2}\)</span>. For example, if <span class="math inline">\(\max|S_i|^2\)</span> is a constant, the bound is still the same. As long as this is <span class="math inline">\(\le O^*\pf{k}{m\ln m}\)</span>, the bound is still OK.</p>
<p>(Basically we get larger terms in <span class="math inline">\(E_2,E_3\)</span>: In <span class="math inline">\(E_2\)</span> we have <span class="math inline">\(\sum_{i,j\in S_k, i\ne j} q_i \be_i\be_i' A_jA_j^T\)</span>—this is larger because <span class="math inline">\(i,j\)</span> cooccur, we have <span class="math inline">\(q_i\)</span> rather than <span class="math inline">\(q_{i,j}\)</span>, <span class="math inline">\(q_i\)</span> is <span class="math inline">\(\Te\pf km\)</span> not <span class="math inline">\(\Te \pf{k^2}{m^2}\)</span>. Lump these terms into the 1st sum. Ditto for <span class="math inline">\(E_3\)</span>.)</p>
<p>See notebook 15 (end) for details.</p>
<p>Rely on <a href="../../../../posts/math/algebra/linear/matrix_analysis/perturbation.html">modification of Davis-Kahan</a>.</p>
<p>Take <span class="math inline">\(u,v\)</span> and estimate <span class="math inline">\(M_{u,v} = \E \an{u,Ax}\an{v,Ax}(Ax)(Ax)^T\)</span>. Instead of taking the largest singular vector, take the subspace spanned by all singular values of size <span class="math inline">\(\ge C_1 \fc{k}{m}\)</span>. What is the angle between this subspace and the projection of this onto the space of <span class="math inline">\(A_{\cdot i}\)</span> where <span class="math inline">\(i\in \Supp(u)\cap \Supp(v)\)</span>? By generalized Davis-Kahan, <span class="math inline">\(\fc{C_0\fc km - C_1\fc km}{C_2 \fc{k}{m\ln m}} = \fc{C_3}{\ln m}\)</span>.</p>
<p>Now iteratively refine the set of subspaces <span class="math inline">\(\mathcal S\)</span>. Start with a subspace <span class="math inline">\(V\)</span>, and set of atoms <span class="math inline">\(\mathcal A=\phi\)</span>. Let <span class="math inline">\(\ep = \fc{C}{\ln m}\)</span>.</p>
<h3 id="from-subspaces-to-vectors">From subspaces to vectors</h3>
<h4 id="try-1">Try 1</h4>
<p>Do a DFS.</p>
<ul>
<li>If the angle between <span class="math inline">\(V\)</span> and any other subspace in <span class="math inline">\(\mathcal S\)</span> is <span class="math inline">\(&gt;\ep\)</span>, not counting subspaces where all canonical angles between <span class="math inline">\(V\)</span> and it are <span class="math inline">\(&lt;\ep\)</span>, then find the orthogonal complement of atoms that are close to being in <span class="math inline">\(V\)</span>, and let that be an atomic subspace. Now go upwards in the tree.</li>
<li>Otherwise, approximately intersect <span class="math inline">\(V\)</span> with a subspace making it smaller, to get <span class="math inline">\(W\)</span>. Draw <span class="math inline">\(V\to W\)</span>. Repeat with <span class="math inline">\(W\)</span>.</li>
</ul>
<p>NO: These subspaces aren’t spanned by the vectors!</p>
<h4 id="try-2">Try 2</h4>
<p>This seems difficult if we don’t make assumptions.</p>
<p>Let’s suppose for each vector, it is in a small subset of things that cooccur. (Make this more precise.) Ex. constant size.</p>
<p>The issue is that some subspaces/vectors will be inside a <span class="math inline">\(S_i\)</span> (one of the groups in group sparsity) completely—if we can get all the vectors inside a <span class="math inline">\(S_i\)</span> all we have to do is get a basis, good—but others will be inside <span class="math inline">\(\spn(S_i,S_j)\)</span>, or more, and we have to tell them apart.</p>
<p>Idea: do a best-first search, adding subspaces until <span class="math inline">\(\si_{C+1}\)</span> is too large, in which case discard it. (Singular value may not be the right measure here. Maybe: max perpendicular to span of <span class="math inline">\(v_1,\ldots, v_C\)</span>?) The large subsets we get will be inside a <span class="math inline">\(S_i\)</span>.</p>
<p>(Or some kind of clustering?)</p>
<h2 id="big-picture">Big picture</h2>
<p>What are we trying to do here?</p>
<ul>
<li>Generalize the settings under which we have provable DL.</li>
<li>Define agnostic DL as something worth studying and approachable.
<ul>
<li>Idea that neural nets are easier to train if you have more nodes. More opportunities to converge to columns of <span class="math inline">\(A\)</span>.
<ul>
<li>Look at convergene in easy case?</li>
</ul></li>
<li>Have <em>any</em> provable result on agnostic DL.
<ul>
<li>Why we would have agnostic rather than normal: some features are correlated, so the basis could rotate within the subspace of correlated vectors.</li>
</ul></li>
</ul></li>
</ul>
<p>We already have practical DL algorithms that just rely on AM and adding sparsity penalty. How could theory improve this? (AGM14, AGMM15 don’t look like they are used in practice.) Are there theoretical guarantees for AM with sparsity penalty? (There are theoretical guarantees for AM w/ decoding.)</p>
<p>What’s a better model in practice: DL with sparsity or <span class="math inline">\(L^1\)</span>? (And how does ICA compare?)</p>
<p>TODO: program DL and look at empirical performance.</p>
<p>TODO: 2-layer NN learning dictionary?</p>
<p>Dual view between <span class="math inline">\(x=Ah\)</span> and <span class="math inline">\(h=A^Tx\)</span>?</p>
<p>Sparse recovery has this thing where <span class="math inline">\(\ved_1\)</span> actually optimizes <span class="math inline">\(\ved_0\)</span>. Not true of sparse coding?</p>
<h3 id="dl-experiments">DL experiments</h3>
<p>What to try?</p>
<ul>
<li>Program classic AM DL with <span class="math inline">\(L^1\)</span> sparsity penalty.</li>
<li>Program AGMM15 DL.</li>
<li>ICA.</li>
</ul>
<p>Test sets</p>
<ul>
<li>Image patches</li>
<li>Words embeddings</li>
<li>Artificial data (sparse, Laplace, etc. <span class="citation" data-cites="Bianca">@Bianca</span>)</li>
<li>Artificial group-sparse data, or data correlated in some way (ex. geometrically).</li>
</ul>
<p>How to evaluate?</p>
<ul>
<li>Closeness of columns.</li>
<li>Loss: how much sparsity, and how far away. (Reconstruction error)
<ul>
<li>How does reconstruction error compare to SVD? (Make dimensions comparable.)</li>
</ul></li>
<li>Put in random SVM on top. Can it learn the SVM well?</li>
</ul>
<p>Experiments with NN</p>
<ul>
<li>Top-<span class="math inline">\(k\)</span> thresholding to enforce <span class="math inline">\(k\)</span>-sparsity. Autoencoder?</li>
<li>Try group-sparse penalty/architecture.</li>
<li>Does adding more nodes improve things (cf. agnostic DL)?</li>
</ul>
<p>I think sparseness helps interpretability. Does convergence become quicker if we have sparsity?</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class="st_facebook_large" displayText="Facebook"></span>
    <span class="st_twitter_large" displayText="Tweet"></span>
    <span class="st_googleplus_large" displayText="Google +"></span>
    <span class="st_reddit_large" displayText="Reddit"></span>
    <span class="st__large" displayText></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>


<!-- Footer -->
<div id="footer">
  <div class="container">
    Built with
    <a href="http://jaspervdj.be/hakyll">Hakyll</a> 
    using 
    <a href="http://www.getbootstrap.com">Bootstrap</a>, 
    <a href="http://www.disqus.com">Disqus</a>,
    <a href="http://ignorethecode.net/blog/2010/04/20/footnotes/">Footnotes.js</a>,
    <a href="http://highlightjs.org/">Highlight.js</a>, 
    <a href="http://www.mathjax.org">MathJax</a>, 
    and <a href="http://www.sharethis.com">ShareThis</a>.
  </div>
</div>
</body>

</html>

<!-- SCRIPTS -->
<!-- jQuery-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="../../../../bootstrap/js/bootstrap.min.js"></script>

<!-- Extension : Highlight.js @ https://highlightjs.org/ -->
<!-- Syntax highlighting tomorrow-night-bright, agate-->
<link rel="stylesheet" href="../../../../highlight/css/tomorrow-night-bright.css">
<script src="../../../../highlight/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- Extension : MathJax @ https://docs.mathjax.org/en/v2.5-latest/tex.html -->
<!-- MathJax/config/local/local.js contains macros. Need to provide entire URL-->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,http://holdenlee.github.io/notebook/MathJax/config/local/local"></script>

<!-- Extension : Footnotes @ http://ignorethecode.net/blog/2010/04/20/footnotes/ -->
<script src="../../../../footnotes/js/footnotes.js"></script>

<!-- Extension : Disqus @ http://disqus.com -->
<!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->

<script src="../../../../disqus/js/disqus.js"></script>



<!-- Extension : Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73261814-1', 'auto');
  ga('send', 'pageview');

</script>

