<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

  <meta name="description" content="Holden Lee's Research Notebook">
  <meta name="author" content="Holden Lee">
    
  <title>[BKS15] Dictionary Learning and Tensor Decomposition via the Sum-of-Squares Method</title>

  <!-- Bootstrap core CSS -->
  <link href="../../../../bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

  <!-- Custom styles for this template -->
  <link href="../../../../css/blog.css" rel="stylesheet">
  <link href="../../../../css/default.css" rel="stylesheet">

  <!-- Extension : Footnotes -->
  <link href="../../../../footnotes/css/footnotes.css" rel="stylesheet">

  <!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->
  

  <!-- Extension : Collapsible lists @ http://code.stephenmorley.org/javascript/collapsible-lists/-->
  <link href="../../../../collapsible_lists/css/collapsible.css" rel="stylesheet">
  <script type="text/javascript" src="../../../../collapsible_lists/js/CollapsibleLists.js"></script>

</head>

<body>

<!-- Navigation bar. navbar-inverse is black, navbar-default is white.-->
<!-- To make a button active (pressed), use <li class="active"> -->
<div id="header">
  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../../">Notebook</a>
      </div>
      <div id="navbar" class="collapse navbar-collapse">
        <ul class="nav navbar-nav">
          <li><a href="../../../../">Home</a></li>
          <li><a href="../../../../sitemap.html">Sitemap</a></li>
<!-- TODO: Distinguish between PAPERS, RESEARCH QUESTIONS, BOOKS -->
<!-- TODO: make this part a for loop over main pages -->
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </nav>
</div>
<!-- Content -->
<!--div id="content">
  <h1>Mental Wilderness</h1>-->



<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[BKS15] Dictionary Learning and Tensor Decomposition via the Sum-of-Squares Method</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-30 
          , Modified: 2016-08-30 
	</p>
      
       <p>Tags: <a href="../../../../tags/dictionary%20learning.html">dictionary learning</a>, <a href="../../../../tags/tensor.html">tensor</a>, <a href="../../../../tags/sparse%20coding.html">sparse coding</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#noisy-tensor-decomposition-1">Noisy tensor decomposition 1</a><ul>
 <li><a href="#reduction">Reduction</a></li>
 <li><a href="#algorithm">Algorithm</a></li>
 </ul></li>
 <li><a href="#noisy-tensor-decomposition">Noisy tensor decomposition</a><ul>
 <li><a href="#algorithm-1">Algorithm</a><ul>
 <li><a href="#sampling-pseudo-distributions">Sampling pseudo-distributions</a></li>
 </ul></li>
 <li><a href="#proof">Proof</a></li>
 </ul></li>
 <li><a href="#dictionary-learning">Dictionary learning</a></li>
 <li><a href="#todo">Todo</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><strong>Theorem</strong>. Given <span class="math inline">\(\ep&gt;0,\si\ge 1, \de&gt;0\)</span> there exists <span class="math inline">\(d\)</span> and a polytime algorithm that given input</p>
<ul>
<li>a <span class="math inline">\(\si\)</span>-dictionary <span class="math inline">\(n\times m\)</span>,</li>
<li><span class="math inline">\((d,\tau=n^{-\de})\)</span>-nice <span class="math inline">\(\{x\}\)</span>,</li>
<li><span class="math inline">\(n^{O(1)}\)</span> samples from <span class="math inline">\(y=Ax\)</span>,</li>
</ul>
<p>outputs with probability <span class="math inline">\(\ge 0.9\)</span> a set <span class="math inline">\(\ep\)</span>-close to columns of <span class="math inline">\(A\)</span>.</p>
<ul>
<li>A <span class="math inline">\(\si\)</span>-dictionary has <span class="math inline">\(AA^T\preceq \si I\)</span> (analytic proxy for overcompleteness <span class="math inline">\(\fc mn\)</span>).</li>
<li>A distribution is <span class="math inline">\((d,\tau)\)</span>-nice if <span class="math inline">\(\E x_i^{d/2}x_j^{d/2}\le \tau\)</span> for all <span class="math inline">\(i\ne j\)</span>, and <span class="math inline">\(\E x^\al=0\)</span> for all <span class="math inline">\(|\al|\le d\)</span>. (Ex. Bernoulli(<span class="math inline">\(\tau\)</span>) times Gaussian with <span class="math inline">\(\E z_i^d = \rc\tau\)</span>.)</li>
</ul>
<p>Note on running time:</p>
<ul>
<li><span class="math inline">\(\ep\)</span>-accuracy with running time depending on <span class="math inline">\(\poly\prc{\ep}\)</span> in the exponent. So this is better for giving an initial solution for local search methods.</li>
</ul>
<h2 id="noisy-tensor-decomposition-1">Noisy tensor decomposition 1</h2>
<p>Given noisy version of <span class="math inline">\(\sumo im \an{a^i, u}^d = \ve{A^Tu}_d^d\)</span>, recover <span class="math inline">\(\{a^1,\ldots, a^m\}\)</span>.</p>
<p><strong>Theorem</strong> (Noisy tensor decomposition). For every <span class="math inline">\(\ep&gt;0,\si\ge 1\)</span> there exist <span class="math inline">\(d,\tau\)</span> such that a probabilistic <span class="math inline">\(n^{O(\ln n)}\)</span>-time agorithm, given <span class="math inline">\(P\)</span> with <span class="math display">\[\ve{A^T u}_d^d - \tau \ve{u}_2^d \preceq P \preceq \ve{A^Tu}_d^d + \tau \ve{u}_2^d,\]</span> outputs with probability 0.9 <span class="math inline">\(S\)</span> that is <span class="math inline">\(\ep\)</span>-close to <span class="math inline">\(\{a^1,\ldots, a^m\}\)</span>. (<span class="math inline">\(\preceq\)</span> means the difference is a sum of squares.)</p>
<p>Note this allows significant noise since we expect <span class="math inline">\(\ve{A^Tu}_d^d\sim mn^{-\fc d2}\ll \tau\)</span>.</p>
<h3 id="reduction">Reduction</h3>
<p>Take <span class="math display">\[ P = \EE_{i} \an{y_i, u}^{2d}.\]</span></p>
<h3 id="algorithm">Algorithm</h3>
<ol type="1">
<li>Use SoS to find degree-<span class="math inline">\(k\)</span> pseudo-distribution <span class="math inline">\(\{u\}\)</span> that maximizes <span class="math inline">\(P(u)\)</span> under <span class="math inline">\(\ve{u}^2=1\)</span>.</li>
<li>Let <span class="math inline">\(W\)</span> be a product of <span class="math inline">\(O(\ln n)\)</span> rndom linear functions.</li>
<li>Output top eigenvector of <span class="math inline">\(M\)</span>, <span class="math inline">\(M_{ij} = \wt \E W(u)^2 u_iu_j\)</span>.</li>
</ol>
<p><strong>SoS Algorithm</strong>: given <span class="math inline">\(\ep&gt;0, k,n,M\)</span>, <span class="math inline">\(P_1,\ldots, P_m\in \R[x_1,\ldots, x_n]\)</span> with coefficients in <span class="math inline">\([0,M]\)</span>, if there exists a degree <span class="math inline">\(k\)</span> pseudo-distribution with <span class="math inline">\(P_i=0,i\in [m]\)</span>, then we can find <span class="math inline">\(u'\)</span> satisfying <span class="math inline">\(P_i\le \ep, P_i\ge -\ep\)</span> for every <span class="math inline">\(i\in [m]\)</span> in time <span class="math inline">\((n\polylog\prc{M}{\ep})^{O(k)}\)</span>.</p>
<p>(Proof: This is a feasibility problem. Use ellipsoid method.)</p>
<p>Steps</p>
<ol type="1">
<li>If <span class="math inline">\(u\)</span> is an actual distribution, the the output is close to a column.</li>
<li>Generalize to pseudo-distributions.</li>
<li>Generalize to getting all columns.</li>
</ol>
<p>Idea: If not correlated, then <span class="math inline">\(P(u)\)</span> is small. The inequalities can be turned into polynomial inequalities provable by SoS.</p>
<h2 id="noisy-tensor-decomposition">Noisy tensor decomposition</h2>
<p><strong>Theorem</strong> (Noisy tensor decomposition, 4.3). There is a probabilistic algorithm that given</p>
<ul>
<li><span class="math inline">\(\ep&gt;0\)</span></li>
<li><span class="math inline">\(\si \ge 1\)</span></li>
<li>degree <span class="math inline">\(d\ge d(\ep,\si) = O\pf{\ln \si}{\ep}\)</span></li>
<li>noise parameter <span class="math inline">\(\tau \le \tau(\ep) = \Om(\ep)\)</span></li>
<li>degree <span class="math inline">\(d\)</span> polynomial <span class="math inline">\(P\in \R[u]\)</span> such that <span class="math display">\[\ve{A^T u}_d^d - \tau \ve{u}_2^d \preceq P \preceq \ve{A^Tu}_d^d + \tau \ve{u}_2^d,\]</span> outputs with high probability <span class="math inline">\(S\)</span> that is <span class="math inline">\(\ep\)</span>-close to <span class="math inline">\(\{a^1,\ldots, a^m\}\)</span>. (<span class="math inline">\(\preceq\)</span> means the difference is a sum of squares.)</li>
</ul>
<h3 id="algorithm-1">Algorithm</h3>
<ol type="1">
<li>Start with <span class="math inline">\(S=\phi\)</span>. Let <span class="math inline">\(\ga = \fc{C}{\ep}\)</span> for <span class="math inline">\(C\)</span> large enough. ?? What’s <span class="math inline">\(k\)</span>?</li>
<li>Loop:
<ol type="1">
<li>Attempt to find a degree <span class="math inline">\(k\)</span> pseudo-distribution <span class="math inline">\(U\)</span> satisfying the constraints <span class="math inline">\(P(U) \ge 1-\tau\)</span>, <span class="math inline">\(\ve{U}_2^2=1\)</span>, <span class="math inline">\(\an{s,U}^2\le 1-\ga\)</span> for every <span class="math inline">\(s\in S\)</span> (to repel it away from vectors we already found). If fail, stop.</li>
<li>Use the “sampling pseudo-distributions” algorithm to obtain a unit vector <span class="math inline">\(c'\)</span> such that
\begin{align}
P(c') &amp; \ge e^{-\ep d} - \tau\\
\ep &amp;:= O(\pf{\tau}{d} + \fc{\ln \si}{d} \fc{\ln m}k)
\end{align}</li>
<li>Add <span class="math inline">\(c'\)</span> to <span class="math inline">\(S\)</span>.</li>
</ol></li>
</ol>
<h4 id="sampling-pseudo-distributions">Sampling pseudo-distributions</h4>
<h3 id="proof">Proof</h3>
<ol type="1">
<li>Existence of <span class="math inline">\(c'\)</span>.
<ul>
<li><span class="math inline">\(U\)</span> exists because any column of <span class="math inline">\(A\)</span> is a solution. (CHECK columns far!!)</li>
<li><span class="math inline">\(U\)</span> is correlated with <span class="math inline">\(A\)</span>: By assumption, for <span class="math inline">\(\ve{U}_2^2=1\)</span>, <span class="math inline">\(P\in \ve{A^T U}_d^d + \tau[-1,1]\)</span>. Thus <span class="math inline">\(P(U)\ge 1-\tau\)</span> implies <span class="math display">\[\ve{A^TU}_d^d \ge 1-2\tau.\]</span></li>
<li><p>If <span class="math inline">\(U\)</span> correlates well with <span class="math inline">\(A\)</span>, then <span class="math inline">\(U\)</span> correlates with some column of <span class="math inline">\(A\)</span>. <!--Any column of $A$ satisfies $\wt \E \an{c,u}^k \ge e^{-\ep' k}$.--></p>
<p><em>Lemma 6.1</em>. Given</p>
<ul>
<li><span class="math inline">\(A\)</span> is <span class="math inline">\(\si\)</span>-overcomplete</li>
<li><span class="math inline">\(u\)</span> is degree <span class="math inline">\(3k\)</span> pd over <span class="math inline">\(\R^n\)</span> satisfying <span class="math inline">\(\ve{A^Tu}_d^d \ge e^{-\de d}, \ve{u}_2^2 = 1\)</span>,</li>
<li><span class="math inline">\(d-2\mid k\)</span> (?? this seems to be necessary. Think of <span class="math inline">\(k\gg d\)</span>.)</li>
</ul>
<p>there exists a column of <span class="math inline">\(A\)</span> such that <span class="math inline">\(\wt \E \an{c,u}^k \ge e^{-\ep k}\)</span>, <span class="math inline">\(\ep=O(\de + \fc{\ln \si}{d} + \fc{\ln m}k)\)</span>.</p>
<ul>
<li>Motivation for proof: First, let’s see how to prove this if <span class="math inline">\(u\)</span> is an actual vector. By averaging, there is a column such that <span class="math display">\[ \ve{A_{\cdot i}^T u}_{k}^k \ge \pf{1-\ep}{m}\implies A_{\cdot i}^Tu \ge \pf{1-\ep}{m}^{\rc k}\approx e^{\fc{\ln m}{k}}.\]</span> (This also shows why we expect <span class="math inline">\(\ln m\)</span> in the exponent.) This averaging argument still works for pseudo-distributions when <span class="math inline">\(d=k\)</span>—the first inequality still holds true, if we have <span class="math inline">\(d=k\)</span>. (The first inequality would no longer imply the second, though.) (?? Why wouldn’t we take <span class="math inline">\(d=k\)</span>?)</li>
<li>?? For some reason, we want to use an inequality not depending on <span class="math inline">\(m\)</span> (depending on <span class="math inline">\(\ve{A^Tu}_2\)</span> is good, because we have control of this using <span class="math inline">\(\si\)</span>), in which case instead of using the inequality <span class="math inline">\(\ve{v}_{\iy} \ge \fc{\ve{v}_d}{m^{\rc d}}\)</span> we use <span class="math inline">\(\ve{v}_{\iy}^{d-2}\ge \fc{\ve{v}_d^d}{\ve{v}_2^2}\)</span>. We need to change this into a SoS inequality, so replace <span class="math inline">\(\iy\)</span> by <span class="math inline">\(k\)</span> for <span class="math inline">\(k\)</span> large, <span class="math display">\[\ve{v}_d^{\fc{dk}{d-2}} \le \ve{v}_k^k (\ve{v}_2^2)^{\fc{k}{d-2}}\]</span> provable by SoS (expand and then use Muirhead/AM-GM).</li>
<li><!-- For the actual proof, we just need to relate the $d$ and $k$ norms. We want to lower-bound the $k$th norm in terms of the $d$th norm. Use
	$$ \ve{v}_k^{d-2} \ve{v}_2^2 \ge \ve{v}_d^d. $$
	(This is true for $k=1$ by expanding, and hence for all $k$.)-->
<em>Proof</em>. <span class="math display">\[
\ve{A^Tu}_k^k \ge \fc{(\ve{A^Tu}_d^d)^{\fc k{d-2}}}{(\ve{A^Tu}_2^2)^{\fc{k}{d-2}}} \ge \fc{e^{-\de d}}{\si^{\fc k{d-2}}}.
\]</span> Now use averaging on <span class="math inline">\(\ve{A^Tu}_k^k\)</span>.</li>
</ul></li>
<li>Here <span class="math inline">\(\de = O\pf{\tau}d\)</span> so we get <span class="math display">\[ \wt E \an{c,u}^k \ge e^{-\ep' k},\quad \ep' = O(\fc{\tau}d + \fc{\ln \si \ln m}{dk}).\]</span></li>
<li>Example: if <span class="math inline">\(u\)</span> is the actual distribution <span class="math inline">\(u=A_{\cdot i}\)</span> with probability <span class="math inline">\(\rc{m}\)</span>, then <span class="math inline">\(\wt E\an{c,u}^k = \rc m\)</span>. Hence we can’t do better than <span class="math inline">\(k=\Om(\ln m)\)</span>.</li>
</ul></li>
<li><p>Now we analyze the algorithm to “sample pseudo-distributions”: given a PD that correlates with a vector, find a vector close to it. We prove:</p>
<p><strong>Theorem 5.1</strong>. The “sample pseudo-distributions” algorithm, given</p>
<ul>
<li>even <span class="math inline">\(k\ge 0\)</span></li>
<li>degree <span class="math inline">\(k\)</span> p.d. with <span class="math inline">\(\ve{u}_2^2=1\)</span> and <span class="math inline">\(\wt \E\an{c,u}^k \ge e^{-\ep k}\)</span> (?? I think we want this for <span class="math inline">\(k-2\)</span>.)</li>
</ul>
<p>outputs <span class="math inline">\(c'\in \R^n\)</span> with <span class="math inline">\(\an{c,c'}\ge 1-O(\ep)\)</span> with probability <span class="math inline">\(2^{-\fc{k}{\poly(\ep)}}\)</span>.</p>
<ul>
<li>Here is the idea. We want to compute the tensor decomposition by using SVD (because there aren’t other good ways to compute overcomplete tensor decomposition…). Suppose that <span class="math inline">\(u\)</span> were an actual distribution; since it is correlated with <span class="math inline">\(A\)</span>, it is approximately supported on columns of <span class="math inline">\(A\)</span>. Then <span class="math inline">\(\wt \E uu^T = \sum p_i a_i a_i^T\)</span>. The <span class="math inline">\(a_i\)</span> are not uniquely identifiable from this (that’s the whole problem with SVD, why we want tensors in the first place!). Idea: consider <span class="math inline">\(\wt \E p(u) uu^T = \sum p_i p(a_i) a_ia_i^T\)</span> instead. If <span class="math inline">\(p\)</span> is a function that is large on <span class="math inline">\(a_i\)</span> and small on all other <span class="math inline">\(a_j\)</span>, then we win. Take <span class="math inline">\(p\)</span> to be a product of linear factors. If we put in enough factors, then with good probability the <span class="math inline">\(p(a_i)\)</span> will become farther apart, and one outruns the others, <span class="math inline">\(|p(a_i)|\gg \sqrt m|p(a_i)|\)</span>. Take <span class="math inline">\(p =W^2\)</span> where <span class="math inline">\(W\)</span> is a product of <span class="math inline">\(O(\ln n)\)</span> random linear functions <span class="math inline">\(\an{u,v}, \ve{v}_2=1\)</span>.</li>
<li><p>We are given <span class="math inline">\(c\)</span> such that <span class="math inline">\(\wt E\an{c,u}^k \ge e^{-\de d}\)</span> (high correlation) and we want that <span class="math inline">\(c\)</span> to be a value of <span class="math inline">\(v\)</span> that makes (the quadratic form) <span class="math inline">\(v\mapsto \wt \E W(U) \an{v,U}^2\)</span> large. In fact, what we want is that <!--the value is way bigger than any other singular values; i.e.,--> when normalized so the sum of the eigenvalues is 1, the singular value corresponding to <span class="math inline">\(c\)</span> is close to 1.</p>
There are 3 parts. (a) We need to understand how “renormalization” of pseudo-distributions works. (b) We need to show that for our choice of <span class="math inline">\(W\)</span>, we have with some probability a lower bound for <span class="math inline">\(\wt \E W(U) \an{c,U}^2\)</span>. This is the hardest part. (c) We need to show that <span class="math inline">\(c\)</span> is close to the singular vector (which we output). (I think (c) is in essence a matrix/eigenvector perturbation result, but it’s cloaked in the language of SoS here—unravel!)</li>
<li><strong>Lemma</strong> (reweighting). Let <span class="math inline">\(U\)</span> be a degree <span class="math inline">\(k\)</span> p.d. Then for every SoS polynomial <span class="math inline">\(W\)</span> of degree <span class="math inline">\(d&lt;k\)</span>, <span class="math inline">\(\wt \E W&gt;0\)</span>, there exists a degree <span class="math inline">\((k-d)\)</span> p.d. <span class="math inline">\(U'\)</span> such that for every <span class="math inline">\(\deg P \le k-d\)</span>, <span class="math display">\[\wt E_{U'} P(U') = \rc{\wt \E_U W(U)} \wt \E_{U} W(U)P(U).\]</span> <em>Proof</em>. Just verify the positivity property.</li>
<li><p><strong>Lemma 5.2</strong>. Let <span class="math inline">\(U\)</span> be a degree-<span class="math inline">\(k+2\)</span> pseudo-distribution over <span class="math inline">\(\R^n\)</span> with <span class="math inline">\(\ve{U}_2^2=1\)</span> and such that there exists a unit vector <span class="math inline">\(c\in \R^n\)</span> such that <span class="math inline">\(\wt \E\an{c,u}^k \ge e^{-\ep k}\)</span>. <span class="math inline">\(W=\rc{M^{k/2}} \prod_{i=1}^{k/2} w^{(i)}\)</span> where <span class="math inline">\(w^{(i)}\)</span> are iid draws from <span class="math inline">\(w=\an{\xi,u}^2\)</span>, <span class="math inline">\(\xi\sim N(0,I_n)\)</span>. Then with probability <span class="math inline">\(2^{-O\pf{k}{\poly(\ep)}}\)</span>, <span class="math inline">\(\wt E\an{c,u}^2 \ge (1-O(\ep))\wt \E W\)</span>.</p>
(?? Discrepancy between <span class="math inline">\(k\)</span> and <span class="math inline">\(k+2\)</span>.)</li>
<li>By reweighting, there exists a degree 2 p.d. <span class="math inline">\(U'\)</span> such that <span class="math display">\[\wt \E_{U'} \an{v,u'}^2 = \rc{\wt \E_U W(U)} \wt\E_{U} W(U) \an{v,U}^2.\]</span> By Lemma 5.2, the value at <span class="math inline">\(c\)</span> is <span class="math inline">\(\ge 1-O(\ep)\)</span>.</li>
<li><em>Proof</em> (Sketch). This is more likely to be true if <span class="math inline">\(\an{c,\xi}\)</span> is large. Let <span class="math inline">\(w=\an{\xi,u}^2\)</span>. Let <span class="math inline">\(\tau_M\)</span> be such that <span class="math inline">\(\E_{\xi_0|\xi_0\ge \tau_M}\xi_0^2 =M\)</span>. Conditioning on <span class="math inline">\(\an{c,\xi}\ge \tau_{M+1}\)</span> and expanding <span class="math inline">\(\xi=\an{c,\xi}c+\xi'\)</span>, find that <span class="math display">\[ \E_{\xi | \an{c,\xi}\ge \tau_{M+1}} w = M\an{c,u}^2 + \ve{U}_2^2.\]</span> Let <span class="math inline">\(\ol W = \E W = \pa{\an{c,U}^2 + \rc M}^{\fc k2}\)</span> (assume <span class="math inline">\(\ve{u}_2^2=1\)</span>). Algebra shows
\begin{align}
\ol W \an{c,U}^2 &amp;\succeq \pa{1-\fc 2M } \ol W - \pa{1-\rc M}^{\fc k2}\\
\EE_W \wt \E W\an{c,u}^2 &amp; \ge (1-O(\ep)) \EE_W \wt \E W.
\end{align}
<p>(Take <span class="math inline">\(M=\rc{\ep}\ln \prc{\ep}\)</span>.) This is almost what we want, except that we conditioned on <span class="math inline">\(\an{c,\xi}\ge \tau_{M+1}\)</span> and that this is an inequality about absolute values.</p>
<p>To get rid of the conditioning, note that the event <span class="math inline">\(\forall i, \an{\xi^{(i)},c}\ge \tau_{M+1}\)</span> has probability <span class="math inline">\(2^{-O(kM^2)}\)</span>.</p>
To get the result with not-too-small probability, rearrange as <span class="math display">\[
O(\ep) \EE_W \wt \E W \ge \EE_W \wt \E W - \EE_W \wt \E W\an{c,U}^2,
\]</span> bound second moments <span class="math inline">\(\E_W(\wt \E W)^2\)</span>, and use
<ul>
<li><strong>Lemma 5.3</strong>. Let <span class="math inline">\(A,B\)</span> be distributions such that <span class="math inline">\(0\le A\le B\)</span>, <span class="math inline">\(\de \in [0,1]\)</span>. If <span class="math inline">\(\E A\le \ep \E B\)</span> and <span class="math inline">\(\E B^2 \le t(\E B)^2\)</span>, then <span class="math inline">\(\Pj(A\le e^\de \ep B) \ge \fc{\de^2}{9t}\)</span>.</li>
</ul></li>
<li><p>(From degree 2 p.d. to a vector)</p>
<p><strong>Lemma 5.4</strong>. Let <span class="math inline">\(c\in \R^n\)</span> be unit and <span class="math inline">\(U\)</span> be a degree-2 p.d. over <span class="math inline">\(\R^n\)</span> satisfying <span class="math inline">\(\ve{U}_2^2=1\)</span>. If <span class="math inline">\(\wt \E\an{c,U}^2 \ge 1-\ep\)</span>, then letting <span class="math inline">\(\xi\)</span> match the first two moments of <span class="math inline">\(U\)</span> and <span class="math inline">\(v=\fc{\xi}{\ve{\xi}_2}\)</span>, we have <span class="math inline">\(\Pj[\an{c,v}^2 \ge 1-2\ep] =\Om(1)\)</span>.</p>
<em>Proof</em>. Again use Lemma 5.3, just check <span class="math inline">\(\E\ve{\xi}_2^t \le O(\E\ve{\xi}_2^2)^2\)</span>.</li>
<li>Summary of parameters in Theorem 5.1.
<ul>
<li>Running time <span class="math inline">\(n^{O(k)}\)</span>: Just to evaluate <span class="math inline">\(W\)</span> on a degree <span class="math inline">\(k\)</span> p.d. takes <span class="math inline">\(n^{O(k)}\)</span> time.</li>
<li>Success probability <span class="math inline">\(2^{-k/\poly(\ep)}\)</span>: <span class="math inline">\(W\)</span> succeeded if each <span class="math inline">\(\an{c,\xi^{(i)}}\)</span> is large, so we get <span class="math inline">\(-k\)</span> in the exponent. (!! This seems like a very crude bound—I expect we can do much better, maybe even subexponential?) <span class="math inline">\(M\)</span> depends polynomially on <span class="math inline">\(\rc{\ep}\)</span>; <span class="math inline">\(\rc{\poly(\ep)}\)</span> is in the exponent because the algorithm relies on sampling to get close enough.</li>
</ul></li>
</ul></li>
<li></li>
</ol>
<h2 id="dictionary-learning">Dictionary learning</h2>
<p>Algorithm: Take ?? samples <span class="math inline">\(x^{(i)}\)</span> and apply noisy tensor decomposition to the polynomial <span class="math display">\[ 
\EE_i x^{(i)} \an{Ax,u}^d
\]</span> where <span class="math inline">\(d=??\)</span>.</p>
<p>Note that <span class="math inline">\(\an{Ax,u}^d\)</span> is the polynomial (<span class="math inline">\(n\)</span>-ic form) corresponding to the tensor <span class="math inline">\((Ax)^{\ot d}\)</span>, just as <span class="math inline">\(u^T vv^Tu = \an{u,v}^2\)</span> is the quadratic form corresponding to the matrix <span class="math inline">\(vv^T\)</span>.</p>
<p><em>Proof</em>.</p>
<ol type="1">
<li><p>(Calculate expectation) <em>Lemma 4.5</em>. If the distribution of <span class="math inline">\(x\)</span> is <span class="math inline">\((d,\tau)\)</span>-nice and <span class="math inline">\(A\)</span> is <span class="math inline">\(\si\)</span>-overcomplete, then <span class="math display">\[\EE_x \an{Ax,u}^d \in \ve{A^Tu}_d^d + [0,\tau \si^dd^d \ve{u}_2^d]\]</span> (where inequalities are in the SoS sense).</p>
<em>Proof</em>. Expand the sum as <span class="math inline">\(\sum_\al x_\al (A^Tu)_\al\)</span>. For <span class="math inline">\(\al=[i,\ldots, i]\)</span> we get the term <span class="math inline">\((A^Tu)_i^d\)</span>. For the other terms, <!--each even term $\al\ne [i,\ldots, i]$ we get the term -->
\begin{align}
0 &amp;\le \sum_{\al \in [n]^d,\al \ne [i,\ldots, i] }(\E x_\al) (A^Tu)_\al \\
&amp; \le \tau \sum_{\al \in [n]^d,\al \ne [i,\ldots, i] } (A^Tu)_\al \\
&amp; \le \tau d^d\sum_{\be \in [n]^{d/2},\be \ne [i,\ldots, i] } (A^Tu)_\be\\
&amp; \le \tau d^d\ve{A^Tu}_2^d \le \tau \si^d \ve{u}_2^d.
\end{align}
(The <span class="math inline">\(d^d\)</span> bound is crude; this is not the bottleneck.) Note the lower inequality depends on there only being even nonzero terms.</li>
<li><p>(Calculate concentration, Proof of 4.2) We use a Chebyshev bound<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. We have <span class="math inline">\(|(A^Tx)_\al|\le 1\)</span>. <strong>I think we need assumption on <span class="math inline">\(\ve{A^Tx}_{\iy}\)</span></strong>. Thus taking <span class="math inline">\(\wt \Om\pf{n^{2d})}{\tau^2}\)</span> samples we get that the coefficients are whp <span class="math inline">\(\fc{\tau}{n^d}\)</span>-close. This means the value at <span class="math inline">\(u\)</span> is <span class="math inline">\(n^d \si^d\fc{\tau}{n^d}\)</span>-close, and we get whp <span class="math display">\[P\in \ve{A^Tu}_d^d + 2\tau \si^d d^d \ve{u}_2^d[-1,1].\]</span></p></li>
</ol>
<h2 id="todo">Todo</h2>
<ul>
<li>Finish noisy tensor decomp proof (look at <span class="math inline">\(S\)</span>, check value of <span class="math inline">\(k\)</span>)</li>
<li>Go through parameters for DL.</li>
<li>Summarize changes to get down to polytime. What’s the key idea?</li>
</ul>
<p>(Q: can you adapt something like this for NMF?)</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Can we do better if we use polynomial concentration?<a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class="st_facebook_large" displayText="Facebook"></span>
    <span class="st_twitter_large" displayText="Tweet"></span>
    <span class="st_googleplus_large" displayText="Google +"></span>
    <span class="st_reddit_large" displayText="Reddit"></span>
    <span class="st__large" displayText></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>


<!-- Footer -->
<div id="footer">
  <div class="container">
    Built with
    <a href="http://jaspervdj.be/hakyll">Hakyll</a> 
    using 
    <a href="http://www.getbootstrap.com">Bootstrap</a>, 
    <a href="http://www.disqus.com">Disqus</a>,
    <a href="http://ignorethecode.net/blog/2010/04/20/footnotes/">Footnotes.js</a>,
    <a href="http://highlightjs.org/">Highlight.js</a>, 
    <a href="http://www.mathjax.org">MathJax</a>, 
    and <a href="http://www.sharethis.com">ShareThis</a>.
  </div>
</div>
</body>

</html>

<!-- SCRIPTS -->
<!-- jQuery-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="../../../../bootstrap/js/bootstrap.min.js"></script>

<!-- Extension : Highlight.js @ https://highlightjs.org/ -->
<!-- Syntax highlighting tomorrow-night-bright, agate-->
<link rel="stylesheet" href="../../../../highlight/css/tomorrow-night-bright.css">
<script src="../../../../highlight/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- Extension : MathJax @ https://docs.mathjax.org/en/v2.5-latest/tex.html -->
<!-- MathJax/config/local/local.js contains macros. Need to provide entire URL-->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,http://holdenlee.github.io/notebook/MathJax/config/local/local"></script>

<!-- Extension : Footnotes @ http://ignorethecode.net/blog/2010/04/20/footnotes/ -->
<script src="../../../../footnotes/js/footnotes.js"></script>

<!-- Extension : Disqus @ http://disqus.com -->
<!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->

<script src="../../../../disqus/js/disqus.js"></script>



<!-- Extension : Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73261814-1', 'auto');
  ga('send', 'pageview');

</script>

