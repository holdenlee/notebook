<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

  <meta name="description" content="Holden Lee's Research Notebook">
  <meta name="author" content="Holden Lee">
    
  <title>(BKS15) Dictionary Learning and Tensor Decomposition via the Sum-of-Squares Method</title>

  <!-- Bootstrap core CSS -->
  <link href="../../../../bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

  <!-- Custom styles for this template -->
  <link href="../../../../css/blog.css" rel="stylesheet">
  <link href="../../../../css/default.css" rel="stylesheet">

  <!-- Extension : Footnotes -->
  <link href="../../../../footnotes/css/footnotes.css" rel="stylesheet">

  <!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->
  

  <!-- Extension : Collapsible lists @ http://code.stephenmorley.org/javascript/collapsible-lists/-->
  <link href="../../../../collapsible_lists/css/collapsible.css" rel="stylesheet">
  <script type="text/javascript" src="../../../../collapsible_lists/js/CollapsibleLists.js"></script>

</head>

<body>

<!-- Navigation bar. navbar-inverse is black, navbar-default is white.-->
<!-- To make a button active (pressed), use <li class="active"> -->
<div id="header">
  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../../">Notebook</a>
      </div>
      <div id="navbar" class="collapse navbar-collapse">
        <ul class="nav navbar-nav">
          <li><a href="../../../../">Home</a></li>
          <li><a href="../../../../sitemap.html">Sitemap</a></li>
<!-- TODO: Distinguish between PAPERS, RESEARCH QUESTIONS, BOOKS -->
<!-- TODO: make this part a for loop over main pages -->
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </nav>
</div>
<!-- Content -->
<!--div id="content">
  <h1>Mental Wilderness</h1>-->



<div class="container">
  <div id="content">
    <div class="page header">
      <h1>(BKS15) Dictionary Learning and Tensor Decomposition via the Sum-of-Squares Method</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-30 
          , Modified: 2016-08-30 
	</p>
      
       <p>Tags: <a href="../../../../tags/dictionary%20learning.html">dictionary learning</a>, <a href="../../../../tags/tensor.html">tensor</a>, <a href="../../../../tags/sparse%20coding.html">sparse coding</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#noisy-tensor-decomposition-1">Noisy tensor decomposition 1</a><ul>
 <li><a href="#reduction">Reduction</a></li>
 <li><a href="#algorithm">Algorithm</a></li>
 </ul></li>
 <li><a href="#noisy-tensor-decomposition">Noisy tensor decomposition</a><ul>
 <li><a href="#algorithm-1">Algorithm</a><ul>
 <li><a href="#sampling-pseudo-distributions">Sampling pseudo-distributions</a></li>
 </ul></li>
 <li><a href="#proof">Proof</a></li>
 </ul></li>
 <li><a href="#dictionary-learning">Dictionary learning</a></li>
 <li><a href="#theorem">Theorem</a><ul>
 <li><a href="#algorithm-2">Algorithm</a></li>
 </ul></li>
 <li><a href="#polytime-algorithm">Polytime algorithm</a></li>
 <li><a href="#todo">Todo</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><strong>Theorem</strong>. Given <span class="math inline">\(\ep&gt;0,\si\ge 1, \de&gt;0\)</span> there exists <span class="math inline">\(d\)</span> and a polytime algorithm that given input</p>
<ul>
<li>a <span class="math inline">\(\si\)</span>-dictionary <span class="math inline">\(n\times m\)</span>,</li>
<li><span class="math inline">\((d,\tau=n^{-\de})\)</span>-nice <span class="math inline">\(\{x\}\)</span>,</li>
<li><span class="math inline">\(n^{O(1)}\)</span> samples from <span class="math inline">\(y=Ax\)</span>,</li>
</ul>
<p>outputs with probability <span class="math inline">\(\ge 0.9\)</span> a set <span class="math inline">\(\ep\)</span>-close to columns of <span class="math inline">\(A\)</span>.</p>
<ul>
<li>A <span class="math inline">\(\si\)</span>-dictionary has <span class="math inline">\(AA^T\preceq \si I\)</span> (analytic proxy for overcompleteness <span class="math inline">\(\fc mn\)</span>).</li>
<li>A distribution is <span class="math inline">\((d,\tau)\)</span>-nice if <span class="math inline">\(\E x_i^{d/2}x_j^{d/2}\le \tau\)</span> for all <span class="math inline">\(i\ne j\)</span>, and <span class="math inline">\(\E x^\al=0\)</span> for all <span class="math inline">\(|\al|\le d\)</span>. (Ex. Bernoulli(<span class="math inline">\(\tau\)</span>) times Gaussian with <span class="math inline">\(\E z_i^d = \rc\tau\)</span>.)</li>
</ul>
<p>Note on running time:</p>
<ul>
<li><span class="math inline">\(\ep\)</span>-accuracy with running time depending on <span class="math inline">\(\poly\prc{\ep}\)</span> in the exponent. So this is better for giving an initial solution for local search methods.</li>
</ul>
<h2 id="noisy-tensor-decomposition-1">Noisy tensor decomposition 1</h2>
<p>Given noisy version of <span class="math inline">\(\sumo im \an{a^i, u}^d = \ve{A^Tu}_d^d\)</span>, recover <span class="math inline">\(\{a^1,\ldots, a^m\}\)</span>.</p>
<p><strong>Theorem</strong> (Noisy tensor decomposition). For every <span class="math inline">\(\ep&gt;0,\si\ge 1\)</span> there exist <span class="math inline">\(d,\tau\)</span> such that a probabilistic <span class="math inline">\(n^{O(\ln n)}\)</span>-time agorithm, given <span class="math inline">\(P\)</span> with <span class="math display">\[\ve{A^T u}_d^d - \tau \ve{u}_2^d \preceq P \preceq \ve{A^Tu}_d^d + \tau \ve{u}_2^d,\]</span> outputs with probability 0.9 <span class="math inline">\(S\)</span> that is <span class="math inline">\(\ep\)</span>-close to <span class="math inline">\(\{a^1,\ldots, a^m\}\)</span>. (<span class="math inline">\(\preceq\)</span> means the difference is a sum of squares.)</p>
<p>Note this allows significant noise since we expect <span class="math inline">\(\ve{A^Tu}_d^d\sim mn^{-\fc d2}\ll \tau\)</span>.</p>
<h3 id="reduction">Reduction</h3>
<p>Take <span class="math display">\[ P = \EE_{i} \an{y_i, u}^{2d}.\]</span></p>
<h3 id="algorithm">Algorithm</h3>
<ol type="1">
<li>Use SoS to find degree-<span class="math inline">\(k\)</span> pseudo-distribution <span class="math inline">\(\{u\}\)</span> that maximizes <span class="math inline">\(P(u)\)</span> under <span class="math inline">\(\ve{u}^2=1\)</span>.</li>
<li>Let <span class="math inline">\(W\)</span> be a product of <span class="math inline">\(O(\ln n)\)</span> rndom linear functions.</li>
<li>Output top eigenvector of <span class="math inline">\(M\)</span>, <span class="math inline">\(M_{ij} = \wt \E W(u)^2 u_iu_j\)</span>.</li>
</ol>
<p><strong>SoS Algorithm</strong>: given <span class="math inline">\(\ep&gt;0, k,n,M\)</span>, <span class="math inline">\(P_1,\ldots, P_m\in \R[x_1,\ldots, x_n]\)</span> with coefficients in <span class="math inline">\([0,M]\)</span>, if there exists a degree <span class="math inline">\(k\)</span> pseudo-distribution with <span class="math inline">\(P_i=0,i\in [m]\)</span>, then we can find <span class="math inline">\(u'\)</span> satisfying <span class="math inline">\(P_i\le \ep, P_i\ge -\ep\)</span> for every <span class="math inline">\(i\in [m]\)</span> in time <span class="math inline">\((n\polylog\prc{M}{\ep})^{O(k)}\)</span>.</p>
<p>(Proof: This is a feasibility problem. Use ellipsoid method.)</p>
<p>Steps</p>
<ol type="1">
<li>If <span class="math inline">\(u\)</span> is an actual distribution, the the output is close to a column.</li>
<li>Generalize to pseudo-distributions.</li>
<li>Generalize to getting all columns.</li>
</ol>
<p>Idea: If not correlated, then <span class="math inline">\(P(u)\)</span> is small. The inequalities can be turned into polynomial inequalities provable by SoS.</p>
<h2 id="noisy-tensor-decomposition">Noisy tensor decomposition</h2>
<p><strong>Theorem</strong> (Noisy tensor decomposition, 4.3). There is a probabilistic algorithm that given</p>
<ul>
<li><span class="math inline">\(\ep&gt;0\)</span></li>
<li><span class="math inline">\(\si \ge 1\)</span></li>
<li>degree <span class="math inline">\(d\ge d(\ep,\si) = O\pf{\ln \si}{\ep}\)</span></li>
<li>noise parameter <span class="math inline">\(\tau \le \tau(\ep) = \Om(\ep)\)</span></li>
<li>degree <span class="math inline">\(d\)</span> polynomial <span class="math inline">\(P\in \R[u]\)</span> such that <span class="math display">\[\ve{A^T u}_d^d - \tau \ve{u}_2^d \preceq P \preceq \ve{A^Tu}_d^d + \tau \ve{u}_2^d,\]</span> outputs with high probability <span class="math inline">\(S\)</span> that is <span class="math inline">\(\ep\)</span>-close to <span class="math inline">\(\{a^1,\ldots, a^m\}\)</span>. (<span class="math inline">\(\preceq\)</span> means the difference is a sum of squares.)</li>
</ul>
<h3 id="algorithm-1">Algorithm</h3>
<ol type="1">
<li>Start with <span class="math inline">\(S=\phi\)</span>. Let <span class="math inline">\(\ga = \fc{C}{\ep}\)</span> for <span class="math inline">\(C\)</span> large enough. ?? What’s <span class="math inline">\(k\)</span>?</li>
<li>Loop:
<ol type="1">
<li>Attempt to find a degree <span class="math inline">\(k\)</span> pseudo-distribution <span class="math inline">\(U\)</span> satisfying the constraints <span class="math inline">\(P(U) \ge 1-\tau\)</span>, <span class="math inline">\(\ve{U}_2^2=1\)</span>, <span class="math inline">\(\an{s,U}^2\le 1-\ga\)</span> for every <span class="math inline">\(s\in S\)</span> (to repel it away from vectors we already found). If fail, stop.</li>
<li>Use the “sampling pseudo-distributions” algorithm to obtain a unit vector <span class="math inline">\(c'\)</span> such that
<span class="math display">\[\begin{align}
P(c') &amp; \ge e^{-\ep d} - \tau\\
\ep &amp;:= O(\pf{\tau}{d} + \fc{\ln \si}{d} \fc{\ln m}k)
\end{align}\]</span></li>
<li>Add <span class="math inline">\(c'\)</span> to <span class="math inline">\(S\)</span>.</li>
</ol></li>
</ol>
<h4 id="sampling-pseudo-distributions">Sampling pseudo-distributions</h4>
<h3 id="proof">Proof</h3>
<ol type="1">
<li>Existence of <span class="math inline">\(c'\)</span>.
<ul>
<li><span class="math inline">\(U\)</span> exists because any column of <span class="math inline">\(A\)</span> is a solution. (CHECK columns far!!)</li>
<li><span class="math inline">\(U\)</span> is correlated with <span class="math inline">\(A\)</span>: By assumption, for <span class="math inline">\(\ve{U}_2^2=1\)</span>, <span class="math inline">\(P\in \ve{A^T U}_d^d + \tau[-1,1]\)</span>. Thus <span class="math inline">\(P(U)\ge 1-\tau\)</span> implies <span class="math display">\[\ve{A^TU}_d^d \ge 1-2\tau.\]</span></li>
<li><p>If <span class="math inline">\(U\)</span> correlates well with <span class="math inline">\(A\)</span>, then <span class="math inline">\(U\)</span> correlates with some column of <span class="math inline">\(A\)</span>. <!--Any column of $A$ satisfies $\wt \E \an{c,u}^k \ge e^{-\ep' k}$.--></p>
<p><em>Lemma 6.1</em>. Given</p>
<ul>
<li><span class="math inline">\(A\)</span> is <span class="math inline">\(\si\)</span>-overcomplete</li>
<li><span class="math inline">\(u\)</span> is degree <span class="math inline">\(3k\)</span> pd over <span class="math inline">\(\R^n\)</span> satisfying <span class="math inline">\(\ve{A^Tu}_d^d \ge e^{-\de d}, \ve{u}_2^2 = 1\)</span>,</li>
<li><span class="math inline">\(d-2\mid k\)</span> (?? this seems to be necessary. Think of <span class="math inline">\(k\gg d\)</span>.)</li>
</ul>
<p>there exists a column of <span class="math inline">\(A\)</span> such that <span class="math inline">\(\wt \E \an{c,u}^k \ge e^{-\ep k}\)</span>, <span class="math inline">\(\ep=O(\de + \fc{\ln \si}{d} + \fc{\ln m}k)\)</span>.</p>
<ul>
<li>Motivation for proof: First, let’s see how to prove this if <span class="math inline">\(u\)</span> is an actual vector. By averaging, there is a column such that <span class="math display">\[ \ve{A_{\cdot i}^T u}_{k}^k \ge \pf{1-\ep}{m}\implies A_{\cdot i}^Tu \ge \pf{1-\ep}{m}^{\rc k}\approx e^{\fc{\ln m}{k}}.\]</span> (This also shows why we expect <span class="math inline">\(\ln m\)</span> in the exponent.) This averaging argument still works for pseudo-distributions when <span class="math inline">\(d=k\)</span>—the first inequality still holds true, if we have <span class="math inline">\(d=k\)</span>. (The first inequality would no longer imply the second, though.) (?? Why wouldn’t we take <span class="math inline">\(d=k\)</span>?)</li>
<li>?? For some reason, we want to use an inequality not depending on <span class="math inline">\(m\)</span> (depending on <span class="math inline">\(\ve{A^Tu}_2\)</span> is good, because we have control of this using <span class="math inline">\(\si\)</span>), in which case instead of using the inequality <span class="math inline">\(\ve{v}_{\iy} \ge \fc{\ve{v}_d}{m^{\rc d}}\)</span> we use <span class="math inline">\(\ve{v}_{\iy}^{d-2}\ge \fc{\ve{v}_d^d}{\ve{v}_2^2}\)</span>. We need to change this into a SoS inequality, so replace <span class="math inline">\(\iy\)</span> by <span class="math inline">\(k\)</span> for <span class="math inline">\(k\)</span> large, <span class="math display">\[\ve{v}_d^{\fc{dk}{d-2}} \le \ve{v}_k^k (\ve{v}_2^2)^{\fc{k}{d-2}}\]</span> provable by SoS (expand and then use Muirhead/AM-GM).</li>
<li><!-- For the actual proof, we just need to relate the $d$ and $k$ norms. We want to lower-bound the $k$th norm in terms of the $d$th norm. Use
	$$ \ve{v}_k^{d-2} \ve{v}_2^2 \ge \ve{v}_d^d. $$
	(This is true for $k=1$ by expanding, and hence for all $k$.)-->
<em>Proof</em>. <span class="math display">\[
\ve{A^Tu}_k^k \ge \fc{(\ve{A^Tu}_d^d)^{\fc k{d-2}}}{(\ve{A^Tu}_2^2)^{\fc{k}{d-2}}} \ge \fc{e^{-\de d}}{\si^{\fc k{d-2}}}.
\]</span> Now use averaging on <span class="math inline">\(\ve{A^Tu}_k^k\)</span>.</li>
</ul></li>
<li>Here <span class="math inline">\(\de = O\pf{\tau}d\)</span> so we get <span class="math display">\[ \wt E \an{c,u}^k \ge e^{-\ep' k},\quad \ep' = O(\fc{\tau}d + \fc{\ln \si}{d} +\fc{\ln m}{k}).\]</span> This means we set <span class="math inline">\(k=\boxed{\frac{C\ln m}{\ep}}\)</span>. This also means we need <span class="math inline">\(d=\Om(\rc{\ep}\ln \si)\)</span>, <span class="math inline">\(\tau=O(\ep)\)</span>.</li>
<li>Example: if <span class="math inline">\(u\)</span> is the actual distribution <span class="math inline">\(u=A_{\cdot i}\)</span> with probability <span class="math inline">\(\rc{m}\)</span>, then <span class="math inline">\(\wt E\an{c,u}^k = \rc m\)</span>. Hence we can’t do better than <span class="math inline">\(k=\Om(\ln m)\)</span>.</li>
</ul></li>
<li><p>Now we analyze the algorithm to “sample pseudo-distributions”: given a PD that correlates with a vector, find a vector close to it. We prove:</p>
<p><strong>Theorem 5.1</strong>. The “sample pseudo-distributions” algorithm, given</p>
<ul>
<li>even <span class="math inline">\(k\ge 0\)</span></li>
<li>degree <span class="math inline">\(k\)</span> p.d. with <span class="math inline">\(\ve{u}_2^2=1\)</span> and <span class="math inline">\(\wt \E\an{c,u}^k \ge e^{-\ep k}\)</span> (?? I think we want this for <span class="math inline">\(k-2\)</span>.)</li>
</ul>
<p>outputs <span class="math inline">\(c'\in \R^n\)</span> with <span class="math inline">\(\an{c,c'}\ge 1-O(\ep)\)</span> with probability <span class="math inline">\(2^{-\fc{k}{\poly(\ep)}}\)</span>.</p>
<ul>
<li>Here is the idea. We want to compute the tensor decomposition by using SVD (because there aren’t other good ways to compute overcomplete tensor decomposition…). Suppose that <span class="math inline">\(u\)</span> were an actual distribution; since it is correlated with <span class="math inline">\(A\)</span>, it is approximately supported on columns of <span class="math inline">\(A\)</span>. Then <span class="math inline">\(\wt \E uu^T = \sum p_i a_i a_i^T\)</span>. The <span class="math inline">\(a_i\)</span> are not uniquely identifiable from this (that’s the whole problem with SVD, why we want tensors in the first place!). Idea: consider <span class="math inline">\(\wt \E p(u) uu^T = \sum p_i p(a_i) a_ia_i^T\)</span> instead. If <span class="math inline">\(p\)</span> is a function that is large on <span class="math inline">\(a_i\)</span> and small on all other <span class="math inline">\(a_j\)</span>, then we win. Take <span class="math inline">\(p\)</span> to be a product of linear factors. If we put in enough factors, then with good probability the <span class="math inline">\(p(a_i)\)</span> will become farther apart, and one outruns the others, <span class="math inline">\(|p(a_i)|\gg \sqrt m|p(a_i)|\)</span>. Take <span class="math inline">\(p =W^2\)</span> where <span class="math inline">\(W\)</span> is a product of <span class="math inline">\(O(\ln n)\)</span> random linear functions <span class="math inline">\(\an{u,v}, \ve{v}_2=1\)</span>.</li>
<li><p>We are given <span class="math inline">\(c\)</span> such that <span class="math inline">\(\wt E\an{c,u}^k \ge e^{-\de d}\)</span> (high correlation) and we want that <span class="math inline">\(c\)</span> to be a value of <span class="math inline">\(v\)</span> that makes (the quadratic form) <span class="math inline">\(v\mapsto \wt \E W(U) \an{v,U}^2\)</span> large. In fact, what we want is that <!--the value is way bigger than any other singular values; i.e.,--> when normalized so the sum of the eigenvalues is 1, the singular value corresponding to <span class="math inline">\(c\)</span> is close to 1.</p>
There are 3 parts. (a) We need to understand how “renormalization” of pseudo-distributions works. (b) We need to show that for our choice of <span class="math inline">\(W\)</span>, we have with some probability a lower bound for <span class="math inline">\(\wt \E W(U) \an{c,U}^2\)</span>. This is the hardest part. (c) We need to show that <span class="math inline">\(c\)</span> is close to the singular vector (which we output). (I think (c) is in essence a matrix/eigenvector perturbation result, but it’s cloaked in the language of SoS here—unravel!)</li>
<li><strong>Lemma</strong> (reweighting). Let <span class="math inline">\(U\)</span> be a degree <span class="math inline">\(k\)</span> p.d. Then for every SoS polynomial <span class="math inline">\(W\)</span> of degree <span class="math inline">\(d&lt;k\)</span>, <span class="math inline">\(\wt \E W&gt;0\)</span>, there exists a degree <span class="math inline">\((k-d)\)</span> p.d. <span class="math inline">\(U'\)</span> such that for every <span class="math inline">\(\deg P \le k-d\)</span>, <span class="math display">\[\wt E_{U'} P(U') = \rc{\wt \E_U W(U)} \wt \E_{U} W(U)P(U).\]</span> <em>Proof</em>. Just verify the positivity property.</li>
<li><p><strong>Lemma 5.2</strong>. Let <span class="math inline">\(U\)</span> be a degree-<span class="math inline">\(k+2\)</span> pseudo-distribution over <span class="math inline">\(\R^n\)</span> with <span class="math inline">\(\ve{U}_2^2=1\)</span> and such that there exists a unit vector <span class="math inline">\(c\in \R^n\)</span> such that <span class="math inline">\(\wt \E\an{c,u}^k \ge e^{-\ep k}\)</span>. <span class="math inline">\(W=\rc{M^{k/2}} \prod_{i=1}^{k/2} w^{(i)}\)</span> where <span class="math inline">\(w^{(i)}\)</span> are iid draws from <span class="math inline">\(w=\an{\xi,u}^2\)</span>, <span class="math inline">\(\xi\sim N(0,I_n)\)</span>. Then with probability <span class="math inline">\(2^{-O\pf{k}{\poly(\ep)}}\)</span>, <span class="math inline">\(\wt \E\an{c,u}^2W \ge (1-O(\ep))\wt \E W\)</span>.</p>
(?? Discrepancy between <span class="math inline">\(k\)</span> and <span class="math inline">\(k+2\)</span>.)</li>
<li>By reweighting, there exists a degree 2 p.d. <span class="math inline">\(U'\)</span> such that <span class="math display">\[\wt \E_{U'} \an{v,u'}^2 = \rc{\wt \E_U W(U)} \wt\E_{U} W(U) \an{v,U}^2.\]</span> By Lemma 5.2, the value at <span class="math inline">\(c\)</span> is <span class="math inline">\(\ge 1-O(\ep)\)</span>.</li>
<li><em>Proof</em> (Sketch). This is more likely to be true if <span class="math inline">\(\an{c,\xi}\)</span> is large. Let <span class="math inline">\(w=\an{\xi,u}^2\)</span>. Let <span class="math inline">\(\tau_M\)</span> be such that <span class="math inline">\(\E_{\xi_0|\xi_0\ge \tau_M}\xi_0^2 =M\)</span>. Conditioning on <span class="math inline">\(\an{c,\xi}\ge \tau_{M+1}\)</span> and expanding <span class="math inline">\(\xi=\an{c,\xi}c+\xi'\)</span>, find that <span class="math display">\[ \E_{\xi | \an{c,\xi}\ge \tau_{M+1}} w = M\an{c,u}^2 + \ve{U}_2^2.\]</span> Let <span class="math inline">\(\ol W = \E W = \pa{\an{c,U}^2 + \rc M}^{\fc k2}\)</span> (assume <span class="math inline">\(\ve{u}_2^2=1\)</span>). Algebra shows
<span class="math display">\[\begin{align}
\ol W \an{c,U}^2 &amp;\succeq \pa{1-\fc 2M } \ol W - \pa{1-\rc M}^{\fc k2}\\
\EE_W \wt \E W\an{c,u}^2 &amp; \ge (1-O(\ep)) \EE_W \wt \E W.
\end{align}\]</span>
<p>(Take <span class="math inline">\(M=\rc{\ep}\ln \prc{\ep}\)</span>.) This is almost what we want, except that we conditioned on <span class="math inline">\(\an{c,\xi}\ge \tau_{M+1}\)</span> and that this is an inequality about absolute values.</p>
<p>To get rid of the conditioning, note that the event <span class="math inline">\(\forall i, \an{\xi^{(i)},c}\ge \tau_{M+1}\)</span> has probability <span class="math inline">\(2^{-O(kM^2)}\)</span>.</p>
To get the result with not-too-small probability, rearrange as <span class="math display">\[
O(\ep) \EE_W \wt \E W \ge \EE_W \wt \E W - \EE_W \wt \E W\an{c,U}^2,
\]</span> bound second moments <span class="math inline">\(\E_W(\wt \E W)^2\)</span>, and use
<ul>
<li><strong>Lemma 5.3</strong>. Let <span class="math inline">\(A,B\)</span> be distributions such that <span class="math inline">\(0\le A\le B\)</span>, <span class="math inline">\(\de \in [0,1]\)</span>. If <span class="math inline">\(\E A\le \ep \E B\)</span> and <span class="math inline">\(\E B^2 \le t(\E B)^2\)</span>, then <span class="math inline">\(\Pj(A\le e^\de \ep B) \ge \fc{\de^2}{9t}\)</span>.</li>
</ul></li>
<li><p>(From degree 2 p.d. to a vector)</p>
<p><strong>Lemma 5.4</strong>. Let <span class="math inline">\(c\in \R^n\)</span> be unit and <span class="math inline">\(U\)</span> be a degree-2 p.d. over <span class="math inline">\(\R^n\)</span> satisfying <span class="math inline">\(\ve{U}_2^2=1\)</span>. If <span class="math inline">\(\wt \E\an{c,U}^2 \ge 1-\ep\)</span>, then letting <span class="math inline">\(\xi\)</span> match the first two moments of <span class="math inline">\(U\)</span> and <span class="math inline">\(v=\fc{\xi}{\ve{\xi}_2}\)</span>, we have <span class="math inline">\(\Pj[\an{c,v}^2 \ge 1-2\ep] =\Om(1)\)</span>.</p>
<em>Proof</em>. Again use Lemma 5.3, just check <span class="math inline">\(\E\ve{\xi}_2^t \le O(\E\ve{\xi}_2^2)^2\)</span>.</li>
<li>Summary of parameters in Theorem 5.1.
<ul>
<li>Running time <span class="math inline">\(n^{O(k)}\)</span>: Just to evaluate <span class="math inline">\(W\)</span> on a degree <span class="math inline">\(k\)</span> p.d. takes <span class="math inline">\(n^{O(k)}\)</span> time.</li>
<li>Success probability <span class="math inline">\(2^{-k/\poly(\ep)}\)</span>: <span class="math inline">\(W\)</span> succeeded if each <span class="math inline">\(\an{c,\xi^{(i)}}\)</span> is large, so we get <span class="math inline">\(-k\)</span> in the exponent. (!! This seems like a very crude bound—I expect we can do much better, maybe even subexponential?) <span class="math inline">\(M\)</span> depends polynomially on <span class="math inline">\(\rc{\ep}\)</span>; <span class="math inline">\(\rc{\poly(\ep)}\)</span> is in the exponent because the algorithm relies on sampling to get close enough.</li>
</ul></li>
</ul></li>
<li>Finish.
<ul>
<li>In step 1, from 6.1, we get <span class="math inline">\(\wt \E \an{c,u}^k \ge e^{-\ep' k}\)</span>, <span class="math inline">\(\ep' = O\pa{\fc\tau d+ \fc{\ln \si}d + \fc{\ln m}k}\)</span>.</li>
<li>From 5.1, we get <span class="math inline">\(c'\)</span> close to a column <span class="math inline">\(c\)</span> of <span class="math inline">\(A\)</span>: <span class="math inline">\(\an{c,c'}\ge 1-O(\ep')\)</span> with probability <span class="math inline">\(2^{-\fc{k}{\poly(\ep')}}\)</span>. Repeat <span class="math inline">\(2^{\fc{k}{\poly(\ep')}}\)</span> times to get this with good probability. (<span class="math inline">\(\ep = \Te(\ep')\)</span>.)</li>
<li>(Far from other vectors already in <span class="math inline">\(S\)</span>) We have <span class="math inline">\(P(c') \ge c^Tc' - \tau\ve{c'}_2^2 \ge e^{-\ep d}-\tau\)</span>. Calculation using the triangle inequality on (after doing <span class="math inline">\(\bullet^{\ot 2}\)</span> because the inequality we have is <span class="math inline">\(\an{s,U}^2\le 1-\ga\)</span>) gives <span class="math inline">\(\an{c',s}\le 1-\fc{\ga}{10}\)</span>. (We chose <span class="math inline">\(\ga = C\ep\)</span>.) <img src="../../../../images/bks15-tri.png"></li>
<li>(Every <span class="math inline">\(s\in S\)</span> is close to some column.) Use 6.1 on <span class="math inline">\(s\)</span> (which is an actual vector! not a p.d.), <span class="math inline">\(\ve{A^T s}_d^d \ge e^{-\ep d}-2\tau\)</span> to get <span class="math inline">\(\an{s,c}^2 \ge 1-O(\ep)\)</span>.</li>
<li>(Can’t have 2 <span class="math inline">\(s\in S\)</span> close to same <span class="math inline">\(c\)</span>) More triangle inequalities.</li>
<li>The algorithm then terminates after <span class="math inline">\(|S|=m\)</span>. The total time is <span class="math inline">\(n^{O(k)/\poly(\ep)} = n^{\fc{\ln m}{\poly(\ep)}}\)</span>. There’s an extra <span class="math inline">\(d\)</span> in the exponent because accessing <span class="math inline">\(P\)</span> takes <span class="math inline">\(n^d\)</span> time. (CHECK!!)</li>
</ul></li>
</ol>
<h2 id="dictionary-learning">Dictionary learning</h2>
<h2 id="theorem">Theorem</h2>
<p>The algorithm below, given</p>
<ul>
<li><span class="math inline">\(\ep&gt;0\)</span></li>
<li>overcompleteness <span class="math inline">\(\si\ge 1\)</span></li>
<li><span class="math inline">\(d\ge D:=O\pf{\ln \si}{\ep}\)</span>,</li>
<li><span class="math inline">\(\tau \le (D\si)^{-D}\)</span>. (In 4.2 it says <span class="math inline">\(D^D\)</span>, but I think it should be this.)</li>
<li><span class="math inline">\(\wt O \pf{n^{2d}}{\tau^2}\)</span> samples from <span class="math inline">\(y=Ax\)</span> for <span class="math inline">\((d,\tau)\)</span>-nice <span class="math inline">\(x\)</span>,</li>
</ul>
<p>outputs in time <span class="math inline">\(n^{\fc{d+\ln m}{\ep^{O(1)}}}\)</span> a set of vectors <span class="math inline">\(\ep\)</span>-close to <span class="math inline">\(A\)</span>.</p>
<h3 id="algorithm-2">Algorithm</h3>
<p>Take <span class="math inline">\(\wt O \fc{n^{2d}}{\tau^2}\)</span> samples <span class="math inline">\(x^{(i)}\)</span> and apply noisy tensor decomposition to the polynomial <span class="math display">\[ 
\EE_i x^{(i)} \an{Ax,u}^d
\]</span> where <span class="math inline">\(d\ge D:=O(\ep^{-1}\ln \si)\)</span>.</p>
<p>Note that <span class="math inline">\(\an{Ax,u}^d\)</span> is the polynomial (<span class="math inline">\(n\)</span>-ic form) corresponding to the tensor <span class="math inline">\((Ax)^{\ot d}\)</span>, just as <span class="math inline">\(u^T vv^Tu = \an{u,v}^2\)</span> is the quadratic form corresponding to the matrix <span class="math inline">\(vv^T\)</span>.</p>
<p><em>Proof</em>.</p>
<ol type="1">
<li><p>(Calculate expectation) <em>Lemma 4.5</em>. If the distribution of <span class="math inline">\(x\)</span> is <span class="math inline">\((d,\tau)\)</span>-nice and <span class="math inline">\(A\)</span> is <span class="math inline">\(\si\)</span>-overcomplete, then <span class="math display">\[\EE_x \an{Ax,u}^d \in \ve{A^Tu}_d^d + [0,\tau \si^dd^d \ve{u}_2^d]\]</span> (where inequalities are in the SoS sense).</p>
<em>Proof</em>. Expand the sum as <span class="math inline">\(\sum_\al x_\al (A^Tu)_\al\)</span>. For <span class="math inline">\(\al=[i,\ldots, i]\)</span> we get the term <span class="math inline">\((A^Tu)_i^d\)</span>. For the other terms, <!--each even term $\al\ne [i,\ldots, i]$ we get the term -->
<span class="math display">\[\begin{align}
0 &amp;\le \sum_{\al \in [n]^d,\al \ne [i,\ldots, i] }(\E x_\al) (A^Tu)_\al \\
&amp; \le \tau \sum_{\al \in [n]^d,\al \ne [i,\ldots, i] } (A^Tu)_\al \\
&amp; \le \tau d^d\sum_{\be \in [n]^{d/2},\be \ne [i,\ldots, i] } (A^Tu)_\be\\
&amp; \le \tau d^d\ve{A^Tu}_2^d \le \tau \si^d \ve{u}_2^d.
\end{align}\]</span>
(The <span class="math inline">\(d^d\)</span> bound is crude; this is not the bottleneck.) Note the lower inequality depends on there only being even nonzero terms.</li>
<li>(Calculate concentration, Proof of 4.2) We use a Chebyshev bound<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. We have <span class="math inline">\(|(A^Tx)_\al|\le 1\)</span>. <strong>I think we need assumption on <span class="math inline">\(\ve{A^Tx}_{\iy}\)</span></strong>. Thus taking <span class="math inline">\(\wt \Om\pf{n^{2d}}{\tau^2}\)</span> samples we get that the coefficients are whp <span class="math inline">\(\fc{\tau}{n^d}\)</span>-close. This means the value at <span class="math inline">\(u\)</span> is <span class="math inline">\(n^d \si^d\fc{\tau}{n^d}\)</span>-close, and we get whp <span class="math display">\[P\in \ve{A^Tu}_d^d + 2\tau \si^d d^d \ve{u}_2^d[-1,1].\]</span></li>
<li><p>Use noisy tensor decomposition. Check parameters. There we needed <span class="math inline">\(d \ge D=O(\rc{\ep}\ln \si)\)</span>. We need <span class="math inline">\(\tau\le \fc{\ep}{\si^DD^D}\)</span>. (<strong>Errata?</strong> The value of <span class="math inline">\(\tau\)</span> is different than in the theorem.)</p></li>
</ol>
<h2 id="polytime-algorithm">Polytime algorithm</h2>
<p>This means polytime for fixed <span class="math inline">\(\ep\)</span>. Note to get <span class="math inline">\(\rc{\ln n}\)</span> closeness (ex. for initialization of an iterative DL algorithm), we need quasipolytime, <span class="math inline">\(n^{O(\ln n)}\)</span>.</p>
<p>The bottleneck is the <span class="math inline">\(\ln m\)</span> in Lemma 6.1, where we used averaging to conclude that if <span class="math inline">\(U\)</span> has large correlation with <span class="math inline">\(A\)</span>, then <span class="math inline">\(U\)</span> has large correlation with a column of <span class="math inline">\(A\)</span>.</p>
<!-- We want to sidestep this! So directly fix a column, and instead of comparing $W$ to -->
<table>
<thead>
<tr class="header">
<th>Before</th>
<th>Now</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>5.2</td>
<td>7.2</td>
</tr>
<tr class="even">
<td>5.1</td>
<td>7.1</td>
</tr>
<tr class="odd">
<td>6.1</td>
<td>7.3</td>
</tr>
</tbody>
</table>
<p>To summarize the previous argument, we show our distribution of <span class="math inline">\(W\)</span> satisfies inequalities given in 5.2, and then use reweighting to obtain that <span class="math inline">\(u\)</span> is correlated with some column in 5.1. Now we use the weak lemma 6.1 which recovers that column, with <span class="math inline">\(\ln m\)</span> loss.</p>
<p>If we weaken the conclusion of 6.1, then we can hope that 6.1’, i.e., 7.3, doesn’t have a <span class="math inline">\(\ln m\)</span> in the place we care about. Then we need 5.1’, i.e., 7.1, to work with these weaker conditions. Sparsity allows us to weaken the conditions on correlation, provided we have better bounds information about the distribution <span class="math inline">\(W\)</span> that we choose.</p>
<p>(Not quite: 5.2 was a lemma for 5.1. 7.1 is a lemma for 7.2, we use 7.1+5.1 to get 7.1.)</p>
<p>Specifically, the condition on the p.d. <span class="math inline">\(U\)</span> (degree <span class="math inline">\(2(1+2k)\)</span>) is (7.2) <span class="math display">\[ \wt \E\an{c,U}^{2(1+2k)} \ge e^{-\ep k} \wt \E \an{c,U}^2, \qquad \wt E\an{c,u}^2\ge \tau^k.
\]</span> This is easier to satisfy: the <span class="math inline">\(\rc m\)</span> that causes problems is moved to the second inequality (as long as we have sparsity <span class="math inline">\(\tau^k\le \rc m\)</span> we are OK). The second inequality is true if sparsity is <span class="math inline">\(\tau\)</span> (CHECK!!). <span class="math inline">\(k\)</span> no longer needs a factor <span class="math inline">\(\rc{\ln m}\)</span> in the denominator to satisfy this.</p>
<p>Note 5.2, 5.1 don’t mention the sparsity at all, while 7.2, 7.1 mention <span class="math inline">\(\tau\)</span> which will be related to the sparsity.</p>
<p>Some changes: instead of reducing from degree <span class="math inline">\(k\)</span> to <span class="math inline">\(2\)</span> we reduce from degree <span class="math inline">\(\approx 4k\)</span> to <span class="math inline">\(2k\)</span>. (??)</p>
<p>So how does the algorithm work now, given that our assumption is (7.2) rather than a bound on <span class="math inline">\(\wt \E \an{c,u}\)</span>? 7.1 has the following inequality on <span class="math inline">\(\ol W= \E_WW\)</span>, (7.1): <span class="math display">\[\an{c,U}^{2(1+k)}\preceq \ol W \preceq (\an{c,U}^2 + \tau\ve{U}_2^2)^{1+k}.\]</span> Using this, get an upper bound on <span class="math inline">\(\wt \E \ol W\)</span> and lower bound on <span class="math inline">\(\wt \E \an{c,U}^{k}\)</span> to get a lower bound on <span class="math inline">\(\wt\E \ol W\an{c,U}^{2k}\)</span>: <span class="math display">\[\wt \E \ol W\an{c,U}^{2k} \wt \E \ol W \ge [\wt \E(\ol W \an{c,U}^k)]^2.\]</span></p>
<p>The bound on <span class="math inline">\(\ol W\)</span> comes from sparsity. Take <span class="math inline">\(W=\prodo i{k/2-1} \an{y_i=Ax_i,U}^2\)</span>. <span class="math inline">\(\ol W\)</span> does not actually satisfy (7.1); it does after reweighting by <span class="math inline">\(\prodo i{k/2-1} x_{i,1}^2\)</span>. (This is a distribution over polynomials. Take the probability that <span class="math inline">\(\prod \an{Ax_i,U}^2\)</span> is chosen and multiply it by <span class="math inline">\(\prod x_{i,1}^2\)</span> and normalize the probabilities. (Not multiply the polynomial.)) Consider <span class="math inline">\(c\an{Ax,u}^2\)</span> reweighted by <span class="math inline">\(x_i^2\)</span>; expand and use sparsity to get a nice bound on expectations.</p>
<p>This is much better than the previous analysis on <span class="math inline">\(W\)</span>! It actually seems to take into account some kind of concentration…</p>
<p>Final theorem is 7.6.</p>
<h2 id="todo">Todo</h2>
<ul>
<li>Summarize changes to get down to polytime. What’s the key idea?</li>
</ul>
<p>(Q: can you adapt something like this for NMF?)</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Can we do better if we use polynomial concentration?<a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class="st_facebook_large" displayText="Facebook"></span>
    <span class="st_twitter_large" displayText="Tweet"></span>
    <span class="st_googleplus_large" displayText="Google +"></span>
    <span class="st_reddit_large" displayText="Reddit"></span>
    <span class="st__large" displayText></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>


<!-- Footer -->
<div id="footer">
  <div class="container">
    Built with
    <a href="http://jaspervdj.be/hakyll">Hakyll</a> 
    using 
    <a href="http://www.getbootstrap.com">Bootstrap</a>, 
    <a href="http://www.disqus.com">Disqus</a>,
    <a href="http://ignorethecode.net/blog/2010/04/20/footnotes/">Footnotes.js</a>,
    <a href="http://highlightjs.org/">Highlight.js</a>, 
    <a href="http://www.mathjax.org">MathJax</a>, 
    and <a href="http://www.sharethis.com">ShareThis</a>.
  </div>
</div>
</body>

</html>

<!-- SCRIPTS -->
<!-- jQuery-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="../../../../bootstrap/js/bootstrap.min.js"></script>

<!-- Extension : Highlight.js @ https://highlightjs.org/ -->
<!-- Syntax highlighting tomorrow-night-bright, agate-->
<link rel="stylesheet" href="../../../../highlight/css/tomorrow-night-bright.css">
<script src="../../../../highlight/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- Extension : MathJax @ https://docs.mathjax.org/en/v2.5-latest/tex.html -->
<!-- MathJax/config/local/local.js contains macros. Need to provide entire URL-->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,http://holdenlee.github.io/notebook/MathJax/config/local/local"></script>

<!-- Extension : Footnotes @ http://ignorethecode.net/blog/2010/04/20/footnotes/ -->
<script src="../../../../footnotes/js/footnotes.js"></script>

<!-- Extension : Disqus @ http://disqus.com -->
<!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->

<script src="../../../../disqus/js/disqus.js"></script>



<!-- Extension : Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73261814-1', 'auto');
  ga('send', 'pageview');

</script>

