<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

  <meta name="description" content="Holden Lee's Research Notebook">
  <meta name="author" content="Holden Lee">
    
  <title>Linear convex regulator 2</title>

  <!-- Bootstrap core CSS -->
  <link href="../../../../bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

  <!-- Custom styles for this template -->
  <link href="../../../../css/blog.css" rel="stylesheet">
  <link href="../../../../css/default.css" rel="stylesheet">

  <!-- Extension : Footnotes -->
  <link href="../../../../footnotes/css/footnotes.css" rel="stylesheet">

  <!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->
  

  <!-- Extension : Collapsible lists @ http://code.stephenmorley.org/javascript/collapsible-lists/-->
  <link href="../../../../collapsible_lists/css/collapsible.css" rel="stylesheet">
  <script type="text/javascript" src="../../../../collapsible_lists/js/CollapsibleLists.js"></script>

</head>

<body>

<!-- Navigation bar. navbar-inverse is black, navbar-default is white.-->
<!-- To make a button active (pressed), use <li class="active"> -->
<div id="header">
  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../../">Notebook</a>
      </div>
      <div id="navbar" class="collapse navbar-collapse">
        <ul class="nav navbar-nav">
          <li><a href="../../../../">Home</a></li>
          <li><a href="../../../../sitemap.html">Sitemap</a></li>
<!-- TODO: Distinguish between PAPERS, RESEARCH QUESTIONS, BOOKS -->
<!-- TODO: make this part a for loop over main pages -->
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </nav>
</div>
<!-- Content -->
<!--div id="content">
  <h1>Mental Wilderness</h1>-->



<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Linear convex regulator 2</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-01-10 
          , Modified: 2017-01-10 
	</p>
      
       <p>Tags: <a href="../../../../tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#convex-optimization">Convex optimization</a><ul>
 <li><a href="#full-information">Full information</a></li>
 <li><a href="#see-only-costs-of-sampled-trajectories">See only costs of sampled trajectories</a></li>
 <li><a href="#unknown-dynamics">Unknown dynamics</a></li>
 <li><a href="#notes">Notes</a></li>
 </ul></li>
 <li><a href="#noisy-dynamics">Noisy dynamics</a></li>
 <li><a href="#references">References</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="lcr.html">Part 1</a></p>
<p>Fix <span class="math inline">\(x_1\)</span>. Let the cost function (jointly convex in <span class="math inline">\(x,a\)</span>) be <span class="math inline">\(c(x,a)\)</span> (for example, <span class="math inline">\(g(x) + \ve{a}^2\)</span>) the actions be <span class="math inline">\(a_1,a_2,...\)</span> and the recurrence describing the dynamics be <span class="math inline">\(x_{n+1} =Ax_n+Ba_n\)</span>, we want</p>
<span class="math display">\[\begin{align}
&amp;\quad
\min_{a_1} [c(x_1,a_1) + \min_{a_2} [c(Ax_1+Ba_1,a_2) + ...]] \\
&amp;=
\min_{a_1,...,a_T} c(x_1,a_1) + c(Ax_1+Ba_1,a_2) + ...
\end{align}\]</span>
<p>This objective is jointly convex because we are assuming <span class="math inline">\(c\)</span> is convex, and a convex function composed with a linear function is convex. Letting <span class="math inline">\(f_a(x) = Ax + Ba\)</span> be the function describing the dynamics and <span class="math inline">\(f_{a_1\cdots a_n} (x) = f_{a_n}\circ \cdots \circ f_{a_1}(x)\)</span>, we can write this as <span class="math display">\[
\min_{a_1,...,a_T} \ub{c(x_1,a_1) + c(f_{a_1}(x_1),a_2) + c(f_{a_1a_2} (x_1),a_3)+\cdots}{F(a_1,\ldots, a_T)}
\]</span></p>
<h2 id="convex-optimization">Convex optimization</h2>
<p>What guarantees do we get from convex optimization? We can consider different settings:</p>
<ul>
<li>We know the function <span class="math inline">\(c\)</span> (full information) vs. we don’t know <span class="math inline">\(c\)</span>, only get access to <span class="math inline">\(c\)</span> through the trajectories that we sample, i.e., for each episode we choose a sequence of actions <span class="math inline">\((a_1,\ldots, a_T)\)</span> and observe <span class="math inline">\(c(x_1,a_1), c(f_{a_1}(x_1),a_2),\ldots\)</span>. (Then we can care about regret bounds, etc.)</li>
<li>If we don’t know <span class="math inline">\(c\)</span>: <span class="math inline">\(c\)</span> can be deterministic or stochastic (so we care about <span class="math inline">\(\min_{a_1,...,a_T} \E c(x_1,a_1) + c(f_{a_1}(x_1),a_2) + c(f_{a_1a_2} (x_1),a_3)+\cdots\)</span>). (Stochasticity not incorporated below, but easy to include.)</li>
<li>Whether we know the dynamics. Otherwise we would have to learn the dynamics. (<a href="../optimization/HMR16.html">HMR16</a> applies if there is noise in the observation.)</li>
</ul>
<h3 id="full-information">Full information</h3>
<p>This is just gradient descent. The dimension is <span class="math inline">\(Tn\)</span>, the gradient is a sum of gradients of <span class="math inline">\(c\)</span>’s, see below. If <span class="math inline">\(c\)</span> is stochastic, then this is SGD.</p>
<h3 id="see-only-costs-of-sampled-trajectories">See only costs of sampled trajectories</h3>
<p><strong>Theorem 6.6 in OCO</strong>. Online gradient descent without a gradient attains regret bound <span class="math display">\[
\E R_T \le 9nDGT^{\fc 34}
\]</span> where</p>
<ul>
<li><span class="math inline">\(n\)</span> is the dimension.</li>
<li><span class="math inline">\(D=\diam K\)</span>, where <span class="math inline">\(K\)</span> is the convex set we’re optimizing over.</li>
<li><span class="math inline">\(G=\sup_{x\in K} \ve{\nb f(x)}\)</span>.</li>
<li><span class="math inline">\(T\)</span> is the number of steps.</li>
</ul>
<p>Applying this here, we have</p>
<ul>
<li>The dimension is <span class="math inline">\(Tn\)</span>.</li>
<li>Suppose each action is in <span class="math inline">\(K\)</span>. Then <span class="math inline">\(D=\sqrt n \diam(B)\)</span>.</li>
<li>Suppose <span class="math inline">\(f_a(x)\)</span> is a function <span class="math inline">\(K\times C \to C\)</span> and <span class="math inline">\(x_1\in C\)</span>. Let
<span class="math display">\[\begin{align}
m &amp;= \max_{x\in C, a\in K^T}\nb_x c(x,a)\\
L &amp;= \max_{x\in C, a\in K^T}\nb_a c(x,a).
\end{align}\]</span>
Let <span class="math inline">\(\ga\)</span> be the max eigenvalue of <span class="math inline">\(B\)</span>. Then
<span class="math display">\[\begin{align}
\ve{\nb_{a_i}F} &amp;= \ve{
\nb_{a_i} c(f_{a_{1:i-1}}(x_1),a_i)
+\nb_{a_i} c(f_{a_{1:i}}(x_1), a_{i+1})
+\cdots \nb_{a_i} c(f_{a_{1:T-1}}(x_1),a_T)}\\
&amp;\le L + \be (1+\ga +\cdots + \ga^{T-1-i})m\\
&amp;\le L + \fc{\be}{1-\ga}m\\
\ve{\nb_{a_{1:T}}F} &amp; \le \sqrt{T} (L + \fc{\be}{1-\ga}m).
\end{align}\]</span></li>
</ul>
<h3 id="unknown-dynamics">Unknown dynamics</h3>
<p><strong>Exploration/exploitation involved in balancing learning the dynamics with choosing the best action given known information.</strong> Think about each linearly independent <span class="math inline">\(a\)</span> as giving more information.</p>
<h3 id="notes">Notes</h3>
<ul>
<li>This does not use any kind of function approximation, so the optimization only gives us information about the optimal action at <span class="math inline">\(x_1\)</span>. <strong>If we choose a different starting point, we have to run the optimization procedure with that new starting point.</strong></li>
<li><strong>We ignore the structure of each point <span class="math inline">\(x\)</span> in the space being a tuple of actions</strong> <span class="math inline">\((a_1,\ldots, a_T)\)</span>. Is there a way to use the structure of the cost function (as a sum of costs) to get better complexity?</li>
<li>Complexity scales as lookahead time <span class="math inline">\(T\)</span>. We can do better by realizing that later actions are less important - so e.g. to estimate the gradient we can do ellipsoid sampling instead of sphere sampling.</li>
</ul>
<h2 id="noisy-dynamics">Noisy dynamics</h2>
<p>The problem changes when there is noise in the dynamics. <span class="math display">\[
x_{n+1} = Ax_n+Ba_n + \ep_n.
\]</span></p>
<ul>
<li>The convex optimization problem solves for the best actions if all actions <span class="math inline">\(a_{1:T}\)</span> have to be chosen ahead of time - you don’t get to choose your action <span class="math inline">\(a_n\)</span> online after observing <span class="math inline">\(x_n\)</span>. This may be very suboptimal.
<ul>
<li>For general MDP’s, there can be a significant gap between what you can achieve with and without an online strategy (not sure how significant the gap is for convex <span class="math inline">\(c\)</span>, but I expect it will still be there).</li>
</ul></li>
<li>(If the noise is nice, ex. Gaussian, then because we are basically convolving with the noise, convexity is preserved.)</li>
<li>If we want an online strategy (which makes much more sense), we can’t unroll like this and get a convex optimization problem. <strong>What to do?</strong></li>
</ul>
<h2 id="references">References</h2>
<p>Might be useful.</p>
<ul>
<li>Kernel methods solve bandit convex optimization: [BEL16] Kernel-based methods for bandit convex optimization (Can something like this work in the RL setting?)</li>
<li>Relationship between sampling and interior point methods: [AH15] Faster Convex Optimization - Simulated Annealing with an Efficient Universal Barrier</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class="st_facebook_large" displayText="Facebook"></span>
    <span class="st_twitter_large" displayText="Tweet"></span>
    <span class="st_googleplus_large" displayText="Google +"></span>
    <span class="st_reddit_large" displayText="Reddit"></span>
    <span class="st__large" displayText></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>


<!-- Footer -->
<div id="footer">
  <div class="container">
    Built with
    <a href="http://jaspervdj.be/hakyll">Hakyll</a> 
    using 
    <a href="http://www.getbootstrap.com">Bootstrap</a>, 
    <a href="http://www.disqus.com">Disqus</a>,
    <a href="http://ignorethecode.net/blog/2010/04/20/footnotes/">Footnotes.js</a>,
    <a href="http://highlightjs.org/">Highlight.js</a>, 
    <a href="http://www.mathjax.org">MathJax</a>, 
    and <a href="http://www.sharethis.com">ShareThis</a>.
  </div>
</div>
</body>

</html>

<!-- SCRIPTS -->
<!-- jQuery-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="../../../../bootstrap/js/bootstrap.min.js"></script>

<!-- Extension : Highlight.js @ https://highlightjs.org/ -->
<!-- Syntax highlighting tomorrow-night-bright, agate-->
<link rel="stylesheet" href="../../../../highlight/css/tomorrow-night-bright.css">
<script src="../../../../highlight/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- Extension : MathJax @ https://docs.mathjax.org/en/v2.5-latest/tex.html -->
<!-- MathJax/config/local/local.js contains macros. Need to provide entire URL-->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,http://holdenlee.github.io/notebook/MathJax/config/local/local"></script>

<!-- Extension : Footnotes @ http://ignorethecode.net/blog/2010/04/20/footnotes/ -->
<script src="../../../../footnotes/js/footnotes.js"></script>

<!-- Extension : Disqus @ http://disqus.com -->
<!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->

<script src="../../../../disqus/js/disqus.js"></script>



<!-- Extension : Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73261814-1', 'auto');
  ga('send', 'pageview');

</script>

