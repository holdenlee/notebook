<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

  <meta name="description" content="Holden Lee's Research Notebook">
  <meta name="author" content="Holden Lee">
    
  <title>Second-order methods</title>

  <!-- Bootstrap core CSS -->
  <link href="../../../../bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

  <!-- Custom styles for this template -->
  <link href="../../../../css/blog.css" rel="stylesheet">
  <link href="../../../../css/default.css" rel="stylesheet">

  <!-- Extension : Footnotes -->
  <link href="../../../../footnotes/css/footnotes.css" rel="stylesheet">

  <!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->
  

  <!-- Extension : Collapsible lists @ http://code.stephenmorley.org/javascript/collapsible-lists/-->
  <link href="../../../../collapsible_lists/css/collapsible.css" rel="stylesheet">
  <script type="text/javascript" src="../../../../collapsible_lists/js/CollapsibleLists.js"></script>

</head>

<body>

<!-- Navigation bar. navbar-inverse is black, navbar-default is white.-->
<!-- To make a button active (pressed), use <li class="active"> -->
<div id="header">
  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../../">Notebook</a>
      </div>
      <div id="navbar" class="collapse navbar-collapse">
        <ul class="nav navbar-nav">
          <li><a href="../../../../">Home</a></li>
          <li><a href="../../../../sitemap.html">Sitemap</a></li>
<!-- TODO: Distinguish between PAPERS, RESEARCH QUESTIONS, BOOKS -->
<!-- TODO: make this part a for loop over main pages -->
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </nav>
</div>
<!-- Content -->
<!--div id="content">
  <h1>Mental Wilderness</h1>-->



<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Second-order methods</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-22 
          , Modified: 2016-04-22 
	</p>
      
       <p>Tags: <a href="../../../../tags/Newton.html">Newton</a>, <a href="../../../../tags/second-order.html">second-order</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#main-points">Main points</a></li>
 <li><a href="#proofs">Proofs</a></li>
 <li><a href="#convergence">Convergence</a></li>
 <li><a href="#more-intuition">More intuition</a></li>
 <li><a href="#implementation-issues">Implementation issues</a></li>
 <li><a href="#review">Review</a></li>
 <li><a href="#scraps">Scraps</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>See <a href="GD.html">gradient descent</a>.</p>
<ul>
<li>What is the general framework?
<ol type="1">
<li>Pick a descent direction <span class="math inline">\(\De x\)</span>.</li>
<li>Choose a step size <span class="math inline">\(\tau&gt;0\)</span>: <span class="math display">\[x^{(t+1)} \leftarrow x+\tau \De x.\]</span></li>
<li>Continue until stop criterion.</li>
</ol></li>
</ul>
<h2 id="main-points">Main points</h2>
<ul>
<li>What’s the shortcoming of gradient descent that we want to fix?
<ul>
<li>It is not invariant under linear transformation.</li>
<li>When the condition number of the Hessian is large, it has bad convergence.</li>
</ul></li>
<li>Steepest descent
<ul>
<li>For a norm <span class="math inline">\(\ved\)</span>, the steepest descent direction is (sd = steepest descent, nsd = normalized steepest descent)
<span class="math display">\[\begin{align}
\De x_{sd} &amp;= \min_{\ve{y}\le 1} \nb f(x)^T y.
\end{align}\]</span></li>
</ul></li>
<li>Newton method
<ul>
<li>Let
<span class="math display">\[\begin{align}
\De x_{sd} &amp;= -H^{-1}\nb f\\
\De x_{nsd} &amp;=\fc{H^{-1} \nb f}{\ve{H^{-\rc 2} \nb f}} = \fc{H^{-1} \nb f}{\la(x)^2}\\
\la(x) &amp;= \ve{H^{-\rc 2}\nb f}^{\rc 2} = (\nb f^T H^{-1} \nb f)^{\rc 2}.
\end{align}\]</span>
Here <span class="math inline">\(\la(x)\)</span> is the Newton decrement. (Use <span class="math inline">\(\Delta x = \Delta x_{nsd}\)</span>.)</li>
</ul></li>
<li>Newton for functions with Lipschitz Hessian: The number of steps to reach <span class="math inline">\(\ep\)</span> is <span class="math display">\[\fc{f(x^{(0)}-p^*)}{\ga} + \ln\ln \fc{\ep_0}{\ep}, \ep_0=\fc{2m^3}{L^2}.\]</span></li>
<li><span class="math inline">\(f\)</span> is <strong>self-concordant</strong> if for all <span class="math inline">\(v\)</span>, <span class="math inline">\(\an{\nb^3 f, \De x^{\ot 3}} \le 2 \an{\nb^2 f, \De x^{\ot2}}^{\fc 32}\)</span>. For self-concordant functions, the number of steps to reach <span class="math inline">\(\ep\)</span> is <span class="math display">\[\fc{f(x^{(0)}-p^*)}{\ga} + \log_2\log_2\prc{\ep}\]</span> where <span class="math inline">\(\ga = \fc{\al \be (1-2\al)^2}{20-8\al}\)</span>.</li>
<li>What are the drawbacks of Newton’s method and how to fix them?
<ul>
<li>Naively it requires computing <span class="math inline">\(H^{-1}\)</span> which (naively?) takes <span class="math inline">\(n^3\)</span> time each iteration, where <span class="math inline">\(n\)</span> is the dimension. <em>For large <span class="math inline">\(n\)</span> this is prohibitive.</em></li>
</ul></li>
</ul>
<h2 id="proofs">Proofs</h2>
<ul>
<li>Newton method is steepest descent for <span class="math inline">\(H\)</span>:
<ul>
<li>Let <span class="math inline">\(A\)</span> be a symmetric positive definite matrix. Defining <span class="math inline">\(\ved_A\)</span> as follows, we note that the dual norm is the norm corresponding to the inverse.
<span class="math display">\[\begin{align}
\ve{A} :&amp;= x^TAx = \ve{\sqrt A}_2\\
\ve{x}_{A}^* &amp; = \ve{x}_{A^{-1}}.
\end{align}\]</span>
<ul>
<li><em>Proof.</em> <span class="math display">\[\ve{x}_A^* = \max_{\ve{y}_A=1} x^Ty \stackrel{z = \sqrt{A}y}{=} \max_{\ve{z}=1} x^T A^{-\rc 2} z = \ve{A^{-\rc 2}x^T}  = \ve{x}_{A^{-1}}.\]</span></li>
<li>This calculation also shows that <span class="math display">\[\amin_{\ve{y}_A=1} v^Ty = \fc{A^{-1} x}{\ve{A^{-\rc 2}x}}.\]</span></li>
<li>Thus,
<span class="math display">\[\begin{align}
\De x_{nsd} &amp;= \fc{H^{-1} \nb f(x)}{\ve{H^{-\rc 2} \nb f}}.
\end{align}\]</span></li>
</ul></li>
<li>Why is <span class="math inline">\(\De x_{nsd}\)</span> the right normalization? <span class="math display">\[ f\pa{x - t\fc{H^{-1} \nb f(x)}{\ve{H^{-\rc 2} \nb f}}} \approx f(x) + (-t+\fc{t^2}2) \fc{H^{-1} \nb f(x)}{\ve{H^{-\rc 2} \nb f}}\]</span> and <span class="math inline">\(t=1\)</span> minimizes this.</li>
</ul></li>
</ul>
<h2 id="convergence">Convergence</h2>
<ul>
<li>What is the main idea of Newton descent? Why/how do we get quadratic convergence?
<ul>
<li>Go in the direction of the minimum suggested by the Hessian (second order approximation).</li>
<li>The proof is organized as follows. If <span class="math inline">\(\ve{\nb f(x^{(k)})}^2\)</span> is
<ul>
<li>large (<span class="math inline">\(\ge \eta\)</span>), then make constant progress by <span class="math inline">\(\ga\)</span>. This is the dampled phase; steps are small. Steps are small because the default Newton step <span class="math inline">\(t=1\)</span> is bad, and backtracking will choose a smaller <span class="math inline">\(t\)</span>. (?) (It’s “slow” in the sense it’s not quadratically convergent, but it makes constant progress.)</li>
<li>small (<span class="math inline">\(&lt;\eta\)</span>), then make quadratic progress, <span class="math inline">\(\ve{\nb f(x^{(k+1)})}_2\le \fc{L}{2m^2}\ve{\nb f(x^{(k)})}_2^2\)</span>.</li>
</ul></li>
</ul></li>
<li>What should <span class="math inline">\(\eta\)</span> depend on? <span class="math inline">\(\al\)</span> (the slope for backtracking), <span class="math inline">\(m\)</span> (strong convexity parameter), <span class="math inline">\(L\)</span> (Lipschitz constant on Hessian).</li>
</ul>
<em>Proof (for Lipschitz)</em>. We do the calculations for 1 dimension. The calculations are the same, except we have to use matrices and vectors. Let <span class="math inline">\(\te(y)\)</span> be a quantity in <span class="math inline">\([-y,y]\)</span>. Suppose we are at <span class="math inline">\((0,0)\)</span>. Let <span class="math inline">\(d=f'(0), a=f''(0), \la = \fc{d}{a^{\rc 2}}\)</span>, <span class="math inline">\(\De x_{nt} = \fc{d}{a} = \fc{\la}{a^{\rc 2}}\)</span>.
<span class="math display">\[\begin{align}
f(x) &amp;= dx + \rc 2 ax^2 + \te\pa{\rc 6 Lx^3}\\
f\pa{-\fc da} &amp;= -\rc 2 \fc{d^2}a\\
&amp;\le -\fc{d^2}{a} \ub{\pa{\rc 2 - \rc 6 L \fc{d}{a^2}}}{\ge \al}.
\end{align}\]</span>
<p>In order for the quantity to be <span class="math inline">\(\ge \al\)</span>, noting <span class="math inline">\(\fc{d}{a^2} = \fc{d}{a^{\fc 32}}\)</span>, we want <span class="math inline">\(\la \le \fc{3(1-2\al) a^{\fc 32}}L\)</span>; it’s sufficient for <span class="math inline">\(f'\le \fc{3(1-2\al)m^2}{L}\)</span>.</p>
Note that unlike for linear convergence, we don’t prove something like <span class="math inline">\(f(x') - f(x^*)\le \cdots\)</span>. We have to work with the gradients to get quadratic convergence. (Gradients also control the distance to the optimum.) We have
<span class="math display">\[\begin{align}
f'(x) &amp;= d+ ax + \te\pa{\rc L x^2}\\
f'\pa{-\fc da} &amp; \le \rc 2 L\fc{d^2}{a^2} \le \fc{L}{2m^2}f'(0).
\end{align}\]</span>
<em>Proof (for self-concordant)</em>. Work in 1-D. Instead of integrating <span class="math inline">\(\int f'''=\int (f'')^{\fc 32}\)</span>, we let <span class="math inline">\(F(y)=y^{-\rc 2}\)</span> and integrate <span class="math inline">\(\int (F(f''))'\)</span>.
<span class="math display">\[\begin{align}
|(f'')^{-\rc 2} (t)| &amp;= \ab{\int_0^t ((f'')^{-\rc 2})'} =\int_0^t \ab{-\rc 2 (f'')^{-\fc 32} f'''}\le t\\
\label{eq:f''}
\implies
\rc{(-t + f''(0)^{-\rc 2})^2} &amp;\ge f''(t) \ge \rc{(t+f''(0)^{-\rc 2})^2}\\
\implies
\rc{-t+f''(0)^{-\rc 2}} - f''(0)^{\rc 2} &amp; \ge f'(t)-f'(0) \ge \rc{t+f''(0)^{-\rc 2}} + f''(0)^{\rc 2}\\
\implies f(t) &amp;\le f(0) + (f'(0) - f''(0)^{\rc 2}) t + \ln \pf{f''(0)^{-\rc 2}}{t + f''(0)^{-\rc 2}}\\
\implies f\pa{-\fc{f'}{f''}t} &amp; \le f(0) - \la^2 t + \la t - \ln (1-t\la(x)).
\end{align}\]</span>
<p>Now consider 2 cases.</p>
<ul>
<li><span class="math inline">\(\la(x^{(k)})&gt;\eta\)</span>. We show <span class="math inline">\(f(x^{(k+1)}) - f(x^{(k)}) \le -\ga\)</span>.
<ul>
<li>Using <span class="math inline">\(-x + \ln (1+x) + \fc{x^2}{2(1+x)}\le 0\)</span>, for <span class="math inline">\(t=\rc{1+\la(x)}\)</span>,
<span class="math display">\[\begin{align}
f(-\fc{f'}{f''} t) &amp; \le f(0) - \al \la(x)^2 t\\
t=\rc{1+\la(x)^2}\implies f(-\fc{f'}{f''} t) &amp; \le f(0) - \al \la(x)^2 t
\end{align}\]</span>
so backtracking line search chooses <span class="math inline">\(t\ge \fc{\be}{1+\la(x)}\)</span> with <span class="math display">\[f(-\fc{f'}{f''} t)  \le -\al \be \fc{\la^2}{1+\la} \le -\al \be \fc{\eta^2}{1+\eta}.\]</span></li>
</ul></li>
<li><span class="math inline">\(\la(x^{(k)})\le \eta\)</span>. We show <span class="math inline">\(2\la(x^{(k+1)})\le (2\la(x^{(k)}))^2\)</span>. (Note we are not getting a bound on <span class="math inline">\(f'(x_{k+1})\)</span> like before because we don’t have strong convexity, which upper-bounds <span class="math inline">\(f''(x)^{-\rc 2}\)</span>.)
<ul>
<li>We take a unit step because <span class="math display">\[f(-\fc{f'}{f''})   = f(0) - \la^2 + \la - \ln (1-\la(x))\le f(0)-\al \la(x)^2\]</span> when <span class="math inline">\(\la(x) \le \fc{1-2\al}{2}\)</span>.</li>
<li>Self-concordance gives multiplicative bound on how <span class="math inline">\(H\)</span> changes, <span class="math inline">\((1-t\al)^2 H \preceq H(x+tv) \preceq \rc{(1-t\al)^2} H(x)\)</span>, where <span class="math inline">\(\al=\ve{v}_{H^{\rc 2}}\)</span>. Proof:
<span class="math display">\[\begin{align}
\ln f''(t) - \ln f''(0) &amp; = \int_0^t (\ln f'')' \\
&amp; \le \int_0^t (2f'')^{\rc 2}\\
&amp;\le \int_0^t \fc{2}{-x+(f'')^{-\rc 2}} &amp;\eqref{eq:f''}\\
&amp;\le 2\ln (-t (f'')^{\rc 2} + 1).
\end{align}\]</span></li>
<li>Now we show <span class="math inline">\(\la(x-\fc{f'}{f''}) \le \fc{\la^2}{(1-\la)^2}\)</span>. Using the multiplicative bound (here <span class="math inline">\(t=1\)</span>, <span class="math inline">\(v=-\fc{f'}{f''}\)</span>, <span class="math inline">\(\al=\la\)</span>.
<span class="math display">\[\begin{align}
f''(x_{k+1})&amp;\in f''(x_k) [1-\la, \rc{1-\la}]\\
f'(x_{k+1}) &amp; \in f'(x_k) + \ba{-\rc{\fc{f'}{f''} + \rc{(f'')^{\rc 2}}} + (f'')^{\rc 2}, \rc{-\fc{f'}{f''}+\rc{(f'')^{\rc2}}} - (f'')^{\rc 2}}\\
&amp;= f'(x_k) \ba{\fc{\la}{-\la + 1}, \fc{\la}{\la+1}}\\
\la(x_{k+1})&amp;\le \fc{\la(x_k)^2}{(1-\la(x_k))^2} \\ &amp; \le 2\la^2
\end{align}\]</span>
where the last inequality is when <span class="math inline">\(\la\le \rc 4\)</span>.</li>
</ul></li>
<li>Finally, bound the distance to optimum by <span class="math inline">\(\la\)</span>, <span class="math inline">\(f(x^{(l)})-p^* \le \la(x^{(l)})^2\)</span>.</li>
</ul>
<h2 id="more-intuition">More intuition</h2>
<ul>
<li>Why do we use <span class="math inline">\(\ved_H\)</span>?
<ul>
<li>It appears in the second-order Taylor approximation. <span class="math inline">\(f(x) + \nb f^T v + \rc 2 v^T \nb^2 f v\)</span> has a minimum at <span class="math inline">\(\fc{H^{-1}f}{\ve{H^{-\rc 2}f}}\)</span>, not in the gradient direction!</li>
<li>It is invariant to linear transformation: <span class="math inline">\(\De x_{nsd} (f\circ A) = \De x_{nsd} f\)</span>.</li>
</ul></li>
<li>What is the relationship to Newton’s method of finding zeros?
<ul>
<li>The Newton’s method here is the Newton zero-finding method applied to <span class="math inline">\(f'\)</span>.</li>
</ul></li>
<li>Does order <span class="math inline">\(c\)</span> give <span class="math inline">\(2^{-n^c}\)</span> convergence?
<ul>
<li>Probably. But it’s rare to get <span class="math inline">\(&gt;2\)</span> order information.</li>
</ul></li>
<li>Give an example where gradient descent performs poorly compared to Newton. <span class="math inline">\(f = x_1^2 + \ep x_2^2\)</span>, <span class="math inline">\(-\nb f = (-2x_1,-2\ep x_2)\)</span>.</li>
<li>Intuition for self-concordance
<ul>
<li>We relax the requirement that <span class="math inline">\(f\)</span> is strongly convex and has Lipschitz Hessian. We’re allowed to have <span class="math inline">\(f''\to 0\)</span>; the requirement is that when <span class="math inline">\(f''\)</span> is small so is <span class="math inline">\(f'''\)</span>. Note <span class="math inline">\(\fc{(f'')^{\fc 32}}{f'''}\)</span> is dimensionless.</li>
</ul></li>
</ul>
<h2 id="implementation-issues">Implementation issues</h2>
<ol type="1">
<li>Precompute line searches: it can be more efficient to simultaneously compute <span class="math inline">\(f(x+t\De x)\)</span> for many values of <span class="math inline">\(t\)</span>, e.g., if it involves a linear/matrix function.</li>
<li>Computing <span class="math inline">\(\De x_{nt}=H^{-1}\nb f(x)\)</span> is more efficient if <span class="math inline">\(H\)</span> has band structure, sparsity, etc.</li>
</ol>
<h2 id="review">Review</h2>
<ol type="1">
<li>Describe the Newton method. What is the step size and Newton decrement?</li>
<li>What is the convergence rate for functions with Lipschitz Hessian?</li>
<li>Define self-concordant. What is the convergence for self-concordant functions?</li>
</ol>
<h2 id="scraps">Scraps</h2>
<p>? 9.31,</p>
<p>estimating Hessian?</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class="st_facebook_large" displayText="Facebook"></span>
    <span class="st_twitter_large" displayText="Tweet"></span>
    <span class="st_googleplus_large" displayText="Google +"></span>
    <span class="st_reddit_large" displayText="Reddit"></span>
    <span class="st__large" displayText></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>


<!-- Footer -->
<div id="footer">
  <div class="container">
    Built with
    <a href="http://jaspervdj.be/hakyll">Hakyll</a> 
    using 
    <a href="http://www.getbootstrap.com">Bootstrap</a>, 
    <a href="http://www.disqus.com">Disqus</a>,
    <a href="http://ignorethecode.net/blog/2010/04/20/footnotes/">Footnotes.js</a>,
    <a href="http://highlightjs.org/">Highlight.js</a>, 
    <a href="http://www.mathjax.org">MathJax</a>, 
    and <a href="http://www.sharethis.com">ShareThis</a>.
  </div>
</div>
</body>

</html>

<!-- SCRIPTS -->
<!-- jQuery-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="../../../../bootstrap/js/bootstrap.min.js"></script>

<!-- Extension : Highlight.js @ https://highlightjs.org/ -->
<!-- Syntax highlighting tomorrow-night-bright, agate-->
<link rel="stylesheet" href="../../../../highlight/css/tomorrow-night-bright.css">
<script src="../../../../highlight/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- Extension : MathJax @ https://docs.mathjax.org/en/v2.5-latest/tex.html -->
<!-- MathJax/config/local/local.js contains macros. Need to provide entire URL-->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,http://holdenlee.github.io/notebook/MathJax/config/local/local"></script>

<!-- Extension : Footnotes @ http://ignorethecode.net/blog/2010/04/20/footnotes/ -->
<script src="../../../../footnotes/js/footnotes.js"></script>

<!-- Extension : Disqus @ http://disqus.com -->
<!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->

<script src="../../../../disqus/js/disqus.js"></script>



<!-- Extension : Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73261814-1', 'auto');
  ga('send', 'pageview');

</script>

