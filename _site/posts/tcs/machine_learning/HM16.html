<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

  <meta name="description" content="Holden Lee's Research Notebook">
  <meta name="author" content="Holden Lee">
    
  <title>(HM16) A non-generative framework and convex relaxations for unsupervised learning</title>

  <!-- Bootstrap core CSS -->
  <link href="../../../bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

  <!-- Custom styles for this template -->
  <link href="../../../css/blog.css" rel="stylesheet">
  <link href="../../../css/default.css" rel="stylesheet">

  <!-- Extension : Footnotes -->
  <link href="../../../footnotes/css/footnotes.css" rel="stylesheet">

  <!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->
  

  <!-- Extension : Collapsible lists @ http://code.stephenmorley.org/javascript/collapsible-lists/-->
  <link href="../../../collapsible_lists/css/collapsible.css" rel="stylesheet">
  <script type="text/javascript" src="../../../collapsible_lists/js/CollapsibleLists.js"></script>

</head>

<body>

<!-- Navigation bar. navbar-inverse is black, navbar-default is white.-->
<!-- To make a button active (pressed), use <li class="active"> -->
<div id="header">
  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../">Notebook</a>
      </div>
      <div id="navbar" class="collapse navbar-collapse">
        <ul class="nav navbar-nav">
          <li><a href="../../../">Home</a></li>
          <li><a href="../../../sitemap.html">Sitemap</a></li>
<!-- TODO: Distinguish between PAPERS, RESEARCH QUESTIONS, BOOKS -->
<!-- TODO: make this part a for loop over main pages -->
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </nav>
</div>
<!-- Content -->
<!--div id="content">
  <h1>Mental Wilderness</h1>-->



<div class="container">
  <div id="content">
    <div class="page header">
      <h1>(HM16) A non-generative framework and convex relaxations for unsupervised learning</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-12-27 
          , Modified: 2016-12-27 
	</p>
      
       <p>Tags: <a href="../../../tags/unsupervised.html">unsupervised</a>, <a href="../../../tags/convex%20relaxation.html">convex relaxation</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#framework">Framework</a></li>
 <li><a href="#generalization-theory">Generalization theory</a></li>
 <li><a href="#pca">PCA</a></li>
 <li><a href="#spectral-autoencoder">Spectral autoencoder</a></li>
 <li><a href="#dictionary-learning">Dictionary learning</a><ul>
 <li><a href="#group-encoding-and-decoding">Group encoding and decoding</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="framework">Framework</h2>
<p>Given</p>
<ol type="1">
<li>Instance domain <span class="math inline">\(X\)</span> and target space <span class="math inline">\(Y\)</span> (ex. <span class="math inline">\(\R^d\)</span>, <span class="math inline">\(\R^k\)</span>, <span class="math inline">\(d\gg k\)</span>).</li>
<li>Unknown distribution <span class="math inline">\(D\)</span> on domain <span class="math inline">\(X\)</span>. (NOT assumed to be generated by a function in <span class="math inline">\(H\)</span>.)</li>
<li>Hypothesis class of decoding and encoding pairs <span class="math display">\[ H \subeq \{X\to Y\}\times \{Y\to X\}.\]</span></li>
<li>Loss function <span class="math inline">\(l:H\times X\to \R_{\ge0}\)</span>. (Ex. <span class="math inline">\(\ell_2\)</span> loss <span class="math inline">\(\ve{g(h(x))-x}_2^2\)</span>.
<ul>
<li>Define <span class="math display">\[
 \text{loss}_D(f) = \EE_{x\sim D} l(f,x).
 \]</span></li>
<li>Define sample loss <span class="math display">\[
 \text{loss}_S(f) = \rc m \sum_{x\in S}l(f,x).
 \]</span></li>
</ul></li>
</ol>
<p><span class="math inline">\(D,X\)</span> is <span class="math inline">\((k,\ga)\)</span> <span class="math inline">\(C\)</span>-learnable wrt <span class="math inline">\(H\)</span> if there exists an algorithm that given <span class="math inline">\(\de,\ep&gt;0\)</span>, after seeing <span class="math inline">\(m(\ep, \de) = \poly\pa{\rc \ep, \ln \prc{\de}, d}\)</span> examples, returns <span class="math inline">\((h,g)\)</span> (NOT necessarily in <span class="math inline">\(H\)</span>) such that</p>
<ol type="1">
<li>wp <span class="math inline">\(\ge 1-\de\)</span>, <span class="math display">\[\text{loss}_D((h,g)) \le \min_{(h,g)\in H} \text{loss}_D((h,g)) + \ep + \ga.\]</span></li>
<li><span class="math inline">\(h\)</span> has an explicit representation with <span class="math inline">\(\le k\)</span> bits.</li>
</ol>
<p>(<span class="math inline">\(\ga\)</span> is the bias.)</p>
<p>(Q: ? how to understand “<span class="math inline">\(\le k\)</span>” bits?)</p>
<h2 id="generalization-theory">Generalization theory</h2>
<span class="math display">\[\begin{align}
\wh f_{ERM} &amp;= \amin_{f\in H} \text{loss}_S(\wh f)\\
R_{S,l}(H) &amp;= \EE_{\si\sim \{\pm 1\}^m} \ba{
\sup_{f\in H} \rc m \sum_{x\in S}\si_i l(f,x)
}\\
\Pj\pa{
\text{loss}_D(\wh f_{ERM})
 \le \min_{f\in H} \text{loss}_D(f) + 6 R_m(H) + \sfc{4\ln \prc{\de}}{2m}}\ge 1-\de.
\end{align}\]</span>
<h2 id="pca">PCA</h2>
For <span class="math inline">\(H_k^{pca}\)</span>,
<span class="math display">\[\begin{align}
X&amp;=\R^n\\
Y&amp;=\R^k\\
h(x) &amp;= Ax\\
g(y) &amp;= A^+y\\
l((g,h),x) &amp; = \ve{(A^+A - I)x}^2.
\end{align}\]</span>
<p>For <span class="math inline">\(H_{k,s}^{pca}\)</span>, replace <span class="math inline">\(d\)</span> by <span class="math inline">\(d^s\)</span>, <span class="math inline">\(x\)</span> by <span class="math inline">\(x^{\ot s}\)</span>.</p>
<p><span class="math inline">\(H_{k,s}^{pca}\)</span> is efficiently <span class="math inline">\((k,0)\)</span> C-learnable.</p>
<p>(Proof: Use SVD. Rademacher complexity is <span class="math inline">\(O\pf{d^s}{m}\)</span>.)</p>
<h2 id="spectral-autoencoder">Spectral autoencoder</h2>
For <span class="math inline">\(H_{k,s}^{s,a}\)</span>,
<span class="math display">\[\begin{align}
h(x) &amp;= Ax^{\ot s},&amp; A&amp;\in \R^{k\times d^s}\\
g(y) &amp;= v_{\max}(By),&amp; B&amp;\in \R^{d^s\times k}
\end{align}\]</span>
<p>where <span class="math inline">\(v_{\max}\)</span> is max eigenvector of tensor.</p>
<p>For <span class="math inline">\(s=2\)</span>:</p>
<p><span class="math inline">\(D\)</span> is <span class="math inline">\((k,\ep)\)</span>-regularly spectral decodable <span class="math inline">\(A\in \R^{k\times d^2}\)</span>, <span class="math inline">\(B\in \R^{d^2\times k}\)</span> with <span class="math inline">\(\ve{BA}\le \tau\)</span> such that for <span class="math inline">\(x\sim D\)</span>, <span class="math display">\[ M(BAx^{\ot 2}) = xx^T + E, \quad \ve{E}\le \ep.\]</span> Here <span class="math inline">\(M\)</span> means matrix representation.</p>
<p>(order of qualifiers for <span class="math inline">\(\tau\)</span>?)</p>
<p><span class="math inline">\(H_{k,2}^{sa}\)</span> is <span class="math inline">\(\pa{O\pa{\tau^4k^4}{\de^4}, \de}\)</span> <span class="math inline">\(C\)</span>-learnable in poly-time.</p>
<p><em>Proof</em>.</p>
<ol type="1">
<li>Rademacher complexity bound for <span class="math inline">\(\Phi = \set{\ve{Rx^{\ot 2}-x^{\ot 2}}}{\ve{R}_{S_1}\le \tau k}\)</span>: <span class="math display">\[R_m(\Phi) \le 2\tau k \sfc1m.\]</span> (CHECK. Omitted.)
<ul>
<li><span class="math inline">\(L^2\)</span> bound on <span class="math inline">\(x\)</span> gives <span class="math inline">\(L^1\)</span> bound on <span class="math inline">\(x^{\ot 2}\)</span>, which is why we need <span class="math inline">\(S_1\)</span> bound.</li>
</ul></li>
<li><span class="math inline">\(f(R) := \E[\ve{Rx^{\ot 2}-x^{\ot 2}}]\)</span> is convex and 1-Lipschitz.
<ul>
<li><em>Proof</em>. <span class="math inline">\((u\ot v)(x^{\ot 2}^T)\)</span> is a subgradient, where <span class="math inline">\(u,v\in \R^d\)</span> are top left/right singular vectors of <span class="math inline">\(M(Rx^{\ot 2} - x^{\ot 2})\)</span>.</li>
</ul></li>
<li>Optimization: Use (non-smooth) Frank-Wolfe algorithm (CHECK).
<ul>
<li>Each update is rank 1.</li>
<li><span class="math inline">\(R^*\)</span> is a feasible solution with <span class="math inline">\(\ve{R^*}_{S_1}\le \rank(R^*)\ve{R}\le \tau k\)</span>, so the diameter of <span class="math inline">\(\Phi\)</span> is <span class="math inline">\(\le \tau k\)</span>.</li>
<li><span class="math inline">\(f\)</span> is 1-Lipschitz.</li>
<li>In <span class="math inline">\(O\pf{\tau^4k^4}{\de^4}\)</span> steps get <span class="math inline">\(\wh R\)</span> with <span class="math inline">\(f(\wh R)\le \de + \ep\)</span>.</li>
</ul></li>
</ol>
<h2 id="dictionary-learning">Dictionary learning</h2>
<span class="math display">\[\begin{align}
h_A(x) &amp;= \amin{\ve{y}_\be\le k} \rc d |x-Ay|\\
g_A(y) &amp;= Ay\\
A&amp;= \amin_{\ve{A}_\al \le c_\al}
\EE_{x\sim D} \ba{
\min_{y\in \R^r: \ve{y}_\be \le k} \rc d |x-Ay|_1
}.
\end{align}\]</span>
<p>Ex. <span class="math inline">\(\ved_\al\)</span> is max (column <span class="math inline">\(\ell_2\)</span> or <span class="math inline">\(\ell_\iy\)</span> norm) and <span class="math inline">\(\ved_b\)</span> is <span class="math inline">\(\ell_0\)</span>.</p>
<p>Here, use max column <span class="math inline">\(\ell_\iy\)</span> norm (i.e., <span class="math inline">\(\ell_1\to \ell_\iy\)</span>) for <span class="math inline">\(A\)</span> and <span class="math inline">\(\ell_1\)</span> norm for <span class="math inline">\(B\)</span>. (Q: How does this compare to usual setting? <span class="math inline">\(\ell_1\)</span> norm is looser than <span class="math inline">\(\ell_0\)</span>, OK. <span class="math inline">\(\ell_\iy\)</span> is also looser than <span class="math inline">\(\ell_2\)</span>.)</p>
<p>For any <span class="math inline">\(\de&gt;0, p\ge 1\)</span>, <span class="math inline">\(H_k^{dict}\)</span> is C-learnable with encoding length <span class="math inline">\(\wt O\pf{k^2 r^{\rc p}}{\de^2}\)</span>, bias <span class="math inline">\(\de+ O(\ep^*)\)</span>, and sample complexity <span class="math inline">\(d^{O(p)}\)</span> in time <span class="math inline">\(n^{O(p^2)}\)</span>.</p>
<p>Interpretation:</p>
<ul>
<li>For <span class="math inline">\(p\to \iy\)</span>, get <span class="math inline">\(\wt \pf{k^2}{\de^2}\)</span>. Can get nontrivial approximation by taking <span class="math inline">\(\de = o(\sqrt k)\)</span>, so can get close to <span class="math inline">\(k\)</span>. This makes sense thinking of the input as <span class="math inline">\(k\)</span>-sparse.</li>
</ul>
<p>Note this isn’t doing “dictionary learning” in the usual sense of finding the dictionary. It only shows that you can reduce dimension and still preserve information. It does not “undo” the dictionary multiplication and give a <em>useful</em> representation, only a <em>succinct</em> one.</p>
<h3 id="group-encoding-and-decoding">Group encoding and decoding</h3>
<p>This is a simpler algorithm which achieves a weaker goal of encoding multiple examples at a time.</p>
<p>Given <span class="math inline">\(N\)</span> points <span class="math inline">\(X\in \R^{d\times N}\sim D^N\)</span>, convex set <span class="math inline">\(Q\)</span>,</p>
<ol type="1">
<li>Group encoding <span class="math inline">\(h(X)\)</span>:
<ul>
<li>Compute <span class="math inline">\(Z=\amin{C\in Q} |X-C|_1\)</span>.</li>
<li>Random sample <span class="math inline">\(B\)</span> taking each entry with probability <span class="math inline">\(\rh\)</span>, <span class="math inline">\(Y=P_{\Om}(Z)\)</span>.</li>
</ul></li>
<li>Group decoding <span class="math inline">\(g(Y)\)</span>:
<ul>
<li><span class="math inline">\(\amin_{C\in Q} |P_\Om(C)-Y|_1\)</span>.</li>
</ul></li>
</ol>
<p>We need a conex relaxation <span class="math inline">\(Q\)</span> <span class="math display">\[
\set{g_{A^*}(h_{A^*}(X))}{X\in \R^{d\times N}} \sub
\set{AY}{\ve{A}_{\ell^1\to \ell^\iy}\le 1, \ve{Y}_{\ell_1\to \ell_1}}\sub Q.
\]</span> with low sampling Rademacher width and that is efficiently optimizable.</p>
<ol type="1">
<li>(Lemma 5.2) If a convex set has low sampling Rademacher width, then we can compress it by random sampling.</li>
<li>Define the factorable “norm” <span class="math inline">\(\Ga_{\al,\be}(Z) = \inf_{Z=AB}\ve{A}_\al\ve{B}_\be\)</span>. <span class="math inline">\(\Ga_{1,q,1,t}(\cdot) = \Ga_{\ell_1\to \ell_q, \ell_1\to \ell_t}\)</span> is a norm (5.3-4). <span class="math inline">\(Q_{1,\iy,1,1} = \set{C\in \R^{N\times d}}{\Ga_{1,\iy,1,1}(C)\le k}\)</span> has low SRW (5.5).</li>
<li>This is not efficiently optimizable. Instead consider the SoS relaxation
<span class="math display">\[\begin{align}
Q_p^{sos} &amp;= \set{C\in \R^{d\times N}}{\exists \text{degree-}O(p^2)\text{pseudo-expectation $\wt\E$ that satisfies }A(C)}\\
A(C) &amp; = \set{C=AB}\cup 
\bc{ \forall i,j, B_{ij} = b_{ij}^{p-1}, \sumo lr b_{lj}^p \le k^{\fc p{p-1}},\forall i,j, A_{ij}^2\le 1}.
\end{align}\]</span>
Now redo the proof for low SRW using a SoS proof. Motivation: for <span class="math inline">\(Q\)</span> we use an inequality involving <span class="math inline">\(L^1,L^{\iy}\)</span>. In oreder to convert to SoS proof, we need to turn this into <span class="math inline">\(L^{\fc{p}{p-1}},L^p\)</span>.</li>
</ol>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class="st_facebook_large" displayText="Facebook"></span>
    <span class="st_twitter_large" displayText="Tweet"></span>
    <span class="st_googleplus_large" displayText="Google +"></span>
    <span class="st_reddit_large" displayText="Reddit"></span>
    <span class="st__large" displayText></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>


<!-- Footer -->
<div id="footer">
  <div class="container">
    Built with
    <a href="http://jaspervdj.be/hakyll">Hakyll</a> 
    using 
    <a href="http://www.getbootstrap.com">Bootstrap</a>, 
    <a href="http://www.disqus.com">Disqus</a>,
    <a href="http://ignorethecode.net/blog/2010/04/20/footnotes/">Footnotes.js</a>,
    <a href="http://highlightjs.org/">Highlight.js</a>, 
    <a href="http://www.mathjax.org">MathJax</a>, 
    and <a href="http://www.sharethis.com">ShareThis</a>.
  </div>
</div>
</body>

</html>

<!-- SCRIPTS -->
<!-- jQuery-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="../../../bootstrap/js/bootstrap.min.js"></script>

<!-- Extension : Highlight.js @ https://highlightjs.org/ -->
<!-- Syntax highlighting tomorrow-night-bright, agate-->
<link rel="stylesheet" href="../../../highlight/css/tomorrow-night-bright.css">
<script src="../../../highlight/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- Extension : MathJax @ https://docs.mathjax.org/en/v2.5-latest/tex.html -->
<!-- MathJax/config/local/local.js contains macros. Need to provide entire URL-->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,http://holdenlee.github.io/notebook/MathJax/config/local/local"></script>

<!-- Extension : Footnotes @ http://ignorethecode.net/blog/2010/04/20/footnotes/ -->
<script src="../../../footnotes/js/footnotes.js"></script>

<!-- Extension : Disqus @ http://disqus.com -->
<!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->

<script src="../../../disqus/js/disqus.js"></script>



<!-- Extension : Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73261814-1', 'auto');
  ga('send', 'pageview');

</script>

