<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

  <meta name="description" content="Holden Lee's Research Notebook">
  <meta name="author" content="Holden Lee">
    
  <title>Multiplicative weights</title>

  <!-- Bootstrap core CSS -->
  <link href="../../../bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

  <!-- Custom styles for this template -->
  <link href="../../../css/blog.css" rel="stylesheet">
  <link href="../../../css/default.css" rel="stylesheet">

  <!-- Extension : Footnotes -->
  <link href="../../../footnotes/css/footnotes.css" rel="stylesheet">

  <!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->
  

  <!-- Extension : Collapsible lists @ http://code.stephenmorley.org/javascript/collapsible-lists/-->
  <link href="../../../collapsible_lists/css/collapsible.css" rel="stylesheet">
  <script type="text/javascript" src="../../../collapsible_lists/js/CollapsibleLists.js"></script>

</head>

<body>

<!-- Navigation bar. navbar-inverse is black, navbar-default is white.-->
<!-- To make a button active (pressed), use <li class="active"> -->
<div id="header">
  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../">Notebook</a>
      </div>
      <div id="navbar" class="collapse navbar-collapse">
        <ul class="nav navbar-nav">
          <li><a href="../../../">Home</a></li>
          <li><a href="../../../sitemap.html">Sitemap</a></li>
<!-- TODO: Distinguish between PAPERS, RESEARCH QUESTIONS, BOOKS -->
<!-- TODO: make this part a for loop over main pages -->
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </nav>
</div>
<!-- Content -->
<!--div id="content">
  <h1>Mental Wilderness</h1>-->



<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Multiplicative weights</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-02-24 
          , Modified: 2017-02-24 
	</p>
      
       <p>Tags: <a href="../../../tags/boosting.html">boosting</a>, <a href="../../../tags/multiplicative%20weights.html">multiplicative weights</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#basic-algorithm-and-guarantees">Basic algorithm and guarantees</a><ul>
 <li><a href="#problem">Problem</a></li>
 </ul></li>
 <li><a href="#algorithm">Algorithm</a></li>
 <li><a href="#guarantees">Guarantees</a><ul>
 <li><a href="#questions">Questions</a></li>
 </ul></li>
 <li><a href="#applications">Applications</a><ul>
 <li><a href="#classification-with-margin">Classification with margin</a></li>
 <li><a href="#game-theory">Game theory</a></li>
 <li><a href="#boosting">Boosting</a></li>
 </ul></li>
 <li><a href="#bayesian-interpretation">Bayesian interpretation</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>Ref:</p>
<ul>
<li>Arora Hazan Kale</li>
</ul>
<p>See also <a href="optimization/multiplicative_updates.md">Curse and blessing</a></p>
<h2 id="basic-algorithm-and-guarantees">Basic algorithm and guarantees</h2>
<h3 id="problem">Problem</h3>
<ul>
<li>Prediction: <span class="math inline">\(n\)</span> experts make predictions in <span class="math inline">\(\{0,1\}\)</span> at times <span class="math inline">\(1,\ldots, T\)</span>. After seeing predictions <span class="math inline">\(y_{t,1:n}\)</span>, you make a prediction. You want your total number of mistakes to be comparable to that of the best expert.</li>
<li>Losses/gains: At each time, you get some loss for choosing 0 or 1, <span class="math inline">\(l_t(0), l_t(1)\)</span>. (Assume losses are bounded.) You want your total loss (gain) to be comparable to that of the best expert.</li>
</ul>
<h2 id="algorithm">Algorithm</h2>
<ul>
<li>Vanilla: Let <span class="math inline">\(w_{t+1,i} = w_{t,i}\pa{1-\eta m_{t,i}}\)</span>.
<ul>
<li>Deterministic: at time <span class="math inline">\(t\)</span>, take weighted majority of experts with weights <span class="math inline">\(w_{t,i}\)</span>.</li>
<li>Random: at time <span class="math inline">\(t\)</span>, randomly choose <span class="math inline">\(i\)</span> with probability proportional to <span class="math inline">\(w_{t,i}\)</span>.</li>
</ul></li>
<li>Hedge: Let <span class="math inline">\(w_{t+1,i} = w_{t,i} e^{-\eta m_{t,i}}\)</span>.</li>
<li>MWU with restriction: <span class="math inline">\(p^{(t+1)} = \amin_{p\in \mathcal P} KL(p||\wh p^{(t)})\)</span>.</li>
</ul>
<p>For “counting” case, <span class="math inline">\(m_i=1\)</span> iff mistake. For “loss” case, <span class="math inline">\(m_i\)</span> is loss. Assume <span class="math inline">\(\eta\le \rc2\)</span>.</p>
<h2 id="guarantees">Guarantees</h2>
<ol type="1">
<li>Deterministic multiplicative weights attains <span class="math display">\[
M^{(T)} \le 2(1+\eta) m_i^{(T)} + \fc{2\ln n}{\eta}.
\]</span></li>
<li>Probabilistic multiplicative weights (let <span class="math inline">\(p^{(t)}=\fc{w^{(t)}}{\Phi^{(t)}}\)</span>)
<span class="math display">\[\begin{align}
\E M^{(T)} &amp;= \sumo tT m^{(t)}\cdot p^{(t)} \le \sumo iT m_i^{(t)} + \eta\sumo tT |m_i^{(t)}| + \fc{\ln n}{\eta} \\
&amp; \le (1+\eta) m_i^{(t)} + \fc{\ln n}{\eta} 
&amp; \text{if }m_i^{(t)}\ge0.
\end{align}\]</span>
<em>Proof</em>. 2 parts.
<ol type="1">
<li><span class="math inline">\(\Phi^{(t+1)} \le \Phi^{(t)} e^{-\eta m^{(t)}\cdot p^{(t)}}\)</span>. (If costs are large, this decreases a lot. <span class="math inline">\(p_i\)</span> are exactly the slice of the pie occupied by expert <span class="math inline">\(i\)</span>.) Look at pie occupied by all experts.</li>
<li><span class="math inline">\((1-\eta)^{\sum_{\ge 0}m_i(t)} (1+\eta)^{-\sum_{\le 0} m_i(t)}\le \Phi^{(t+1)}\)</span>. Look at slice occupied by <span class="math inline">\(i\)</span>th expert.</li>
</ol></li>
<li>Hedge attains <span class="math display">\[
\sumo tT m^{(t)}\cdot p^{(t)} \le \sumo tT m_i^{(t)} + \eta\sumo tT m_i^{(t)} + \eta\sumo tT (m^{(t)})^2 \cdot p^{(t)} + \fc{\ln n}{\eta}.
\]</span> (Note this bound depends on <span class="math inline">\(p\)</span> on the RHS.</li>
<li>For MWU with restrictions, <span class="math display">\[
\sumo tT m^{(t)} \cdot p^{(t)} \le \sumo tT (m^{(t)} + \eta |m^{(t)}|)\cdot p + \fc{KL(p||p^{(1)})}{\eta}.
\]</span> (The farther away <span class="math inline">\(p\)</span> is from <span class="math inline">\(p^{(1)}\)</span> the more loss you might incur.)</li>
</ol>
<h3 id="questions">Questions</h3>
<p>What if we replace a finite number of experts with an infinite number of experts with finite VC dimension? This is typically not tractable, but can still be analyzed. Is this like the online version of ERM? Bayesian version/version with uncertainty?</p>
<h2 id="applications">Applications</h2>
<h3 id="classification-with-margin">Classification with margin</h3>
Solve
<span class="math display">\[\begin{align}
a_j^T x &amp;\ge 0\\
\one^T x&amp;=1\\
x_i &amp;\ge 0
\end{align}\]</span>
<p>under the promise that there exists <span class="math inline">\(x^*\)</span> with <span class="math inline">\(a_j^Tx^*\ge \ep\)</span>. (More generally, can solve other LP’s with margin.) Let <span class="math inline">\(\rh = \max_j \ve{a_j}_\iy\)</span>.</p>
<p>Ideas:</p>
<ol type="1">
<li>The experts are the coordinates. They do nothing except existing. It is the <em>weighting</em> that is the solution we’re looking for. (In the original formulation, the default interpretations is that the experts are some different algorithms, and we seek an algorithm using them as black boxes that does well. Here, the experts are nothing but points, and we find an algorithm that classifies well—where the “algorithm” is restricted to be in the class of linear classifiers.)
<ul>
<li>The experts are not the points <span class="math inline">\(a_j\)</span>.</li>
</ul></li>
<li>The bulk of this comes from how we define the gains. These gains should be related to the points <span class="math inline">\(a_j\)</span>.</li>
<li>Note the right sign: we should reward a coordinate <span class="math inline">\(i\)</span> if <span class="math inline">\(a_{ij}\)</span> is large, because increasing <span class="math inline">\(x_i\)</span> would increase the dot product more. (cf. gradient - but this is projected back onto the simplex by scaling (what norm is this?)) The gain is <span class="math inline">\({a_{ji}}\)</span>. <!--shouldn't be a /\rh here because that's in $\eta$--></li>
<li>We are not going through the <span class="math inline">\(a_j\)</span> in order. Instead, at each step we pick an <span class="math inline">\(a_j\)</span> where <span class="math inline">\(a_j^Tx^{(t)}&lt;0\)</span>. (! For GAN, pick <span class="math inline">\(D\)</span> that is violated?)</li>
<li>To analyze this, we consider the total number of steps where there is some <span class="math inline">\(j\)</span> such that <span class="math inline">\(a_j^T x^{(t)}&lt;0\)</span>. Let <span class="math inline">\(T\)</span> be the largest step for which this the case.</li>
</ol>
<p>The inequality is <span class="math display">\[
0 &gt; \sumo tT m^{(t)}\cdot x^{(t)} \ge 
\sumo tT m^{(T)} \cdot x^* 
- \eta \fc{\sumo tT \one^T a_{j(t)}}{\rh}
- \fc{\ln n}{\eta} = 
\pa{\fc{\eta}{\rh} - \fc{\eta}{2\rh}}T + \fc{\ln n}{\eta/(2\rh)}.
\]</span> <!-- not 1-\eta formulation --> Solving, get <span class="math inline">\(T = \ce{\fc{4\rh^2\ln n}{\ep^2}}\)</span>.</p>
<p>Notes:</p>
<ol type="1">
<li>Thinking of the difference in margins as regret, this says that we get <span class="math inline">\(O\prc{\sqrt T}\)</span> regret.</li>
<li>We can think of this as a game, <span class="math inline">\(\max_x \min_j a_j^Tx\)</span> over probability vectors. At each step, the “max/column” player chooses a mixed strategy <span class="math inline">\(x\)</span>, and the “min/row” player plays the best response. Best response is not actually necessary, just some choice of <span class="math inline">\(j\)</span> such that <span class="math inline">\(a_j^Tx&lt;0\)</span> (since that’s what we’re aiming for).</li>
<li>This is logarithmic in <span class="math inline">\(n\)</span>. So if <span class="math inline">\(x\)</span> has super-many coordinates, a sparse solution is good enough! We can prove this existentially—that there exists a <span class="math inline">\(O(\rh^2\ln n/\ep^2)\)</span> sparse solution—existentially using sampling from <span class="math inline">\(x^*\)</span> and using Chernoff.</li>
</ol>
<p>This naturally generalizes!</p>
<h3 id="game-theory">Game theory</h3>
We want to solve a zero-sum game approximately: find <span class="math inline">\(\wt q\)</span> (column mixed strategy) and <span class="math inline">\(\wt p\)</span> (row mixed strategy) such that
<span class="math display">\[\begin{align}
\la^* - \ep &amp; \le \min_i e_i^TA\wt q\\
\la^* &amp;= \max_j \wt p A e_j \le \la^* + \ep
\end{align}\]</span>
<p>where <span class="math display">\[
\la^* = \min_p \max_j A(p,j) = \max_q \min_i A(i,q).
\]</span> Note <span class="math inline">\(\ge\)</span> is clear.</p>
<p>Result: Assume all entries of <span class="math inline">\(A\)</span> bounded in absolute value by 1. We can approximate <span class="math inline">\(\la^*\)</span> up to <span class="math inline">\(\ep\)</span> with <span class="math inline">\(O\pf{\ln n}{\ep^2}\)</span> calls to oracle and time <span class="math inline">\(O(n)\)</span> per call.</p>
<p><em>Proof</em>. T use the algorithm, we specify:</p>
<ol type="1">
<li>Experts: row coordinates. (We take row’s POV.)</li>
<li>Losses: column responds with best response <span class="math inline">\(j\)</span> at each step. The loss for <span class="math inline">\(i\)</span> is <span class="math inline">\((Ae_j)_i\)</span>.</li>
</ol>
For all distributions <span class="math inline">\(p\)</span>,
<span class="math display">\[\begin{align}
\sumo tT p^{(t)} Ae_j
&amp;\le\sumo tT p^* A e_{j(t)} + \eta \sumo tT p^* Ae_{j(t)}+ \fc{\ln n}{\eta}\\
&amp;\le \la^* + \eta\ve{A}_{\iy} + \fc{\ln n}{\eta}\\
&amp; \le \la^* T + \ub{2\sqrt{T\ln n}}{\ep}
\end{align}\]</span>
<p>with <span class="math inline">\(\eta = \sfc{\ln n}T\)</span>. Thus can take <span class="math inline">\(T=\ce{\fc{4\ln n}{\ep^2}}\)</span>.</p>
<p>Notes:</p>
<ol type="1">
<li>Suppose we didn’t know the minimax theorem. Then <span class="math inline">\(\la^*\)</span> should be defined as <span class="math inline">\(\min\max\)</span>. This goes through.</li>
<li>Does this give a strategy for the column player? We can let <span class="math inline">\(\wt q = \fc{\set{t}{j(t)=j}}{T}\)</span>. Then we get <span class="math display">\[
\ol p A \wt q \le p A \wt q + \ep.
\]</span> Note we can set <span class="math inline">\(p\)</span> to anything; we don’t need to choose <span class="math inline">\(p^*\)</span>. We have <span class="math display">\[
\la^{*} - \ep \le \ol p A\wt q \le p A \wt q
\]</span> the LHS because <span class="math inline">\(j(t)\)</span> is the best response. Thus we not only found a good <span class="math inline">\(\wt p\)</span>, we also found <span class="math inline">\(\wt q\)</span> which attains <span class="math inline">\(\ge \la^*-\ep\)</span> for any <span class="math inline">\(p\)</span>.</li>
<li>Alas, this does not prove the minimax theorem because the <span class="math inline">\(\ge\)</span> part is clear.</li>
</ol>
<h3 id="boosting">Boosting</h3>
<p>Again specify</p>
<ol type="1">
<li>Experts: experts are now the data points.</li>
<li>Loss: reward the experts (data points) for fooling the algorithm.</li>
</ol>
<p>The role of “algorithm” and “data point” is swapped from the initial interpretation of multiplicative weights as operating on “experts = algorithms” shown “data points = incur losses from mistaks”, and the sign is flipped: we get rewards for fooling the algorithm.</p>
<p>(The “linear classifier with margin” still had “data points” being shown. The data points there were also adversarial in order to get improvement. We were rewarding coordinates for doing well.)</p>
<p>MAKE A TABLE.</p>
<p>If there are <span class="math inline">\(\rc 2+\ga\)</span>-weak learners, then there is a <span class="math inline">\(1-\ep\)</span>-strong learner with <span class="math inline">\(T=\ce{\fc{2}{\ga^2}\ln \prc{\ep}}\)</span>. (To be more precise, talk about weak-learners on somewhat dense distributions…)</p>
<p><em>Proof</em>. Let <span class="math inline">\(m_x=1-|h(x)-c(x)|\)</span>: a data point is penalized for being correctly classified. WL means that at each step we force many mistakes, <span class="math inline">\(\rc2+\ga\)</span> proportion at each time step.</p>
<p>Now comes the tricky part. We want to bound the number of steps until the mistakes (wrt original, uniform distribution) is small. Thus, we want: when we can find a large set <span class="math inline">\(E\)</span> on which the algorithm misclassifies, then this forces <span class="math inline">\(T\)</span> to be small. Using the refined inequality with KL divergence and <span class="math inline">\(p\)</span> the uniform distribution on <span class="math inline">\(E\)</span>, (“counting in 2 ways”, noting that each row of <span class="math inline">\(x\in E\)</span> sums to at most <span class="math inline">\(\fc T2\)</span>) <span class="math display">\[
\pa{\rc 2 + \ga}T \le (1+\eta)\fc{T}{2} + \fc{\ln \pf{n}{|E|}}{\eta}.
\]</span> Note that it’s key that we get a ratio <span class="math inline">\(\fc{n}{|E|}\)</span>. We have <span class="math inline">\(\fc{|E|}{n}\le \ep\)</span>. We get <span class="math inline">\(T&lt; \fc{2}{\ga^2} \ln \prc{\ep}\)</span>, so for larger <span class="math inline">\(T\)</span>, we can’t find such a <span class="math inline">\(E\)</span>. The “majority vote” comes from <span class="math inline">\(\fc T2\)</span>.</p>
<p>(You can also probably penalize differently depending on how the weak learner does, getting different weights, and probably improved bounds depending on performance of weak learners.)</p>
<h2 id="bayesian-interpretation">Bayesian interpretation</h2>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class="st_facebook_large" displayText="Facebook"></span>
    <span class="st_twitter_large" displayText="Tweet"></span>
    <span class="st_googleplus_large" displayText="Google +"></span>
    <span class="st_reddit_large" displayText="Reddit"></span>
    <span class="st__large" displayText></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>


<!-- Footer -->
<div id="footer">
  <div class="container">
    Built with
    <a href="http://jaspervdj.be/hakyll">Hakyll</a> 
    using 
    <a href="http://www.getbootstrap.com">Bootstrap</a>, 
    <a href="http://www.disqus.com">Disqus</a>,
    <a href="http://ignorethecode.net/blog/2010/04/20/footnotes/">Footnotes.js</a>,
    <a href="http://highlightjs.org/">Highlight.js</a>, 
    <a href="http://www.mathjax.org">MathJax</a>, 
    and <a href="http://www.sharethis.com">ShareThis</a>.
  </div>
</div>
</body>

</html>

<!-- SCRIPTS -->
<!-- jQuery-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="../../../bootstrap/js/bootstrap.min.js"></script>

<!-- Extension : Highlight.js @ https://highlightjs.org/ -->
<!-- Syntax highlighting tomorrow-night-bright, agate-->
<link rel="stylesheet" href="../../../highlight/css/tomorrow-night-bright.css">
<script src="../../../highlight/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- Extension : MathJax @ https://docs.mathjax.org/en/v2.5-latest/tex.html -->
<!-- MathJax/config/local/local.js contains macros. Need to provide entire URL-->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,http://holdenlee.github.io/notebook/MathJax/config/local/local"></script>

<!-- Extension : Footnotes @ http://ignorethecode.net/blog/2010/04/20/footnotes/ -->
<script src="../../../footnotes/js/footnotes.js"></script>

<!-- Extension : Disqus @ http://disqus.com -->
<!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->

<script src="../../../disqus/js/disqus.js"></script>



<!-- Extension : Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73261814-1', 'auto');
  ga('send', 'pageview');

</script>

