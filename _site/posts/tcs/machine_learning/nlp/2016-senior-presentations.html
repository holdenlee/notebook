<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

  <meta name="description" content="Holden Lee's Research Notebook">
  <meta name="author" content="Holden Lee">
    
  <title>NLP</title>

  <!-- Bootstrap core CSS -->
  <link href="../../../../bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

  <!-- Custom styles for this template -->
  <link href="../../../../css/blog.css" rel="stylesheet">
  <link href="../../../../css/default.css" rel="stylesheet">

  <!-- Extension : Footnotes -->
  <link href="../../../../footnotes/css/footnotes.css" rel="stylesheet">

  <!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->
  

  <!-- Extension : Collapsible lists @ http://code.stephenmorley.org/javascript/collapsible-lists/-->
  <link href="../../../../collapsible_lists/css/collapsible.css" rel="stylesheet">
  <script type="text/javascript" src="../../../../collapsible_lists/js/CollapsibleLists.js"></script>

</head>

<body>

<!-- Navigation bar. navbar-inverse is black, navbar-default is white.-->
<!-- To make a button active (pressed), use <li class="active"> -->
<div id="header">
  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../../">Notebook</a>
      </div>
      <div id="navbar" class="collapse navbar-collapse">
        <ul class="nav navbar-nav">
          <li><a href="../../../../">Home</a></li>
          <li><a href="../../../../sitemap.html">Sitemap</a></li>
<!-- TODO: Distinguish between PAPERS, RESEARCH QUESTIONS, BOOKS -->
<!-- TODO: make this part a for loop over main pages -->
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </nav>
</div>
<!-- Content -->
<!--div id="content">
  <h1>Mental Wilderness</h1>-->



<div class="container">
  <div id="content">
    <div class="page header">
      <h1>NLP</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-05-14 
          , Modified: 2016-05-14 
	</p>
      
       <p>Tags: <a href="../../../../tags/nlp.html">nlp</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#personality-through-sentiment-analysis-sidharth">Personality through sentiment analysis (Sidharth)</a><ul>
 <li><a href="#harvard-inquirer">Harvard Inquirer</a></li>
 <li><a href="#general-event-analysis">General event analysis</a></li>
 </ul></li>
 <li><a href="#answering-sat-reading-comprehension-questions-with-tensor-decomposition-and-neural-networks-saahil-madge">Answering SAT reading comprehension questions with tensor decomposition and neural networks (Saahil Madge)</a><ul>
 <li><a href="#tensor-factorization-actually-matrix-factorization-optimizing-over-2-matrices">Tensor factorization (actually, matrix factorization, optimizing over 2 matrices)</a></li>
 <li><a href="#memory-networks">Memory networks</a></li>
 <li><a href="#results">Results</a></li>
 </ul></li>
 <li><a href="#automated-wordnet-construction-using-word-embeddings-misha">Automated WordNet construction using word embeddings (Misha)</a><ul>
 <li><a href="#associate-words-with-synsets">Associate words with synsets</a></li>
 <li><a href="#infer-word-relations">Infer word relations</a></li>
 <li><a href="#translational-synset-matching">Translational synset matching</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <p>NLP group meeting 5-14-16</p>
<h2 id="personality-through-sentiment-analysis-sidharth">Personality through sentiment analysis (Sidharth)</h2>
<ul>
<li>Big five personality</li>
<li>Predicting personality from Twitter data, etc.</li>
</ul>
<p>Longitudinal, with 5000 documents per person spanning years, with data from private correspondences (more unfiltered). Use <a href="http://rotunda.upress.virginia.edu/founders">presidential letters</a> from Adams, Jefferson, and Washington.</p>
<!-- I am so much more expressive and unreserved in the expression of my sentiments-->
<h3 id="harvard-inquirer">Harvard Inquirer</h3>
<p>Use Harvard Inquirer Categories which puts words in categories (ex. “strong” contains audacity, battle, wealthy, charisma, clever, climax, further). These are hand-crafted categories. There are 200 categories; number of words vary. Words can appear in multiple categories.</p>
<p>Start with a specific event and analyze the way they respond. For example, use Washington’s early campaigns from the French and Indian War. Find keywords and phrases relating specifically to the event. Use these keywords to find other letters in the same category.</p>
<p>Differentiate between attitude in these letters and overall attitude by the difference of means statistical test. There were differences in overstatement, negative, understtement, vice, and arousal (negative).</p>
<p>In the early French and Indian War, Washington had limited resources, Americans would desert, etc.</p>
<h3 id="general-event-analysis">General event analysis</h3>
<p>Use Lasswell categories for general event analysis:</p>
<ul>
<li>power gain, loss, conflict, cooperation</li>
<li>respect gain or loss</li>
<li>affection gain or loss</li>
<li>wealth</li>
<li>well-being loss</li>
</ul>
<p>Use a Bayesian classifier for all letters. Put in the category that it correlates with the most, provided it’s at least a certain threshold.</p>
<p>Washington and Adams react to power loss differently. For example, Washington was more active and passive, and both understated and overstated more.</p>
<p>Vice put together with self-pronouns means self-doubt.</p>
<p>The goal was to find an interesting way to measure the resilience of a president in the face of tough events. I found some metrics which yielded useful, informative results.</p>
<p>Another example: Adams was pessimistic and powerless about not being able to go home; Jefferson was closely involved with the education of his kids.</p>
<p>How to use this? Facebook posts, emails (Clinton, Enron, Wikileaks). Panama papers?</p>
<h2 id="answering-sat-reading-comprehension-questions-with-tensor-decomposition-and-neural-networks-saahil-madge">Answering SAT reading comprehension questions with tensor decomposition and neural networks (Saahil Madge)</h2>
<!-- you spend your whole life stuck in the labyrinth-->
<p>Goal: create a system for machine comprehension. Previous work focused on reading questions for young children (MCTest), with very low accuracy on “why” questions. Most of these are “bag of words” models.</p>
<p>But we want to capture semantic and structural information. We use SAT questions, which are longer and more complex, and answering requires understanding deeper information about the context. They use different words, “angry” vs. “incensed”, etc.</p>
<h3 id="tensor-factorization-actually-matrix-factorization-optimizing-over-2-matrices">Tensor factorization (actually, matrix factorization, optimizing over 2 matrices)</h3>
<p>We use a knowledge graph, consist of entities and relations <span class="math inline">\(\an{e_1,r,e_2}\)</span>. Entities are nodes, and relations are directed edges. We can ask queries <span class="math inline">\(\an{e_1,?,e_2},\an{e_1,r,?}\)</span>.</p>
<p>(Google’s knowledge graph is FreeBase, 300GB. Nell at Carnegie Mellon is trained to extract triples from the Internet.)</p>
<ul>
<li>First, convert text to sets of triples on the knowledge graph. (Use the Stanford NLP toolkit.)</li>
<li>Use word2vec to convert questions to queries on the graph.</li>
</ul>
<p>We focus on Subject-Verb-Direct object (ex. dog likes sausage) or copular relations (subject and description, ex. dog is green). <!-- --></p>
<p>We used Stanford high-dependency parser. We can do pronoun-antecedent resolutions as well.</p>
<p>Represent the knowledge graph as a 3-D array where each <span class="math inline">\(X_k\)</span> is the adjacency matrix for relation type <span class="math inline">\(k\)</span>. Approach: find rank <span class="math inline">\(r\)</span> approximation <span class="math display">\[X_k\approx AR_kA^T,\]</span> (<span class="math inline">\(n\times r, r\times r\)</span>, <span class="math inline">\(n\)</span> is the number of entities) for latent features. Use alternating least squares, semantically smoth embedding. Answer questions using factorization. (The slices are dependent because the <span class="math inline">\(A\)</span> is shared.)</p>
<p>Stories about a character make one of the slices dense; explanatory articles are a lot sparser. Most tensors are sparse. <!-- slices are not symmetric. (one way) binary and sparse --></p>
<p>(Each slice is factorized separately? Use <a href="http://www.mit.edu/~mnick/">Rescal</a>.)</p>
<!-- entries.-->
<h3 id="memory-networks">Memory networks</h3>
<p>Use neural network to predict answers. Embed the tensor and vectorized query <span class="math inline">\(q\)</span> as inputs. Train the network to recognize what information is relevant, and output an answer choice.</p>
<h3 id="results">Results</h3>
<p>Tensor model trains independently, while the NN needs to be trained on 2/3 of the questions. Tensor model is consistent but with low ceiling.</p>
<p>The tensor model completes a triple rather than just give an answer choice.</p>
<!-- small data. transfer learning. replaceable characters-->
<!-- metamind. semantic vs. episodic. DMM. default neural network -->
<h2 id="automated-wordnet-construction-using-word-embeddings-misha">Automated WordNet construction using word embeddings (Misha)</h2>
<p>Can we use a mathematical approach to constructing the WordNet graph?</p>
<p>Current automated methods include merging or extending. Use multi-language translations (French WordNet), scrape Wiktionary, or use rule-based/automatic methods.</p>
<ol type="1">
<li>Can we associate words with synsets (sets of synonyms)? Use translations. <!--meaning independent of word--></li>
<li>Can we find relations between synsets or words? Not yet. (Only using translation.)</li>
<li>Can we link to synsets in the Princeton WordNet? Yes.</li>
</ol>
<p>Use word embeddings. Optimize the squared norm objective <span class="math display">\[\min_{v\in \R^d, C\in \R} \sum_{(i,j)\in [V]^2} f(X_{ij}) (\ve{v_w+v_j}_2^2 + C-\log X_{ij})^2.\]</span> (Distributional assumption.) Word vectors have been used in solving analogies, named-entity recognition, and word similarity.</p>
<p>Use the discourse model: a corpus is generated by a random walk over the discourse space <span class="math inline">\(\R^d\)</span>. <!-- lower bound for d. Why can't you take d=1,2? --></p>
<h3 id="associate-words-with-synsets">Associate words with synsets</h3>
<p>Arora et al. used a learned dictionary of atoms to find representation of the meanings of a word <span class="math inline">\(w\in V\)</span> with a dictionary <span class="math inline">\(A\in \R^{K\times d}\)</span> and sparsity constraint <span class="math inline">\(s\)</span>:</p>
<ol type="1">
<li>Find <span class="math inline">\(R_w\in \R^K\)</span> minimizing <span class="math inline">\(\ve{v_w-R_w^TA}_2\)</span> such that <span class="math inline">\(\ve{R_w}_0\le s\)</span>.</li>
<li>For each <span class="math inline">\(i\)</span> such that <span class="math inline">\(R_{w,i}&gt;0\)</span> find a dense cluster <span class="math inline">\(C\sub \R^d\)</span> of word vectors close to <span class="math inline">\(A_i\in \R^d\)</span>.</li>
<li>For each cluster <span class="math inline">\(C\)</span>, the set of words <span class="math inline">\(\set{w'}{v_{w'}\in C}\)</span> represents one meaning of <span class="math inline">\(w\)</span>.</li>
</ol>
<h3 id="infer-word-relations">Infer word relations</h3>
<p>Consider hypernyms.</p>
<ul>
<li>For supervised learning, use SVM or logistic regression classification on word pair features.</li>
<li>For unsupervised learning, learn a mixture of 2 gaussians on feature vectors for the word pair, or prune with similarity thresholds or requiring shared atoms.</li>
</ul>
<p>This fails on real datasets because of word-relation sparsity and similar relations problem (problems discerning hypernyms and cohypernyms).</p>
<p>There is a spatial distinction between hyponym-hypernym and random pairs, but not between hyponym-hypernym and co-hypernym (sharing the same hypernym) pairs.</p>
<h3 id="translational-synset-matching">Translational synset matching</h3>
<p>We can link to synsets in the Princeton WordNet.</p>
<p>Given <span class="math inline">\(w\)</span> in a foreign language to synsets in the Princeton WordNet,</p>
<ol type="1">
<li>find meanings of <span class="math inline">\(w\)</span> using polysemy.</li>
<li>find the synsets of all English translations of <span class="math inline">\(w\)</span>.</li>
<li>Match each meaning to one of the synsets by a similarity measure between the words in the meaning and the foreign languge translation of the synset definition.</li>
</ol>
<!--Sometimes don't match to correct atom. -->
<p>Ex. The word “container” doesn’t exist in Dutch. (cf. Hofstadter, Surfaces and Essences.)</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class="st_facebook_large" displayText="Facebook"></span>
    <span class="st_twitter_large" displayText="Tweet"></span>
    <span class="st_googleplus_large" displayText="Google +"></span>
    <span class="st_reddit_large" displayText="Reddit"></span>
    <span class="st__large" displayText></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>


<!-- Footer -->
<div id="footer">
  <div class="container">
    Built with
    <a href="http://jaspervdj.be/hakyll">Hakyll</a> 
    using 
    <a href="http://www.getbootstrap.com">Bootstrap</a>, 
    <a href="http://www.disqus.com">Disqus</a>,
    <a href="http://ignorethecode.net/blog/2010/04/20/footnotes/">Footnotes.js</a>,
    <a href="http://highlightjs.org/">Highlight.js</a>, 
    <a href="http://www.mathjax.org">MathJax</a>, 
    and <a href="http://www.sharethis.com">ShareThis</a>.
  </div>
</div>
</body>

</html>

<!-- SCRIPTS -->
<!-- jQuery-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="../../../../bootstrap/js/bootstrap.min.js"></script>

<!-- Extension : Highlight.js @ https://highlightjs.org/ -->
<!-- Syntax highlighting tomorrow-night-bright, agate-->
<link rel="stylesheet" href="../../../../highlight/css/tomorrow-night-bright.css">
<script src="../../../../highlight/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- Extension : MathJax @ https://docs.mathjax.org/en/v2.5-latest/tex.html -->
<!-- MathJax/config/local/local.js contains macros. Need to provide entire URL-->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,http://holdenlee.github.io/notebook/MathJax/config/local/local"></script>

<!-- Extension : Footnotes @ http://ignorethecode.net/blog/2010/04/20/footnotes/ -->
<script src="../../../../footnotes/js/footnotes.js"></script>

<!-- Extension : Disqus @ http://disqus.com -->
<!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->

<script src="../../../../disqus/js/disqus.js"></script>



<!-- Extension : Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73261814-1', 'auto');
  ga('send', 'pageview');

</script>

