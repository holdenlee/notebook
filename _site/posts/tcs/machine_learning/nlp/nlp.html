<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

  <meta name="description" content="Holden Lee's Research Notebook">
  <meta name="author" content="Holden Lee">
    
  <title>NLP</title>

  <!-- Bootstrap core CSS -->
  <link href="../../../../bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

  <!-- Custom styles for this template -->
  <link href="../../../../css/blog.css" rel="stylesheet">
  <link href="../../../../css/default.css" rel="stylesheet">

  <!-- Extension : Footnotes -->
  <link href="../../../../footnotes/css/footnotes.css" rel="stylesheet">

  <!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->
  

  <!-- Extension : Collapsible lists @ http://code.stephenmorley.org/javascript/collapsible-lists/-->
  <link href="../../../../collapsible_lists/css/collapsible.css" rel="stylesheet">
  <script type="text/javascript" src="../../../../collapsible_lists/js/CollapsibleLists.js"></script>

</head>

<body>

<!-- Navigation bar. navbar-inverse is black, navbar-default is white.-->
<!-- To make a button active (pressed), use <li class="active"> -->
<div id="header">
  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../../">Notebook</a>
      </div>
      <div id="navbar" class="collapse navbar-collapse">
        <ul class="nav navbar-nav">
          <li><a href="../../../../">Home</a></li>
          <li><a href="../../../../sitemap.html">Sitemap</a></li>
<!-- TODO: Distinguish between PAPERS, RESEARCH QUESTIONS, BOOKS -->
<!-- TODO: make this part a for loop over main pages -->
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </nav>
</div>
<!-- Content -->
<!--div id="content">
  <h1>Mental Wilderness</h1>-->



<div class="container">
  <div id="content">
    <div class="page header">
      <h1>NLP</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-05-04 
          , Modified: 2016-05-04 
	</p>
      
       <p>Tags: <a href="../../../../tags/nlp.html">nlp</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#language-models">Language models</a></li>
 <li><a href="#hmms-and-tagging">HMMs and tagging</a></li>
 <li><a href="#pcfgs">PCFGs</a></li>
 <li><a href="#lexicalized-pcfgs">Lexicalized PCFGs</a></li>
 <li><a href="#ibm">IBM</a></li>
 <li><a href="#section">6</a></li>
 <li><a href="#log-linear-models">7 Log-linear models</a></li>
 <li><a href="#memms-maximum-entropy-markov-models-log-linear-tagging-models">MEMMs (Maximum entropy Markov models = Log-linear tagging models)</a></li>
 <li><a href="#crfs">CRFs</a></li>
 <li><a href="#scraps">Scraps</a></li>
 </ul> </div>

  <div class="blog-main">
    <ul>
<li><a href="http://www.cs.columbia.edu/~mcollins/">Notes by Mike Collins</a>.</li>
<li><a href="http://cs224d.stanford.edu/syllabus.html">Deep learning for NLP</a></li>
</ul>
<h2 id="language-models">Language models</h2>
<p>A language model is a probability distribution over sentences consisting of words from a finite set <span class="math inline">\(\mathcal V\)</span>.</p>
<blockquote>
<p>in speech recognition the language model is combined with an acoustic model that models the pronunciation of different words: one way to think about it is that the acoustic model generates a large number of candidate sentences, together with probabilities; the language model is then used to reorder these possibilities based on how likely they are to be a sentence in the language.</p>
</blockquote>
<p>A <span class="math inline">\(t\)</span>th order Markov model (<span class="math inline">\(n+1\)</span>-grams) assumes the probability distribution of the next word depends only on the previous <span class="math inline">\(t\)</span>. To model variable-length sentences, include STOP as a word. Treat the <span class="math inline">\(0,\ldots,-(t-1)\)</span> words as <span class="math inline">\(*\)</span>.</p>
<p>The most naive estimate is to let <span class="math display">\[wh p_{x_{t+1}|x_1\ldots x_t} = \fc{\#(x_1,\ldots, x_{t+1})}{\#(x_1,\ldots, x_t)}\]</span> but this requires lots of computations and makes many counts 0.</p>
<p>Let <span class="math inline">\(x^{(i)}\)</span> be the test sentences. Letting <span class="math inline">\(l=\rc{M} \sumo im \lg p(x^{(i)})\)</span> (average log probability), the <strong>perplexity</strong> is <span class="math inline">\(2^{-l}\)</span> (geometric mean of probabilities).</p>
<ol type="1">
<li><strong>Linear interpolation</strong> is <span class="math display">\[ \wh p(x_{t+1}|x_1\cdots x_t) = \sumo i{t+1} \la_i p(x_i|x_{[1,i-1]}).\]</span> Find <span class="math inline">\(\amax_{\la} L(\la)\)</span> over the development data (separate from training and test data).</li>
<li><strong>Bucketing</strong> is replacing <span class="math inline">\(\la_{t+1} = \fc{\#(x_1,\ldots, x_{t+1})}{#+\ga}\)</span>, <span class="math inline">\(\la_{t} = (1-\la_{t+1}) \fc{\#(x_1,\ldots, x_t)}{\#(x_1,\ldots, x_{t-1})+\ga}\)</span>,…</li>
<li><strong>Discount counts</strong> by <span class="math inline">\(c^(\mathbf x) = c(\mathbf x) - \be\)</span>, <span class="math inline">\(\be\in [0,1]\)</span>, let <span class="math inline">\(\wh p = \fc{c^*(\cdots)}{c(\cdots)}\)</span>. Spread the missing mass evenly over those that have not appeared.</li>
<li><strong>Linear interpolation with bucketing</strong>: Let <span class="math inline">\(\Pi: \mathcal V^{t} \to [K]\)</span> depending on counts seen in the training data. Now make the smoothing parameters depend on <span class="math inline">\(\Pi(u,v)\)</span>.</li>
</ol>
<p>Note that bucketing and discounting only have 1 parameter.</p>
<h2 id="hmms-and-tagging">HMMs and tagging</h2>
<h2 id="pcfgs">PCFGs</h2>
<p>A CFG is <span class="math inline">\(G=(N,\Si,R,S)\)</span> where</p>
<ul>
<li><span class="math inline">\(N\)</span> has non-terminals</li>
<li><span class="math inline">\(\Si\)</span> has terminals</li>
<li><span class="math inline">\(R\)</span> has rules <span class="math inline">\(X\to Y_1\ldots, Y_n\)</span>.</li>
<li><span class="math inline">\(S\in N\)</span> is te start symbol.</li>
</ul>
<p>A leftmost derivation is one where we choose to replace the leftmost nonterminal each time.</p>
<p>Strings can be ambiguous under a CFG.</p>
<p>Notation:</p>
<ul>
<li><span class="math inline">\(\mathcal T_G\)</span> is the set of possible leftmost derivations (parse trees).</li>
<li>For <span class="math inline">\(t\in \mathcal T_G\)</span>, yield<span class="math inline">\((t)\)</span> is the yield.</li>
<li><span class="math inline">\(\mathcal T_G(s)\)</span> is the set of possible parse trees for <span class="math inline">\(s\)</span>.</li>
<li>A sentence is ambiguous/grammatical if <span class="math inline">\(|\mathcal T_G(s)|&gt;1\)</span> and grammatical if <span class="math inline">\(&gt;0\)</span>.</li>
</ul>
<p>We want * define a probability distribution <span class="math inline">\(p\)</span> on parse trees * learn parameters * find <span class="math inline">\(\amax_{t\in \mathcal T_G(s)}p(t)\)</span> (decoding/parsing)</p>
<p>A <strong>PCFG</strong> has probabilities with <span class="math inline">\(\sum_{X\to \be\in R} a(X\to \be)=1\)</span>. The probability is the product of the probabilities of the derivations.</p>
<p>To learn parameters given <em>parse trees</em>, let <span class="math inline">\(\wh p (\al \to \be) = \fc{\#(\al\to \be)}{\#\al}\)</span>.</p>
<p><strong>Chomsky normal form</strong> has rules in the form <span class="math inline">\(X\to Y_1Y_2\)</span> and <span class="math inline">\(X\to Y\)</span> where <span class="math inline">\(Y_1,Y_2\)</span> are nonterminal and <span class="math inline">\(Y\)</span> is terminal.</p>
<ul>
<li>Parsing: The CKY algorithm is a dynamic programming algorithm. For a CFG, just keep track of possibility. For a PCFG, keep track of the max and argmax at each level.</li>
<li>Finding probability: The inside-outside algorithm is a dynamic program that takes the sum at each step.</li>
</ul>
<h2 id="lexicalized-pcfgs">Lexicalized PCFGs</h2>
<p>These have much higher parsing accuracy.</p>
<p>Weaknesses of PCFG’s are</p>
<ol type="1">
<li><p>Lack of sensitivity to lexical information: PCFGs make a strong independence assumption. The identity of each lexical item depends only on the part-of-speech (POS) above that lexical item, but does not depend directly on other information in the tree.</p>
<p>Example: Workers dumped sacks into a bin. What parsing is picked depends only on comparing <span class="math inline">\(q(VP\to VP,PP), q(NP\to NP, PP)\)</span>.</p>
<p><code>Into</code> is 9 times more likely to attach to a VP rather than NP.</p>
Example (coordination ambiguity): Dogs in houses and cats.</li>
<li><p>Lack of sensitivity to structural preferences.</p>
<p>Ex. President of a company in Africa.</p>
<p>President of a (company in Africa) is <em>close attachment</em>. Close attachment is twice as frequent.</p>
<p>When a PP can attach to 2 potential verbs, it is 20 times more likely to attach to the most recent verb.</p></li>
</ol>
<p>To lexicalize a treebank,</p>
<ul>
<li>label terminal words with lexical information (POS).</li>
<li>For each <span class="math inline">\(X\to Y_1\ldots Y_n\)</span> identify <span class="math inline">\(h\in [n]\)</span> the head of the rule, the most important child. (In CNF, <span class="math inline">\(n=2\)</span>.) Do this recursively to get a word associated with the POS. Now label the POS with that word. (See p. 12 for an example.) Write <span class="math inline">\(\to_h\)</span> to specify that the <span class="math inline">\(h\)</span>th child is the head.</li>
<li>Overline the heads.</li>
</ul>
<p>Thus, each node in the parse tree is now POS(word). Note the number of non-terminals is now much larger.</p>
<p>For a rule <span class="math inline">\(X(H) \to_2 Y_1(M) Y_2(H)\)</span> corresponding to <span class="math inline">\(R:X\to_2 Y_1Y_2\)</span>, factor the probability <span class="math display">\[\Pj(X\to_2 Y_1Y_2, M|X, H) = \Pj(X \to_2 Y_1Y_2 |X, H) \Pj(M|S\to_2 Y_1Y_2, X, H)\]</span> and estimate each of the factors with counts.</p>
<ul>
<li>For <span class="math inline">\(\Pj(X \to_2 Y_1Y_2 |X, H)\)</span>, estimate by linearly interpolating <span class="math inline">\(\wh p(X\to_2 Y_1Y_2|X)\)</span> and <span class="math inline">\(\wh p(X\to_2 Y_1Y_2|X,H)\)</span>.</li>
<li>For the other model, linearly interpolate <span class="math inline">\(M|R,H\)</span> and <span class="math inline">\(M|R\)</span>.</li>
</ul>
<p>For DP, include also the head position <span class="math inline">\(h\in [i,j]\)</span>. Complexity goes up to <span class="math inline">\(O(n^4)\)</span>!</p>
<h2 id="ibm">IBM</h2>
<h2 id="section">6</h2>
<h2 id="log-linear-models">7 Log-linear models</h2>
<p>Log-linear models are more flexible; they allow a rich set of features. We can combine a much larger set of estimates with <span class="math inline">\(\la\)</span> parameters. Linear interpolation becomes unwieldy.</p>
<p>We want to model the conditional probability <span class="math inline">\(p(y|x)\)</span>, <span class="math inline">\(x\in \mathcal X\)</span>, <span class="math inline">\(y\in \mathcal Y\)</span>. For example, <span class="math inline">\(\mathcal Y=\mathcal V\)</span>, <span class="math inline">\(\mathcal X = \mathcal Y^{i-1}\)</span>. Or <span class="math inline">\(\mathcal Y=\mathcal T\)</span>, the set of tags.</p>
<p>A loglinear model has <span class="math display">\[\Pj_v(y|x) =\Pj(y|x;v) \propto e^{v\cdot f(x,y)}\]</span> with <span class="math inline">\(f:\mathcal X\times \mathcal Y\to \R^d\)</span>.</p>
<p>Often features are indicator functions. Features can include all bigrams, trigrams… Other features include</p>
<ul>
<li>skip-grams (skip words)</li>
<li>part-of-speech of previous word(s)</li>
<li>suffixes, etc.</li>
</ul>
<p>A key idea is <strong>feature templates</strong>.</p>
<p>For any trigram <em>seen in training data</em>, create an indicator feature. Hash each <span class="math inline">\((u,v,w)\)</span> to a unique integer (for trigrams). We can do the same for suffixes, etc.</p>
<p>Models can have millions of features.</p>
<p>Key observation: for any pair <span class="math inline">\((x,y)\)</span>, the number of values for <span class="math inline">\(k\)</span> in <span class="math inline">\([d]\)</span> such that <span class="math inline">\(f_k(x,y)=1\)</span> is often very small.</p>
<p>Implementation: for any pair <span class="math inline">\((x,y)\)</span>, compute indices of nonzero features (ex. use hash functions).</p>
<p>ML estimates are bad because they overfit—ex. give 0 probability to <span class="math inline">\(n\)</span>-grams that haven’t appeared. Include a regularization term like <span class="math inline">\(\ve{v}^2\)</span>, <span class="math display">\[L(v) = \sumo in \ln p(y^{(i)}|x^{(i)};v) - \fc\la2\ve{v}^2.\]</span> This is convex. Use gradient ascent. LBFGs is a gradient method that makes a more intelligent choice of search direction.</p>
<p>The gradient is <span class="math display">\[\nb L(v) = \sumo in \pa{f(x^{(i)},y^{(i)}) - (p(y|x^{(i)};v))_{y} \cdot (f_k(x^{(i)},y))_{yk}}.\]</span> (Think of the second part as the expected number of times <span class="math inline">\(f_k\)</span> is equal to 1.)</p>
<h2 id="memms-maximum-entropy-markov-models-log-linear-tagging-models">MEMMs (Maximum entropy Markov models = Log-linear tagging models)</h2>
<p>A generative tagging model defines a joint distribution <span class="math inline">\(p(x_1,\ldots, x_n,y_1,\ldots, y_n)\)</span> where <span class="math inline">\(x_i\)</span> is a sentence tagged with <span class="math inline">\(y_i\)</span>. A <strong>conditional tagging model</strong> defines <span class="math inline">\(p(y_1\ldots, y_n|x_1,\ldots, x_n)\)</span>.</p>
<p>Tag by taking the argmax.</p>
<ol type="1">
<li>How to define a conditional tagging model?</li>
<li>How to estimate the parameters?</li>
<li>How to find argmax?</li>
</ol>
<p>Consider trigrams for example. The independence assumption is that conditioned on <span class="math inline">\(X_{[1,n]}\)</span>, the distribution of <span class="math inline">\(Y_i\)</span> depends only on <span class="math inline">\(Y_{i-1},Y_{i-2}\)</span>.</p>
<ol type="1">
<li><p>Let the <span class="math inline">\(i\)</span>th history <span class="math inline">\(h_i=(y_{i-2},y_{i-1},x_{[1,n]}, i)\)</span>. Let <span class="math inline">\(f:H\times \mathcal K\to \R^d\)</span> be the feature-vector representation (<span class="math inline">\(H\)</span> is the space of histories and <span class="math inline">\(\mathcal K\)</span> is the space of tags). Define the probability as a loglinear model on <span class="math inline">\(f\)</span>.</p>
Features may include
<ul>
<li>Word/tag</li>
<li>Prefix, suffix (Ex. -ing is often with VBG, gerunds)</li>
<li><span class="math inline">\(n\)</span>-gram (on <span class="math inline">\(y\)</span>) (Having <span class="math inline">\(i\)</span>-grams for <span class="math inline">\(1\le i\le n\)</span> is useful with regularized approaches to parameter estimation)</li>
<li>Word before/after</li>
<li>Spelling features</li>
<li>Contextual features (<span class="math inline">\(x_{i-2},x_{i+2}\)</span>)</li>
</ul></li>
<li>Use gradient methods.</li>
<li><p>Use the Viterbi algorithm.</p></li>
</ol>
<h2 id="crfs">CRFs</h2>
<blockquote>
<p>There are close connections to support vector machines, where linear models are learned in very high dimensional spaces, with good generalization guarantees hold as long as the margins on training examples are large. Margins are closely related to norms of parameter vectors.</p>
</blockquote>
<h2 id="scraps">Scraps</h2>
<p>Can we extract simple features/algorithms out of neural nets?</p>
<p>Extracting code from a neural net may be NP-hard?</p>
<p>Can a neural net do at least as well as <span class="math inline">\(n\)</span>-gram, and can you crystallize a <span class="math inline">\(n\)</span>-gram from it?</p>
<p>What information is present in memory?</p>
<p>Allow a neural net to revise, or wait before it outputs…</p>
<p>Matrix indexed by Indexable.</p>
<ul>
<li>[KJF14] Deep Fragment Embeddings for bidirectional sentence mapping: &gt; Inspired &gt; by previous work [5, 22] we observe that a dependency tree of a sentence provides a rich set of &gt; typed relationships that can serve this purpose more effectively than individual words or bigrams. &gt; We discard the tree structure in favor of a simpler model and interpret each relation (edge) as an &gt; individual sentence fragment (Figure 2, right shows 5 example dependency relations)</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class="st_facebook_large" displayText="Facebook"></span>
    <span class="st_twitter_large" displayText="Tweet"></span>
    <span class="st_googleplus_large" displayText="Google +"></span>
    <span class="st_reddit_large" displayText="Reddit"></span>
    <span class="st__large" displayText></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>


<!-- Footer -->
<div id="footer">
  <div class="container">
    Built with
    <a href="http://jaspervdj.be/hakyll">Hakyll</a> 
    using 
    <a href="http://www.getbootstrap.com">Bootstrap</a>, 
    <a href="http://www.disqus.com">Disqus</a>,
    <a href="http://ignorethecode.net/blog/2010/04/20/footnotes/">Footnotes.js</a>,
    <a href="http://highlightjs.org/">Highlight.js</a>, 
    <a href="http://www.mathjax.org">MathJax</a>, 
    and <a href="http://www.sharethis.com">ShareThis</a>.
  </div>
</div>
</body>

</html>

<!-- SCRIPTS -->
<!-- jQuery-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="../../../../bootstrap/js/bootstrap.min.js"></script>

<!-- Extension : Highlight.js @ https://highlightjs.org/ -->
<!-- Syntax highlighting tomorrow-night-bright, agate-->
<link rel="stylesheet" href="../../../../highlight/css/tomorrow-night-bright.css">
<script src="../../../../highlight/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- Extension : MathJax @ https://docs.mathjax.org/en/v2.5-latest/tex.html -->
<!-- MathJax/config/local/local.js contains macros. Need to provide entire URL-->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,http://holdenlee.github.io/notebook/MathJax/config/local/local"></script>

<!-- Extension : Footnotes @ http://ignorethecode.net/blog/2010/04/20/footnotes/ -->
<script src="../../../../footnotes/js/footnotes.js"></script>

<!-- Extension : Disqus @ http://disqus.com -->
<!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->

<script src="../../../../disqus/js/disqus.js"></script>



<!-- Extension : Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73261814-1', 'auto');
  ga('send', 'pageview');

</script>

