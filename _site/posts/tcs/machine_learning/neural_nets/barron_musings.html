<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

  <meta name="description" content="Holden Lee's Research Notebook">
  <meta name="author" content="Holden Lee">
    
  <title>Musings on Barron's Theorem</title>

  <!-- Bootstrap core CSS -->
  <link href="../../../../bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

  <!-- Custom styles for this template -->
  <link href="../../../../css/blog.css" rel="stylesheet">
  <link href="../../../../css/default.css" rel="stylesheet">

  <!-- Extension : Footnotes -->
  <link href="../../../../footnotes/css/footnotes.css" rel="stylesheet">

  <!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->
  

  <!-- Extension : Collapsible lists @ http://code.stephenmorley.org/javascript/collapsible-lists/-->
  <link href="../../../../collapsible_lists/css/collapsible.css" rel="stylesheet">
  <script type="text/javascript" src="../../../../collapsible_lists/js/CollapsibleLists.js"></script>

</head>

<body>

<!-- Navigation bar. navbar-inverse is black, navbar-default is white.-->
<!-- To make a button active (pressed), use <li class="active"> -->
<div id="header">
  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../../">Notebook</a>
      </div>
      <div id="navbar" class="collapse navbar-collapse">
        <ul class="nav navbar-nav">
          <li><a href="../../../../">Home</a></li>
          <li><a href="../../../../sitemap.html">Sitemap</a></li>
<!-- TODO: Distinguish between PAPERS, RESEARCH QUESTIONS, BOOKS -->
<!-- TODO: make this part a for loop over main pages -->
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </nav>
</div>
<!-- Content -->
<!--div id="content">
  <h1>Mental Wilderness</h1>-->



<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Musings on Barron's Theorem</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-12-22 
          , Modified: 2017-03-15 
	</p>
      
       <p>Tags: <a href="../../../../tags/neural%20net.html">neural net</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#directions-1222">Directions 12/22</a></li>
 <li><a href="#generalization-bounds">Generalization bounds</a></li>
 <li><a href="#kernel-methods">Kernel methods</a><ul>
 <li><a href="#from-email">From email</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="directions-1222">Directions 12/22</h2>
<ul>
<li>Wild baseless ideas: given distribution <span class="math inline">\(\mu\)</span>, there is some sequence of eigenfunctions associated with it of increasing oscillations; writing a function as a combination of these, if most weight is on small eigenfunctions, then it can be represented by a 1-hidden layer NN on <span class="math inline">\(\mu\)</span>. Estimate <span class="math inline">\(\mu\)</span> with kernels around sampled points?</li>
<li>Forget NN. Can we estimate a Barron function (in MISE sense) well using Fourier sampling?</li>
<li>Rademacher for real-valued functions?</li>
<li>Barron’s theorem has a hard-to-check condition: that <span class="math inline">\(f\)</span> has some extension <span class="math inline">\(f_1\)</span> such that <span class="math inline">\(\ve{\wh{f_1'}}_1\)</span> is small. Can we replace this by a more natural, easy to check condition? Ex. use the wavelet transform or windowed FT (both are localized) instead of Fourier.</li>
<li>Converse of Barron: a function computed by a 1-hidden layer NN with <span class="math inline">\(a_i\)</span>’s small is Barron. (Note difference from ES16: a Barron counterexample will be a NN counterexample only if we limit the <span class="math inline">\(a_i\)</span>’s.)</li>
<li>(Warning: If you only care about being close in <span class="math inline">\(\mu\)</span>…)</li>
<li>Can we make [ES16]’s criterion into an if-and-only-if criteria for representability by 1-hidden layer NN? Ex. concentration on some tubes.</li>
<li>What is the advantage of sigmoids over Fourier basis? Somehow easier to train by monotonicity (less minima)? The relationship to sigmoids is tenuous. In NN the hidden layer often isn’t vastly overcomplete like in Barron to get <span class="math inline">\(\ep\)</span> error, certainly you don’t have multiple in the same direction (I think? - what if you add kernel to fit for that direction). Also you don’t get a lot of oscillation in any direction - like the counterexamples, or even like <span class="math inline">\(\cos\)</span> for large phase. In fact, maybe even it is unimodal. So why sigmoids? Is there a function class for which sigmoids work better? Or are they just better for optimization?</li>
<li>Boundary of accepting set (threshold) rather than function values?</li>
</ul>
<h2 id="generalization-bounds">Generalization bounds</h2>
<p>229T p. 198</p>
<p>Let <span class="math inline">\(h\)</span> be 1-Lipschitz sigmoid function.</p>
<span class="math display">\[\begin{align}
f_{w,\al}(x) &amp;=\sumo jm v_j h(w_j\cdot x)\\
\mathcal F &amp;= \set{f_{w,\al}}{\forall j\in [m],\ve{\al}_2\le B_2', \ve{w_j}_2\le B_2}\\
R_n(\mathcal F)&amp;\le \fc{2B_2B_2'C_2\sqrt m}{\sqrt n}
\end{align}\]</span>
<p>Composition properties of Rademacher complexity aligns very nicely with the layer-by-layer compositionality of neural networks.</p>
<p>Problems</p>
<ul>
<li>Set of all bounded continuous functions is too rich.</li>
<li>Optimization?</li>
</ul>
<p>[Andoni, Panigrahy, Valiant, Zhang] Neural networks approximate bounded-degree polynomials.</p>
<h2 id="kernel-methods">Kernel methods</h2>
<ul>
<li>[GKKT16] Reliably Learning the ReLU in Polynomial Time</li>
<li>[ZLJ16] l1-regularized Neural Networks are Improperly Learnable in Polynomial Time</li>
</ul>
<h3 id="from-email">From email</h3>
<ul>
<li>We can bound the Barron norm of a 1-hidden layer NN if we bound the size of the the vectors: <span class="math inline">\(c \sigma (a^Tx + b)\)</span> has Barron norm <span class="math inline">\(O(c||a||_2)\)</span>. We can bound Barron norm <span class="math inline">\(\int||\omega|||\hat f|\)</span> by
<ul>
<li><span class="math inline">\(\sqrt{(\int||\omega||^{2s}|\hat f|^2)(\int ||\omega||^{2-2s})}\)</span> (but this requires <span class="math inline">\(s&gt;n/2\)</span> for the second integral to converge, and I suspect the first integral might be undesirably large)</li>
<li><span class="math inline">\(\sqrt{(\int||\omega||^{2}|\hat f|^2)(\int_S 1)}\)</span> if we somehow “band-limit” <span class="math inline">\(f\)</span> by zeroing out its Fourier transform outside <span class="math inline">\(S\)</span>.</li>
</ul></li>
<li>I need to compare bound <span class="math inline">\(f(x)\)</span> by a norm in a Hilbert space and the Barron norm is not a norm in a Hilbert space.</li>
</ul>
<p>Some things:</p>
<ul>
<li>I’m confused about <span class="math inline">\(n\)</span> dimensions. There’s the weird thing that even though <span class="math inline">\(\si=1\)</span> in <span class="math inline">\(e^{-\fc{\ve{x}^2}{2}}\)</span>, the standard deviation is <span class="math inline">\(\sqrt n\)</span>. cf. Mass of sphere is mostly near boundary.</li>
<li>Why aren’t most reasonable functions approximable by band-limited functions? If you can approximate with <span class="math inline">\(\ve{\om}\)</span>’s <span class="math inline">\(\le C_n\sqrt n\)</span>, then Barron’s Theorem applies trivially, because you don’t have an exponentially large volume!</li>
<li>Another problem: for <span class="math inline">\(\si(a^Tx+b)\)</span>, <span class="math inline">\(L^1\)</span> norm is bounded so Barron is OK, but <span class="math inline">\(L^2\)</span> norm is infinite. In order to get a Hilbert space bound, you have to smooth, and you don’t get finite <span class="math inline">\(L^2\)</span> until you smooth by <span class="math inline">\(e^{-O(x^2)}\)</span>.</li>
<li>Bandlimiting at <span class="math inline">\(\sqrt n\)</span> is like convolving with something that is <span class="math inline">\(e^{-O(x^2)}\)</span>—this is bad because it smooths everything out! ? Not really, you want a bump function, not a Gaussian, this is incorrect intuition?</li>
</ul>
<p>Conclusion:</p>
<ul>
<li>Hilbert space bound seems useless for <span class="math inline">\(\si(a^Tx+b)\)</span> because the amount of smoothing needed for bounded <span class="math inline">\(L^2\)</span> seems like it kills the function.</li>
<li>Even if we only care about a Barron constant bound, I have a problem: why aren’t bandlimited functions good enough to approximate NN’s? (Additive <span class="math inline">\(L^\iy\)</span> error OK? I.e., get signs? Don’t care about total <span class="math inline">\(L^2\)</span> error?)
<ul>
<li>Suppose measure is in unit sphere. (Is this silly, should I consider the box instead?) It seems like there is additive error <span class="math inline">\(k_n\to 0\)</span> in <span class="math inline">\(\si(a^Tx+b)\)</span> approximation as <span class="math inline">\(n\to \iy\)</span>. (I’m looking at Barron converse.)</li>
<li>Actual function approximation doesn’t get better as <span class="math inline">\(n\to \iy\)</span> (otherwise it would be a paradox). If we consider point/line masses in Fourier space, there is no contradiction.</li>
<li>Radial functions without too much oscillation are also good. (This actually seems counterintuitive to me.)</li>
<li>What’s an example of a function that is Lipschitz and not easily expressible? Consider a grid of triangle functions. What does this look like when bandlimited? I guess it gets killed by the convolution, while a few faraway directions do not? So instead of looking at Barron directly look at: ability to approximate with convolved/decayed function? (Although bounds are probably not too good.) Check how convolution plays out in the Hankel transform? ! For 1-D things, we only care about the projection onto that dimension—so the convolution doesn’t kill it. Formalize: something with low <span class="math inline">\(L^0\)</span> (sparsity) isn’t too affected (in <span class="math inline">\(L^\iy\)</span>) by convolution? And this is NOT true (directly) for <span class="math inline">\(L^1\)</span>? Although Barron says that things small in <span class="math inline">\(L^1\)</span> can be approximated by things small in <span class="math inline">\(L^0\)</span>—so we can always use that as a first step. (norm different though?) Conclusion/steps:
<ul>
<li>Show that every NN can be approximated in <span class="math inline">\(L^\iy\)</span> by bandlimited function. Possibly restrict to NN’s with some incoherence condition (so we lose a chunk, although possibly an unimportant chunk, of the Barron functions).</li>
<li>Simply use RKHS with norm <span class="math inline">\(\pa{\int\ve{\om}^2|\wh f(\om)|^2}^{\rc 2}\)</span>. (Proof of boundedness by this norm goes through Barron norm, but there is no use of Barron’s theorem.)</li>
<li>Find the kernel for this! I’m having trouble doing this. I’m not dealing with the <span class="math inline">\(f(0)\)</span> properly so am getting divergence. <span class="math inline">\(\wh k_y(\om) = \fc{e^{-i\an{\om,y}}}{\ve{\om}^2}\)</span>. If use <span class="math inline">\(1+\ve{\om}^2\)</span> I get <span class="math inline">\(\wh k_y(\om) = \fc{e^{-i\an{\om,y}}}{1+\ve{\om}^2}\)</span>.</li>
<li>Write an <span class="math inline">\(f\)</span> you’re trying to approximate as <span class="math inline">\(\sum a_xk_x\)</span> where <span class="math inline">\((x,y)\)</span> are training pairs.</li>
<li>Careful with either compact support or <span class="math inline">\(f(0)=0\)</span> condition.</li>
</ul></li>
<li>What’s transform of a sphere (surface)?</li>
<li>What’s difference with using Sobolev norm <span class="math inline">\(s&gt;\fc n2\)</span>? For that you have to bandwidth to 1, not <span class="math inline">\(\Te(\sqrt n)\)</span>.</li>
</ul></li>
<li>Fundamental solution <span class="math inline">\(\rc{(4Dt)^{\fc n2}} e^{-\fc{\ve{x}^2}{4Dt}}\)</span>.</li>
</ul>
<p>Other musings:</p>
<ul>
<li>For convolutional? Lipschitz control? Translation invariance? Think about torus case?</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class="st_facebook_large" displayText="Facebook"></span>
    <span class="st_twitter_large" displayText="Tweet"></span>
    <span class="st_googleplus_large" displayText="Google +"></span>
    <span class="st_reddit_large" displayText="Reddit"></span>
    <span class="st__large" displayText></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>


<!-- Footer -->
<div id="footer">
  <div class="container">
    Built with
    <a href="http://jaspervdj.be/hakyll">Hakyll</a> 
    using 
    <a href="http://www.getbootstrap.com">Bootstrap</a>, 
    <a href="http://www.disqus.com">Disqus</a>,
    <a href="http://ignorethecode.net/blog/2010/04/20/footnotes/">Footnotes.js</a>,
    <a href="http://highlightjs.org/">Highlight.js</a>, 
    <a href="http://www.mathjax.org">MathJax</a>, 
    and <a href="http://www.sharethis.com">ShareThis</a>.
  </div>
</div>
</body>

</html>

<!-- SCRIPTS -->
<!-- jQuery-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="../../../../bootstrap/js/bootstrap.min.js"></script>

<!-- Extension : Highlight.js @ https://highlightjs.org/ -->
<!-- Syntax highlighting tomorrow-night-bright, agate-->
<link rel="stylesheet" href="../../../../highlight/css/tomorrow-night-bright.css">
<script src="../../../../highlight/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- Extension : MathJax @ https://docs.mathjax.org/en/v2.5-latest/tex.html -->
<!-- MathJax/config/local/local.js contains macros. Need to provide entire URL-->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,http://holdenlee.github.io/notebook/MathJax/config/local/local"></script>

<!-- Extension : Footnotes @ http://ignorethecode.net/blog/2010/04/20/footnotes/ -->
<script src="../../../../footnotes/js/footnotes.js"></script>

<!-- Extension : Disqus @ http://disqus.com -->
<!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->

<script src="../../../../disqus/js/disqus.js"></script>



<!-- Extension : Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73261814-1', 'auto');
  ga('send', 'pageview');

</script>

