<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

  <meta name="description" content="Holden Lee's Research Notebook">
  <meta name="author" content="Holden Lee">
    
  <title>Adversarial thoughts</title>

  <!-- Bootstrap core CSS -->
  <link href="../../../../bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

  <!-- Custom styles for this template -->
  <link href="../../../../css/blog.css" rel="stylesheet">
  <link href="../../../../css/default.css" rel="stylesheet">

  <!-- Extension : Footnotes -->
  <link href="../../../../footnotes/css/footnotes.css" rel="stylesheet">

  <!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->
  

  <!-- Extension : Collapsible lists @ http://code.stephenmorley.org/javascript/collapsible-lists/-->
  <link href="../../../../collapsible_lists/css/collapsible.css" rel="stylesheet">
  <script type="text/javascript" src="../../../../collapsible_lists/js/CollapsibleLists.js"></script>

</head>

<body>

<!-- Navigation bar. navbar-inverse is black, navbar-default is white.-->
<!-- To make a button active (pressed), use <li class="active"> -->
<div id="header">
  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../../">Notebook</a>
      </div>
      <div id="navbar" class="collapse navbar-collapse">
        <ul class="nav navbar-nav">
          <li><a href="../../../../">Home</a></li>
          <li><a href="../../../../sitemap.html">Sitemap</a></li>
<!-- TODO: Distinguish between PAPERS, RESEARCH QUESTIONS, BOOKS -->
<!-- TODO: make this part a for loop over main pages -->
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </nav>
</div>
<!-- Content -->
<!--div id="content">
  <h1>Mental Wilderness</h1>-->



<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Adversarial thoughts</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-04-16 
          , Modified: 2017-04-16 
	</p>
      
       <p>Tags: <a href="../../../../tags/neural%20nets.html">neural nets</a>, <a href="../../../../tags/uncertainty.html">uncertainty</a>, <a href="../../../../tags/aaml.html">aaml</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#not-surprising">Not surprising</a></li>
 <li><a href="#a-formalization">A formalization</a></li>
 <li><a href="#a-key-question">A key question</a></li>
 <li><a href="#several-subproblems">Several subproblems</a></li>
 <li><a href="#hypotheses-and-approaches">Hypotheses and approaches</a><ul>
 <li><a href="#glue-approach">Glue approach</a></li>
 <li><a href="#mixture">Mixture</a></li>
 <li><a href="#sampling-from-nns">Sampling from NN’s</a></li>
 <li><a href="#regularization">Regularization</a></li>
 <li><a href="#conditioning">Conditioning</a></li>
 <li><a href="#thresholding-and-quantization">Thresholding and quantization</a></li>
 <li><a href="#is-the-first-layer-the-problem">Is the first layer the problem?</a></li>
 <li><a href="#conservative-concepts-and-detection">Conservative concepts and detection</a></li>
 <li><a href="#more-human-approaches">More human approaches</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <p>See also</p>
<ul>
<li><a href="adversarial.html">intro to problem</a>.</li>
<li><a href="adversarial_experiments.html">my experiments</a>.</li>
<li><a href="confidence.html">confidence</a>.</li>
</ul>
<p>Some explanations for adversarial examples.</p>
<h2 id="not-surprising">Not surprising</h2>
<p>Firstly, I think adversarial examples aren’t very surprising: we shouldn’t expect neural nets to do well against an adversary if we didn’t train against it. Neural nets will do the “laziest” thing, which does not involve the “broader” conceptual class we want them to learn (ex. everything close to a ‘5’ is also a ‘5’); vanilla training doesn’t communicate to them this broader class.</p>
<p>That said, why can’t neural nets do well after adversarial training?</p>
<h2 id="a-formalization">A formalization</h2>
<p>Let <span class="math inline">\(A_a\)</span> be a set of “adversarial” modifications to inputs <span class="math inline">\(x\)</span>. We say that an algorithm adversarially-PAC learns <span class="math inline">\((x,y)\sim D\)</span> if after poly samples and time, it produces <span class="math inline">\(f\in F\)</span> such that <span class="math display">\[
\Pj_{x,y} (\max_a L(f(A_a(x)), y)\ne y) \le
\ep
\]</span> in the realizable case, or <span class="math display">\[
\Pj_{x,y} (\max_a L(f(A_a(x)), y)\ne y) \le
\min_{f\in F} \Pj_{x,y} (\max_a L(f(A_a(x)), y)\ne y) + \ep
\]</span> in the agnostic case, where <span class="math inline">\(L\)</span> is the loss function (ex. 0-1).</p>
<p>We probably also want to assume access to an oracle, which given <span class="math inline">\(f\)</span> and <span class="math inline">\(x\)</span>, produces some <span class="math inline">\(a\)</span> that maximizes <span class="math inline">\(L(f(A_a(x)),y)\)</span>.</p>
<p>Some work has been done here: see Yishay Mansour, Robust learning and inference. Cf. also boosting vs. game theory.</p>
<p>Example: <span class="math inline">\(A_a(x) = x+a\)</span>, restricted to <span class="math inline">\(\ve{a}\le \ep\)</span>.</p>
<p>Now there are settings where the adversarial setting isn’t harder - for example, for SVM, in the above example, if there is still a margin <span class="math inline">\(\ga\)</span> between positive and negative examples even after adversarial perturbation, then we’ll make at most <span class="math inline">\(\rc{\ga^2}\)</span> mistakes by perceptron analysis, which doesn’t care about the distribution.</p>
<p>But this is not the case for neural nets. (The fact that we can get training error to 0 suggests that there may be some “margin” at play (cf. Telgarsky). However, we <em>can’t</em> get to 0 training error if we include adversarial examples!)</p>
<p>Can we study a toy example here like dictionary learning? Generate <span class="math inline">\(Ah = x\)</span>, there is a SVM <span class="math inline">\(\sign(w^Th)=y\)</span>. Make it robust to <span class="math inline">\(+a\)</span>.</p>
<p>(This doesn’t seem to capture what’s going on though in <span class="math inline">\(L^2\)</span> - here the adversary’s best bet is to change in the direction <span class="math inline">\(a_i\)</span> where <span class="math inline">\(i=\amax_i |w_i|\)</span>, which would in fact change the sign of <span class="math inline">\(w^Th\)</span>.)</p>
<h2 id="a-key-question">A key question</h2>
<p>It’s not surprising that adversarial examples exist. A better question is why we can’t “train them away” by normal training methods, and what can we do to fix this.</p>
<p>I think there are 2 possibilities.</p>
<ol type="1">
<li>There exists a neural network, with reasonable parameter sizes (under whatever regularization we are using), that can do well on most adversarial inputs. The issue is that using our optimization methods aren’t finding these parameters.</li>
<li>There is something fundamentally limited about the (typical) neural net architecture that doesn’t capture human concept boundaries.</li>
</ol>
<p>The “margin” analysis above is in favor of (2). (1) could still be true if there are e.g. exponentially more choices of parameters that would do well on normal examples than choices that also do well on the adversarial examples. But this isn’t enough to explain it, because adversarial training stalls eventually.</p>
<h2 id="several-subproblems">Several subproblems</h2>
<p>We can talk about <span class="math inline">\(L^\iy\)</span> or <span class="math inline">\(L^2\)</span> adversarial examples. Also whatever threshold we choose, there shouldn’t be human-adversarial examples below that threshold. (I don’t think this is so much of an issue now.)</p>
<h2 id="hypotheses-and-approaches">Hypotheses and approaches</h2>
<h3 id="glue-approach">Glue approach</h3>
<p>Just take a hodgepodge of things that reduce adversarial examples, including things that detect and reject adversarial examples. For example,</p>
<ul>
<li>check if it has abnormally large coefficients for singular vectors for small singular values.</li>
<li>train a detector for adversarial examples on the internal representation.</li>
</ul>
<p>However, this would be an arms race: likely, for many of these, you can find adversarial examples that pass the test (ex. restrict search to top singular vectors). If this were not the case, that would be <em>very interesting</em>.</p>
<p>Are there things like clipping, normalizing, that you do on input before feeding into the network that could help? (Ex. can damping the smaller singular vectors help?)</p>
<p>These are interesting questions but not as satisfying an approach.</p>
<h3 id="mixture">Mixture</h3>
<p>This doesn’t seem to help.</p>
<p>One question: after doing adversarial training for a while, does the NN still do well against the <em>original</em> adversarial examples? If not, then it’s catastrophically forgetting. If it is forgetting, this should be easy to fix.</p>
<h3 id="sampling-from-nns">Sampling from NN’s</h3>
<p>Train a bunch of NN’s on the real examples and then use majority vote.</p>
<p>Problem: this doesn’t work.</p>
<p>Are we somehow not exploring the space of NN’s which do well on the real examples? Can we use Fisher information, Langevin to sample better?</p>
<p>Probably we can’t do much better. Also the existence of a universal perturbation and the fact that adversarial examples from a “linear” model transfer suggest that most NN’s which do well on real examples suffer from the same adversarial examples. (cf. there being exponentially more NN’s which are weak against these adversarial examples, then good.)</p>
<h3 id="regularization">Regularization</h3>
<p>The objective/regularization is wrong. Ex. we should be encouraging sparsity, using exponentiated gradient/multiplicative weights, etc.</p>
<h3 id="conditioning">Conditioning</h3>
<p>Perhaps the Lipschitz constant is just really large. Can we train a NN to have small Lipschitz constant in the correct norm? (Note this can be challenging because for example the <span class="math inline">\(\iy\to 2\)</span> norm is difficult to compute; even approximating it takes a linear program, which is too computationally intensive to do at every step.)</p>
<p>Also if we can’t find a NN with small Lipschitz constant why not? Does it not exist or is there something wrong with the geometry of the optimization problem?</p>
<p>Path norm, batch normalization seem attractive here.</p>
<h3 id="thresholding-and-quantization">Thresholding and quantization</h3>
<p>ReLUs seem like a very bad idea - the adversary can just keep increasing. But is the problem more for</p>
<ul>
<li>ReLU(x) where <span class="math inline">\(x\)</span> is large, or</li>
<li>ReLU(x) where <span class="math inline">\(x\)</span> is small?</li>
</ul>
<p>The first suggests capping - having <span class="math inline">\(y=ReLU(x)\)</span> grow unlimitedly is a bad idea. (We still have the same problem for sigmoids.) The second suggests that we are getting past thresholds by adding a lot of correlated noise, so we should zero out small ReLU(x)’s.</p>
<p>In the brain we don’t have thresholds exceeded by contributions from hundreds of neurons - there’s some kind of attenuation or normalization (of activations because of limited energy?) that prevents this. What if we just take the top <span class="math inline">\(k\)</span> activations? (During training or testing?)</p>
<p>What about quantization/binarization? For MNIST, binarizing all pixels to be 0 or 1 is fine, and helps against adversarial examples (because small norm changes don’t have much effect - although you should consider other attacks here!). But this is in some sense cheating because MNIST is basically black-white. Can we binarize/quantize intermediate features?</p>
<h3 id="is-the-first-layer-the-problem">Is the first layer the problem?</h3>
<p>I.e. are we just screwed after the first layer because it somehow destroyed the input?</p>
<p>Ex. consider the toy problem of dictionary learning. If the layers of a NN were <em>really</em> doing sparse coding, would this be a problem? I.e. does sparse coding suffer from the same problem? Or are NN really <em>not</em> doing sparse coding? While we sort-of say it is, I don’t think it is at all! For the convolutional NN, the convolution <em>isn’t</em> computing <span class="math inline">\(h\)</span> such that <span class="math inline">\(Ah=x\)</span>. In fact, the overlaps between different patches might make things badly conditioned.</p>
<p>If we actually did DL on the first layer, then solved the sparse recovery problem for every input, would we still have the same problem?</p>
<h3 id="conservative-concepts-and-detection">Conservative concepts and detection</h3>
<p>Have the network be able to “abstain” (like a confidence score). This should be similar in principle to training detection between real and adversarial examples.</p>
<p>What if we use RBF’s on the first layer? Even something like pretraining to find the right RBF’s, and keeping them fixed. The idea is that RBF’s are very conservative. Maybe we’ll need the <span class="math inline">\(L^\iy\)</span> analogue if we’re protecting against that…</p>
<h3 id="more-human-approaches">More human approaches</h3>
<p>A vital thing that’s missing in current NN’s is back-and-forth between higher and lower layers. Humans can reinterpret lower-level data when they see a higher-level pattern.</p>
<p>Also humans have some kind of idea of a “prototype” of a digit - another instance must have its curves match up with that prototype in some fashion. There are these ideas of strokes, etc. Something hierarchical, probabilistic model? cf. Tenenbaum.</p>
<p>It may be that there needs to be something fundamentally new in the architecture - something more principled. Anything from neuroscience?</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class="st_facebook_large" displayText="Facebook"></span>
    <span class="st_twitter_large" displayText="Tweet"></span>
    <span class="st_googleplus_large" displayText="Google +"></span>
    <span class="st_reddit_large" displayText="Reddit"></span>
    <span class="st__large" displayText></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>


<!-- Footer -->
<div id="footer">
  <div class="container">
    Built with
    <a href="http://jaspervdj.be/hakyll">Hakyll</a> 
    using 
    <a href="http://www.getbootstrap.com">Bootstrap</a>, 
    <a href="http://www.disqus.com">Disqus</a>,
    <a href="http://ignorethecode.net/blog/2010/04/20/footnotes/">Footnotes.js</a>,
    <a href="http://highlightjs.org/">Highlight.js</a>, 
    <a href="http://www.mathjax.org">MathJax</a>, 
    and <a href="http://www.sharethis.com">ShareThis</a>.
  </div>
</div>
</body>

</html>

<!-- SCRIPTS -->
<!-- jQuery-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="../../../../bootstrap/js/bootstrap.min.js"></script>

<!-- Extension : Highlight.js @ https://highlightjs.org/ -->
<!-- Syntax highlighting tomorrow-night-bright, agate-->
<link rel="stylesheet" href="../../../../highlight/css/tomorrow-night-bright.css">
<script src="../../../../highlight/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- Extension : MathJax @ https://docs.mathjax.org/en/v2.5-latest/tex.html -->
<!-- MathJax/config/local/local.js contains macros. Need to provide entire URL-->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,http://holdenlee.github.io/notebook/MathJax/config/local/local"></script>

<!-- Extension : Footnotes @ http://ignorethecode.net/blog/2010/04/20/footnotes/ -->
<script src="../../../../footnotes/js/footnotes.js"></script>

<!-- Extension : Disqus @ http://disqus.com -->
<!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->

<script src="../../../../disqus/js/disqus.js"></script>



<!-- Extension : Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73261814-1', 'auto');
  ga('send', 'pageview');

</script>

