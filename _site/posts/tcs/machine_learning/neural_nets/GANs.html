<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

  <meta name="description" content="Holden Lee's Research Notebook">
  <meta name="author" content="Holden Lee">
    
  <title>GANs</title>

  <!-- Bootstrap core CSS -->
  <link href="../../../../bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

  <!-- Custom styles for this template -->
  <link href="../../../../css/blog.css" rel="stylesheet">
  <link href="../../../../css/default.css" rel="stylesheet">

  <!-- Extension : Footnotes -->
  <link href="../../../../footnotes/css/footnotes.css" rel="stylesheet">

  <!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->
  

  <!-- Extension : Collapsible lists @ http://code.stephenmorley.org/javascript/collapsible-lists/-->
  <link href="../../../../collapsible_lists/css/collapsible.css" rel="stylesheet">
  <script type="text/javascript" src="../../../../collapsible_lists/js/CollapsibleLists.js"></script>

</head>

<body>

<!-- Navigation bar. navbar-inverse is black, navbar-default is white.-->
<!-- To make a button active (pressed), use <li class="active"> -->
<div id="header">
  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../../">Notebook</a>
      </div>
      <div id="navbar" class="collapse navbar-collapse">
        <ul class="nav navbar-nav">
          <li><a href="../../../../">Home</a></li>
          <li><a href="../../../../sitemap.html">Sitemap</a></li>
<!-- TODO: Distinguish between PAPERS, RESEARCH QUESTIONS, BOOKS -->
<!-- TODO: make this part a for loop over main pages -->
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </nav>
</div>
<!-- Content -->
<!--div id="content">
  <h1>Mental Wilderness</h1>-->



<div class="container">
  <div id="content">
    <div class="page header">
      <h1>GANs</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-12-28 
          , Modified: 2016-12-28 
	</p>
      
       <p>Tags: <a href="../../../../tags/neural%20nets.html">neural nets</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#gans">GANs</a><ul>
 <li><a href="#objective">Objective</a></li>
 <li><a href="#interpretation-of-objective">Interpretation of objective</a></li>
 <li><a href="#another-interpretation">Another interpretation</a></li>
 <li><a href="#training">Training</a></li>
 <li><a href="#notes">Notes</a></li>
 </ul></li>
 <li><a href="#improved-techniques.">Improved techniques.</a><ul>
 <li><a href="#semi-supervised-learning">Semi-supervised learning</a></li>
 </ul></li>
 <li><a href="#infogan">InfoGAN</a></li>
 <li><a href="#two-views-of-gans">Two views of GANs</a></li>
 <li><a href="#adversarial-training-with-maximum-mean-discrepancy">Adversarial training with maximum mean discrepancy</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>References</p>
<ul>
<li>Papers
<ul>
<li><a href="https://arxiv.org/pdf/1606.03657v1.pdf">InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</a></li>
<li><a href="https://arxiv.org/abs/1606.03498">Improved techniques for training GANs</a></li>
<li><a href="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf">Generative adversarial nets</a></li>
</ul></li>
<li>Blog posts
<ul>
<li><a href="http://www.inference.vc/my-summary-of-adversarial-training-nips-workshop/">New perspectives, NIPS2016</a> <a href="http://scrible.com/s/g5QQ6">h</a></li>
<li><a href="http://www.inference.vc/how-to-train-your-generative-models-why-generative-adversarial-networks-work-so-well-2/">How to train your generative models</a> <a href="http://scrible.com/s/i5AQ6">h</a></li>
<li><a href="http://www.inference.vc/infogan-variational-bound-on-mutual-information-twice/">InfoGAN</a> <a href="http://scrible.com/s/6BAQ6">h</a></li>
<li><a href="https://openai.com/blog/generative-models/">OpenAI</a></li>
<li><a href="http://www.yingzhenli.net/home/blog/?p=421">GANs, MI, and algorithm selection</a></li>
</ul></li>
</ul>
<h2 id="gans">GANs</h2>
<p>Ferenc: When the goal is to train a model that can generate natural-looking samples, maximum likelihood is not a desirable training objective. Under model misspecification and finite data (that is, in pretty much every practically interesting scenario), it has a tendency to produce models that overgeneralise.</p>
<h3 id="objective">Objective</h3>
<p>Train 2 neural networks <span class="math inline">\(D\)</span> and <span class="math inline">\(G\)</span>. <span class="math inline">\(P\)</span> generates real data, and <span class="math inline">\(G=\wh P\)</span> generates fake data. Suppose either <span class="math inline">\(P\)</span> or <span class="math inline">\(\wh P\)</span> is chosen with probability <span class="math inline">\(\rc2\)</span> and then one same <span class="math inline">\(x\)</span> is drawn. (I.e., consider <span class="math inline">\(\rc2 (P+\wh P)\)</span>.)</p>
<ul>
<li><span class="math inline">\(G\)</span> is generator: it tries to generate <span class="math inline">\(x\)</span> with distribution close to <span class="math inline">\(P\)</span>.</li>
<li><span class="math inline">\(D\)</span> is discriminator: given <span class="math inline">\(x\)</span>, it tries to output the probability that <span class="math inline">\(x\)</span> is real data.</li>
</ul>
<p><span class="math inline">\(G\)</span> tries to minimize the objective and <span class="math inline">\(D\)</span> tries to maximize it: <span class="math display">\[
V(D, G) = \E_P \ln D + \E_G \ln (1-D).
\]</span> (The second expectation is over <span class="math inline">\(G(z)\)</span>, <span class="math inline">\(z\sim p_z\)</span> a fixed distribution.) If the data was real, there is a loss <span class="math inline">\(\ln \wh\Pj\pat{real}\)</span>, and if the data was fake, there is a loss <span class="math inline">\(\ln \wh \Pj\pat{fake}\)</span>. (Penalize for mistakes.)</p>
Note that given <span class="math inline">\(G\)</span>, the optimal <span class="math inline">\(D\)</span> is <span class="math inline">\(\ln \fc{p(x)}{p(x) + \wh p(x)}\)</span>,
<span class="math display">\[\begin{align}
\max_D V(D,G) &amp;= \EE_p \ln \fc{p}{p+\wh p} + \EE_{\wh p} \ln \fc{\wh p}{p+\wh p}\\
&amp;= -2\ln 2 + KL\ba{p||\fc{p+\wh p}2} + KL\ba{\wh p ||\fc{p+\wh p}2}.
\end{align}\]</span>
<h3 id="interpretation-of-objective">Interpretation of objective</h3>
<span class="math display">\[
KL[Q||P] := \EE_Q\ln \fc{Q}{P} = H(Q) - \EE_Q\ln P = H(Q) + CE(Q,P)
\]</span> where CE is cross-entropy.
<span class="math display">\[\begin{align}
JSD[P||Q] &amp;= \rc 2 KL\ba{P||\fc{P+Q}2}
+ \rc 2 KL\ba{Q||\fc{P+Q}2}\\
JSD[Q||P] &amp;= \pi KL\ba{P||\pi P+(1-\pi)Q}
+ (1-\pi) KL\ba{Q||\pi P+(1-\pi)Q}
\end{align}\]</span>
<p>Let <span class="math inline">\(P\)</span> be the natural distribution and <span class="math inline">\(Q\)</span> be the estimated one.</p>
<ul>
<li><span class="math inline">\(KL[Q||P] = H(Q) + CE(Q,P)\)</span>, <span class="math inline">\(CE(Q,P)\)</span> is the perplexity of seeing samples from <span class="math inline">\(P\)</span> when you think the distribution is <span class="math inline">\(Q\)</span>. It is maximized by a model <span class="math inline">\(Q\)</span> that deterministically picks the most likely stimulus. <span class="math inline">\(H(Q)\)</span> enforces diversity.
<ul>
<li>Favors approximations <span class="math inline">\(Q\)</span> to overgeneralize <span class="math inline">\(P\)</span>. It’s allowed to introduce probability mass where <span class="math inline">\(P\)</span> has no or little mass.</li>
<li>Hard to optimize based on a finite sample. (Why?)</li>
</ul></li>
<li><span class="math inline">\(KL[P||Q] = H(P) + CE(P,Q)\)</span>, <span class="math inline">\(H(P)\)</span> is constant and <span class="math inline">\(CE(P,Q)\)</span> is the average negative log likelihood. Optimizing this corresponds to maximizing the log likelihood.
<ul>
<li>Favors undergeneralization, ex. describing the largest mode only. It’s infinitely penalized in introducing probability mass when <span class="math inline">\(P\)</span> has none.</li>
</ul></li>
<li>JSD is a compromise.</li>
</ul>
<h3 id="another-interpretation">Another interpretation</h3>
<p>Let <span class="math inline">\(s=0,1\)</span> wp <span class="math inline">\(\rc2\)</span>, determining whether we see real or fake data. Let <span class="math inline">\(\wt p\)</span> be combined distribution. <span class="math display">\[I[s;x] = KL[\wt p(s,x)||\wt p(s) \wt p(x)].\]</span> This is intractable to estimate. Subtract out KL divergence: <span class="math display">\[
L[p;q] := I[s;x] - \E_{\wt p(x)} [KL[\wt p(s|x)||w(s|x)]] = H[s] + \E_{\wt p(s,x)} [\ln q(s|x)]
\]</span> (CHECK THIS) The second term is the GAN objective function. “GAN can be viewed as an augmented generative model which is trained by minimizing mutual information.” <a href="http://www.yingzhenli.net/home/blog/?p=421">YZL</a></p>
<h3 id="training">Training</h3>
<ul>
<li>Alternate between <span class="math inline">\(k\)</span> steps of optimizing <span class="math inline">\(D\)</span> and one step of optimizing <span class="math inline">\(G\)</span>.</li>
<li>At the beginning <span class="math inline">\(G\)</span> is poor so <span class="math inline">\(D\)</span> predicts close to 1, and <span class="math inline">\(\ln (1-D)\)</span> can blow up. So at the beginning train <span class="math inline">\(G\)</span> to maximize <span class="math inline">\(\E_G\ln D\)</span> instead.</li>
<li>Use stochastic backprop.</li>
<li>Use momentum.</li>
<li>Generator nets used ReLU and sigmoid, discriminator used maxout, dropout.</li>
<li>Estimate probability of test set data under <span class="math inline">\(p_g\)</span> by fitting Gaussian Parzen window. (What is this? [7])</li>
</ul>
<h3 id="notes">Notes</h3>
<ul>
<li>No explicit representation of <span class="math inline">\(p_g\)</span>.</li>
<li><span class="math inline">\(D\)</span> must be synchronized well with <span class="math inline">\(G\)</span> during training. (? Avoid “Helvetica”)</li>
</ul>
<p>Table p. 8</p>
<h2 id="improved-techniques.">Improved techniques.</h2>
<p>“GANs are typically trained using gradient descent techniques that are designed to find a low value of a cost function, rather than to find the Nash equilibrium of a game.”</p>
<ul>
<li>Feature matching: new objective for generator that prevents it from overtraining on current discriminator. Train the generator to match the expected value of features <span class="math inline">\(f(x)\)</span> on intermediate layer of discriminator. <span class="math display">\[ \ve{\E_{x\sim p_{\text{data}}}f(x) - \E_{z\sim p_z(z)} f(G(z))}_2^2.\]</span></li>
<li>Minibatch discrimination: A failure mode for GAN is for the generator to collapse to a point. So allow discriminator to look at multiple data examples in combination.</li>
<li>Historical averaging: include term <span class="math inline">\(\ve{\te - \rc t \sumo it \te[i]}^2\)</span>. cf. fictitious play [16] algorithm that works for other games.</li>
<li>One-sided label smoothing. Replace positive and negative classification targets 1, 0 with <span class="math inline">\(\al,\be\)</span>. This makes it less vulnerable to adversarial examples. Actually, don’t change <span class="math inline">\(\be=0\)</span>.</li>
<li>Virtual batch normalization</li>
</ul>
<h3 id="semi-supervised-learning">Semi-supervised learning</h3>
<p>Label generated samples with “generated” class <span class="math inline">\(K+1\)</span>. For unlabeled data, maximize <span class="math inline">\(\ln p_{\text{model}}(y\in [K]|x)\)</span>.</p>
<h2 id="infogan">InfoGAN</h2>
<ul>
<li>Extend the GAN objective with new term encouraging high mutual information between generated samples and subset of latent variables <span class="math inline">\(c\)</span>.</li>
<li>Hopefully <span class="math inline">\(c\)</span> will represent the most meaningful sources of variation.</li>
<li>Use a variational lower bound to maximize mutual information. (cf. variational autoencoders)</li>
</ul>
<p><span class="math display">\[L_{infoGAN}(\te) = I[x,y] - \la I[x_{\text{fake}},c].\]</span></p>
<p>Want <span class="math inline">\(c\)</span> to effectively explain most variation in fake data.</p>
<p>Variational bound on MI: <span class="math display">\[I[X,Y] = \max_q \bc{H[Y] + \EE_{x,y} \ln q(y|x)}.\]</span></p>
<p>GANs use this bound in the wrong direction. InfoGANs use it twice.</p>
<h2 id="two-views-of-gans">Two views of GANs</h2>
<ol type="1">
<li>Divergence minimization: minimize divergence between real data and implicit generative model <span class="math inline">\(q_\te\)</span>. Problems
<ul>
<li>GAN algorithms minimize lower bounds. The discriminator must be powerful.</li>
<li>Degenerate distributions</li>
</ul></li>
<li>Constrast function view: learn a function that takes low values on data manifold and high values everywhere else. The generator is a smart way of generating contrastive points.</li>
</ol>
<h2 id="adversarial-training-with-maximum-mean-discrepancy">Adversarial training with maximum mean discrepancy</h2>
<ul>
<li><a href="http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf">A kernel two sample test</a></li>
<li><a href="https://arxiv.org/abs/1505.03906">Training generative neural networks via Maximum Mean Discrepancy optimization</a></li>
<li><a href="https://arxiv.org/pdf/1502.02761v1.pdf">Generative Moment Matching Networks</a></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class="st_facebook_large" displayText="Facebook"></span>
    <span class="st_twitter_large" displayText="Tweet"></span>
    <span class="st_googleplus_large" displayText="Google +"></span>
    <span class="st_reddit_large" displayText="Reddit"></span>
    <span class="st__large" displayText></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>


<!-- Footer -->
<div id="footer">
  <div class="container">
    Built with
    <a href="http://jaspervdj.be/hakyll">Hakyll</a> 
    using 
    <a href="http://www.getbootstrap.com">Bootstrap</a>, 
    <a href="http://www.disqus.com">Disqus</a>,
    <a href="http://ignorethecode.net/blog/2010/04/20/footnotes/">Footnotes.js</a>,
    <a href="http://highlightjs.org/">Highlight.js</a>, 
    <a href="http://www.mathjax.org">MathJax</a>, 
    and <a href="http://www.sharethis.com">ShareThis</a>.
  </div>
</div>
</body>

</html>

<!-- SCRIPTS -->
<!-- jQuery-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="../../../../bootstrap/js/bootstrap.min.js"></script>

<!-- Extension : Highlight.js @ https://highlightjs.org/ -->
<!-- Syntax highlighting tomorrow-night-bright, agate-->
<link rel="stylesheet" href="../../../../highlight/css/tomorrow-night-bright.css">
<script src="../../../../highlight/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- Extension : MathJax @ https://docs.mathjax.org/en/v2.5-latest/tex.html -->
<!-- MathJax/config/local/local.js contains macros. Need to provide entire URL-->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,http://holdenlee.github.io/notebook/MathJax/config/local/local"></script>

<!-- Extension : Footnotes @ http://ignorethecode.net/blog/2010/04/20/footnotes/ -->
<script src="../../../../footnotes/js/footnotes.js"></script>

<!-- Extension : Disqus @ http://disqus.com -->
<!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->

<script src="../../../../disqus/js/disqus.js"></script>



<!-- Extension : Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73261814-1', 'auto');
  ga('send', 'pageview');

</script>

