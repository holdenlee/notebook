<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2017-03-08T00:00:00Z</updated>
    <entry>
    <title>Stochastic calculus</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/probability/stochastic_calculus.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/probability/stochastic_calculus.html</id>
    <published>2017-03-08T00:00:00Z</published>
    <updated>2017-03-08T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Stochastic calculus</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-03-08 
          , Modified: 2017-03-08 
	</p>
      
       <p>Tags: <a href="/tags/stochastic%20calculus.html">stochastic calculus</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#section">4</a><ul>
 <li><a href="#ito-integral">4.2 Ito integral</a></li>
 <li><a href="#some-elementary-properties">4.3 Some elementary properties</a></li>
 <li><a href="#ito-calculus">4.4 Ito calculus</a></li>
 <li><a href="#girsanovs-theorem">4.5 Girsanov’s theorem</a></li>
 <li><a href="#martingale-representation-theorem">4.6 Martingale representation theorem</a></li>
 </ul></li>
 <li><a href="#stochastic-differential-equations">5 Stochastic differential equations</a><ul>
 <li><a href="#sde-existence-and-uniqueness">5.1 SDE existence and uniqueness</a></li>
 <li><a href="#markov-property-and-kolmogorovs-equations">5.2 Markov property and Kolmogorov’s equations</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="section">4</h2>
<p>Stieltjes function can be defined in terms of simple functions first, <span class="math inline">\(I(f_n) = \sum_i f_n(t_i^n) (g(t_{i+1}^n) - g(t_i^n))\)</span>.</p>
<p>What’s wrong with the Stieltjes integral? If <span class="math inline">\(g\)</span> has infinite variation on <span class="math inline">\([0,1]\)</span>, then there exist simple functions <span class="math inline">\(f_n\to f\)</span> uniformly such that <span class="math inline">\(I(f_n)\)</span> diverges.</p>
<p><em>Proof</em>. Take a partition so that <span class="math inline">\(\sum |g({t_{i+1}}) - g(t_i)|\to \iy\)</span>, <span class="math inline">\(h_n(t_i) = \sign(g(t_{i+1})-g(t_i))\)</span>.</p>
<p>But when <span class="math inline">\(g\)</span> is a Wiener process, this example would have to choose <span class="math inline">\(h_n\)</span> adaptively to it. If <span class="math inline">\(h_n\)</span> is nonrandom, this is not a problem. There are certain sample paths of <span class="math inline">\(W_t\)</span> for which the integral diverges, but the set of such sample paths has probability 0! But we would like to integrate random processes.</p>
<p>The lemma cheated by looking into the future!</p>
<ol type="1">
<li>We only define stochastic integrals with repect to <span class="math inline">\(W_t\)</span> of stochastic processes which are <span class="math inline">\(\mathcal F_t\)</span>-adapted.</li>
<li>Even though finite variation of <span class="math inline">\(W_t\)</span> is a.s. infinite, quadratic variation is finite, <span class="math inline">\(\sum_{t_i\in \pi_n} (W_{t_{i+1}}-W_{t_i})^2\to 1\)</span> in probability. Define stochastic integrals as limits in <span class="math inline">\(L^2\)</span>. By independence of increments, <span class="math display">\[
\E [ I(f_n)^2] = \int_0^1 f_n(s)^2\,ds.
\]</span></li>
</ol>
<p>Wiener integral: <span class="math inline">\(f_n\to f\)</span> in <span class="math inline">\(L^2([0,1])\)</span>. Then <span class="math inline">\(\E[(I(f_n)-I(f_m))^2]\to 0\)</span>, so <span class="math inline">\(I(f_n)\)</span> converges to some <span class="math inline">\(I(f)\)</span>. Note: choosing <span class="math inline">\(f_n\)</span> so <span class="math inline">\(\sumo n{\iy} \int_0^1 (f_n(s)-f(s))^2\,ds&lt;\iy\)</span>, can define as a.s. limit (CHECK). Counterexample: take wavelet, divide by <span class="math inline">\(\sqrt{\ln \prc{n}}\)</span>.</p>
<h3 id="ito-integral">4.2 Ito integral</h3>
<p>!! Note the Ito integral is ITSELF a random variable over the probability measure underlying the Wiener process. You can take a <span class="math inline">\(\E\)</span> over Brownian motions.</p>
<ol type="1">
<li>Let <span class="math inline">\(\{X_t^n\}_{t\in [0,T]}\)</span> be a simple, square-integrable, <span class="math inline">\(F_t\)</span>-adapted stochastic process. Define <span class="math inline">\(I(X^n) = \int_0^T X_t^n \,dW_t = \sumz iN X_{t_i}^N (W_{t_{i+1}} - W_{t_i})\)</span>.</li>
<li>Ito isometry: <span class="math inline">\(X_{t_i}^n\)</span> is independent of <span class="math inline">\(W_{t_{i+1}} - W_{t_i}\)</span>, so <span class="math display">\[ \E\ba{\pa{\int_0^T X_t^n \,dW_t}^2} = \E\ba{\int_0^T (X_t^n)^2\,dt}.\]</span> Succinctly, <span class="math display">\[\ve{I(X_\cdot^n)}_{2,\Pj} = \ve{X_\cdot^n}_{2,\mu_T\times \Pj}.\]</span> I.e., <span class="math inline">\(I:L^2(\mu_T\times \Pj)\to L^2(\Pj)\)</span> preserves <span class="math inline">\(L^2\)</span> distance when applied to <span class="math inline">\(F_t\)</span> adapted simple integrands.</li>
<li>Extend to <span class="math inline">\(X_\cdot^n \to X_\cdot\)</span> in <span class="math inline">\(L^2(\mu_T\times \Pj)\)</span>.</li>
<li>Let <span class="math inline">\(X_\cdot\in L^2(\mu_T\times \Pj)\)</span> be <span class="math inline">\(F_t\)</span>-adapted. Then there exists a sequence of <span class="math inline">\(F_t\)</span>_adapted simple <span class="math inline">\(X_\cdot^n\to X\)</span>. <!--DCT-->
<ul>
<li>Continuous and bounded sample paths: uniform continuity, DCT.</li>
<li>Bounded and progressively measurable (?): <span class="math inline">\(X^\ep_\cdot \to X^\ep\)</span>, where <span class="math inline">\(X_t^\ep = \rc\ep\int_{t-\ep}^t X_{\max\{s,0\}}\,ds\)</span>.</li>
<li>Progressively measurable: <span class="math inline">\(X_t I_{|X_t|\le M}\)</span>. DCT.</li>
</ul></li>
</ol>
<p>Ex. <span class="math inline">\(W_T^2 = 2\int_0^T W_t\,dW_t+T\)</span>.</p>
<p>Now what?</p>
<ol type="1">
<li>Consider Ito integral itself as a stochastic process, <span class="math inline">\(t\mapsto \int_0^t X_s\,dW_s\)</span>.
<ul>
<li>For <span class="math inline">\(t\le T\)</span>, define <span class="math inline">\(I_t(X_\cdot^n) = I(X_\cdot^n I_{\le t})\)</span>.</li>
<li><span class="math inline">\(I_t(X_\cdot^n)\)</span> is a <span class="math inline">\(F_t\)</span>-martingale.</li>
<li>Ito integral can be chosen to have continuous sample paths. (Pf. Discontinuous paths live on null set. Use subsequence argument and Borel-Cantelli.) …</li>
</ul></li>
<li>Extend the class of integrable processes, to have nice closure properties. (Product of 2 integrals can be expressed as a integral.)
<ul>
<li>Note we don’t want to define <span class="math inline">\(I_\iy\)</span>, only for every <span class="math inline">\(t\in [0,\iy)\)</span>.</li>
<li>Localization: to define on <span class="math inline">\([0,\iy)\)</span>, define it on every interval <span class="math inline">\([0,T]\)</span>. Require <span class="math inline">\(X_{[0,T]}\in L^2(\mu_T\times \Pj)\)</span> for every <span class="math inline">\(T&lt;\iy\)</span>, i.e. <span class="math inline">\(X\in \bigcap_{T&lt;\iy} L^2(\mu_T\times \Pj)\)</span>.</li>
<li>Check local property: <span class="math inline">\(I_t(X_\cdot)\)</span> does not depend on which <span class="math inline">\(T&gt;t\)</span> we choose.</li>
<li>Behavior under stopping: <span class="math inline">\(X_t\)</span> is <span class="math inline">\(F_t\)</span>-adapted in <span class="math inline">\(\bigcap_{T&lt;\iy} L^2(\mu_T\times \Pj)\)</span>, <span class="math inline">\(\tau\)</span> is <span class="math inline">\(F_t\)</span>-stopping time. Then <span class="math inline">\(I_{\min(t,\tau)}(X_\cdot) = I_t(X_\cdot I_{&lt;\tau})\)</span>. Pf. Let <span class="math inline">\(\tau^n\)</span> be <span class="math inline">\(\tau\)</span> rounded upwards to the earliest jump time.</li>
<li>Localizing sequence for <span class="math inline">\(X_t\)</span>: <span class="math inline">\(F_t\)</span>-stopping times <span class="math inline">\(\tau_n\nearrow \iy\)</span>, <span class="math inline">\(\E\ba{\int_0^{\tau_n} X_t^2\,dt}&lt;\iy\)</span>. “Allow localization intervals to be random.” This is independent of localizing sequence (4.2.10, CHECK).</li>
<li>There is a natural cloass of integrands whose elements admit localizing sequence: <span class="math inline">\(A_T(X_\cdot) = \int_0^T X_t^2&lt;\iy\)</span> a.s. for all <span class="math inline">\(T&lt;\iy\)</span>. Let <span class="math inline">\(\tau_n=\inf\set{t\le n}{A_t(X_{\cdot})\ge n}\)</span>.</li>
</ul></li>
</ol>
<p>Finally: Let <span class="math inline">\(X_t\)</span> be any <span class="math inline">\(F_t\)</span>-adapted stochastic process with <span class="math inline">\(\Pj\ba{\int_0^T X_t^2\,dt&lt;\iy}=1\)</span> for all <span class="math inline">\(T&lt;\iy\)</span>. Then Ito integral <span class="math inline">\(I_t(X_\cdot)\)</span> is uniquely defined by localization and choice of continuous modification as <span class="math inline">\(F_t\)</span>-adapted stochastic process on <span class="math inline">\([0,\iy)\)</span> with continuous sample paths.</p>
<h3 id="some-elementary-properties">4.3 Some elementary properties</h3>
<ol type="1">
<li>Linearity</li>
<li>Stopping time <span class="math inline">\(\int_0^{\min(t,\tau)} X_s\,dW_s = \int_0^t X_s I_{&lt;\tau}\,dW_s\)</span>.</li>
<li>If <span class="math inline">\(X\in \bigcap_{T&lt;\iy} L^2(\mu_T\times \Pj)\)</span> then <span class="math inline">\(\E[]=0\)</span> and <span class="math inline">\(\E[()^2] = \E[\int_0^TX_t^2\,dt]\)</span>.</li>
<li><span class="math inline">\(X^n\to X\)</span> in <span class="math inline">\(L^2(\mu_T\times \Pj)\)</span> means <span class="math inline">\(I_t(X_\cdot^n) \to I_t(X_\cdot)\)</span> in <span class="math inline">\(L^2(\Pj)\)</span>. If convergence fast enough, a.s.</li>
<li><span class="math inline">\(X_t\)</span> is <span class="math inline">\(F_t\)</span>-local martingale if there exists <span class="math inline">\(\tau_n\nearrow \iy\)</span> (reducing sequence), <span class="math inline">\(X_{\min(t,\tau_n)}\)</span> is martingale. Any Ito integral is a local martingale. (Take a localizing sequence.)</li>
</ol>
<h3 id="ito-calculus">4.4 Ito calculus</h3>
<p>Setup</p>
<ul>
<li><span class="math inline">\((\Om, F, \{F_t\}_{t\in [0,\iy)}, \Pj)\)</span></li>
<li><span class="math inline">\(W_t\)</span> is <span class="math inline">\(m\)</span>-dimensional <span class="math inline">\(F_t\)</span>-Wiener process.</li>
<li>Ito process <span class="math inline">\(X_t^i = X_0^i + \int_0^t F_s^i\,ds + \sumo jm \int_0^t G_s^{ij}\,dW_s^j\)</span>.
<ul>
<li><span class="math inline">\(\int_0^t |F_s^i|\,ds&lt;\iy\)</span> a.s.</li>
<li><span class="math inline">\(\int_0^t (G_s^{ij})^2\,ds&lt;\iy\)</span> a.s.</li>
<li>Shorthand: <span class="math inline">\(X_t=X_0+\int_0^t F_s\,ds + \int_0^t G_s\,dW_s\)</span>.</li>
</ul></li>
</ul>
<strong>Theorem</strong> (Ito rule): <span class="math inline">\(u(t,x)\)</span> is <span class="math inline">\(C^1\)</span> in <span class="math inline">\(t\)</span> and <span class="math inline">\(C^2\)</span> in <span class="math inline">\(x\)</span>. Then <span class="math inline">\(u(t,X_t)\)</span> is Ito.
<span class="math display">\[\begin{align}
u(t,X_t) &amp;= u(0,X_0) + \sumo in \sumo km \int_0^t u_i(s,X_s) G_s^{ik} \,dW_s^k\\
&amp;\quad + \int_0^t\ba{u'(s,X_s)+\sumo in u_i(s,X_s)F_s^i + \rc 2 \sum_{i,j=1}^n\sumo km u_{ij}(s,X_s)G_s^{ik}G_s^{jk}}\,ds\\
&amp;=u(0,X_0) + \int_0^t (\nb u)^T G \,dW + \int_0^t u'(s,X_s) + (\nb u)^TF_s + \rc 2 \Tr(G_s^T (\nb^2 u) G_s)\,ds.
\end{align}\]</span>
<p>Alternate notation: <span class="math inline">\(dX_t = F_t\,dt + G_t\,dW_t\)</span>, <span class="math display">\[
du(t,X_t) = u'(t,X_t)\,dt + \pl u(t,X_t)\,dX_t + \rc 2 \Tr(\pl^2 u(t,X_t)dX_t(dX_t)^*),
\]</span> where <span class="math inline">\(dW_t^i \,dW_t^j=\de_{ij}\,dt\)</span> and other derivatives are 0.</p>
<p>First two terms are chain rule. When stochastic integrals are present, we evidently need to take a second-order term into account as well.</p>
<p>Cor. Ito processes form an algebra.</p>
<h3 id="girsanovs-theorem">4.5 Girsanov’s theorem</h3>
<p>What happens to Wiener process under change of measure? We can often simplify a problem by changing to a more convenient probability measure.</p>
<p><strong>Theorem</strong> (Girsanov). Let <span class="math inline">\(W_t\)</span> be <span class="math inline">\(m\)</span>-dimensional <span class="math inline">\(\mathcal F_t\)</span>-Wiener on <span class="math inline">\((\Om, \mathcal F, \{\mathcal F_t\}_{t\in [0,T]}, \Pj)\)</span>, <span class="math inline">\(X_t=\int_0^t F_s\,ds + W_t\)</span> be Ito, <span class="math inline">\(F_t\)</span> Ito integrable, <span class="math display">\[
\La = \exp\pa{-\int_0^T (F_s)^* dW_s - \rc 2\int_0^T \ve{F_s}^2\,ds},
\]</span> Novikov’s condition <span class="math inline">\(\E_{\Pj} \ba{\exp\pa{\rc 2\int_0^T \ve{F_s}^2\,ds}}&lt;\iy\)</span>. THen <span class="math inline">\(\{X_t\}_{t\in [0,T]}\)</span> is <span class="math inline">\(\mathcal F_t\)</span>-Wiener under <span class="math inline">\(\mathbb Q(A) = \E_\Pj (\Ga I_A)\)</span>.</p>
<p>Intuition (discrete case): If <span class="math inline">\(d\Pj\)</span> is the probability measure of standard gaussian, and <span class="math inline">\(d\mathbb Q\)</span> is probability measure where <span class="math inline">\(a_k+\xi_k\)</span> are standard gaussian (<span class="math inline">\(a_k\)</span> is predictable process), write <span class="math inline">\(d\mathbb Q\)</span> wrt <span class="math inline">\(d\Pj\)</span>. <span class="math display">\[
\fc{d\mathbb Q}{d\mathbb P} = \prod\fc{e^{-(\xi_i+a_i)^2/2}}{e^{-\xi_i^2/2}} = \exp\ba{\sumo kn \pa{-a_k \xi_k - \rc 2a_k^2}}.
\]</span></p>
<p>READ PROOF.</p>
<h3 id="martingale-representation-theorem">4.6 Martingale representation theorem</h3>
<p>Gives converse to Ito integral. Every martingale <span class="math inline">\(\{M_t\}_{t\in [0,T]}\)</span> with <span class="math inline">\(M_T\in L^2(\Pj)\)</span> is the Ito integral of a unique process in <span class="math inline">\(L^2(\mu_T\times \Pj)\)</span>.</p>
<p><strong>Theorem</strong>.</p>
<ol type="1">
<li>(Martingale representation) Let <span class="math inline">\(M_t\)</span> be <span class="math inline">\(\mathcal F_t^W = \si\set{W_s}{s\le t}\)</span>-martingale, <span class="math inline">\(M_T\in L^2(\Pj)\)</span>. For a unique <span class="math inline">\(\mathcal F_t^W\)</span>-adapted process <span class="math inline">\(\{H_t\}_{t\in [0,T]}\)</span> in <span class="math inline">\(L^2(\mu_T\times \Pj)\)</span>, <span class="math inline">\(M_t=M_0 + \int_0^t H_s\,dW_s\)</span> a.s.</li>
<li>(Ito representation, more general) Let <span class="math inline">\(X\)</span> be <span class="math inline">\(\mathcal F_T^W\)</span>-measurable rv in <span class="math inline">\(L^2(\Pj)\)</span>. Then for … <span class="math inline">\(X=\E X + \int_0^T H_s\,dW_s\)</span> a.s. <!--can differentiate --></li>
</ol>
<p><em>Proof</em>.</p>
<ol type="1">
<li>Show that any <span class="math inline">\(X\)</span> can be approximated arbitrarily well by Ito integral, <span class="math inline">\(\ve{X-I_T(H_\cdot^\ep)}_2&lt; \ep\)</span>.
<ol type="1">
<li>Identify class of random variables that can approximatate <span class="math inline">\(X\)</span> arbitrarily well. <span class="math inline">\(S=\set{f(W_{t_1},\ldots, W_{t_n})}{n&lt;\iy, t_1,\ldots, t_n\in [0,T], f\in C_0^\iy}\)</span>.
<ul>
<li>Show this holds if allow Borel-measurable <span class="math inline">\(f\)</span>. Filter by slicing into intervals <span class="math inline">\(2^{-n}\)</span>, <span class="math inline">\(X^n = f(W_{2^{-n}T},\ldots, W_T)\)</span>.
<ul>
<li>Levy’s upward theorem: let <span class="math inline">\(X\in L^2(\Pj)\)</span> be <span class="math inline">\(G\)</span>-measurable, <span class="math inline">\(G=\si\{G_n\}\)</span>. Then <span class="math inline">\(\E[X|G_n]\to X\)</span> a.s. and in <span class="math inline">\(L^2(\Pj)\)</span>.</li>
</ul></li>
<li>Any Borel function can be approximated arbitrarily well by <span class="math inline">\(f^n\in C^\iy\)</span>.</li>
</ul></li>
<li>Show any rv in this class can be represented as Ito integral
<ul>
<li>Ito’s rule: <span class="math display">\[g(t,W_t) = g(0,0) + \int_0^t (g_s + \rc2 g_{xx}) (s,W_s)\,ds + \int_0^t g_x (s,W_s)\,dW_s.\]</span> Solve the heat equation for <span class="math inline">\(g\)</span>, <span class="math inline">\(g = \rc{\sqrt{2\pi (t-s)}}\int_{-\iy}^{\iy} f(y) e^{-(x-y)^2/(2(t-s))} \dy\)</span>. Still works for multivariate.</li>
</ul></li>
</ol></li>
<li>Take limits.</li>
</ol>
<h2 id="stochastic-differential-equations">5 Stochastic differential equations</h2>
<p>Existence, uniqueness, Markov property.</p>
<p>Kolmogorov forward (Fokker-Planck) and backward equations.</p>
<h3 id="sde-existence-and-uniqueness">5.1 SDE existence and uniqueness</h3>
<span class="math display">\[\begin{align}
dX_t &amp;= b(t,X_t)dt + \si(t,X_t) dW_t, &amp; X_0=x\\
\iff 
X_t &amp;= x+\int_0^t b(s,X_s) \,ds + \int_0^t \si(s,X_s)\,dW_s.
\end{align}\]</span>
<p>Ex. <span class="math inline">\(dX_t = AX_t dt + B\,dW_t\)</span>, <span class="math inline">\(X_0=x\)</span> has solution <span class="math display">\[
X_t = e^{At}x + \int_0^t e^{A(t-s)}B\,dW_s.
\]</span></p>
<p><strong>Theorem</strong>. Suppose</p>
<ol type="1">
<li><span class="math inline">\(X_0\in L^2(\Pj)\)</span></li>
<li><span class="math inline">\(b,\si\)</span> Lipschitz uniformly on <span class="math inline">\([0,T]\)</span> (in <span class="math inline">\(x\)</span>).</li>
<li><span class="math inline">\(\ve{b(t,0)}, \ve{\si(t,0)}\)</span> bounded on <span class="math inline">\(t\in [0,T]\)</span>.</li>
</ol>
<p>Then there exists solution <span class="math inline">\(X_t\)</span>, and <span class="math inline">\(b(t,X_t),\si(t,X_t)\in L^2(\mu_T\times \Pj)\)</span>, and it is unique a.s.</p>
<h3 id="markov-property-and-kolmogorovs-equations">5.2 Markov property and Kolmogorov’s equations</h3>
<p>A large class of Markov processes with continuous sample paths can be obtained as solution of appropriate SDE.</p>
<p><strong>Theorem</strong>. Suppose conditions hold. Then <span class="math inline">\(X_t\)</span> is <span class="math inline">\(\mathcal F_t\)</span>-Markov process. (Actually it satisfies the strong Markov property, even with random stopping times.)</p>
<p><em>Proof</em>. Calculate <span class="math inline">\(X_t-X_s\)</span>. Note <span class="math inline">\(W_{r+s}-W_s\)</span> is Wiener. <span class="math inline">\(Y_r=X_{r+s}\)</span> satisfies a SDE… ?</p>
<p>Assume <span class="math inline">\(b, \si\)</span> independent of <span class="math inline">\(t\)</span>. Markov property gives <span class="math inline">\(\E[f(X_t)|\mathcal F_s] = g_{t-s}(X_s)\)</span>. This suggests <span class="math inline">\(\ddd t P_t f = \mathcal L P_tf\)</span>.</p>
<p><strong>Theorem</strong> (Kolmogorov backward equation). For <span class="math inline">\(g\in C^2\)</span>, <span class="math display">\[
\mathcal Lg = b^T\nb g + \rc 2 \Tr(\si^T \nb^2 g \si).
\]</span> If <span class="math inline">\(u(t,x)\)</span> is <span class="math inline">\(C^1\)</span> in <span class="math inline">\(t\)</span>, <span class="math inline">\(C^2\)</span> in <span class="math inline">\(x\)</span>, <span class="math inline">\(f\in C^2\)</span> such that <span class="math display">\[
u_t=\mathcal L u, \quad u(0,x)=f(x)
\]</span> then <span class="math inline">\(u(t,x)=P_tf(x)\)</span>.</p>
<p>(Note we can write this backwards as <span class="math inline">\(v_t + \mathcal Lv = 0\)</span>, <span class="math inline">\(v(T,x)=f(x)\)</span>.)</p>
<p>(Note: in principle we would like to define <span class="math inline">\(\E[f(X_t)|\mathcal F_s] =: u(t-s,X_s)\)</span> and show <span class="math inline">\(u\)</span> satisfies the PDE. This is more technical because we need to show smoothness or interpret the PDE in a weak sense.)</p>
<p><em>Proof</em>. Ito’srule on <span class="math inline">\(Y_r=v(r,X_r)\)</span>. (The Ito integral <span class="math inline">\(\int_0^t (\nb v)^T G\,dW_s\)</span> is a “local martingale” here.) Take <span class="math inline">\(\E[\cdot |\mathcal F_s]\)</span> and the martingale part disappears.</p>
<p>Forwards equation: If law of <span class="math inline">\(X_t\)</span> is absolutely continuous, <span class="math inline">\(\E[f(X_t)] = \int_{\R^n} f(y)p_t(y)\dy\)</span> for some <span class="math inline">\(p\)</span>, and more generally, <span class="math display">\[
\E[f(X_t)|\mathcal F_s] = \int_{\R^n} f(y) p_{t-s}(X_s,y)\dy.
\]</span> Can <span class="math inline">\(p\)</span> be obtained as solution to PDE?</p>
<p>“Kolmogorov forward equation is dual of backward equation”: <span class="math inline">\(\int fp_t = \int P_tfp_0\)</span>.</p>
<p><strong>Theorem</strong> (Kolmogorov forward, Fokker-Planck): Assume niceness of the SDE, and <span class="math inline">\(b\in C^1, \si\in C^2\)</span>, <span class="math inline">\(\rh\in C^2\)</span>, <span class="math display">\[
\mathcal L^* \rh = -\sumo in \pd{x^i}(b^i\rh) + \rc2 \suij n \sumo km \pd{{}^2}{x^i\pl x^j} (\si^{ik}\si^{ij}\rh),
\]</span> <span class="math inline">\(p_t\)</span> exists, <span class="math inline">\(C^1\)</span> in <span class="math inline">\(t\)</span>, <span class="math inline">\(C^2\)</span> in <span class="math inline">\(x\)</span>. Then <span class="math display">\[(p_t)_t = \mathcal L^* p_t, \quad t\in [0,T].\]</span></p>
<p><em>Proof</em>.</p>
<ol type="1">
<li>Write Ito’s rule for <span class="math inline">\(f\)</span>.</li>
<li>Take <span class="math inline">\(\E\)</span> so the martingale disappears.</li>
<li>Substitute definition of <span class="math inline">\(p_t(y)\)</span>, integrate by parts to take <span class="math inline">\(\mathcal L\)</span> from <span class="math inline">\(\mathcal L f\)</span> to <span class="math inline">\(\mathcal L^* p_s\)</span>. This holds for all <span class="math inline">\(f\)</span> so remove the <span class="math inline">\(f\)</span>.</li>
<li>Take time derivative.</li>
</ol>
<blockquote>
<p>As a rule of thumb, the backward equation is very well behaved, and will often have a solution provided only that f is sufficiently smooth; the forward equation is much less well behaved and requires stronger conditions on the coefficients</p>
</blockquote>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2017-03-11</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2017-03-11.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2017-03-11.html</id>
    <published>2017-03-07T00:00:00Z</published>
    <updated>2017-03-07T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2017-03-11</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-03-07 
          , Modified: 2017-03-07 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#adversarial-training">Adversarial training</a></li>
 <li><a href="#langevin-monte-carlo">Langevin Monte Carlo</a></li>
 <li><a href="#boosting-etc.">Boosting, etc.</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="adversarial-training">Adversarial training</h2>
<h2 id="langevin-monte-carlo">Langevin Monte Carlo</h2>
<p>Background</p>
<ul>
<li>Brownian motion (and other probability prerequisites)</li>
<li>Stochastic calculus</li>
<li>High-dimensional probability (LSI, etc.)</li>
<li>Langevin papers</li>
</ul>
<h2 id="boosting-etc.">Boosting, etc.</h2>
<p>Fix notes!</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Brownian Motion</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/probability/brownian_motion.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/probability/brownian_motion.html</id>
    <published>2017-03-07T00:00:00Z</published>
    <updated>2017-03-07T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Brownian Motion</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-03-07 
          , Modified: 2017-03-07 
	</p>
      
       <p>Tags: <a href="/tags/brownian%20motion.html">brownian motion</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#robert-browns-new-thing">Robert Brown’s new thing</a></li>
 <li><a href="#brownian-motion-as-a-gaussian-process">Brownian motion as a Gaussian process</a><ul>
 <li><a href="#invariance-properties">2.2 Invariance properties</a></li>
 <li><a href="#rd">2.3 <span class="math inline">$\R^d$</span></a></li>
 </ul></li>
 <li><a href="#constructions-of-brownian-motion">3 Constructions of Brownian motion</a><ul>
 <li><a href="#levy-ciesielski">3.1 Levy-Ciesielski</a></li>
 <li><a href="#levys-original-argument">3.2 Levy’s original argument</a></li>
 </ul></li>
 <li><a href="#the-canonical-model">4 The canonical model</a><ul>
 <li><a href="#kolmogorovs-theorem">4.2 Kolmogorov’s Theorem</a></li>
 </ul></li>
 <li><a href="#brownian-motion-as-a-markov-process">6 Brownian motion as a Markov process</a></li>
 <li><a href="#brownian-motion-and-transition-semigroups">7 Brownian motion and transition semigroups</a><ul>
 <li><a href="#the-generator">7.2 The generator</a></li>
 </ul></li>
 <li><a href="#the-pde-connection">8 The PDE connection</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>Notes from Schilling, Partzsch.</p>
<h2 id="robert-browns-new-thing">Robert Brown’s new thing</h2>
<p>A particle</p>
<ul>
<li>starts at <span class="math inline">\(x=0\)</span>.</li>
<li>at times <span class="math inline">\(k\De t\)</span>, move <span class="math inline">\(\De x\)</span> units to L or R with equal probability.</li>
</ul>
<p>Let <span class="math inline">\(X_t\)</span> be position at time <span class="math inline">\(t\in [0,T]\)</span>.</p>
<ul>
<li><span class="math inline">\(\Var(X_t)=\si^2 T\)</span> where <span class="math inline">\(\fc{(\De x)^2}{\De t}=\si^2\)</span>.</li>
<li>CLT: <span class="math inline">\(X_T \to \sqrt T \si N(0,1)\)</span>.</li>
</ul>
<strong>Definition</strong>. A <span class="math inline">\(d\)</span>-dimensional Brownian motion <span class="math inline">\(B=(B_t)_{t\ge 0}\)</span> is a stochastic process indexed by <span class="math inline">\([0,\iy)\)</span>,
<span class="math display">\[\begin{align}
B_0(\om)&amp;=0\\
B_{t_n}-B_{t_{n-1}},\ldots, B_{t_1}-B_{t_0} &amp; \text{ independent}, 0=t_0\le t_1&lt;\cdots &lt; t_n\\
B_t - B_s &amp;\sim B_{t+h} - B_{s+h}\\
B_t &amp;= N(0,t)^{\ot d}\\
t&amp;\mapsto B_t(\om) \text{ continuous.}
\end{align}\]</span>
<p>Note the last property is implied by the previous.</p>
<h2 id="brownian-motion-as-a-gaussian-process">Brownian motion as a Gaussian process</h2>
<p>Characteristic function of Gaussian: <span class="math display">\[
\E e^{i\an{\xi,\Ga}} = e^{i \E\an{\xi, \Ga} - \rc \Var\an{\xi, \Ga}} = e^{i\an{\xi, \Ga}} = e^{i\an{\xi, m} - \rc 2\an{\xi, \Si\xi}}.
\]</span></p>
<ul>
<li><span class="math inline">\(B_t\)</span> is Gaussian with mean 0, variance <span class="math inline">\(t\)</span>. <span class="math inline">\(\E e^{\ze B_t} = e^{t\ze^2/2}\)</span>.</li>
<li><span class="math inline">\(\E (B_t^{2k}) = t^k \fc{2^k \Ga(k+\rc 2)}{\sqrt \pi}\)</span>.</li>
<li><span class="math inline">\(\Cov(B_s,B_t) = \min(s,t)\)</span>.</li>
</ul>
<p><span class="math inline">\((X_t)_{t\ge 0}\)</span> is a Gaussian process if for all <span class="math inline">\(0\le t_1&lt;t_2&lt;\cdots\)</span>, <span class="math inline">\((X_{t_1},\ldots, X_{t_n})\)</span> is a Gaussian random vector.</p>
<p><strong>Theorem</strong>. <span class="math inline">\((B_t)_{t\ge 0}\)</span> is a Gaussian process. The covariance of <span class="math inline">\((B_{t_1},\ldots, B_{t_n})\)</span> is <span class="math inline">\(C = (\min(t_j,t_k))_{j,k}\)</span>.</p>
<p><em>Proof</em>. Linearly transform <span class="math inline">\((t_1-t_0,t_2-t_1,\ldots)\)</span>.</p>
<p>Converse: If the covariance matrices are given by the above and <span class="math inline">\((X_t)_{t\ge 0}\)</span> has continuous sample paths, then <span class="math inline">\((X_t)_{t\ge 0}\)</span> is 1-D Brownian.</p>
<h3 id="invariance-properties">2.2 Invariance properties</h3>
<ul>
<li>Reflection <span class="math inline">\(-B_t\)</span></li>
<li>Renewal <span class="math inline">\(W(t) = B(t+a)-B(a)\)</span>.</li>
<li>Markov property: <span class="math inline">\(B(t):0\le t\le a\)</span> independent to <span class="math inline">\(W(t), t\ge 0\)</span>.</li>
<li>Time inversion <span class="math inline">\(W_t=B_{a-t}-B_a\)</span>.</li>
<li>Scaling <span class="math inline">\(B_{ct}\sim c^{\rc 2}B_t\)</span>.</li>
<li>Projective reflection <span class="math inline">\(W(t) = tB(\rc t)\)</span></li>
</ul>
<h3 id="rd">2.3 <span class="math inline">\(\R^d\)</span></h3>
<p><span class="math inline">\(B_t\)</span> is <span class="math inline">\(BM^d\)</span> iff its coordinates are <span class="math inline">\(BM^1\)</span>.</p>
<p><em>Proof</em>. Forward: Show increments are independent—characteristic function factorizes.</p>
<p><span class="math inline">\(Q\)</span>-Brownian motion: <span class="math inline">\(X_t-X_s\sim N(0,(t-s)Q)\)</span>, <span class="math inline">\(s&lt;t\)</span>.</p>
<h2 id="constructions-of-brownian-motion">3 Constructions of Brownian motion</h2>
<h3 id="levy-ciesielski">3.1 Levy-Ciesielski</h3>
<p>Write paths as random series wrt complete orthonormal system of <span class="math inline">\(L^2([0,1],dt)\)</span>. Let <span class="math inline">\((G_n)_{n\ge 0}\)</span> be sequence of <span class="math inline">\(N(0,1)\)</span> variables. <span class="math display">\[
W_N(t):=\sumz n{N-1} G_n\an{\one_{[0,t)}, \phi_n}_{L^2}.
\]</span></p>
<p><strong>Theorem</strong>. <span class="math inline">\(\lim_{N\to \iy} W_N(t)\)</span> is Brownian motion.</p>
<p><em>Proof</em>.</p>
<ul>
<li>Convergence: <span class="math inline">\(\E [W_N(t)^2] \to t\)</span>.</li>
<li>Covariances correct: <span class="math inline">\(\E[(W(t)-W(s))(W(v)-W(u))] = \an{\one_{[s,t)}, \one_{[u,v)}}\)</span>.</li>
<li>Continuity of <span class="math inline">\(t\mapsto W(t,\om)\)</span>: Proof for a specific system: Haar functions <span class="math inline">\(H_n\)</span>.
<ul>
<li>(? Uniform convergence <span class="math inline">\(\liminf_{n,N\to \iy} \ve{W_{2^N}-W_{2^n}}_{L^{\iy}(t)}=0\)</span>. By Arzela-Ascoli (?) there is convergent subsequence.</li>
</ul></li>
</ul>
<!-- * (Invariance - translation is orthonormal transformation?) (not necessary)-->
<h3 id="levys-original-argument">3.2 Levy’s original argument</h3>
<h2 id="the-canonical-model">4 The canonical model</h2>
<p>Identify <span class="math inline">\(\Om\)</span> (in <span class="math inline">\((\Om, A, \Pj)\)</span>) as a subset of <span class="math inline">\((\R^d)^I\)</span>, actually <span class="math inline">\(C_{(0)}\)</span> consisting of continuous <span class="math inline">\(w:[0,\iy)\to \R^d\)</span>, <span class="math inline">\(w(0)=0\)</span>.</p>
<p>Consider the product <span class="math inline">\(\si\)</span>-algebra <span class="math display">\[
B^I(\R^d) = \si\set{\pi_t^{-1}(B)}{B\in B(\R^d), t\in I} = \si\set{\pi_t}{t\in I}.
\]</span> Consider the intersection with <span class="math inline">\(C_{(0)}\)</span>, <span class="math display">\[
C_{(0)}\cap B^T(\R^d) = \si(\pi_t|_{C_{(0)}}:t\in I).
\]</span> <span class="math inline">\(C_{(0)}\)</span> is complete separable metric space with metric of locally uniform convergence <span class="math inline">\(\rh(w,v)=\sumo n{\iy} (1\wedge \sup_{0\le t\le n} |w(t)-v(t)|)2^{-n}\)</span>.</p>
<p>The finite-dimensional distributions uniquely determine <span class="math inline">\(\mu\)</span> (cylinder sets generate). <span class="math inline">\(\mu\)</span> is <strong>Wiener measure</strong>, the space is the <strong>path space</strong>.</p>
<p><strong>Theorem</strong> <span class="math inline">\((C_{(0)}, B(C_{(0)}), \mu)\)</span>: <span class="math inline">\((\pi_t)_{t\ge 0}\)</span> is Brownian motion (<strong>canonical model of Brownian motion</strong>). <!-- *-stable generator? --></p>
<p>Some properties</p>
<ul>
<li><span class="math inline">\(\Ga\in B^I(\R^d)\)</span> is determined by countably many indices: for every <span class="math inline">\(\Ga\in B^I(\R^d)\)</span> there exists countable <span class="math inline">\(S\sub I\)</span>, such that <span class="math inline">\(f\in (\R^d)^I, w\in \Ga, f|_S=w|_S \implies f\in \Ga\)</span>. <em>Proof</em>. <span class="math inline">\(\set{\Ga \subeq (\R^d)^I}{\pat{holds for some countable }S}\)</span> is a <span class="math inline">\(\si\)</span>-algebra.</li>
<li><span class="math inline">\(C_{(0)}\nin B^{[0,\iy)}\nin B^I (\R^d)\)</span>. Proof: can’t enforce continuity using values at countably many points. (What is the point of this? Saying we can’t work in <span class="math inline">\(B^I(\R^d)\)</span>, have to work in <span class="math inline">\(\cap C_{(0)}\)</span>?)</li>
</ul>
<h3 id="kolmogorovs-theorem">4.2 Kolmogorov’s Theorem</h3>
<p><strong>Theorem</strong>. Let <span class="math inline">\(I\subeq [0,\iy)\)</span>, <span class="math inline">\(p_{t_1,\ldots, t_n}\)</span> be probability measures defined on <span class="math inline">\((\R^d)^n\)</span>. If the family is consistent (<span class="math inline">\(p_t(C) = p_{t_{\si}} (C_\si)\)</span> and <span class="math inline">\(p_{t_{1:n-1},t_n}(C_{1:n-1}\times \R^d) = p_{t_{1:n-1}} (C_{1:n-1})\)</span>), then there exists <span class="math inline">\(\mu\)</span> on <span class="math inline">\(((\R^d)^I, B^I(\R^d))\)</span>, <span class="math inline">\(p_{t_{1:n}}(C) = \mu(\pi_{t_{1:n}}^{-1}(C))\)</span>.</p>
<p>Corollary: can construct canonical process for any family of consistent finite dimensional probability distributions.</p>
<p>(Still not too clear on it sufficing to consider finite-dim projections…)</p>
<p>Can use this theorem to construct BM. Continuity follows from Theorem 4.11.</p>
<h2 id="brownian-motion-as-a-markov-process">6 Brownian motion as a Markov process</h2>
<p>(I’m confused about what more 6.1 says. Is this related to stopping times? <span class="math inline">\(F_t\)</span> is info up to time <span class="math inline">\(t\)</span>?)</p>
<p>Let <span class="math display">\[
\Pj^x(B_{t_i}\in A_i:1\le i\le n) = \Pj(B_{t_i}+x\in A_i:1\le i\le n).
\]</span></p>
<h2 id="brownian-motion-and-transition-semigroups">7 Brownian motion and transition semigroups</h2>
Linear operators: transition semigroup and resolvent
<span class="math display">\[\begin{align}
P_t u(x) &amp;= \E^x u(B_t)\\
U_\al u(x) &amp;= \E^x \ba{\iiy u(B_t)e^{-\al t}\,dt}
\end{align}\]</span>
<p>A semigroup <span class="math inline">\((P_t)_{t\ge 0}\)</span> on a Banach space is family of linear operators <span class="math inline">\(P_t:B\to B, t\ge 0\)</span>, satisfying <span class="math inline">\(P_tP_s=P_{t+s}\)</span>, <span class="math inline">\(P_0=\id\)</span>.</p>
<p>Banach spaces:</p>
<ul>
<li><span class="math inline">\(B_b\)</span>: Borel measurable <span class="math inline">\(\R^d\to \R\)</span> with uniform norm <span class="math inline">\(\ved_{\iy}\)</span>.</li>
<li><span class="math inline">\(C_\iy\)</span>: continuous functions vanishing at infinity with uniform norm <span class="math inline">\(\ved_{\iy}\)</span>.</li>
</ul>
<p>Lemma 7.2. <span class="math inline">\((B_t)_{t\ge 0}\)</span> is uniformly stochastically continuous, <span class="math display">\[ \lim_{t\to 0}\sup_{x\in \R^d} \Pj^x (|B_t-x|&gt;\de)=0\]</span> for all <span class="math inline">\(\de&gt;0\)</span>.</p>
<p>Properties</p>
<ol type="1">
<li>Conservative <span class="math inline">\(P_t1=1\)</span>.</li>
<li>Contraction on <span class="math inline">\(B_b\)</span>: <span class="math inline">\(\ve{P_t u}_\iy \le \ve{u}_{\iy}\)</span>, <span class="math inline">\(u\in B_b\)</span></li>
<li>Positive preserving: <span class="math inline">\(u\ge 0\implies P_tu\ge 0\)</span>, <span class="math inline">\(u\in B_b\)</span>.</li>
<li>Sub-Markovian: <span class="math inline">\(0\le u\le 1\)</span>, <span class="math inline">\(u\in B_b\implies 0\le P_tu\le 1\)</span>.</li>
<li>Feller: <span class="math inline">\(u\in C_\iy\implies P_t u\in C_\iy\)</span>.</li>
<li>Strongly continuous on <span class="math inline">\(C_\iy\)</span>: <span class="math inline">\(u\in C_\iy\implies \lim_{t\to 0}\ve{P_tu-u}_\iy = 0\)</span>.</li>
<li>Strong Feller: <span class="math inline">\(u\in B_b\implies P_tu\in C_b\)</span>.</li>
</ol>
<p>(1-4 is Markov, 2-4 is sub-Markov, 2-6 is Feller, 2-4+7 is strong Feller.)</p>
<p>7.5: <span class="math display">\[\Pj^x(B_{t_i}\in C_i:1\le i\le n) = P_{t_1}[\one_{C_1} P_{t_2-t_1}[\one_{C_2}\cdots ]].\]</span></p>
<p>This gives a way to construct a Markov process from a semigroup of operators. (Apply to indicator functions.) A Feller semigroup has a corresponding Feller process.</p>
<h3 id="the-generator">7.2 The generator</h3>
<p>Motivation: If <span class="math inline">\(\phi:[0,\iy)\to \R\)</span>, additive, <span class="math inline">\(\phi(0)=1\)</span>, then <span class="math inline">\(\phi(t) = e^{at}\)</span>, <span class="math inline">\(a=\ddd t^{+}\phi(t)|_{t=0}\)</span>.</p>
<strong>Definition</strong>. Let <span class="math inline">\((P_t)_{t\ge 0}\)</span> be Feller semigroup on <span class="math inline">\(C_\iy(\R^d)\)</span>. Then
<span class="math display">\[\begin{align}
Au :&amp;= \lim_{t\to 0} \fc{P_t u-u}t\\
D(A) :&amp;= \set{u\in C_\iy(\R^d)}{\exists g\in C_\iy(\R^d), \lim_{t\to 0} \ve{\fc{P_tu-u}t-g}_{\iy}=0}
\end{align}\]</span>
<p>is the infinitesimal generator of <span class="math inline">\((P_t)_{t\ge 0}\)</span>. <span class="math inline">\(A\)</span> is a function <span class="math inline">\(A:D(A)\to C_\iy(\R^d)\)</span>.</p>
<p>Ex. for <span class="math inline">\(BM^d\)</span>, <span class="math inline">\(P_tu(x) = \E^xu(B_t)\)</span>, <span class="math inline">\(A=\rc 2 \De\)</span> on <span class="math inline">\(C_\iy^2(\R^d)\)</span>. Proof: Taylor expansion.</p>
Lemma 7.10. Let <span class="math inline">\(P_t\)</span> be Feller semigroup with generator <span class="math inline">\(A\)</span>. <span class="math inline">\(P_t\)</span> and <span class="math inline">\(\int_0^t P_s\cdot\,ds\)</span> are <span class="math inline">\(D(A)\to D(A)\)</span>.
<span class="math display">\[\begin{align}
\ddd tP_tu &amp;= AP_tu = P_tAu\\
P_t u - u &amp;= A\int_0^t P_su\,ds.
\end{align}\]</span>
<p><em>Proof.</em> Use: <span class="math inline">\(P_t\)</span> is contraction to get <span class="math inline">\(\ddd t^+\)</span>. Use this and strong continuity to get <span class="math inline">\(\ddd t^-\)</span>. Fubini’s Theorem for part 2.</p>
<p>Corollary 7.11. Let <span class="math inline">\((P_t)_{t\ge 0}\)</span> be Feller semigroup with generator <span class="math inline">\(A\)</span>.</p>
<ol type="1">
<li><span class="math inline">\(D(A)\)</span> is dense in <span class="math inline">\(C_\iy(\R^d)\)</span>.</li>
<li><span class="math inline">\(A\)</span> is a closed operator.</li>
<li>If <span class="math inline">\((T_t)_{t\ge 0}\)</span> is also Feller with generator <span class="math inline">\(A\)</span>, <span class="math inline">\(P_t=T_t\)</span>.</li>
</ol>
<p><em>Proof</em>.</p>
<ol type="1">
<li><span class="math inline">\(\ep^{-1}\int_0^\ep P_su\,ds\to u\)</span>. (Decay it a little!)</li>
<li>If <span class="math inline">\(u_n\to u\)</span>, <span class="math inline">\(Au_n\to w\)</span>, then <span class="math inline">\(Au=w\)</span>.</li>
<li><span class="math inline">\(\ddd sP_{t-s} T_su = 0\)</span>.</li>
</ol>
<p><strong>Proposition 7.13</strong>. Let <span class="math inline">\(P_t\)</span> be Feller. Then <span class="math inline">\(\al U_\al\)</span> is conservative, contraction on <span class="math inline">\(B_b\)</span>, positivity preserving, Feller, strongly continuous on <span class="math inline">\(C_\iy\)</span>. Moreover</p>
<ol type="1">
<li><span class="math inline">\((U_\al)_{\al&gt;0}\)</span> is resolvent: <span class="math inline">\(U_\al = (\al \id -A)^{-1}\)</span>.</li>
<li>Resolvent equation <span class="math inline">\(U_\al u - U_\be u = (\be-\al) U_\be U_\al u\)</span>, <span class="math inline">\(u\in B_b\)</span>.</li>
<li>There is 1-to-1 relationship between <span class="math inline">\((P_t)\)</span> and <span class="math inline">\((U_\al)\)</span>.</li>
<li><span class="math inline">\((U_\al)\)</span> is sub-Markovian iff <span class="math inline">\((P_t)\)</span> is sub-Markovian.</li>
</ol>
<p>(Intuition for (1). <span class="math inline">\(\iiy e^{(A-\al I) t}u\,dt = (A-\al I)^{-1}\)</span>.)</p>
<p>Example 7.14. For Brownian motion <span class="math display">\[
U_\al u(x) = \begin{cases}
\int \fc{e^{-\sqrt{2\al}y}{\sqrt{2\al}}u(x+y)\dy,&amp;d=1\\
\int \rc{\pi^{\fc d2}}\pf{\al}{2y^2}^{\fc d4-\rc2} K_{\fc d2-1}(\sqrt{2\al}y) u(x+y)\dy,&amp;d\ge 2.
\end{cases}
\]</span></p>
<p>Determining domain of generator:</p>
<p><strong>Theorem</strong>. If <span class="math inline">\((A', D(A'))\)</span> extends <span class="math inline">\((A,D(A))\)</span>, and for any <span class="math inline">\(u\in D(A)\)</span>, <span class="math inline">\(A'u=u\implies u=0\)</span>&lt; then <span class="math inline">\((A,D(A))=(A', D(A'))\)</span>. (? I don’t get this.)</p>
<h2 id="the-pde-connection">8 The PDE connection</h2>
<blockquote>
<p>For many classical PDE problems probability theory yields concrete representation formulae for the solutions in the form of expected values of a Brownian functional. These formulae can be used to get generalized solutions of PDEs (which require less smoothness of the initial/boundary data or the boundary itself) and they are amenable to Monte-Carlo simulations</p>
</blockquote>
<p><strong>Lemma</strong> 8.1. <span class="math inline">\(f\in D(\De)\)</span>, <span class="math inline">\(u(t,x):=\E^x f(B_t)\)</span>. <span class="math inline">\(u\)</span> is unique bounded solution of heat equation with initial value <span class="math inline">\(f\)</span>.</p>
<p><em>Proof</em>. Laplace transform and IbP. Use uniqueness of resolvent operator.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Tensorflow setup</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/tensorflow.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/tensorflow.html</id>
    <published>2017-03-03T00:00:00Z</published>
    <updated>2017-03-03T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Tensorflow setup</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-03-03 
          , Modified: 2017-03-03 
	</p>
      
       <p>Tags: <a href="/tags/neural%20nets.html">neural nets</a>, <a href="/tags/programming.html">programming</a>, <a href="/tags/tensorflow.html">tensorflow</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#setting-up-tensorflow-project">Setting up tensorflow project</a></li>
 <li><a href="#tensorboard">Tensorboard</a></li>
 <li><a href="#restoring">Restoring</a></li>
 <li><a href="#sessions-and-graphs">Sessions and graphs</a></li>
 <li><a href="#scopes">Scopes</a></li>
 <li><a href="#questions-to-figure-out">Questions to figure out</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="setting-up-tensorflow-project">Setting up tensorflow project</h2>
<pre><code>sudo apt-get install python-pip python-dev python-virtualenv 
virtualenv --system-site-packages ~/tensorflow
source ~/tensorflow/bin/activate # do every time
pip3 install --upgrade tensorflow-gpu</code></pre>
<p>With keras (<a href="http://www.pyimagesearch.com/2016/11/14/installing-keras-with-tensorflow-backend/">tutorial</a>)</p>
<pre><code>virtualenv --system-site-packages ~/keras_tf
source ~/keras_tf/bin/activate # do every time
#mkvirtualenv keras_tf
#workon keras_tf
pip install --upgrade tensorflow-gpu
pip install numpy scipy
pip install scikit-learn
pip install pillow
pip install h5py
pip install keras</code></pre>
<p>To exit, <code>deactivate</code>.</p>
<p>Cleverhans: at <code>holdenl</code></p>
<pre><code>git clone https://github.com/openai/cleverhans
export PYTHONPATH=&quot;/home/optml/holdenl/cleverhans&quot;:$PYTHONPATH # do every time
pip install matplotlib</code></pre>
<p>For cycles, <code>export PYTHONPATH=&quot;/u/holdenl/code/cleverhans&quot;:$PYTHONPATH # do every time</code></p>
<p>Cycles: do every time</p>
<pre><code>export PYTHONPATH=&quot;/u/holdenl/code/cleverhans&quot;:$PYTHONPATH # do every time
source ~/keras_tf/bin/activate # do every time</code></pre>
<h2 id="tensorboard">Tensorboard</h2>
<p>Do <code>ssh</code> linking port 6006:</p>
<pre><code>ssh -t -t -L 6006:localhost:6006 holdenl@portal.cs.princeton.edu &quot;ssh -L 6006:localhost:6006 optml@optml.cs.princeton.edu&quot;</code></pre>
<p>Run tensorboard.</p>
<pre><code>tensorboard --logdir=tf-slim/train/</code></pre>
<h2 id="restoring">Restoring</h2>
<div class="sourceCode"><pre class="sourceCode py"><code class="sourceCode python">restorer <span class="op">=</span> tf.train.Saver()
<span class="cf">with</span> tf.Session() <span class="im">as</span> sess:
	<span class="co"># Restore variables from disk. #tf-slim/</span>
	restorer.restore(sess, <span class="st">&quot;./tf-slim/train/model.ckpt-20000&quot;</span>)
	<span class="bu">print</span>(<span class="st">&quot;Model restored.&quot;</span>)
	<span class="bu">print</span>(<span class="st">&quot;test accuracy </span><span class="sc">%g</span><span class="st">&quot;</span><span class="op">%</span>accuracy.<span class="bu">eval</span>(feed_dict<span class="op">=</span>{
            x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="fl">1.0</span>}, session<span class="op">=</span>sess))</code></pre></div>
<h2 id="sessions-and-graphs">Sessions and graphs</h2>
<p>A graph is just the skeleton. A session is when the graph is actually run.</p>
<p>Two ways to run a session:</p>
<div class="sourceCode"><pre class="sourceCode py"><code class="sourceCode python">sess <span class="op">=</span> tf.Session()
sess.run(tf.global_variables_initializer())
accuracy.<span class="bu">eval</span>(feed_dict<span class="op">=</span>{
	x:batch[<span class="dv">0</span>], y_: batch[<span class="dv">1</span>], keep_prob: <span class="fl">1.0</span>}, session<span class="op">=</span>sess)</code></pre></div>
<p>Second way: (Probably this is preferred because “with” invokes some magic.)</p>
<div class="sourceCode"><pre class="sourceCode py"><code class="sourceCode python">sess <span class="op">=</span> tf.Session()
<span class="cf">with</span> tf.Session() <span class="im">as</span> sess:
	...</code></pre></div>
<h2 id="scopes">Scopes</h2>
<pre><code>with tf.variable_scope(&quot;conv1&quot;):</code></pre>
<h2 id="questions-to-figure-out">Questions to figure out</h2>
<ul>
<li>What CPU/GPU is actually used?</li>
<li>How do virtual environments work?</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Adversarial experiments</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/adversarial_experiments.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/adversarial_experiments.html</id>
    <published>2017-03-02T00:00:00Z</published>
    <updated>2017-03-02T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Adversarial experiments</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-03-02 
          , Modified: 2017-03-02 
	</p>
      
       <p>Tags: <a href="/tags/neural%20nets.html">neural nets</a>, <a href="/tags/uncertainty.html">uncertainty</a>, <a href="/tags/aaml.html">aaml</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#results">Results</a></li>
 <li><a href="#todo">Todo</a><ul>
 <li><a href="#nice-things">Nice things</a></li>
 </ul></li>
 <li><a href="#code-to-run">Code to run</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="results">Results</h2>
<p>Trained for 100 epochs. Accuracy (out of 1)</p>
<table>
<thead>
<tr class="header">
<th>Mixture\Attack</th>
<th>0.3 fgs</th>
<th>0.5 fgs</th>
<th>1.0 fgs (sanity check)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>0.9133</td>
<td>0.8890</td>
<td>0.9862</td>
</tr>
<tr class="even">
<td>100</td>
<td>0.9933</td>
<td>0.9932</td>
<td>0.9922</td>
</tr>
</tbody>
</table>
<h2 id="todo">Todo</h2>
<ul>
<li>Look at which samples are misclassified by which networks: are they the same or different?
<ul>
<li>Jaccard similarity</li>
<li>How about if you train an ensemble together on regular examples?</li>
</ul></li>
<li>Baselines
<ul>
<li>How transferable are adversarial examples between the 100 networks?</li>
<li>How well does majority do against adversarial example against the mixture?</li>
</ul></li>
<li>Does clamping help?</li>
<li>Examine activations of adversarial examples in hidden layer. Do they look different for adversarial examples?</li>
<li>What are the weights given to the networks in the mixture? How does this change over time?</li>
<li>Try training with independent updates, multiplicative weights.
<ul>
<li>(Check YZ’s code.)</li>
<li>Try sleeping, etc. - regularize more strongly so that weights don’t become too small/large.</li>
</ul></li>
<li>Hyperparameter search</li>
<li>Autoencoder idea</li>
<li>Regularization/Lipschitz/wavelet idea</li>
<li>Is pretraining necessary?</li>
<li>Compare 100 networks to 1 network with 100x size. Which does better?</li>
</ul>
<h3 id="nice-things">Nice things</h3>
<ul>
<li>Set up tensorboard to show histograms, real and adversarial images, etc.</li>
<li>Run with many different settings.</li>
<li>Port over to own training loop.</li>
<li>Output accuracies, etc. over time as list.</li>
</ul>
<h2 id="code-to-run">Code to run</h2>
<p>In <code>adversarial/</code>,</p>
<pre><code>. start # do every time to set up virtualenv
# pretrain 100  nets
mkdir pretrain
nohup python train_many_run.py --t=100 &gt; pretrain/log.txt 2&gt;&amp;1 &amp;
# control
mkdir baseline_epochs100_ep0.3 
nohup python mnist_tutorial_tf.py --nb_epochs=100 &gt; baseline_epochs100_ep0.3/log.txt 2&gt;&amp;1 &amp;
# train mixture of 100
mkdir train_mix100_pretrain1_epochs100_ep0.3_reg1
nohup python train_mix.py --t=100 --nb_epochs=100 &gt; train_mix100_pretrain1_epochs100_ep0.3_reg1/log.txt 2&gt;&amp;1 &amp;
# control 0.5
mkdir baseline_epochs100_ep0.5
nohup python mnist_tutorial_tf.py --nb_epochs=100 --epsilon=0.5 &gt; baseline_epochs100_ep0.5/log.txt 2&gt;&amp;1 &amp;
# train mixture of 100 @ 0.5
mkdir train_mix100_pretrain1_epochs100_ep0.5_reg1
nohup python train_mix.py --t=100 --nb_epochs=100 --epsilon=0.5 &gt; train_mix100_pretrain1_epochs100_ep0.5_reg1/log.txt 2&gt;&amp;1 &amp;
# Sanity check
mkdir baseline_epochs100_ep1
nohup python mnist_tutorial_tf.py --nb_epochs=100 --epsilon=1.0 --train_dir=baseline_epochs100_ep1/ &gt; baseline_epochs100_ep1/log.txt 2&gt;&amp;1 &amp;
mkdir train_mix100_pretrain1_epochs100_ep1_reg1
nohup python train_mix.py --t=100 --nb_epochs=100 --epsilon=1.0 &gt; train_mix100_pretrain1_epochs100_ep1_reg1/log.txt 2&gt;&amp;1 &amp;</code></pre>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Neural net learning, Anthony and Bartlett</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/nn_learning.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/nn_learning.html</id>
    <published>2017-02-28T00:00:00Z</published>
    <updated>2017-02-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Neural net learning, Anthony and Bartlett</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-02-28 
          , Modified: 2017-02-28 
	</p>
      
       <p>Tags: <a href="/tags/neural%20nets.html">neural nets</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#classification-with-real-valued-functions">9 Classification with real-valued functions</a><ul>
 <li><a href="#intro">9.1 Intro</a></li>
 <li><a href="#large-margin-classifiers">9.2 Large margin classifiers</a></li>
 </ul></li>
 <li><a href="#covering-numbers-and-uniform-convergence">10 Covering numbers and uniform convergence</a><ul>
 <li><a href="#uniform-convergence">10.3 Uniform convergence</a></li>
 </ul></li>
 <li><a href="#pseudo-dimension-and-fat-shattering-dimension">11 Pseudo-dimension and fat-shattering dimension</a><ul>
 <li><a href="#pseudo-dimension">11.2 Pseudo-dimension</a></li>
 <li><a href="#fat-shattering-dimension">11.3 Fat-shattering dimension</a></li>
 </ul></li>
 <li><a href="#bounding-covering-numbers-with-dimensions">12 Bounding covering numbers with dimensions</a></li>
 <li><a href="#section">13</a></li>
 <li><a href="#dimensions-of-neural-networks">14 Dimensions of neural networks</a><ul>
 <li><a href="#in-terms-of-number-of-parameters">14.3 In terms of number of parameters</a></li>
 <li><a href="#on-fat-shattering-dimensions-independent-of-number-of-parameters">14.4 On Fat-shattering dimensions, independent of number of parameters</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="classification-with-real-valued-functions">9 Classification with real-valued functions</h2>
<h3 id="intro">9.1 Intro</h3>
<ul>
<li>VC dim grows with number of parameters</li>
<li>Training set <span class="math inline">\(\ll\)</span> number of parameters, but still avoids overfitting.</li>
<li>7.1: arbitrarily small modification to activation makes VC-dimension infinite</li>
<li>In theo theory model, learning algorithm is required to perform well for any probability distribution.</li>
<li>Learning model not appropriate for NN learning algorithms. Many learning algorithms take into account the real output.</li>
<li>Take into account the margin</li>
<li>Get bounds on misclassification <span class="math inline">\(\pat{error}\le \pat{estimate of error} + \pat{complexity penalty}\)</span>. Complexity parameter depends on size of parameters in the network.</li>
<li>Interpret: value of the function as an estimate of the probability that the correct label is 1</li>
</ul>
<h3 id="large-margin-classifiers">9.2 Large margin classifiers</h3>
<ul>
<li><span class="math inline">\(Z=X\times \{0,1\}\)</span>. Margin of <span class="math inline">\(f\)</span> on <span class="math inline">\((x,y)\in Z\)</span> is <span class="math inline">\(\sign(y) (f(x)-\rc 2)\)</span>.</li>
<li>Error <span class="math inline">\(err_P^\ga(f) = \Pj(\text{margin}(f(x),y)&lt;\ga)\)</span>.</li>
<li>Classification learning algorithm for real-valued functions: <span class="math inline">\(err_P(L(\ga,z)) &lt; \inf_{g\in F}err_P^\ga(g)+\ep\)</span>.</li>
<li><span class="math inline">\(\wh{err}_z^\ga(f) = \rc m |\set{i}{\text{margin}(f(x_i),y_i)&lt;\ga}|\)</span>.</li>
<li>Chebyshev: if <span class="math inline">\(\ep\)</span> is <span class="math inline">\(L_2\)</span> error, <span class="math inline">\(\wh{err}_z^\ga (f) &lt; \fc{\ep}{(\rc2-\ga)^2}\)</span>.</li>
</ul>
<h2 id="covering-numbers-and-uniform-convergence">10 Covering numbers and uniform convergence</h2>
<ul>
<li><span class="math inline">\(N(\ep, W, d_\iy)\)</span> is the minimum cardinality of a cover of <span class="math inline">\(W\)</span> by <span class="math inline">\(\ep\)</span>-balls in <span class="math inline">\(L^{\iy}\)</span>.</li>
<li>Ex. if <span class="math inline">\(F|_{x_{1:k}}\)</span> is contained in a linear subspace of dimension <span class="math inline">\(d\)</span>, then grows as <span class="math inline">\(\pf k\ep^d\)</span>.</li>
<li><span class="math inline">\(N_\iy(\ep, F, k) =\max_{x\in X^k} N(\ep, F|_x, d_\iy)\)</span>.</li>
</ul>
<h3 id="uniform-convergence">10.3 Uniform convergence</h3>
<p><span class="math display">\[
\Pj^m [\exists f\in F, err_P(f) \ge \wh{err}_z^\ga (f) + \ep] \le 2N_\iy(\fc \ga2, F, 2m) e^{-\fc{\ep^2m}{8}}.
\]</span> (<span class="math inline">\(\max\)</span> here signals that <span class="math inline">\(L^\iy\)</span> is the right covering.)</p>
<p>If covering number grows as <span class="math inline">\(\poly(m)\)</span>, then this approaches 0 exponentially.</p>
<h2 id="pseudo-dimension-and-fat-shattering-dimension">11 Pseudo-dimension and fat-shattering dimension</h2>
<p>Is there a combo measure like the VC dimension to bound covering numbers?</p>
<h3 id="pseudo-dimension">11.2 Pseudo-dimension</h3>
<ul>
<li><span class="math inline">\(F\)</span> is a set of functions <span class="math inline">\(X\to \R\)</span>, <span class="math inline">\(S=\{x_1,\ldots, x_m\}\subeq X\)</span>. <span class="math inline">\(S\)</span> is <strong>pseudoshattered</strong> by <span class="math inline">\(F\)</span> if there are <span class="math inline">\(r_i\)</span>, s.t. for all <span class="math inline">\(b\in \{0,1\}^m\)</span>, there is <span class="math inline">\(f_b\in F\)</span>, <span class="math inline">\(\sign(f_b(x_i)-r_i)=b_i\)</span>. (Achieve all possible above/below combinations.)</li>
<li><strong>Pseudo-dimension</strong> is maximum <span class="math inline">\(|S|\)</span> pseudo-shattered by <span class="math inline">\(F\)</span>.</li>
<li>The pseudodimension of <span class="math inline">\(F\)</span> is the VC dimension of <span class="math inline">\(\set{\sign(f(x)-y)}{f\in F}\)</span>.</li>
<li>Pseudoshattered iff some translate of <span class="math inline">\(F|_x\)</span> intersects all orthants.</li>
<li>Composing with nondecreasing function doesn’t increase.</li>
<li>If <span class="math inline">\(F\)</span> is vector space, <span class="math inline">\(Pdim(F)=\dim(F)\)</span>.</li>
<li><span class="math inline">\(F=\set{w_0+w^Tx}{w\in \R^n}\)</span>: <span class="math inline">\(P\dim(F)=n+1\)</span>.</li>
<li>Poly transformations of degree <span class="math inline">\(\le k\)</span>: <span class="math inline">\(P\dim(F) = \binom{n+k}k\)</span>.</li>
<li>Poly transformations on <span class="math inline">\(\{0,1\}^k\)</span> of degree <span class="math inline">\(\le k\)</span>: <span class="math inline">\(\sumz ik \binom ni\)</span>.</li>
</ul>
<h3 id="fat-shattering-dimension">11.3 Fat-shattering dimension</h3>
<ul>
<li><span class="math inline">\(S\)</span> is <span class="math inline">\(\ga\)</span>-shattered by <span class="math inline">\(F\)</span> if there are <span class="math inline">\(r_i\)</span> such that for each <span class="math inline">\(b\in \{0,1\}^m\)</span>, there is <span class="math inline">\(f_b\in F\)</span>,
<span class="math display">\[\begin{align}
f_b(x_i) &amp; \ge r_i+\ga,&amp;b_i=1\\
f_b(x_i) &amp;\le r_i - \ga,&amp;b_i=0.
\end{align}\]</span></li>
<li><span class="math inline">\(\ga\)</span>-dimension is max cardinality of <span class="math inline">\(S\)</span> that is <span class="math inline">\(\ga\)</span>-shattered by <span class="math inline">\(F\)</span>, <span class="math inline">\(fat_F(\ga)\)</span>.</li>
<li>This is a scale-sensitive dimension.</li>
<li>Ex. <span class="math inline">\(F\)</span> set of functions <span class="math inline">\([0,1]\to [0,1]\)</span> with total variation <span class="math inline">\(\le V\)</span>. <span class="math inline">\(fat_F(\ga) = 1+\ff{V}{2\ga}\)</span>.</li>
<li><span class="math inline">\(fat_F(\ga)\le P\dim(F)\)</span>. <span class="math inline">\(P\dim(F) = \lim_{\ga\to 0} fat_F(\ga)\)</span>.</li>
<li>If closed under scalar multiplication, <span class="math inline">\(fat_F(\ga) = Pdim(F)\)</span>.</li>
</ul>
<h2 id="bounding-covering-numbers-with-dimensions">12 Bounding covering numbers with dimensions</h2>
<ul>
<li><span class="math inline">\(W\subeq A\)</span> is <span class="math inline">\(\ep\)</span>-separated/packing if <span class="math inline">\(\forall x,y\in P\)</span> distinct, <span class="math inline">\(d(x,y)\ge \ep\)</span>.</li>
<li><span class="math inline">\(\ep\)</span>-packing number <span class="math inline">\(M(\ep,W,d)\)</span> max cardinality of <span class="math inline">\(\ep\)</span>-separated subset.</li>
<li>Uniform packing number <span class="math inline">\(M_p(\ep, H, k) = \max_{x\in X^k} M(\ep, H|_x, d_p)\)</span>.</li>
<li><span class="math inline">\(M(2\ep, W, d)\le N(\ep, W, d)\le M(\ep, W,d)\)</span>.</li>
<li>If <span class="math inline">\(F:\{X\to [0,B]\}\)</span>, <span class="math inline">\(N_\iy(\ep, F, m)\le \sumo id \binom mi \pf B\ep^i\le \pf{emB}{\ep d}^d\)</span>.</li>
<li>(Bound covering number by fat-shattering dimension)
<span class="math display">\[\begin{align}
M_\iy(\ep, F, m) &amp;&lt; 2(mb^2)^{\ce{\lg y}}\\
b&amp;=\ff{2B}{\ep}\\
d&amp;=fat_F\pf{\ep}4\\
y&amp;=\sumo id \binom mi b^i.
\end{align}\]</span></li>
<li>Fat-shattering dimension determines in a fairly precise manner the covering numbers of a function class: <span class="math inline">\(F:\{X\to [0,B]\}\)</span>. <span class="math inline">\(m\ge fat_F\pf{\ep}4\ge 1\)</span>.
<span class="math display">\[\begin{align}
\fc{\lg e}8 fat_F(16\ep) \le \lg N_1(\ep, F, m) \le \lg N_\iy(\ep, F, m) \le 3fat_F\pf{\ep}4 \lg\pf{4e Bm}{\ep}^2.
\end{align}\]</span></li>
</ul>
<h2 id="section">13</h2>
<p>Large margin SEM algorithm: gives <span class="math inline">\(\wh{err}_z^\ga (L(\ga, z)) = \min_{f\in F}\wh{err}_z^\ga (f)\)</span>.</p>
<h2 id="dimensions-of-neural-networks">14 Dimensions of neural networks</h2>
<p>We bound the covering numbers and fat-shattering dimensions for net- works that are fully connected between adjacent layers, that have units with a bounded activation function satisfying a Lipschitz constraint, and that have all weights (or all weights in certain layers) constrained to be small.</p>
<p>Two types of bounds. Two notions of complexity: parameters and size of parameters</p>
<h3 id="in-terms-of-number-of-parameters">14.3 In terms of number of parameters</h3>
<ul>
<li><span class="math inline">\(F:\{X\to Y_1\}\)</span>, <span class="math inline">\(G:\{Y_1\to \R\}\)</span>, <span class="math inline">\(G\)</span> is <span class="math inline">\(L\)</span>-lipschitz. Let <span class="math inline">\(d_\iy^\rh(y,z) = \max_{1\le i\le m}\rh(y_i,z_i)\)</span>. Then <span class="math display">\[N_\iy(\ep, G\circ F_1, m)\le \max_{x\in X^m} N\pa{\fc{\ep}{2L}, F_1|_x, d_\iy^\rh} N(\pf \ep2, G, d_{L_\iy}).\]</span></li>
<li>Note finite <span class="math inline">\(L_\iy\)</span> cover is very strong.</li>
<li>Neural network
<ul>
<li><span class="math inline">\(l\)</span> layers</li>
<li><span class="math inline">\(W\)</span> weights</li>
<li>Computaion maps into <span class="math inline">\([-b,b]\)</span></li>
<li>For each unit, <span class="math inline">\(\ve{w}_1\le V\)</span>.</li>
<li>Activation functions <span class="math inline">\(L\)</span>-Lipschitz</li>
</ul></li>
<li><span class="math inline">\(N_\iy (\ep, F, m) \le \pf{4embW(LV)^l}{\ep(LV-1)}^W\)</span>.</li>
</ul>
<h3 id="on-fat-shattering-dimensions-independent-of-number-of-parameters">14.4 On Fat-shattering dimensions, independent of number of parameters</h3>
<p><span class="math inline">\(fat_F(\ga)\)</span> increases with <span class="math inline">\(\rc{\ga}\)</span>, weight bound, and number of layers.</p>
<ul>
<li>Neural network
<ul>
<li><span class="math inline">\(B=b\)</span> bound on magnitude of input values and activation function</li>
<li><span class="math inline">\(l\)</span>-layer</li>
<li><span class="math inline">\(\ve{w}_1\le V\)</span>.</li>
<li><span class="math inline">\(s\)</span> is <span class="math inline">\(L\)</span>-Lipschitz</li>
</ul></li>
<li><span class="math inline">\(l\ge 1, b\ge 1, V\ge \rc{2L}, \ep\le VbL\)</span>. Then <span class="math inline">\(\lg N_2(\ep, F_l,m) \le \rc2 \pf{2b}{\ep}^{2l} (2VL)^{l(l+1)} \lg (2n+2)\)</span>.</li>
<li><span class="math inline">\(fat_{F_l}(\ep) \le 4 \pf{32b}{\ep}^{2l} (2VL)^{l(l+1)} \ln (2n+2)\)</span>.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Conversation with Paul Christiano</title>
    <link href="http://holdenlee.github.io/notebook/posts/cs/ai/control/paul.html" />
    <id>http://holdenlee.github.io/notebook/posts/cs/ai/control/paul.html</id>
    <published>2017-02-28T00:00:00Z</published>
    <updated>2017-02-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Conversation with Paul Christiano</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-02-28 
          , Modified: 2017-02-28 
	</p>
      
       <p>Tags: <a href="/tags/ai%20safety.html">ai safety</a>, <a href="/tags/machine%20learning.html">machine learning</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p>See <a href="alignment_ml.html">alignment</a> and <a href="concrete.html">Concrete</a></p>
<ol type="1">
<li>Overseeing RL systems
<ul>
<li>What to do when verification is hard, ex. plagiarism?
<ul>
<li>Ex. Have an adversarial system which gives you information that would help you discriminate. How do you make sure the optimization is on the same footing?</li>
<li>Concrete problem: GAN where discriminator recognizes and penalizes perfect memorization.
<ul>
<li>Step 1. Give discriminator access to images. Ex. train the discriminator with an attention model over the images.</li>
<li>Step 2. Give discriminator access to generator’s insides. If the generator has a pointer to the image, the generator discovers that.</li>
</ul></li>
</ul></li>
<li>Suppose an AI comes up with a plan to execute in the real world. You lack physical control. How do you make sure the AI doesn’t trick you?</li>
</ul></li>
<li>Semi-supervised learning
<ul>
<li>Find algorithms that work in this setting.</li>
<li>What is the sample complexity? Info-theoretic vs. for efficient algorithms?</li>
<li>Learning human values</li>
</ul></li>
<li>Learning by imitation
<ul>
<li>Inverse reinforcement learning</li>
<li>ELBA:
<ul>
<li>How can level <span class="math inline">\(n\)</span>-agents meditate a dispute among <span class="math inline">\(n+1\)</span>-level agents, possibly given more time? (<span class="math inline">\(n+1\)</span>-level agents have to “make their case”)</li>
<li>Even if <span class="math inline">\(n+1\)</span>-level agents are opaque. (ML method. The logic method is to bake safety into it.)</li>
<li>Presupposes diversity</li>
</ul></li>
</ul></li>
</ol>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>AdaGAN</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/AdaGAN.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/AdaGAN.html</id>
    <published>2017-02-24T00:00:00Z</published>
    <updated>2017-02-24T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>AdaGAN</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-02-24 
          , Modified: 2017-02-24 
	</p>
      
       <p>Tags: <a href="/tags/neural%20nets.html">neural nets</a>, <a href="/tags/GAN.html">GAN</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#adagan-algorithm">AdaGAN algorithm</a></li>
 <li><a href="#question">Question</a></li>
 <li><a href="#ideas">Ideas</a></li>
 <li><a href="#concreter-questions">Concreter questions</a></li>
 <li><a href="#ideas-2">Ideas 2</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>See <a href="GANs.html#adagan-boosting-generative-models">GANs</a>.</p>
<h2 id="adagan-algorithm">AdaGAN algorithm</h2>
<p>Loop:</p>
<ul>
<li><span class="math inline">\(D\leftarrow DGAN(S_N,G_{t-1})\)</span>, <span class="math inline">\(S_N\)</span> is unweighted sample</li>
<li><span class="math inline">\(\la^*\leftarrow \la(\be_t,D)\)</span></li>
<li><span class="math inline">\(W_t^i \leftarrow ...\)</span> update weights</li>
<li><span class="math inline">\(G_t^c=GAN(S_N,W_t)\)</span></li>
<li><span class="math inline">\(G_t = (1-\be_t)G_{t-1} + \be_t G_t^c\)</span>.</li>
</ul>
<p>Here, assume the discriminator is good enough to force to get some multiple closer to distribution.</p>
<h2 id="question">Question</h2>
<p>Can you reanalyze this with a more stable version of GAN, not based on KL divergence, but</p>
<ul>
<li>Wasserstein distance?</li>
<li>Neural net MMD?</li>
</ul>
<h2 id="ideas">Ideas</h2>
<p>From known results: (see <a href="../multiplicative_weights.html">MW</a>)</p>
<ul>
<li>MU for game theory: maintain probability distribution, get best column response.
<ul>
<li>This means maintaining the probability distribution generated by G and doing MU on it. No!</li>
</ul></li>
<li>MU for boosting: keep weights on data points, response is weak learner.
<ul>
<li>Here, response is generator. At end, take mixture of generators, cf. weighted majority of experts.</li>
<li>How to compute the loss? Before, it was: just the probability of WL being wrong. Now: loss is itself a maximization over discriminator. (not some linear function!)</li>
<li>Weird because in the table, it isn’t that the row/columns are the player, but rather data and 1 player; other player is missing.</li>
<li>You can’t fix the discriminator for this step—ex. the part of the data that doesn’t seem to fool this discriminator, there might be another discriminator that isn’t fooled. (You might not want to include the first generator at all???)</li>
</ul></li>
<li>Dense model theorem, regularity theorems
<ul>
<li>Label with <span class="math inline">\((0,x\sim X)\)</span>, <span class="math inline">\((1,x\sim G)\)</span>. But this is the formulation for boosting the discriminator, not the generator.</li>
</ul></li>
</ul>
<p>Can you do better than simply mixture: ex. decide whether to regenerate according to some criterion, like <span class="math inline">\(\mathcal F_k \mathcal T\)</span> for a different class <span class="math inline">\(\mathcal F\)</span>?</p>
<p>Note MU/boosting doesn’t run into RPS problem if you mix/average.</p>
<h2 id="concreter-questions">Concreter questions</h2>
<ol type="1">
<li>Suppose you want to match <span class="math inline">\(f\)</span> using combination of functions in <span class="math inline">\(\mathcal H\)</span>. What is the right formulation of boosting here?
<ul>
<li>Convex combination - this is just projection to a polytope in a subspace.</li>
<li>What are interesting <span class="math inline">\(\mathcal F\)</span> here?</li>
</ul></li>
<li>In MU for GT: if column only plays <span class="math inline">\(\rc2+\ep\)</span> good strategy, do you get anything?</li>
<li>What is the guarantee of mixture of generators?</li>
</ol>
<p>We’re actually training to completion at each step… What does “good” mean? Analogue of “over half can’t be distinguished”? MMD is bounded away from 0.</p>
<h2 id="ideas-2">Ideas 2</h2>
<ol type="1">
<li>Simpler, boolean formulation. At step <span class="math inline">\(t\)</span>, have weights on data points, increase weight if it was successfully separated. (How did they calculate weights in AdaGAN?) At end, let <span class="math inline">\(D\)</span> be best discriminator against uniform. What’s <span class="math inline">\(\E D - \sumo tT \al_t \E D(G_t)\)</span>?</li>
<li>Is there something more sophisticated/suitable than simple mixture? Ex. pick a data point and then try to generate something close to it? (not this actually…) Something like, run generators together and pick the best point from them?</li>
</ol>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Multiplicative weights</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/multiplicative_weights.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/multiplicative_weights.html</id>
    <published>2017-02-24T00:00:00Z</published>
    <updated>2017-02-24T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Multiplicative weights</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-02-24 
          , Modified: 2017-02-24 
	</p>
      
       <p>Tags: <a href="/tags/boosting.html">boosting</a>, <a href="/tags/multiplicative%20weights.html">multiplicative weights</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#basic-algorithm-and-guarantees">Basic algorithm and guarantees</a><ul>
 <li><a href="#problem">Problem</a></li>
 </ul></li>
 <li><a href="#algorithm">Algorithm</a></li>
 <li><a href="#guarantees">Guarantees</a><ul>
 <li><a href="#questions">Questions</a></li>
 </ul></li>
 <li><a href="#applications">Applications</a><ul>
 <li><a href="#classification-with-margin">Classification with margin</a></li>
 <li><a href="#game-theory">Game theory</a></li>
 <li><a href="#boosting">Boosting</a></li>
 </ul></li>
 <li><a href="#bayesian-interpretation">Bayesian interpretation</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>Ref:</p>
<ul>
<li>Arora Hazan Kale</li>
</ul>
<p>See also <a href="optimization/multiplicative_updates.md">Curse and blessing</a></p>
<h2 id="basic-algorithm-and-guarantees">Basic algorithm and guarantees</h2>
<h3 id="problem">Problem</h3>
<ul>
<li>Prediction: <span class="math inline">\(n\)</span> experts make predictions in <span class="math inline">\(\{0,1\}\)</span> at times <span class="math inline">\(1,\ldots, T\)</span>. After seeing predictions <span class="math inline">\(y_{t,1:n}\)</span>, you make a prediction. You want your total number of mistakes to be comparable to that of the best expert.</li>
<li>Losses/gains: At each time, you get some loss for choosing 0 or 1, <span class="math inline">\(l_t(0), l_t(1)\)</span>. (Assume losses are bounded.) You want your total loss (gain) to be comparable to that of the best expert.</li>
</ul>
<h2 id="algorithm">Algorithm</h2>
<ul>
<li>Vanilla: Let <span class="math inline">\(w_{t+1,i} = w_{t,i}\pa{1-\eta m_{t,i}}\)</span>.
<ul>
<li>Deterministic: at time <span class="math inline">\(t\)</span>, take weighted majority of experts with weights <span class="math inline">\(w_{t,i}\)</span>.</li>
<li>Random: at time <span class="math inline">\(t\)</span>, randomly choose <span class="math inline">\(i\)</span> with probability proportional to <span class="math inline">\(w_{t,i}\)</span>.</li>
</ul></li>
<li>Hedge: Let <span class="math inline">\(w_{t+1,i} = w_{t,i} e^{-\eta m_{t,i}}\)</span>.</li>
<li>MWU with restriction: <span class="math inline">\(p^{(t+1)} = \amin_{p\in \mathcal P} KL(p||\wh p^{(t)})\)</span>.</li>
</ul>
<p>For “counting” case, <span class="math inline">\(m_i=1\)</span> iff mistake. For “loss” case, <span class="math inline">\(m_i\)</span> is loss. Assume <span class="math inline">\(\eta\le \rc2\)</span>.</p>
<h2 id="guarantees">Guarantees</h2>
<ol type="1">
<li>Deterministic multiplicative weights attains <span class="math display">\[
M^{(T)} \le 2(1+\eta) m_i^{(T)} + \fc{2\ln n}{\eta}.
\]</span></li>
<li>Probabilistic multiplicative weights (let <span class="math inline">\(p^{(t)}=\fc{w^{(t)}}{\Phi^{(t)}}\)</span>)
<span class="math display">\[\begin{align}
\E M^{(T)} &amp;= \sumo tT m^{(t)}\cdot p^{(t)} \le \sumo iT m_i^{(t)} + \eta\sumo tT |m_i^{(t)}| + \fc{\ln n}{\eta} \\
&amp; \le (1+\eta) m_i^{(t)} + \fc{\ln n}{\eta} 
&amp; \text{if }m_i^{(t)}\ge0.
\end{align}\]</span>
<em>Proof</em>. 2 parts.
<ol type="1">
<li><span class="math inline">\(\Phi^{(t+1)} \le \Phi^{(t)} e^{-\eta m^{(t)}\cdot p^{(t)}}\)</span>. (If costs are large, this decreases a lot. <span class="math inline">\(p_i\)</span> are exactly the slice of the pie occupied by expert <span class="math inline">\(i\)</span>.) Look at pie occupied by all experts.</li>
<li><span class="math inline">\((1-\eta)^{\sum_{\ge 0}m_i(t)} (1+\eta)^{-\sum_{\le 0} m_i(t)}\le \Phi^{(t+1)}\)</span>. Look at slice occupied by <span class="math inline">\(i\)</span>th expert.</li>
</ol></li>
<li>Hedge attains <span class="math display">\[
\sumo tT m^{(t)}\cdot p^{(t)} \le \sumo tT m_i^{(t)} + \eta\sumo tT m_i^{(t)} + \eta\sumo tT (m^{(t)})^2 \cdot p^{(t)} + \fc{\ln n}{\eta}.
\]</span> (Note this bound depends on <span class="math inline">\(p\)</span> on the RHS.</li>
<li>For MWU with restrictions, <span class="math display">\[
\sumo tT m^{(t)} \cdot p^{(t)} \le \sumo tT (m^{(t)} + \eta |m^{(t)}|)\cdot p + \fc{KL(p||p^{(1)})}{\eta}.
\]</span> (The farther away <span class="math inline">\(p\)</span> is from <span class="math inline">\(p^{(1)}\)</span> the more loss you might incur.)</li>
</ol>
<h3 id="questions">Questions</h3>
<p>What if we replace a finite number of experts with an infinite number of experts with finite VC dimension? This is typically not tractable, but can still be analyzed. Is this like the online version of ERM? Bayesian version/version with uncertainty?</p>
<h2 id="applications">Applications</h2>
<h3 id="classification-with-margin">Classification with margin</h3>
Solve
<span class="math display">\[\begin{align}
a_j^T x &amp;\ge 0\\
\one^T x&amp;=1\\
x_i &amp;\ge 0
\end{align}\]</span>
<p>under the promise that there exists <span class="math inline">\(x^*\)</span> with <span class="math inline">\(a_j^Tx^*\ge \ep\)</span>. (More generally, can solve other LP’s with margin.) Let <span class="math inline">\(\rh = \max_j \ve{a_j}_\iy\)</span>.</p>
<p>Ideas:</p>
<ol type="1">
<li>The experts are the coordinates. They do nothing except existing. It is the <em>weighting</em> that is the solution we’re looking for. (In the original formulation, the default interpretations is that the experts are some different algorithms, and we seek an algorithm using them as black boxes that does well. Here, the experts are nothing but points, and we find an algorithm that classifies well—where the “algorithm” is restricted to be in the class of linear classifiers.)
<ul>
<li>The experts are not the points <span class="math inline">\(a_j\)</span>.</li>
</ul></li>
<li>The bulk of this comes from how we define the gains. These gains should be related to the points <span class="math inline">\(a_j\)</span>.</li>
<li>Note the right sign: we should reward a coordinate <span class="math inline">\(i\)</span> if <span class="math inline">\(a_{ij}\)</span> is large, because increasing <span class="math inline">\(x_i\)</span> would increase the dot product more. (cf. gradient - but this is projected back onto the simplex by scaling (what norm is this?)) The gain is <span class="math inline">\({a_{ji}}\)</span>. <!--shouldn't be a /\rh here because that's in $\eta$--></li>
<li>We are not going through the <span class="math inline">\(a_j\)</span> in order. Instead, at each step we pick an <span class="math inline">\(a_j\)</span> where <span class="math inline">\(a_j^Tx^{(t)}&lt;0\)</span>. (! For GAN, pick <span class="math inline">\(D\)</span> that is violated?)</li>
<li>To analyze this, we consider the total number of steps where there is some <span class="math inline">\(j\)</span> such that <span class="math inline">\(a_j^T x^{(t)}&lt;0\)</span>. Let <span class="math inline">\(T\)</span> be the largest step for which this the case.</li>
</ol>
<p>The inequality is <span class="math display">\[
0 &gt; \sumo tT m^{(t)}\cdot x^{(t)} \ge 
\sumo tT m^{(T)} \cdot x^* 
- \eta \fc{\sumo tT \one^T a_{j(t)}}{\rh}
- \fc{\ln n}{\eta} = 
\pa{\fc{\eta}{\rh} - \fc{\eta}{2\rh}}T + \fc{\ln n}{\eta/(2\rh)}.
\]</span> <!-- not 1-\eta formulation --> Solving, get <span class="math inline">\(T = \ce{\fc{4\rh^2\ln n}{\ep^2}}\)</span>.</p>
<p>Notes:</p>
<ol type="1">
<li>Thinking of the difference in margins as regret, this says that we get <span class="math inline">\(O\prc{\sqrt T}\)</span> regret.</li>
<li>We can think of this as a game, <span class="math inline">\(\max_x \min_j a_j^Tx\)</span> over probability vectors. At each step, the “max/column” player chooses a mixed strategy <span class="math inline">\(x\)</span>, and the “min/row” player plays the best response. Best response is not actually necessary, just some choice of <span class="math inline">\(j\)</span> such that <span class="math inline">\(a_j^Tx&lt;0\)</span> (since that’s what we’re aiming for).</li>
<li>This is logarithmic in <span class="math inline">\(n\)</span>. So if <span class="math inline">\(x\)</span> has super-many coordinates, a sparse solution is good enough! We can prove this existentially—that there exists a <span class="math inline">\(O(\rh^2\ln n/\ep^2)\)</span> sparse solution—existentially using sampling from <span class="math inline">\(x^*\)</span> and using Chernoff.</li>
</ol>
<p>This naturally generalizes!</p>
<h3 id="game-theory">Game theory</h3>
We want to solve a zero-sum game approximately: find <span class="math inline">\(\wt q\)</span> (column mixed strategy) and <span class="math inline">\(\wt p\)</span> (row mixed strategy) such that
<span class="math display">\[\begin{align}
\la^* - \ep &amp; \le \min_i e_i^TA\wt q\\
\la^* &amp;= \max_j \wt p A e_j \le \la^* + \ep
\end{align}\]</span>
<p>where <span class="math display">\[
\la^* = \min_p \max_j A(p,j) = \max_q \min_i A(i,q).
\]</span> Note <span class="math inline">\(\ge\)</span> is clear.</p>
<p>Result: Assume all entries of <span class="math inline">\(A\)</span> bounded in absolute value by 1. We can approximate <span class="math inline">\(\la^*\)</span> up to <span class="math inline">\(\ep\)</span> with <span class="math inline">\(O\pf{\ln n}{\ep^2}\)</span> calls to oracle and time <span class="math inline">\(O(n)\)</span> per call.</p>
<p><em>Proof</em>. T use the algorithm, we specify:</p>
<ol type="1">
<li>Experts: row coordinates. (We take row’s POV.)</li>
<li>Losses: column responds with best response <span class="math inline">\(j\)</span> at each step. The loss for <span class="math inline">\(i\)</span> is <span class="math inline">\((Ae_j)_i\)</span>.</li>
</ol>
For all distributions <span class="math inline">\(p\)</span>,
<span class="math display">\[\begin{align}
\sumo tT p^{(t)} Ae_j
&amp;\le\sumo tT p^* A e_{j(t)} + \eta \sumo tT p^* Ae_{j(t)}+ \fc{\ln n}{\eta}\\
&amp;\le \la^* + \eta\ve{A}_{\iy} + \fc{\ln n}{\eta}\\
&amp; \le \la^* T + \ub{2\sqrt{T\ln n}}{\ep}
\end{align}\]</span>
<p>with <span class="math inline">\(\eta = \sfc{\ln n}T\)</span>. Thus can take <span class="math inline">\(T=\ce{\fc{4\ln n}{\ep^2}}\)</span>.</p>
<p>Notes:</p>
<ol type="1">
<li>Suppose we didn’t know the minimax theorem. Then <span class="math inline">\(\la^*\)</span> should be defined as <span class="math inline">\(\min\max\)</span>. This goes through.</li>
<li>Does this give a strategy for the column player? We can let <span class="math inline">\(\wt q = \fc{\set{t}{j(t)=j}}{T}\)</span>. Then we get <span class="math display">\[
\ol p A \wt q \le p A \wt q + \ep.
\]</span> Note we can set <span class="math inline">\(p\)</span> to anything; we don’t need to choose <span class="math inline">\(p^*\)</span>. We have <span class="math display">\[
\la^{*} - \ep \le \ol p A\wt q \le p A \wt q
\]</span> the LHS because <span class="math inline">\(j(t)\)</span> is the best response. Thus we not only found a good <span class="math inline">\(\wt p\)</span>, we also found <span class="math inline">\(\wt q\)</span> which attains <span class="math inline">\(\ge \la^*-\ep\)</span> for any <span class="math inline">\(p\)</span>.</li>
<li>Alas, this does not prove the minimax theorem because the <span class="math inline">\(\ge\)</span> part is clear.</li>
</ol>
<h3 id="boosting">Boosting</h3>
<p>Again specify</p>
<ol type="1">
<li>Experts: experts are now the data points.</li>
<li>Loss: reward the experts (data points) for fooling the algorithm.</li>
</ol>
<p>The role of “algorithm” and “data point” is swapped from the initial interpretation of multiplicative weights as operating on “experts = algorithms” shown “data points = incur losses from mistaks”, and the sign is flipped: we get rewards for fooling the algorithm.</p>
<p>(The “linear classifier with margin” still had “data points” being shown. The data points there were also adversarial in order to get improvement. We were rewarding coordinates for doing well.)</p>
<p>MAKE A TABLE.</p>
<p>If there are <span class="math inline">\(\rc 2+\ga\)</span>-weak learners, then there is a <span class="math inline">\(1-\ep\)</span>-strong learner with <span class="math inline">\(T=\ce{\fc{2}{\ga^2}\ln \prc{\ep}}\)</span>. (To be more precise, talk about weak-learners on somewhat dense distributions…)</p>
<p><em>Proof</em>. Let <span class="math inline">\(m_x=1-|h(x)-c(x)|\)</span>: a data point is penalized for being correctly classified. WL means that at each step we force many mistakes, <span class="math inline">\(\rc2+\ga\)</span> proportion at each time step.</p>
<p>Now comes the tricky part. We want to bound the number of steps until the mistakes (wrt original, uniform distribution) is small. Thus, we want: when we can find a large set <span class="math inline">\(E\)</span> on which the algorithm misclassifies, then this forces <span class="math inline">\(T\)</span> to be small. Using the refined inequality with KL divergence and <span class="math inline">\(p\)</span> the uniform distribution on <span class="math inline">\(E\)</span>, (“counting in 2 ways”, noting that each row of <span class="math inline">\(x\in E\)</span> sums to at most <span class="math inline">\(\fc T2\)</span>) <span class="math display">\[
\pa{\rc 2 + \ga}T \le (1+\eta)\fc{T}{2} + \fc{\ln \pf{n}{|E|}}{\eta}.
\]</span> Note that it’s key that we get a ratio <span class="math inline">\(\fc{n}{|E|}\)</span>. We have <span class="math inline">\(\fc{|E|}{n}\le \ep\)</span>. We get <span class="math inline">\(T&lt; \fc{2}{\ga^2} \ln \prc{\ep}\)</span>, so for larger <span class="math inline">\(T\)</span>, we can’t find such a <span class="math inline">\(E\)</span>. The “majority vote” comes from <span class="math inline">\(\fc T2\)</span>.</p>
<p>(You can also probably penalize differently depending on how the weak learner does, getting different weights, and probably improved bounds depending on performance of weak learners.)</p>
<h2 id="bayesian-interpretation">Bayesian interpretation</h2>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Adversarial examples in neural networks</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/adversarial.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/adversarial.html</id>
    <published>2017-02-21T00:00:00Z</published>
    <updated>2017-02-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Adversarial examples in neural networks</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-02-21 
          , Modified: 2017-02-21 
	</p>
      
       <p>Tags: <a href="/tags/neural%20nets.html">neural nets</a>, <a href="/tags/uncertainty.html">uncertainty</a>, <a href="/tags/aaml.html">aaml</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#introduction">Introduction</a><ul>
 <li><a href="#statement">Statement</a></li>
 <li><a href="#blog-posts">Blog posts</a></li>
 <li><a href="#literature">Literature</a></li>
 <li><a href="#experiments">Experiments</a></li>
 <li><a href="#theory">Theory</a></li>
 </ul></li>
 <li><a href="#lcls17-delving-into-transferable-adversarial-examples">[LCLS17] Delving into Transferable Adversarial Examples</a><ul>
 <li><a href="#experiments-1">Experiments</a></li>
 <li><a href="#ensemble-based-approaches">Ensemble-based approaches</a></li>
 <li><a href="#geometry">Geometry</a></li>
 </ul></li>
 <li><a href="#questions">Questions</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>See <a href="adversarial_experiments.html">my experiments</a>.</p>
<h2 id="introduction">Introduction</h2>
<h3 id="statement">Statement</h3>
<p>Neural networks can be easily fooled—ex. an adversary adding a small amount of noise can change the classification from “dog” to “cat” with high confidence. It can be fooled even by a weak adversary with just black-box access!</p>
<p>Related to making NN’s resistant: Have NN’s give a confidence bound.</p>
<p>Ideas:</p>
<ul>
<li>Use uncertainty quantification from statistics: Fisher information. See personal communication with Jacob S.</li>
<li>Use an ensemble of neural nets. Train an ensemble in parallel, vs. train together against a discriminator.</li>
<li>Sleeping in NN</li>
<li>Use some kind of calibration. I have a suspicion that cross-entropy simply doesn’t generalize because losses are unbounded.</li>
<li>Active learning</li>
<li>Make Lipschitz/other regularization. Give noisy example with the kind of noise you want to be resistant against.</li>
<li>Boosting</li>
</ul>
<h3 id="blog-posts">Blog posts</h3>
<ul>
<li><a href="http://karpathy.github.io/2015/03/30/breaking-convnets/">breaking linear classifiers</a> <a href="http://scrible.com/s/6wE0Q">h</a></li>
<li><a href="http://www.kdnuggets.com/2015/07/deep-learning-adversarial-examples-misconceptions.html">myths</a> <a href="http://scrible.com/s/4wU0Q">h</a></li>
<li><a href="https://openai.com/blog/adversarial-example-research/">OpenAI blog post</a></li>
</ul>
<h3 id="literature">Literature</h3>
<ul>
<li>[SZSB14] Intriguing properties of neural networks <a href="https://arxiv.org/pdf/1312.6199.pdf?not-changed">paper</a></li>
<li>Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.</li>
<li>[NYC15] Deep Neural Networks are Easily Fooled - High Confidence Predictions for Unrecognizable Images</li>
<li></li>
<li></li>
<li>[LCLS17] Delving into Transferable Adversarial Examples and Black-box Attacks</li>
</ul>
<h3 id="experiments">Experiments</h3>
<ul>
<li>Reproduce adversarial examples result on a simple dataset, e.g. MNIST.</li>
<li>Try training an ensemble of NN in parallel and compare to predictions of a single one.</li>
</ul>
<h3 id="theory">Theory</h3>
<ul>
<li>Think in terms of learning theory, VC dimension…</li>
</ul>
<h2 id="lcls17-delving-into-transferable-adversarial-examples">[LCLS17] Delving into Transferable Adversarial Examples</h2>
<ol type="1">
<li>Model, training data, training process, test label set unknown to attacker.</li>
<li>Large dataset (ImageNet)</li>
<li>Do not construct substitute model</li>
</ol>
<p>What is the difference between targeted and non-targeted transferability?</p>
<ol type="1">
<li>Non-targeted: <span class="math inline">\(x^*\approx x\)</span>, <span class="math inline">\(f_\te(x^*)\ne f_\te(x) = y\)</span>. (Constrain <span class="math inline">\(d(x,x^*)\le B\)</span>.)</li>
<li>Targeted: <span class="math inline">\(x^*\approx x\)</span>, <span class="math inline">\(f_\te(x^*)=y^*\)</span>.</li>
</ol>
<p>3 approaches: Suppose <span class="math inline">\(f = \max_i J_\te(x)_i\)</span>, where <span class="math inline">\(J_\te(x)\)</span> is vector of probabilities.</p>
<ol type="1">
<li>Optimization <span class="math inline">\(\amin_{x^*} \la d(x,x^*) - \ell(\one_y, J_\te(x^*))\)</span>. Ex. <span class="math inline">\(\ell(u,v) = \ln (1-u\cdot v)\)</span>.</li>
<li>Fast gradient <span class="math inline">\(x^* \leftarrow \text{clamp}(x+B\sign (\nb_x \ell(\one_y, J_\te(x))))\)</span>.</li>
<li>Fast gradient sign <span class="math inline">\(x^* \leftarrow \text{clamp}\pa{x+B\nv{\nb_x\ell(\one_y, J_\te(x))}}\)</span>.</li>
</ol>
<p>Approaches for targeted: Replace constraint with <span class="math inline">\(f_\te(x^*)=y^*\)</span></p>
<ol type="1">
<li>Optimization <span class="math inline">\(\amin_{x^*} \la d(x,x^*) \redd{+} \redd{\ell'(\one_{y^*}}, J_\te(x^*))\)</span>. Ex. <span class="math inline">\(\ell'(u,v) = \redd{-\sum_i u_i \lg v_i}\)</span>.</li>
<li>Fast gradient <span class="math inline">\(x^* \leftarrow \text{clamp}(x\redd{-}B\sign (\nb_x \redd{\ell'(\one_{y^*}}, J_\te(x))))\)</span>.</li>
<li>Fast gradient sign <span class="math inline">\(x^* \leftarrow \text{clamp}\pa{x\redd{-}B\nv{\nb_x\redd{\ell'(\one_y}, J_\te(x))}}\)</span>.</li>
</ol>
<h3 id="experiments-1">Experiments</h3>
<p>Choose 100 images (ILSVRC2012 dataset) which can be correctly classified by all 5 models.</p>
<p>Non-target transferability: accuracy = percentage of adversarial examples for one model correctly classified for the other. (For NN to be good, want this to be high)</p>
<p>Targeted transferability: matching rate = percentage of adversarial examples classified as target label by other model. (Want this to be low)</p>
<p>Root mean square deviation <span class="math inline">\(d(x^*,x) = \sfc{\sum_i (x_i^*-x_i^2)}{N}\)</span>.</p>
<p>Q: isn’t the optimizer using gradient information? (We can estimate it by sampling though!)</p>
<p>Use small learning rate to generate images with RMSD&lt;2. Actually can set <span class="math inline">\(\la=0\)</span>.</p>
<p>(Accuracy is low. But what is the confidence?)</p>
<ul>
<li>Optimization can mislead the models.</li>
<li>FG cannot fully mislead the models. A potential reason is that, FG can be viewed as approximating the optimization, but is tailored for speed over accuracy.</li>
</ul>
<p>Find the minimal transferable RMSD by linear search.</p>
<p>Note FGS minimizes distortion’s <span class="math inline">\(L_\iy\)</span> norm while FG minimizes <span class="math inline">\(L_2\)</span> norm.</p>
<p>Target labels do not transfer. Fast gradient-based approaches don’t do well because they only search in 1-D subspace.</p>
<h3 id="ensemble-based-approaches">Ensemble-based approaches</h3>
<p>These do better! If an adversarial image remains adversarial for multiple models, it is more likely to transfer to other models. <span class="math display">\[
\amin_{x^*} -\ln \pa{\pa{\sumo ik \al_i J_i(x^*)}\cdot \one_{y^*}} + \la d(x,x^*)
\]</span> For each of the five models, we treat it as the black-box model to attack, and generate adversarial images for the ensemble of the rest four, which is considered as white-box. This attack does well!</p>
<p>Non-targeted adversarial images have almost perfect transferability!</p>
<p>Fast gradient doesn’t work with ensemble.</p>
<h3 id="geometry">Geometry</h3>
<ul>
<li>The gradient directions of different models in our evaluation are almost orthogonal to each other. - this makes sense</li>
<li>Choose 2 orthogonal directions, one being a gradient direction. There are up to 21 different regions
<ul>
<li>Boundaries align well.</li>
<li>Boundary diameter along gradient direction smaller. (Even in the direction of increasing the prediction probability!)</li>
</ul></li>
</ul>
<h2 id="questions">Questions</h2>
<ul>
<li>Can you use adversarial examples to improve training?</li>
<li>What if you try denoising first?</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
