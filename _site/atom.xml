<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2016-09-04T00:00:00Z</updated>
    <entry>
    <title>LDA (Linear discriminant analysis)</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/lda.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/lda.html</id>
    <published>2016-09-04T00:00:00Z</published>
    <updated>2016-09-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>LDA (Linear discriminant analysis)</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-09-04 
          , Modified: 2016-09-04 
	</p>
      
       <p>Tags: <a href="/tags/LDA.html">LDA</a>, <a href="/tags/dimensionality%20reduction.html">dimensionality reduction</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    \begin{align}
\Ga_c &amp;= \rc{|S_c|} \sum_{i\in S_c} \ol X_i\\
\Si_c &amp;= \rc{|S_c|} \sum_{i\in S_c} (\ol X_i - \Ga_c)(\ol X_i - \Ga_c)^T\\
\Phi &amp;= \rc{C} \sumo cC  (\Ga_c-\Ga) (\Ga_c-\Ga)^T\\
\max_{V\in \R^{k\times L}, V^TV=I_L} \fc{\Tr(V^T\Phi V)}{\Tr(V^T \sumo cC \si_c v)}
&amp;= L_1\text{ principal eigenvectors of }(\wt \Phi = \pa{\sumo cC \Si_c}^+ \Phi).
\end{align}

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2016-09-10</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-09-10.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-09-10.html</id>
    <published>2016-09-04T00:00:00Z</published>
    <updated>2016-09-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2016-09-10</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-09-04 
          , Modified: 2016-09-04 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#threads">Threads</a></li>
 <li><a href="#pcanet-a-simple-deep-learning-baseline-for-image-classification">PCANet: A simple deep learning baseline for image classification?</a></li>
 <li><a href="#architecture">Architecture</a><ul>
 <li><a href="#variations">Variations</a></li>
 </ul></li>
 <li><a href="#experiments">Experiments</a></li>
 <li><a href="#questions">Questions</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="threads">Threads</h2>
<p>Continue from <a href="2016-09-03.html">last week</a>.</p>
<h2 id="pcanet-a-simple-deep-learning-baseline-for-image-classification">PCANet: A simple deep learning baseline for image classification?</h2>
<p>Each stage in a CNN consists of 3 layers:</p>
<ul>
<li>convolutional filter bank</li>
<li>nonlinear processing</li>
<li>feature pooling</li>
</ul>
<p>Replace these:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">CNN</th>
<th style="text-align: left;">PCANet</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">convolutional filter bank</td>
<td style="text-align: left;">PCA filter</td>
</tr>
<tr class="even">
<td style="text-align: left;">nonlinear processing</td>
<td style="text-align: left;">binary quantization (hashing)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">feature pooling</td>
<td style="text-align: left;">block-wise histograms of binary codes</td>
</tr>
</tbody>
</table>
<p>Lesson: Deep learning has a lot of hype, but in fact simpler, better theoretically justifiable architectures can do just as well or better! In particular, one weakness of DL is that it depends on parameter tuning expertise and <em>ad hoc</em> tricks.</p>
<p>Examples:</p>
<ul>
<li>ScatNet (wavelet scattering network): filters are wavelet operators, so no learning is needed. (These don’t work so well when there is intra-class variability with illumination change and corruption.)</li>
<li>PCANet cf. OPCA.</li>
</ul>
<p>Motivations:</p>
<ol type="1">
<li>design a simple deep learning network which should be very easy, even trivial, to train and to adapt to different data and tasks.</li>
<li>such a basic network could serve as a good baseline for people to empirically justify the use of more advanced processing components or more sophisticated architectures for their deep learning networks.</li>
</ol>
<h2 id="architecture">Architecture</h2>
<p>Input: <span class="math inline">\(N\)</span> training images <span class="math inline">\(\{I_i\}_{i=1}^N\)</span> of size <span class="math inline">\(m\times n\)</span>.</p>
<ol type="1">
<li><p>Learn PCA: Vectorize all <span class="math inline">\(k_1\times k_2\)</span> patches, mean-center and put them all in a matrix <span class="math inline">\(X\in \R^{k_1k_2\times Nmn}\)</span><a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. Do rank <span class="math inline">\(L_1\)</span> PCA on <span class="math inline">\(X\)</span>. The <span class="math inline">\(l\)</span>th filter (<span class="math inline">\(l\in [L_1]\)</span>) is <span class="math display">\[ W_l^1 := mat_{k_1,k_2}(q_l(XX^T))\]</span> where <span class="math inline">\(mat_{k_1,k_2}\)</span> maps <span class="math inline">\(\R^{k_1k_2}\to \R^{k_1\times k_2}\)</span> and <span class="math inline">\(q_l\)</span> is the <span class="math inline">\(l\)</span> largest eigenvectors.</p>
<p>Now let <span class="math inline">\(I_i^l = I_i * W_l^1\)</span>. (Zero-pad <span class="math inline">\(I_i\)</span> so that <span class="math inline">\(I_i^l\)</span> has the same size.)</p>
You can apply this multiple times. Suppose we apply it twice to get <span class="math inline">\(I_i^{l_1, l_2}\)</span>.</li>
<li>Hashing: Binarize the outputs to get <span class="math display">\[\{b_{l_1,l_2} = H(I_i^{l_1,l_2})\}\]</span> where <span class="math inline">\(H\)</span> is Heaviside function. Let <span class="math display">\[T_i^{l_1} := \sumo{l_2}{L_2} 2^{l_2-1} b_{l_1,l_2}.\]</span> (I.e., treat <span class="math inline">\((b_{l_1,l_2})_{l_2=1}^{L_2}\)</span> categorically.)</li>
<li><p>Histogram: Express histogram with <span class="math inline">\(L_2\)</span> bins as a vector <span class="math inline">\(Bhist(T_i^l)\)</span>. The feature vector of <span class="math inline">\(I_i\)</span> is <span class="math display">\[f_i := \text{map Bhist }[T_i^{1:L_1}]\in \R^{2^{L_2}L_1B}.\]</span></p></li>
</ol>
<p><strong>On many layers</strong>: Note that we DON’T stack by repeating 1-3. Instead, 2-3 happen only once at the end. The stacking happens within 1—doing PCA multiple times.</p>
<p>Note:</p>
<ul>
<li>Nonoverlapping blocks are suitable for faces.</li>
<li>Overlapping blacks are useful for digits, textures, and objects.</li>
<li>Histogram gives some translation invariance (why??).</li>
<li>Model parameters <span class="math inline">\(k_1,k_2,L_1,L_2\)</span>. Ex. <span class="math inline">\(L_1=L_2=8\)</span>.</li>
<li>Two-stage PCANet is good.</li>
<li>PCANet with absolute rectification layer (??) after the first stage doesn’t help.</li>
<li>The overall process is linear. ?? Combining two stages. Two stages works better. Two-stage PCA filters have a low-rank factorization. It has <span class="math inline">\(L_1k_1^2 + L_2k_2^2\)</span> rather than $L_1L_2(k_1+k_2-1)^2 $ variables.</li>
</ul>
<h3 id="variations">Variations</h3>
<ul>
<li>RandNet: use random filters (from standard Gaussian)</li>
<li>LDANet: Use multi-class linear discriminant analysis. (Think: supervised version of PCA.) <a href="../tcs/machine_learning/matrices/lda.html">LDA</a></li>
</ul>
<p>(Does this mean LDANet will fail on things like concentric circles?)</p>
<h2 id="experiments">Experiments</h2>
<p>Face recognition, MNIST.</p>
<p>MNIST: basic (10000-2000-50000), rot, noise bg, image bg, etc.</p>
<h2 id="questions">Questions</h2>
<ul>
<li>What is “block size”? (vs. filter, image size)</li>
<li>How do you classify using PCANet? (ex. do you train a SVM on top?)</li>
<li>How about stacking?
<ul>
<li>An intriguing research direction will then be how to construct a more complicated (say more sophisticated filters possibly with discriminative learning) or deeper (more number of stages) PCANet.</li>
<li>Some preprocessing of pose alignment and scale normalization might be needed for good performance guarantee. The current bottleneck that keeps PCANet from growing deeper (e.g., more than two stages) is that the dimension of the resulted feature would increase exponentially with the number of stages.</li>
</ul></li>
</ul>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Or <span class="math inline">\(N(m-k_1+1)(n-k_1+1)\)</span> if you don’t go past the border.<a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Transductive learning</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/transduction.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/transduction.html</id>
    <published>2016-08-31T00:00:00Z</published>
    <updated>2016-08-31T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Transductive learning</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-31 
          , Modified: 2016-08-31 
	</p>
      
       <p>Tags: <a href="/tags/self-taught%20learning.html">self-taught learning</a>, <a href="/tags/dictionary%20learning.html">dictionary learning</a>, <a href="/tags/sparse%20coding.html">sparse coding</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p>From <a href="https://en.wikipedia.org/wiki/Transduction_(machine_learning)">wikipedia</a></p>
<blockquote>
<p>In logic, statistical inference, and supervised learning, transduction or transductive inference is reasoning from observed, specific (training) cases to specific (test) cases. In contrast, induction is reasoning from observed training cases to general rules, which are then applied to the test cases. The distinction is most interesting in cases where the predictions of the transductive model are not achievable by any inductive model. Note that this is caused by transductive inference on different test sets producing mutually inconsistent predictions.</p>
</blockquote>
<p>Ex. Semi-supervised clustering</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Self-taught learning ([RBLPN07] Self-taught Learning: Transfer Learning from Unlabeled Data)</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/self_taught_learning.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/self_taught_learning.html</id>
    <published>2016-08-31T00:00:00Z</published>
    <updated>2016-08-31T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Self-taught learning ([RBLPN07] Self-taught Learning: Transfer Learning from Unlabeled Data)</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-31 
          , Modified: 2016-08-31 
	</p>
      
       <p>Tags: <a href="/tags/self-taught%20learning.html">self-taught learning</a>, <a href="/tags/dictionary%20learning.html">dictionary learning</a>, <a href="/tags/sparse%20coding.html">sparse coding</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#definition">Definition</a></li>
 <li><a href="#approach">Approach</a></li>
 <li><a href="#discussion">Discussion</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="http://www.andrewng.org/portfolio/self-taught-learning-transfer-learning-from-unlabeled-data/">website</a></p>
<h2 id="definition">Definition</h2>
<p>Unlabeled data (ex. random Internet images, unlimited audio) need not have the same class labels or generative distribution as labeled data. This is different from</p>
<ul>
<li>semi-supervised learning setting (same distribution/classes)</li>
<li>transfer learning (requires labels for both groups)</li>
</ul>
<p>Makes ML easier and cheaper. Much of human learning is believed to be from unlabeled data. (Order of magnitude: <span class="math inline">\(10^{14}\pat{synapses}/10^9s=10^5\)</span>bits/s)</p>
<!-- (? Ex. maybe just overlap in dictionary features?)-->
<h2 id="approach">Approach</h2>
<ol type="1">
<li><p>Use sparse coding to construct higher-level features using unlabeled data. (cf. representation learning)</p>
They use <span class="math inline">\(L^1\)</span> norm regularization. <span class="math inline">\(\min_{A,x, \ve{A_{\cdot j}}\le 1} \sum_i \ve{y^{(i)} - Ax}_2^2 + \be \ve{x}_1\)</span>. Use AM.</li>
<li>For each input, do sparse recovery (same objective, fixing the <span class="math inline">\(A\)</span> now). This is a convex problem.</li>
<li><p>Now apply supervised learning algorithm, e.g. SVM.</p></li>
</ol>
<h2 id="discussion">Discussion</h2>
<p>?? Sparse coding model also suggests a specific specialized similarity function (kernel) for the learned representations. Once the bases b have been learned using unlabeled data, we obtain a complete generative model for the input x. Thus, we can compute the Fisher kernel to measure the similarity between new inputs.</p>
<p>(Disadvantages of PCA: linear and undercomplete)</p>
<p>What about auto-encoders and NMF?</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Nash equilibrium</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/algorithmic_game_theory/nash_equilibrium.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/algorithmic_game_theory/nash_equilibrium.html</id>
    <published>2016-08-31T00:00:00Z</published>
    <updated>2016-08-31T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Nash equilibrium</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-31 
          , Modified: 2016-08-31 
	</p>
      
       <p>Tags: <a href="/tags/game%20theory.html">game theory</a>, <a href="/tags/Nash.html">Nash</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#definitions">Definitions</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="definitions">Definitions</h2>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Tensor decomposition</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/tensors/tensor_decomposition.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/tensors/tensor_decomposition.html</id>
    <published>2016-08-30T00:00:00Z</published>
    <updated>2016-08-30T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Tensor decomposition</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-30 
          , Modified: 2016-08-30 
	</p>
      
       <p>Tags: <a href="/tags/tensors.html">tensors</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#basics">Basics</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="basics">Basics</h2>
<p>See “Tensor methods” in <code>new_thread.pdf</code>.</p>
<p>Review questions:</p>
<ol type="1">
<li>Define tensor and tensor decomposition. When is it unique? Discuss the intractability of tensor problems. Why would we work with them? Give an example where matrix methods fail but tensor methods work because of uniqueness.</li>
<li>What properties of matrix rank fail for tensor rank?</li>
<li>(*) Give Jeinrich’s algorithm. When does it work, and what is its complexity?
<ul>
<li>Suppose <span class="math inline">\(T=\sum^r u_i\ot v_i\ot w_i\)</span>, where <span class="math inline">\(\{u_i\}\)</span> is linearly independent, <span class="math inline">\(\{v_i\}\)</span> is l.i., and every pair in <span class="math inline">\(\{w_i\}\)</span> is l.i. Then this decomposition is unique and there is an efficient algorithm to find it.</li>
<li>For a tensor <span class="math inline">\(T\in V^{\ot 3}\)</span>, view <span class="math inline">\(T_{\bullet}\in V\ot V^*\ot V^*\)</span>, with <span class="math inline">\(\bullet\)</span> corresponding to the last <span class="math inline">\(V^*\)</span>. The slices are sums of rank 1 matrices <span class="math inline">\(u_iv_i^*\)</span> with coefficients given by the projections of <span class="math inline">\(w_i\)</span> onto <span class="math inline">\(a\)</span>.
\begin{align}
T_a &amp;=\sum \an{w_i,a}u_iv_i^* =: UD_aV^*
T_aT_b^+ &amp;= UD_aD_b^+ U^+\\
T_b^+T_a &amp;= VD_b^+D_a V^+
\end{align}
WHP diagonalization recovers <span class="math inline">\(U,V\)</span>.</li>
</ul></li>
<li>Give a perturbation bound for finding eigenvalues and eigenvectors of a matrix. What does the bound depend on, in what way (polynomial?)? Make it quantitative. Use this to give a bound on convergence in tensor decomposition.</li>
<li>What is the Kruskal rank, why is it important?</li>
<li>Describe the phylogenetic tree/HMM setup. How can you solve it with tensor methods?</li>
<li>What about the non-full-rank case? (Give a reduction from noisy parity.)</li>
<li>Give the block stochastic model. When is it information theoretically posible to find the groups (is this exact, or up to some error)? Compare spectral algorithms with tensor methods. (What are the assumptions? Why are tensor methods more flexible?)</li>
<li>(*) Describe and prove the tensor algorithm for the block stochastic model.</li>
<li>(*) Give a tensor algorithm for a pure topic model, and then generalize under latent dirichlet allocation. Why is the Dirichlet distribution particularly nice here? Find the complexity and error in terms of parameters.</li>
<li>(*) Give a tensor algorithm for independent component analysis. There is a dependence on non-Gaussianlity: explain.</li>
<li>Define the cumulants of a distribution. Why are they nice to work with, and how do they help in algorithms?</li>
</ol>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[BKS15] Dictionary Learning and Tensor Decomposition via the Sum-of-Squares Method</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/tensors/BKS15.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/tensors/BKS15.html</id>
    <published>2016-08-30T00:00:00Z</published>
    <updated>2016-08-30T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[BKS15] Dictionary Learning and Tensor Decomposition via the Sum-of-Squares Method</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-30 
          , Modified: 2016-08-30 
	</p>
      
       <p>Tags: <a href="/tags/dictionary%20learning.html">dictionary learning</a>, <a href="/tags/tensor.html">tensor</a>, <a href="/tags/sparse%20coding.html">sparse coding</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#noisy-tensor-decomposition-1">Noisy tensor decomposition 1</a><ul>
 <li><a href="#reduction">Reduction</a></li>
 <li><a href="#algorithm">Algorithm</a></li>
 </ul></li>
 <li><a href="#noisy-tensor-decomposition">Noisy tensor decomposition</a><ul>
 <li><a href="#algorithm-1">Algorithm</a><ul>
 <li><a href="#sampling-pseudo-distributions">Sampling pseudo-distributions</a></li>
 </ul></li>
 <li><a href="#proof">Proof</a></li>
 </ul></li>
 <li><a href="#dictionary-learning">Dictionary learning</a></li>
 <li><a href="#theorem">Theorem</a><ul>
 <li><a href="#algorithm-2">Algorithm</a></li>
 </ul></li>
 <li><a href="#polytime-algorithm">Polytime algorithm</a></li>
 <li><a href="#todo">Todo</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><strong>Theorem</strong>. Given <span class="math inline">\(\ep&gt;0,\si\ge 1, \de&gt;0\)</span> there exists <span class="math inline">\(d\)</span> and a polytime algorithm that given input</p>
<ul>
<li>a <span class="math inline">\(\si\)</span>-dictionary <span class="math inline">\(n\times m\)</span>,</li>
<li><span class="math inline">\((d,\tau=n^{-\de})\)</span>-nice <span class="math inline">\(\{x\}\)</span>,</li>
<li><span class="math inline">\(n^{O(1)}\)</span> samples from <span class="math inline">\(y=Ax\)</span>,</li>
</ul>
<p>outputs with probability <span class="math inline">\(\ge 0.9\)</span> a set <span class="math inline">\(\ep\)</span>-close to columns of <span class="math inline">\(A\)</span>.</p>
<ul>
<li>A <span class="math inline">\(\si\)</span>-dictionary has <span class="math inline">\(AA^T\preceq \si I\)</span> (analytic proxy for overcompleteness <span class="math inline">\(\fc mn\)</span>).</li>
<li>A distribution is <span class="math inline">\((d,\tau)\)</span>-nice if <span class="math inline">\(\E x_i^{d/2}x_j^{d/2}\le \tau\)</span> for all <span class="math inline">\(i\ne j\)</span>, and <span class="math inline">\(\E x^\al=0\)</span> for all <span class="math inline">\(|\al|\le d\)</span>. (Ex. Bernoulli(<span class="math inline">\(\tau\)</span>) times Gaussian with <span class="math inline">\(\E z_i^d = \rc\tau\)</span>.)</li>
</ul>
<p>Note on running time:</p>
<ul>
<li><span class="math inline">\(\ep\)</span>-accuracy with running time depending on <span class="math inline">\(\poly\prc{\ep}\)</span> in the exponent. So this is better for giving an initial solution for local search methods.</li>
</ul>
<h2 id="noisy-tensor-decomposition-1">Noisy tensor decomposition 1</h2>
<p>Given noisy version of <span class="math inline">\(\sumo im \an{a^i, u}^d = \ve{A^Tu}_d^d\)</span>, recover <span class="math inline">\(\{a^1,\ldots, a^m\}\)</span>.</p>
<p><strong>Theorem</strong> (Noisy tensor decomposition). For every <span class="math inline">\(\ep&gt;0,\si\ge 1\)</span> there exist <span class="math inline">\(d,\tau\)</span> such that a probabilistic <span class="math inline">\(n^{O(\ln n)}\)</span>-time agorithm, given <span class="math inline">\(P\)</span> with <span class="math display">\[\ve{A^T u}_d^d - \tau \ve{u}_2^d \preceq P \preceq \ve{A^Tu}_d^d + \tau \ve{u}_2^d,\]</span> outputs with probability 0.9 <span class="math inline">\(S\)</span> that is <span class="math inline">\(\ep\)</span>-close to <span class="math inline">\(\{a^1,\ldots, a^m\}\)</span>. (<span class="math inline">\(\preceq\)</span> means the difference is a sum of squares.)</p>
<p>Note this allows significant noise since we expect <span class="math inline">\(\ve{A^Tu}_d^d\sim mn^{-\fc d2}\ll \tau\)</span>.</p>
<h3 id="reduction">Reduction</h3>
<p>Take <span class="math display">\[ P = \EE_{i} \an{y_i, u}^{2d}.\]</span></p>
<h3 id="algorithm">Algorithm</h3>
<ol type="1">
<li>Use SoS to find degree-<span class="math inline">\(k\)</span> pseudo-distribution <span class="math inline">\(\{u\}\)</span> that maximizes <span class="math inline">\(P(u)\)</span> under <span class="math inline">\(\ve{u}^2=1\)</span>.</li>
<li>Let <span class="math inline">\(W\)</span> be a product of <span class="math inline">\(O(\ln n)\)</span> rndom linear functions.</li>
<li>Output top eigenvector of <span class="math inline">\(M\)</span>, <span class="math inline">\(M_{ij} = \wt \E W(u)^2 u_iu_j\)</span>.</li>
</ol>
<p><strong>SoS Algorithm</strong>: given <span class="math inline">\(\ep&gt;0, k,n,M\)</span>, <span class="math inline">\(P_1,\ldots, P_m\in \R[x_1,\ldots, x_n]\)</span> with coefficients in <span class="math inline">\([0,M]\)</span>, if there exists a degree <span class="math inline">\(k\)</span> pseudo-distribution with <span class="math inline">\(P_i=0,i\in [m]\)</span>, then we can find <span class="math inline">\(u'\)</span> satisfying <span class="math inline">\(P_i\le \ep, P_i\ge -\ep\)</span> for every <span class="math inline">\(i\in [m]\)</span> in time <span class="math inline">\((n\polylog\prc{M}{\ep})^{O(k)}\)</span>.</p>
<p>(Proof: This is a feasibility problem. Use ellipsoid method.)</p>
<p>Steps</p>
<ol type="1">
<li>If <span class="math inline">\(u\)</span> is an actual distribution, the the output is close to a column.</li>
<li>Generalize to pseudo-distributions.</li>
<li>Generalize to getting all columns.</li>
</ol>
<p>Idea: If not correlated, then <span class="math inline">\(P(u)\)</span> is small. The inequalities can be turned into polynomial inequalities provable by SoS.</p>
<h2 id="noisy-tensor-decomposition">Noisy tensor decomposition</h2>
<p><strong>Theorem</strong> (Noisy tensor decomposition, 4.3). There is a probabilistic algorithm that given</p>
<ul>
<li><span class="math inline">\(\ep&gt;0\)</span></li>
<li><span class="math inline">\(\si \ge 1\)</span></li>
<li>degree <span class="math inline">\(d\ge d(\ep,\si) = O\pf{\ln \si}{\ep}\)</span></li>
<li>noise parameter <span class="math inline">\(\tau \le \tau(\ep) = \Om(\ep)\)</span></li>
<li>degree <span class="math inline">\(d\)</span> polynomial <span class="math inline">\(P\in \R[u]\)</span> such that <span class="math display">\[\ve{A^T u}_d^d - \tau \ve{u}_2^d \preceq P \preceq \ve{A^Tu}_d^d + \tau \ve{u}_2^d,\]</span> outputs with high probability <span class="math inline">\(S\)</span> that is <span class="math inline">\(\ep\)</span>-close to <span class="math inline">\(\{a^1,\ldots, a^m\}\)</span>. (<span class="math inline">\(\preceq\)</span> means the difference is a sum of squares.)</li>
</ul>
<h3 id="algorithm-1">Algorithm</h3>
<ol type="1">
<li>Start with <span class="math inline">\(S=\phi\)</span>. Let <span class="math inline">\(\ga = \fc{C}{\ep}\)</span> for <span class="math inline">\(C\)</span> large enough. ?? What’s <span class="math inline">\(k\)</span>?</li>
<li>Loop:
<ol type="1">
<li>Attempt to find a degree <span class="math inline">\(k\)</span> pseudo-distribution <span class="math inline">\(U\)</span> satisfying the constraints <span class="math inline">\(P(U) \ge 1-\tau\)</span>, <span class="math inline">\(\ve{U}_2^2=1\)</span>, <span class="math inline">\(\an{s,U}^2\le 1-\ga\)</span> for every <span class="math inline">\(s\in S\)</span> (to repel it away from vectors we already found). If fail, stop.</li>
<li>Use the “sampling pseudo-distributions” algorithm to obtain a unit vector <span class="math inline">\(c'\)</span> such that
\begin{align}
P(c') &amp; \ge e^{-\ep d} - \tau\\
\ep &amp;:= O(\pf{\tau}{d} + \fc{\ln \si}{d} \fc{\ln m}k)
\end{align}</li>
<li>Add <span class="math inline">\(c'\)</span> to <span class="math inline">\(S\)</span>.</li>
</ol></li>
</ol>
<h4 id="sampling-pseudo-distributions">Sampling pseudo-distributions</h4>
<h3 id="proof">Proof</h3>
<ol type="1">
<li>Existence of <span class="math inline">\(c'\)</span>.
<ul>
<li><span class="math inline">\(U\)</span> exists because any column of <span class="math inline">\(A\)</span> is a solution. (CHECK columns far!!)</li>
<li><span class="math inline">\(U\)</span> is correlated with <span class="math inline">\(A\)</span>: By assumption, for <span class="math inline">\(\ve{U}_2^2=1\)</span>, <span class="math inline">\(P\in \ve{A^T U}_d^d + \tau[-1,1]\)</span>. Thus <span class="math inline">\(P(U)\ge 1-\tau\)</span> implies <span class="math display">\[\ve{A^TU}_d^d \ge 1-2\tau.\]</span></li>
<li><p>If <span class="math inline">\(U\)</span> correlates well with <span class="math inline">\(A\)</span>, then <span class="math inline">\(U\)</span> correlates with some column of <span class="math inline">\(A\)</span>. <!--Any column of $A$ satisfies $\wt \E \an{c,u}^k \ge e^{-\ep' k}$.--></p>
<p><em>Lemma 6.1</em>. Given</p>
<ul>
<li><span class="math inline">\(A\)</span> is <span class="math inline">\(\si\)</span>-overcomplete</li>
<li><span class="math inline">\(u\)</span> is degree <span class="math inline">\(3k\)</span> pd over <span class="math inline">\(\R^n\)</span> satisfying <span class="math inline">\(\ve{A^Tu}_d^d \ge e^{-\de d}, \ve{u}_2^2 = 1\)</span>,</li>
<li><span class="math inline">\(d-2\mid k\)</span> (?? this seems to be necessary. Think of <span class="math inline">\(k\gg d\)</span>.)</li>
</ul>
<p>there exists a column of <span class="math inline">\(A\)</span> such that <span class="math inline">\(\wt \E \an{c,u}^k \ge e^{-\ep k}\)</span>, <span class="math inline">\(\ep=O(\de + \fc{\ln \si}{d} + \fc{\ln m}k)\)</span>.</p>
<ul>
<li>Motivation for proof: First, let’s see how to prove this if <span class="math inline">\(u\)</span> is an actual vector. By averaging, there is a column such that <span class="math display">\[ \ve{A_{\cdot i}^T u}_{k}^k \ge \pf{1-\ep}{m}\implies A_{\cdot i}^Tu \ge \pf{1-\ep}{m}^{\rc k}\approx e^{\fc{\ln m}{k}}.\]</span> (This also shows why we expect <span class="math inline">\(\ln m\)</span> in the exponent.) This averaging argument still works for pseudo-distributions when <span class="math inline">\(d=k\)</span>—the first inequality still holds true, if we have <span class="math inline">\(d=k\)</span>. (The first inequality would no longer imply the second, though.) (?? Why wouldn’t we take <span class="math inline">\(d=k\)</span>?)</li>
<li>?? For some reason, we want to use an inequality not depending on <span class="math inline">\(m\)</span> (depending on <span class="math inline">\(\ve{A^Tu}_2\)</span> is good, because we have control of this using <span class="math inline">\(\si\)</span>), in which case instead of using the inequality <span class="math inline">\(\ve{v}_{\iy} \ge \fc{\ve{v}_d}{m^{\rc d}}\)</span> we use <span class="math inline">\(\ve{v}_{\iy}^{d-2}\ge \fc{\ve{v}_d^d}{\ve{v}_2^2}\)</span>. We need to change this into a SoS inequality, so replace <span class="math inline">\(\iy\)</span> by <span class="math inline">\(k\)</span> for <span class="math inline">\(k\)</span> large, <span class="math display">\[\ve{v}_d^{\fc{dk}{d-2}} \le \ve{v}_k^k (\ve{v}_2^2)^{\fc{k}{d-2}}\]</span> provable by SoS (expand and then use Muirhead/AM-GM).</li>
<li><!-- For the actual proof, we just need to relate the $d$ and $k$ norms. We want to lower-bound the $k$th norm in terms of the $d$th norm. Use
	$$ \ve{v}_k^{d-2} \ve{v}_2^2 \ge \ve{v}_d^d. $$
	(This is true for $k=1$ by expanding, and hence for all $k$.)-->
<em>Proof</em>. <span class="math display">\[
\ve{A^Tu}_k^k \ge \fc{(\ve{A^Tu}_d^d)^{\fc k{d-2}}}{(\ve{A^Tu}_2^2)^{\fc{k}{d-2}}} \ge \fc{e^{-\de d}}{\si^{\fc k{d-2}}}.
\]</span> Now use averaging on <span class="math inline">\(\ve{A^Tu}_k^k\)</span>.</li>
</ul></li>
<li>Here <span class="math inline">\(\de = O\pf{\tau}d\)</span> so we get <span class="math display">\[ \wt E \an{c,u}^k \ge e^{-\ep' k},\quad \ep' = O(\fc{\tau}d + \fc{\ln \si}{d} +\fc{\ln m}{k}).\]</span> This means we set <span class="math inline">\(k=\boxed{\frac{C\ln m}{\ep}}\)</span>. This also means we need <span class="math inline">\(d=\Om(\rc{\ep}\ln \si)\)</span>, <span class="math inline">\(\tau=O(\ep)\)</span>.</li>
<li>Example: if <span class="math inline">\(u\)</span> is the actual distribution <span class="math inline">\(u=A_{\cdot i}\)</span> with probability <span class="math inline">\(\rc{m}\)</span>, then <span class="math inline">\(\wt E\an{c,u}^k = \rc m\)</span>. Hence we can’t do better than <span class="math inline">\(k=\Om(\ln m)\)</span>.</li>
</ul></li>
<li><p>Now we analyze the algorithm to “sample pseudo-distributions”: given a PD that correlates with a vector, find a vector close to it. We prove:</p>
<p><strong>Theorem 5.1</strong>. The “sample pseudo-distributions” algorithm, given</p>
<ul>
<li>even <span class="math inline">\(k\ge 0\)</span></li>
<li>degree <span class="math inline">\(k\)</span> p.d. with <span class="math inline">\(\ve{u}_2^2=1\)</span> and <span class="math inline">\(\wt \E\an{c,u}^k \ge e^{-\ep k}\)</span> (?? I think we want this for <span class="math inline">\(k-2\)</span>.)</li>
</ul>
<p>outputs <span class="math inline">\(c'\in \R^n\)</span> with <span class="math inline">\(\an{c,c'}\ge 1-O(\ep)\)</span> with probability <span class="math inline">\(2^{-\fc{k}{\poly(\ep)}}\)</span>.</p>
<ul>
<li>Here is the idea. We want to compute the tensor decomposition by using SVD (because there aren’t other good ways to compute overcomplete tensor decomposition…). Suppose that <span class="math inline">\(u\)</span> were an actual distribution; since it is correlated with <span class="math inline">\(A\)</span>, it is approximately supported on columns of <span class="math inline">\(A\)</span>. Then <span class="math inline">\(\wt \E uu^T = \sum p_i a_i a_i^T\)</span>. The <span class="math inline">\(a_i\)</span> are not uniquely identifiable from this (that’s the whole problem with SVD, why we want tensors in the first place!). Idea: consider <span class="math inline">\(\wt \E p(u) uu^T = \sum p_i p(a_i) a_ia_i^T\)</span> instead. If <span class="math inline">\(p\)</span> is a function that is large on <span class="math inline">\(a_i\)</span> and small on all other <span class="math inline">\(a_j\)</span>, then we win. Take <span class="math inline">\(p\)</span> to be a product of linear factors. If we put in enough factors, then with good probability the <span class="math inline">\(p(a_i)\)</span> will become farther apart, and one outruns the others, <span class="math inline">\(|p(a_i)|\gg \sqrt m|p(a_i)|\)</span>. Take <span class="math inline">\(p =W^2\)</span> where <span class="math inline">\(W\)</span> is a product of <span class="math inline">\(O(\ln n)\)</span> random linear functions <span class="math inline">\(\an{u,v}, \ve{v}_2=1\)</span>.</li>
<li><p>We are given <span class="math inline">\(c\)</span> such that <span class="math inline">\(\wt E\an{c,u}^k \ge e^{-\de d}\)</span> (high correlation) and we want that <span class="math inline">\(c\)</span> to be a value of <span class="math inline">\(v\)</span> that makes (the quadratic form) <span class="math inline">\(v\mapsto \wt \E W(U) \an{v,U}^2\)</span> large. In fact, what we want is that <!--the value is way bigger than any other singular values; i.e.,--> when normalized so the sum of the eigenvalues is 1, the singular value corresponding to <span class="math inline">\(c\)</span> is close to 1.</p>
There are 3 parts. (a) We need to understand how “renormalization” of pseudo-distributions works. (b) We need to show that for our choice of <span class="math inline">\(W\)</span>, we have with some probability a lower bound for <span class="math inline">\(\wt \E W(U) \an{c,U}^2\)</span>. This is the hardest part. (c) We need to show that <span class="math inline">\(c\)</span> is close to the singular vector (which we output). (I think (c) is in essence a matrix/eigenvector perturbation result, but it’s cloaked in the language of SoS here—unravel!)</li>
<li><strong>Lemma</strong> (reweighting). Let <span class="math inline">\(U\)</span> be a degree <span class="math inline">\(k\)</span> p.d. Then for every SoS polynomial <span class="math inline">\(W\)</span> of degree <span class="math inline">\(d&lt;k\)</span>, <span class="math inline">\(\wt \E W&gt;0\)</span>, there exists a degree <span class="math inline">\((k-d)\)</span> p.d. <span class="math inline">\(U'\)</span> such that for every <span class="math inline">\(\deg P \le k-d\)</span>, <span class="math display">\[\wt E_{U'} P(U') = \rc{\wt \E_U W(U)} \wt \E_{U} W(U)P(U).\]</span> <em>Proof</em>. Just verify the positivity property.</li>
<li><p><strong>Lemma 5.2</strong>. Let <span class="math inline">\(U\)</span> be a degree-<span class="math inline">\(k+2\)</span> pseudo-distribution over <span class="math inline">\(\R^n\)</span> with <span class="math inline">\(\ve{U}_2^2=1\)</span> and such that there exists a unit vector <span class="math inline">\(c\in \R^n\)</span> such that <span class="math inline">\(\wt \E\an{c,u}^k \ge e^{-\ep k}\)</span>. <span class="math inline">\(W=\rc{M^{k/2}} \prod_{i=1}^{k/2} w^{(i)}\)</span> where <span class="math inline">\(w^{(i)}\)</span> are iid draws from <span class="math inline">\(w=\an{\xi,u}^2\)</span>, <span class="math inline">\(\xi\sim N(0,I_n)\)</span>. Then with probability <span class="math inline">\(2^{-O\pf{k}{\poly(\ep)}}\)</span>, <span class="math inline">\(\wt \E\an{c,u}^2W \ge (1-O(\ep))\wt \E W\)</span>.</p>
(?? Discrepancy between <span class="math inline">\(k\)</span> and <span class="math inline">\(k+2\)</span>.)</li>
<li>By reweighting, there exists a degree 2 p.d. <span class="math inline">\(U'\)</span> such that <span class="math display">\[\wt \E_{U'} \an{v,u'}^2 = \rc{\wt \E_U W(U)} \wt\E_{U} W(U) \an{v,U}^2.\]</span> By Lemma 5.2, the value at <span class="math inline">\(c\)</span> is <span class="math inline">\(\ge 1-O(\ep)\)</span>.</li>
<li><em>Proof</em> (Sketch). This is more likely to be true if <span class="math inline">\(\an{c,\xi}\)</span> is large. Let <span class="math inline">\(w=\an{\xi,u}^2\)</span>. Let <span class="math inline">\(\tau_M\)</span> be such that <span class="math inline">\(\E_{\xi_0|\xi_0\ge \tau_M}\xi_0^2 =M\)</span>. Conditioning on <span class="math inline">\(\an{c,\xi}\ge \tau_{M+1}\)</span> and expanding <span class="math inline">\(\xi=\an{c,\xi}c+\xi'\)</span>, find that <span class="math display">\[ \E_{\xi | \an{c,\xi}\ge \tau_{M+1}} w = M\an{c,u}^2 + \ve{U}_2^2.\]</span> Let <span class="math inline">\(\ol W = \E W = \pa{\an{c,U}^2 + \rc M}^{\fc k2}\)</span> (assume <span class="math inline">\(\ve{u}_2^2=1\)</span>). Algebra shows
\begin{align}
\ol W \an{c,U}^2 &amp;\succeq \pa{1-\fc 2M } \ol W - \pa{1-\rc M}^{\fc k2}\\
\EE_W \wt \E W\an{c,u}^2 &amp; \ge (1-O(\ep)) \EE_W \wt \E W.
\end{align}
<p>(Take <span class="math inline">\(M=\rc{\ep}\ln \prc{\ep}\)</span>.) This is almost what we want, except that we conditioned on <span class="math inline">\(\an{c,\xi}\ge \tau_{M+1}\)</span> and that this is an inequality about absolute values.</p>
<p>To get rid of the conditioning, note that the event <span class="math inline">\(\forall i, \an{\xi^{(i)},c}\ge \tau_{M+1}\)</span> has probability <span class="math inline">\(2^{-O(kM^2)}\)</span>.</p>
To get the result with not-too-small probability, rearrange as <span class="math display">\[
O(\ep) \EE_W \wt \E W \ge \EE_W \wt \E W - \EE_W \wt \E W\an{c,U}^2,
\]</span> bound second moments <span class="math inline">\(\E_W(\wt \E W)^2\)</span>, and use
<ul>
<li><strong>Lemma 5.3</strong>. Let <span class="math inline">\(A,B\)</span> be distributions such that <span class="math inline">\(0\le A\le B\)</span>, <span class="math inline">\(\de \in [0,1]\)</span>. If <span class="math inline">\(\E A\le \ep \E B\)</span> and <span class="math inline">\(\E B^2 \le t(\E B)^2\)</span>, then <span class="math inline">\(\Pj(A\le e^\de \ep B) \ge \fc{\de^2}{9t}\)</span>.</li>
</ul></li>
<li><p>(From degree 2 p.d. to a vector)</p>
<p><strong>Lemma 5.4</strong>. Let <span class="math inline">\(c\in \R^n\)</span> be unit and <span class="math inline">\(U\)</span> be a degree-2 p.d. over <span class="math inline">\(\R^n\)</span> satisfying <span class="math inline">\(\ve{U}_2^2=1\)</span>. If <span class="math inline">\(\wt \E\an{c,U}^2 \ge 1-\ep\)</span>, then letting <span class="math inline">\(\xi\)</span> match the first two moments of <span class="math inline">\(U\)</span> and <span class="math inline">\(v=\fc{\xi}{\ve{\xi}_2}\)</span>, we have <span class="math inline">\(\Pj[\an{c,v}^2 \ge 1-2\ep] =\Om(1)\)</span>.</p>
<em>Proof</em>. Again use Lemma 5.3, just check <span class="math inline">\(\E\ve{\xi}_2^t \le O(\E\ve{\xi}_2^2)^2\)</span>.</li>
<li>Summary of parameters in Theorem 5.1.
<ul>
<li>Running time <span class="math inline">\(n^{O(k)}\)</span>: Just to evaluate <span class="math inline">\(W\)</span> on a degree <span class="math inline">\(k\)</span> p.d. takes <span class="math inline">\(n^{O(k)}\)</span> time.</li>
<li>Success probability <span class="math inline">\(2^{-k/\poly(\ep)}\)</span>: <span class="math inline">\(W\)</span> succeeded if each <span class="math inline">\(\an{c,\xi^{(i)}}\)</span> is large, so we get <span class="math inline">\(-k\)</span> in the exponent. (!! This seems like a very crude bound—I expect we can do much better, maybe even subexponential?) <span class="math inline">\(M\)</span> depends polynomially on <span class="math inline">\(\rc{\ep}\)</span>; <span class="math inline">\(\rc{\poly(\ep)}\)</span> is in the exponent because the algorithm relies on sampling to get close enough.</li>
</ul></li>
</ul></li>
<li>Finish.
<ul>
<li>In step 1, from 6.1, we get <span class="math inline">\(\wt \E \an{c,u}^k \ge e^{-\ep' k}\)</span>, <span class="math inline">\(\ep' = O\pa{\fc\tau d+ \fc{\ln \si}d + \fc{\ln m}k}\)</span>.</li>
<li>From 5.1, we get <span class="math inline">\(c'\)</span> close to a column <span class="math inline">\(c\)</span> of <span class="math inline">\(A\)</span>: <span class="math inline">\(\an{c,c'}\ge 1-O(\ep')\)</span> with probability <span class="math inline">\(2^{-\fc{k}{\poly(\ep')}}\)</span>. Repeat <span class="math inline">\(2^{\fc{k}{\poly(\ep')}}\)</span> times to get this with good probability. (<span class="math inline">\(\ep = \Te(\ep')\)</span>.)</li>
<li>(Far from other vectors already in <span class="math inline">\(S\)</span>) We have <span class="math inline">\(P(c') \ge c^Tc' - \tau\ve{c'}_2^2 \ge e^{-\ep d}-\tau\)</span>. Calculation using the triangle inequality on (after doing <span class="math inline">\(\bullet^{\ot 2}\)</span> because the inequality we have is <span class="math inline">\(\an{s,U}^2\le 1-\ga\)</span>) gives <span class="math inline">\(\an{c',s}\le 1-\fc{\ga}{10}\)</span>. (We chose <span class="math inline">\(\ga = C\ep\)</span>.) <img src="/images/bks15-tri.png"></li>
<li>(Every <span class="math inline">\(s\in S\)</span> is close to some column.) Use 6.1 on <span class="math inline">\(s\)</span> (which is an actual vector! not a p.d.), <span class="math inline">\(\ve{A^T s}_d^d \ge e^{-\ep d}-2\tau\)</span> to get <span class="math inline">\(\an{s,c}^2 \ge 1-O(\ep)\)</span>.</li>
<li>(Can’t have 2 <span class="math inline">\(s\in S\)</span> close to same <span class="math inline">\(c\)</span>) More triangle inequalities.</li>
<li>The algorithm then terminates after <span class="math inline">\(|S|=m\)</span>. The total time is <span class="math inline">\(n^{O(k)/\poly(\ep)} = n^{\fc{\ln m}{\poly(\ep)}}\)</span>. There’s an extra <span class="math inline">\(d\)</span> in the exponent because accessing <span class="math inline">\(P\)</span> takes <span class="math inline">\(n^d\)</span> time. (CHECK!!)</li>
</ul></li>
</ol>
<h2 id="dictionary-learning">Dictionary learning</h2>
<h2 id="theorem">Theorem</h2>
<p>The algorithm below, given</p>
<ul>
<li><span class="math inline">\(\ep&gt;0\)</span></li>
<li>overcompleteness <span class="math inline">\(\si\ge 1\)</span></li>
<li><span class="math inline">\(d\ge D:=O\pf{\ln \si}{\ep}\)</span>,</li>
<li><span class="math inline">\(\tau \le (D\si)^{-D}\)</span>. (In 4.2 it says <span class="math inline">\(D^D\)</span>, but I think it should be this.)</li>
<li><span class="math inline">\(\wt O \pf{n^{2d}}{\tau^2}\)</span> samples from <span class="math inline">\(y=Ax\)</span> for <span class="math inline">\((d,\tau)\)</span>-nice <span class="math inline">\(x\)</span>,</li>
</ul>
<p>outputs in time <span class="math inline">\(n^{\fc{d+\ln m}{\ep^{O(1)}}}\)</span> a set of vectors <span class="math inline">\(\ep\)</span>-close to <span class="math inline">\(A\)</span>.</p>
<h3 id="algorithm-2">Algorithm</h3>
<p>Take <span class="math inline">\(\wt O \fc{n^{2d}}{\tau^2}\)</span> samples <span class="math inline">\(x^{(i)}\)</span> and apply noisy tensor decomposition to the polynomial <span class="math display">\[ 
\EE_i x^{(i)} \an{Ax,u}^d
\]</span> where <span class="math inline">\(d\ge D:=O(\ep^{-1}\ln \si)\)</span>.</p>
<p>Note that <span class="math inline">\(\an{Ax,u}^d\)</span> is the polynomial (<span class="math inline">\(n\)</span>-ic form) corresponding to the tensor <span class="math inline">\((Ax)^{\ot d}\)</span>, just as <span class="math inline">\(u^T vv^Tu = \an{u,v}^2\)</span> is the quadratic form corresponding to the matrix <span class="math inline">\(vv^T\)</span>.</p>
<p><em>Proof</em>.</p>
<ol type="1">
<li><p>(Calculate expectation) <em>Lemma 4.5</em>. If the distribution of <span class="math inline">\(x\)</span> is <span class="math inline">\((d,\tau)\)</span>-nice and <span class="math inline">\(A\)</span> is <span class="math inline">\(\si\)</span>-overcomplete, then <span class="math display">\[\EE_x \an{Ax,u}^d \in \ve{A^Tu}_d^d + [0,\tau \si^dd^d \ve{u}_2^d]\]</span> (where inequalities are in the SoS sense).</p>
<em>Proof</em>. Expand the sum as <span class="math inline">\(\sum_\al x_\al (A^Tu)_\al\)</span>. For <span class="math inline">\(\al=[i,\ldots, i]\)</span> we get the term <span class="math inline">\((A^Tu)_i^d\)</span>. For the other terms, <!--each even term $\al\ne [i,\ldots, i]$ we get the term -->
\begin{align}
0 &amp;\le \sum_{\al \in [n]^d,\al \ne [i,\ldots, i] }(\E x_\al) (A^Tu)_\al \\
&amp; \le \tau \sum_{\al \in [n]^d,\al \ne [i,\ldots, i] } (A^Tu)_\al \\
&amp; \le \tau d^d\sum_{\be \in [n]^{d/2},\be \ne [i,\ldots, i] } (A^Tu)_\be\\
&amp; \le \tau d^d\ve{A^Tu}_2^d \le \tau \si^d \ve{u}_2^d.
\end{align}
(The <span class="math inline">\(d^d\)</span> bound is crude; this is not the bottleneck.) Note the lower inequality depends on there only being even nonzero terms.</li>
<li>(Calculate concentration, Proof of 4.2) We use a Chebyshev bound<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. We have <span class="math inline">\(|(A^Tx)_\al|\le 1\)</span>. <strong>I think we need assumption on <span class="math inline">\(\ve{A^Tx}_{\iy}\)</span></strong>. Thus taking <span class="math inline">\(\wt \Om\pf{n^{2d}}{\tau^2}\)</span> samples we get that the coefficients are whp <span class="math inline">\(\fc{\tau}{n^d}\)</span>-close. This means the value at <span class="math inline">\(u\)</span> is <span class="math inline">\(n^d \si^d\fc{\tau}{n^d}\)</span>-close, and we get whp <span class="math display">\[P\in \ve{A^Tu}_d^d + 2\tau \si^d d^d \ve{u}_2^d[-1,1].\]</span></li>
<li><p>Use noisy tensor decomposition. Check parameters. There we needed <span class="math inline">\(d \ge D=O(\rc{\ep}\ln \si)\)</span>. We need <span class="math inline">\(\tau\le \fc{\ep}{\si^DD^D}\)</span>. (<strong>Errata?</strong> The value of <span class="math inline">\(\tau\)</span> is different than in the theorem.)</p></li>
</ol>
<h2 id="polytime-algorithm">Polytime algorithm</h2>
<p>This means polytime for fixed <span class="math inline">\(\ep\)</span>. Note to get <span class="math inline">\(\rc{\ln n}\)</span> closeness (ex. for initialization of an iterative DL algorithm), we need quasipolytime, <span class="math inline">\(n^{O(\ln n)}\)</span>.</p>
<p>The bottleneck is the <span class="math inline">\(\ln m\)</span> in Lemma 6.1, where we used averaging to conclude that if <span class="math inline">\(U\)</span> has large correlation with <span class="math inline">\(A\)</span>, then <span class="math inline">\(U\)</span> has large correlation with a column of <span class="math inline">\(A\)</span>.</p>
<!-- We want to sidestep this! So directly fix a column, and instead of comparing $W$ to -->
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Before</th>
<th style="text-align: left;">Now</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">5.2</td>
<td style="text-align: left;">7.2</td>
</tr>
<tr class="even">
<td style="text-align: left;">5.1</td>
<td style="text-align: left;">7.1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">6.1</td>
<td style="text-align: left;">7.3</td>
</tr>
</tbody>
</table>
<p>To summarize the previous argument, we show our distribution of <span class="math inline">\(W\)</span> satisfies inequalities given in 5.2, and then use reweighting to obtain that <span class="math inline">\(u\)</span> is correlated with some column in 5.1. Now we use the weak lemma 6.1 which recovers that column, with <span class="math inline">\(\ln m\)</span> loss.</p>
<p>If we weaken the conclusion of 6.1, then we can hope that 6.1’, i.e., 7.3, doesn’t have a <span class="math inline">\(\ln m\)</span> in the place we care about. Then we need 5.1’, i.e., 7.1, to work with these weaker conditions. Sparsity allows us to weaken the conditions on correlation, provided we have better bounds information about the distribution <span class="math inline">\(W\)</span> that we choose.</p>
<p>(Not quite: 5.2 was a lemma for 5.1. 7.1 is a lemma for 7.2, we use 7.1+5.1 to get 7.1.)</p>
<p>Specifically, the condition on the p.d. <span class="math inline">\(U\)</span> (degree <span class="math inline">\(2(1+2k)\)</span>) is (7.2) <span class="math display">\[ \wt \E\an{c,U}^{2(1+2k)} \ge e^{-\ep k} \wt \E \an{c,U}^2, \qquad \wt E\an{c,u}^2\ge \tau^k.
\]</span> This is easier to satisfy: the <span class="math inline">\(\rc m\)</span> that causes problems is moved to the second inequality (as long as we have sparsity <span class="math inline">\(\tau^k\le \rc m\)</span> we are OK). The second inequality is true if sparsity is <span class="math inline">\(\tau\)</span> (CHECK!!). <span class="math inline">\(k\)</span> no longer needs a factor <span class="math inline">\(\rc{\ln m}\)</span> in the denominator to satisfy this.</p>
<p>Note 5.2, 5.1 don’t mention the sparsity at all, while 7.2, 7.1 mention <span class="math inline">\(\tau\)</span> which will be related to the sparsity.</p>
<p>Some changes: instead of reducing from degree <span class="math inline">\(k\)</span> to <span class="math inline">\(2\)</span> we reduce from degree <span class="math inline">\(\approx 4k\)</span> to <span class="math inline">\(2k\)</span>. (??)</p>
<p>So how does the algorithm work now, given that our assumption is (7.2) rather than a bound on <span class="math inline">\(\wt \E \an{c,u}\)</span>? 7.1 has the following inequality on <span class="math inline">\(\ol W= \E_WW\)</span>, (7.1): <span class="math display">\[\an{c,U}^{2(1+k)}\preceq \ol W \preceq (\an{c,U}^2 + \tau\ve{U}_2^2)^{1+k}.\]</span> Using this, get an upper bound on <span class="math inline">\(\wt \E \ol W\)</span> and lower bound on <span class="math inline">\(\wt \E \an{c,U}^{k}\)</span> to get a lower bound on <span class="math inline">\(\wt\E \ol W\an{c,U}^{2k}\)</span>: <span class="math display">\[\wt \E \ol W\an{c,U}^{2k} \wt \E \ol W \ge [\wt \E(\ol W \an{c,U}^k)]^2.\]</span></p>
<p>The bound on <span class="math inline">\(\ol W\)</span> comes from sparsity. Take <span class="math inline">\(W=\prodo i{k/2-1} \an{y_i=Ax_i,U}^2\)</span>. <span class="math inline">\(\ol W\)</span> does not actually satisfy (7.1); it does after reweighting by <span class="math inline">\(\prodo i{k/2-1} x_{i,1}^2\)</span>. (This is a distribution over polynomials. Take the probability that <span class="math inline">\(\prod \an{Ax_i,U}^2\)</span> is chosen and multiply it by <span class="math inline">\(\prod x_{i,1}^2\)</span> and normalize the probabilities. (Not multiply the polynomial.)) Consider <span class="math inline">\(c\an{Ax,u}^2\)</span> reweighted by <span class="math inline">\(x_i^2\)</span>; expand and use sparsity to get a nice bound on expectations.</p>
<p>This is much better than the previous analysis on <span class="math inline">\(W\)</span>! It actually seems to take into account some kind of concentration…</p>
<p>Final theorem is 7.6.</p>
<h2 id="todo">Todo</h2>
<ul>
<li>Summarize changes to get down to polytime. What’s the key idea?</li>
</ul>
<p>(Q: can you adapt something like this for NMF?)</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Can we do better if we use polynomial concentration?<a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Ellipsoid method</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/ellipsoid.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/ellipsoid.html</id>
    <published>2016-08-30T00:00:00Z</published>
    <updated>2016-08-30T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Ellipsoid method</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-30 
          , Modified: 2016-08-30 
	</p>
      
       <p>Tags: <a href="/tags/linear%20programming.html">linear programming</a>, <a href="/tags/algorithms.html">algorithms</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <ul>
<li><a href="http://ocw.mit.edu/courses/mathematics/18-433-combinatorial-optimization-fall-2003/lecture-notes/l1617.pdf">OCW 18.433</a></li>
<li><a href="http://www.cs.princeton.edu/courses/archive/fall15/cos521/lecnotes/lec17.pdf">Arora’s course</a> (<a href="http://www.cs.princeton.edu/courses/archive/fall15/cos521/">course site</a>)</li>
<li>LSW15 <a href="http://arxiv.org/abs/1508.04874">paper</a></li>
</ul>
<h2 id="ellipsoid-method">Ellipsoid method</h2>
<p>This is a very general <em>polynomial time</em> algorithm for convex optimization. We can use it to solve convex optimization problems that are even too large to write down. <!--faster than gradient descent.--></p>
<ol type="1">
<li>LP: <span class="math inline">\(f(x)=c^T\cdot x\)</span>.</li>
<li>SDP: Infinitely many constraints <span class="math inline">\(a^TXa\ge 0\)</span>.</li>
<li>Held-Karp relaxation for traveling salesman: <span class="math inline">\(\min \sum_{i,j}c_{i,j}X_{i,j}\)</span> under conditions
\begin{align}
0 \le X_{ij} &amp;\le 1\\
\forall S\ne \phi, V, \quad \sum_{i\in S, j\in \ol S} X_{ij} &amp;\ge 2
\end{align}
(Last constraint is subtour elimination constraints. We can find a violation by finding a minimum cut with capacity <span class="math inline">\(&lt;2\)</span>.)</li>
</ol>
<!--
Recall convex optimization. In general there is no succinct description for $K$.
\begin{enumerate}
\item
LP: $f(x)=c^T\cdot x$.
\item
SDP: Think of $a^TXa\ge 0$ as infinitely many linear constraints. 
\item
Held-Karp relaxation for traveling salesman: $\min \sum_{i,j}c_{i,j}X_{i,j}$; for all $\sum_j X_{ij}=2$. 
\fixme{Can't require eigenvalue $>$something?}
To prevent disjoint cycles, for all $S\subeq [n],S\ne \phi,[n]$, $\sum_{i\in S,j\in \ol S} X_{ij}\ge 2$. (Exponentially many constraints. Nevertheless we can solve it!)
\end{enumerate}
We only need to be able to project to the convex body.-->
<p><strong>Separation oracle</strong> for convex <span class="math inline">\(K\)</span>: given <span class="math inline">\(x\)</span>, gives a plane that separates <span class="math inline">\(K\)</span> from <span class="math inline">\(x\)</span> in polynomial time. Think of hyperplane as “feedback” on why <span class="math inline">\(x\nin K\)</span>.</p>
<p><strong>Farkas’s Lemma</strong>: If <span class="math inline">\(K\)</span> is convex, for all <span class="math inline">\(x\nin K\)</span>, there is a hyperplane <span class="math inline">\(a^T\cdot x=b\)</span> such that <span class="math inline">\(K\)</span> lies on one side and <span class="math inline">\(y\)</span> on the other.</p>
<p>Ex. 1. PSD: Compute eigenvalues of <span class="math inline">\(Y\)</span>. Say there is eigenvectors <span class="math inline">\(u\)</span> such that <span class="math inline">\(u^TYu&lt;0\)</span>. Use this hyperplane. 2. Traveling salesman: Cut with <span class="math inline">\(&lt;2\)</span>.</p>
<p>An ellipsoid is <span class="math inline">\((X-a)^TBB^T(X-a)\le 1\)</span> where <span class="math inline">\(B\)</span> is PSD. The ellipsoid algorithm: given: an ellipsoid <span class="math inline">\(\cal E\)</span> containing <span class="math inline">\(K\)</span> and <span class="math inline">\(K\)</span> has a poly-time separation oracle.</p>
<p>To find a point of <span class="math inline">\(K\)</span>, recurse:</p>
<ol type="1">
<li>Is the center <span class="math inline">\(x\)</span> in <span class="math inline">\(K\)</span>? If so, done.</li>
<li>Else, find a separating hyperplane <span class="math inline">\(H\)</span> going through <span class="math inline">\(x\)</span>. Find the smallest ellipsoid containing the half cut by <span class="math inline">\(H\)</span>. (<span class="math inline">\(E_{i+1}=E_i\cap \set{x}{a^Tx\le b}\)</span>) This can be found in poly time with ellipsoids.</li>
</ol>
<p><strong>Theorem</strong>: <span class="math inline">\(\Vol(E_{i+1})\le \pa{1-\rc{2n}}\Vol(E_i)\)</span>.</p>
<p>What we need: 1. Rephrase optimization as feasibility. (Binary search.) 2. Find a “reasonably snug” bounding ellipsoid for <span class="math inline">\(K\)</span>. 3. Implement separation oracle for <span class="math inline">\(K\)</span>. 4. Implement computation to find <span class="math inline">\(E_{i+1}\)</span> given <span class="math inline">\(E_i\)</span> and separation oracle.</p>
<strong>Lemma</strong>. The minimum volume ellipsoid containing <span class="math inline">\(Ell(D,z)\cap \set{x}{a\cdot x\le a\cdot z}\)</span> is <span class="math inline">\(E'=Ell(D',z')\)</span> where
\begin{align}
z' &amp;= z-\rc{n+1} \fc{Da}{\sqrt{a^TDa}}\\
D' &amp;= \fc{n^2}{n^2-1} \pa{D-\fc{2}{n+1}\fc{Daa^TD}{a^TDa}}\\
\fc{\Vol(E')}{\Vol(E)} &amp;\le e^{-\rc{2n+2}}.
\end{align}
<p><em>Proof</em>. It suffices to prove this for <span class="math inline">\(D=I\)</span>, <span class="math inline">\(a=e_1\)</span>. Here <span class="math display">\[ D' = \fc{n^2}{n^2-1} \mattn{1-\fc{2}{n+1}}0{\cdots}0{1}{\cdots}{\vdots}{\vdots}{\ddots}.\]</span> Volume bound follows from determinant calculation.</p>
<p>The number of steps needed is <span class="math inline">\(n\ln \pf{V_1}{V_0}\)</span> where <span class="math inline">\(V_1\)</span> is the volume of the smallest ellipsoid containing the body and <span class="math inline">\(V_0\)</span> is volume of the starting ellipsoid. Ex. If <span class="math inline">\(P\)</span> is a polyhedron, and <span class="math inline">\(\nu\)</span> is the number of bits required to write down any <span class="math inline">\(n\times n\)</span> subset of <span class="math inline">\((A,b)\)</span>, then <span class="math inline">\(\Vol(P)\ge 2^{-2n\nu}\)</span>. (Use Cramer’s rule to get expressions for vertices of <span class="math inline">\(Ax\le b\)</span>.) Then the number of iterations is <span class="math inline">\(O(n^2)\)</span>. ?? Each step takes <span class="math inline">\(O(n^2L)O(mn)\)</span> time (<span class="math inline">\(L\)</span>-bit numbers, check validity of point) for a total of <span class="math inline">\(O(mn^5L)\)</span>.</p>
<blockquote>
<p>How do you find a lion in the Sahara? Split it in half and recurse.</p>
</blockquote>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2016-09-03</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-09-03.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-09-03.html</id>
    <published>2016-08-30T00:00:00Z</published>
    <updated>2016-08-30T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2016-09-03</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-30 
          , Modified: 2016-08-30 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#threads">Threads</a></li>
 <li><a href="#summary">Summary</a></li>
 <li><a href="#conversation-with-arora">Conversation with Arora</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="threads">Threads</h2>
<ul>
<li>Relaxing sparsity assumption: Independent sparsity is an unrealistic assumption. For example, many features tend to co-occur with each other. Re-analyze algorithms that rely on independent sparsity under looser conditions on the distribution.
<ul>
<li>Two ways to loosen the conditions:
<ul>
<li>Drop the condition of independence.
<ul>
<li>A first step is “group sparsity”. See <a href="/posts/tcs/machine_learning/representation.html">representation learning</a> for references.</li>
<li>A next step is to do away with independence entirely. (Certain distributions may be intractable… if so, obtain a hardness result and then add some looser assumption.)</li>
</ul></li>
<li>Drop the condition of sparsity.
<ul>
<li>Instead have some condition on moments/tails so that each vector is well-approximated by a sparse vector, cf. “flatness”, ex. 99% of weight is on <span class="math inline">\(o\prc{\sqrt n}\)</span> of coordinates.</li>
<li>The difference with adding noise is that
<ul>
<li>Noise is added to <span class="math inline">\(x\)</span> (pre-coding) rather than <span class="math inline">\(y\)</span> (post-coding)</li>
<li>Noise can be correlated with the vector, not independent. (if it were independent, there’s not that much difference from adding on the <span class="math inline">\(y\)</span>-side)</li>
</ul></li>
</ul></li>
</ul></li>
<li>Sparse recovery. Ex. basis pursuit.</li>
<li>Dictionary learning
<ul>
<li>Q (<span class="citation" data-cites="Arora">@Arora</span>): How does dictionary learning perform in real life? What is it applied to? Applying it to problems traditionally solved with SVM’s and neural nets, how does it compare? How do AGM14, AGMM15 perform?</li>
<li>Re-analyze AGM14, AGMM15 with group sparsity.</li>
<li>Re-analyze with arbitrary sparselike distribution.</li>
</ul></li>
</ul></li>
<li>Representation learning: general problem
<ul>
<li>The main bottleneck is that the problem “sort-of” reduces to tensor decomposition, but I don’t understand much of how TD performs in theory and in practice.</li>
<li>Read papers on TD (Ge, Anandkumar).</li>
</ul></li>
<li>PMI for images
<ul>
<li>I don’t have any positive results when I build on top of the CKN.
<ul>
<li>May try some alternatives: work with CIFAR instead, do some thresholding thing, do things in middle layer… (but unlikely to work if first thing didn’t work?)</li>
</ul></li>
<li>Do experiments on a more basic level.
<ul>
<li>Try unsupervised multiclass SVM on both the original image and on the CKN features. How does this compare to clustering? (Look at the actual pics.)</li>
<li>Have a programming setup where you can visualize clusters and features.</li>
</ul></li>
<li>DL for images
<ul>
<li>If you do DL, do you obtain the categories? (ex. digits)</li>
<li>If you do DL, is training on top of the DL easier?</li>
<li>How does (H)DL compare to the CKN and other unsupervised methods? Make DL convolutional.</li>
<li>What is the typical sparsity of a trained NN? (cf. dropout?)</li>
</ul></li>
<li>Use DL/weighted SVD with the natural proximity/distance in images. Get “context vectors” for different parts of the image. (Hierarchical fashion?) A global context vector would be the classification?</li>
<li>(images not just DL/SVM because adding something in perpendicular direction wrecks things?)</li>
</ul></li>
</ul>
<p>@Arora: in what way is DL “just” tensor decomposition?</p>
<p>(Is there a relationship between weighted SVD for PMI and DL? It’s not at all clear to me. Somehow neural nets do something “like” DL (does the CKN do this though?) but then we’re trying to see if the features can be SVD’d with PMI. Learning with 2 “layers”:</p>
<ol type="1">
<li>Representation - with NN/DL</li>
<li>Classification - (on top) with SVD/PMI.</li>
</ol>
<p>Also how do hierarchical methods come in?)</p>
<p>Things meriting a further look</p>
<ul>
<li>Traditional learning theory (ex. learning SAT)</li>
<li>Probabilistic view of dictionary learning (LDA, hierarchical DP, etc.) <span class="citation" data-cites="Bianca">@Bianca</span></li>
<li>Pattern theory—how they model images. Traditional image things—wavelets etc.</li>
</ul>
<h2 id="summary">Summary</h2>
<ul>
<li>Reading
<ul>
<li><a href="../tcs/machine_learning/self_taught_learning.html">Self-taught learning</a></li>
<li><a href="../tcs/machine_learning/transduction.html">Transduction</a></li>
<li><a href="../tcs/machine_learning/tensor/BKS15.html">BKS15 Dictionary Learning and Tensor Decomposition via the Sum-of-Squares Method</a></li>
</ul></li>
</ul>
<h2 id="conversation-with-arora">Conversation with Arora</h2>
<ul>
<li>“Show that backprop works.”
<ul>
<li>Assume a generative distribution: <span class="math inline">\(x\)</span> from sparse distribution, observe <span class="math inline">\(y\approx Ax\)</span>, there is a SVM classifier depending on <span class="math inline">\(x\)</span>. Show that backprop for a 2-layer NN works. (<span class="citation" data-cites="Tengyu">@Tengyu</span>)</li>
</ul></li>
<li>PMI: restrict to features that co-occur.</li>
<li>SoS for TD + AM for DL gives <span class="math inline">\(n^{O(\log n)}\)</span> algorithm for sparsity <span class="math inline">\(n^{1-\ep}\)</span>. Check this (don’t need sparsity after initialization).</li>
<li>(Hard) Kernel SVM’s can’t learn DL + classifier (essentially 2-layer NN).</li>
</ul>
<!--1-layer NN is like sparse recovery.-->

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Matrix perturbation</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/linear_algebra/matrix_analysis/perturbation.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/linear_algebra/matrix_analysis/perturbation.html</id>
    <published>2016-08-30T00:00:00Z</published>
    <updated>2016-08-30T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Matrix perturbation</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-30 
          , Modified: 2016-08-30 
	</p>
      
       <p>Tags: <a href="/tags/matrices.html">matrices</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#davis-kahan">Davis-Kahan</a></li>
 <li><a href="#weyls-theorem">Weyl’s Theorem</a></li>
 <li><a href="#wedins-theorem">Wedin’s Theorem</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="davis-kahan">Davis-Kahan</h2>
<p>(Theorem 4.8 <a href="http://ocw.mit.edu/courses/mathematics/18-s997-high-dimensional-statistics-spring-2015/lecture-notes/MIT18_S997S15_Chapter4.pdf">here</a>)</p>
<p>Distance between eigenvectors.</p>
<h2 id="weyls-theorem">Weyl’s Theorem</h2>
<h2 id="wedins-theorem">Wedin’s Theorem</h2>
<p>Let <span class="math inline">\(v_1(A)\)</span> be the top eigenvector of <span class="math inline">\(A\)</span>. If <span class="math inline">\(\de=|\la_1(A)-\la_2(A)|\)</span>, then <span class="math inline">\(\sin(\angle (v_1(A), v_1(A+E)))\le \fc{\ve{E}_2}{\de}\)</span>.</p>
<p>?? What’s the analogue of this for subspaces? Ex. <span class="math inline">\(\la_1,\ldots, \la_c\)</span> large and <span class="math inline">\(\la_{c+1}\)</span> small.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
