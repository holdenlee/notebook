<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2017-04-03T00:00:00Z</updated>
    <entry>
    <title>Inverse RL</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/inverse_rl.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/inverse_rl.html</id>
    <published>2017-04-03T00:00:00Z</published>
    <updated>2017-04-03T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Inverse RL</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-04-03 
          , Modified: 2017-04-03 
	</p>
      
       <p>Tags: <a href="/tags/reinforcement%20learning.html">reinforcement learning</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#nr00-inverse-reinforcement-learning">[NR00] Inverse reinforcement learning</a></li>
 <li><a href="#hdar16-cooperative-inverse-reinforcement-learning">[HDAR16] Cooperative inverse reinforcement learning</a><ul>
 <li><a href="#formalism">Formalism</a></li>
 <li><a href="#simple-approximation-scheme">Simple approximation scheme</a></li>
 <li><a href="#future-work">Future work</a></li>
 </ul></li>
 <li><a href="#other-papers">Other papers</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="nr00-inverse-reinforcement-learning">[NR00] Inverse reinforcement learning</h2>
<p>Compute constraints that characterize set of reward functions so observed behavior maximizes reward. Use max-margin heuristic.</p>
<h2 id="hdar16-cooperative-inverse-reinforcement-learning">[HDAR16] Cooperative inverse reinforcement learning</h2>
<p>Human knows reward function. Robot does not. Robot payoff is human reward.</p>
<p>IRL: <span class="math inline">\(\Pj(u|\te, x_0) \propto e^{U_\te(x_0,u)}\)</span>.</p>
<ul>
<li>Reduction to POMDP and sufficient statistics</li>
<li>Apprenticeship learning and suboptimality of IRL-like solutions (because H can use a suboptimal action to convey more information to R).</li>
</ul>
<p>Desiderata:</p>
<ol type="1">
<li>Leverage action to improve learning.</li>
<li>Human is not uninterested expert, but cooperative teacher.</li>
</ol>
<h3 id="formalism">Formalism</h3>
<ul>
<li><span class="math inline">\(\Te\)</span>: static reward parameters observed by <span class="math inline">\(H\)</span>.</li>
<li><span class="math inline">\(R: S\times A^H\times A^R\times \Te \to \R\)</span>, reward.</li>
<li><span class="math inline">\(\ga\)</span>: discount</li>
</ul>
<p>At time <span class="math inline">\(t\)</span>, observe <span class="math inline">\(s_t\)</span> and select action <span class="math inline">\(a_t^H, a_t^R\)</span>. Both achieve reward <span class="math inline">\(r_t=R(s_t,a_t^H, a_t^R;\te)\)</span>.</p>
<p>Note: decentralized POMDP - compute optimal joint policy is NEXP-complete.</p>
<p>Here, private info is restricted to <span class="math inline">\(\te\)</span>, so reduction to coordination-POMDP does not blow up state space. (<span class="math inline">\(|S_C|=|S||\Te|\)</span>). (State is tuple or world state, reward parameters, and R’s belief.)</p>
<p>Belief about <span class="math inline">\(\te\)</span> is sufficient statistic for optimal behavior. <span class="math inline">\((\pi^{H*},\pi^{R*})\)</span> depends only on current state and R’s belief.</p>
<p>Apprenticeship learning: imitate demonstrations.</p>
<p>ACIRL: 2 phases, human and robot takes turns; then robot acts independently (deployment).</p>
<p>Ex. With linear dependence on <span class="math inline">\(\te\)</span>, in deployment, optimal policy is to maximize reward induced by mean.</p>
<p>DBE (demonstration by expert): greedily maximizes immediate reward. Best response is to compute posterior over <span class="math inline">\(\te\)</span>.</p>
<p>There exist ACIRL games where <span class="math inline">\(br(br(\pi^E))\ne \pi^E\)</span>.</p>
<!--Human objective $U_\te(x_0,u_R,u_H)$.-->
<ul>
<li>? Seems to require human knowing how robot learns. Unrealistic teaching assumption.</li>
<li>? Is reward observed by robot? No.</li>
</ul>
<h3 id="simple-approximation-scheme">Simple approximation scheme</h3>
<p>Suppose reward is <span class="math inline">\(\phi(s)^T\te\)</span>.</p>
<p><span class="math display">\[\tau^H = \amax_\tau \phi(\tau)^T \te - \eta\ve{\phi_\te-\phi(\tau)}^2.\]</span></p>
<p>Optimal <span class="math inline">\(\pi^R\)</span> under DBE tries to match observed feature counts. (<strong>I don’t get this.</strong>.)</p>
<h3 id="future-work">Future work</h3>
<p>Coordination problem.</p>
<h2 id="other-papers">Other papers</h2>
<ul>
<li>[RA07], [Z…08]: <span class="math inline">\(\pi^H\)</span> is noisy expert. Bayesian approach: prior on rewards, vs. prior on reward functions.</li>
<li>[Nat…10] observe cooperating multiple actors.</li>
<li>[Wau…11], [KS15]: infer payoffs from observed behavior in general.</li>
<li>[Fer…14], hidden-goal MDP, goal unobserved. Human as part of environment</li>
<li>[CL12] Teach learner reward for MDP.</li>
<li>[DS13] Motion best communicating agent’s intention.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Confidence in neural nets</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/confidence.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/confidence.html</id>
    <published>2017-04-03T00:00:00Z</published>
    <updated>2017-04-03T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Confidence in neural nets</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-04-03 
          , Modified: 2017-04-03 
	</p>
      
       <p>Tags: <a href="/tags/neural%20net.html">neural net</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#hg17-baseline-for-detecting-misclassified-and-out-of-distribution-examples">[HG17] Baseline for detecting misclassified and out-of-distribution examples</a><ul>
 <li><a href="#abnormality-detection-with-auxiliary-decoders">Abnormality detection with auxiliary decoders</a></li>
 <li><a href="#discussion">Discussion</a></li>
 </ul></li>
 <li><a href="#hg17-early-methods-for-detecting-adversarial-images">[HG17] EARLY METHODS FOR DETECTING ADVERSARIAL IMAGES</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>Related: <a href="adversarial.html">adversarial examples</a>.</p>
<p>From AAML: RBF/conservative classifier for in vs. out-of-distribution examples. <a href="/posts/cs/ai/control/aaml_workshop.html">AAML workshop notes</a></p>
<p>Q: how go get a neural net to keep a confidence bound?</p>
<h2 id="hg17-baseline-for-detecting-misclassified-and-out-of-distribution-examples">[HG17] Baseline for detecting misclassified and out-of-distribution examples</h2>
<p>High-confidence predictions frequently produced by softmaxes. Ex. random Gaussian noise gives 90+% confidence. (Q: what if you do before softmaxes?)</p>
<p>Prediction probability of incorrect/ood examples are lower.</p>
<p>Give tasks to evaluate.</p>
<p>2 problems</p>
<ol type="1">
<li>error and success prediction: Can we predict whether a classifier will make an error on a held-out test example? (Use this to output <span class="math inline">\(\perp\)</span>.) Tradeoff between false negatives and positives.</li>
<li>In/out-of-distribution detection: Predict whether test example is from different distribution.</li>
</ol>
<p>ROC (receiver operating characteristic) shows <span class="math display">\[
\pa{tpr = \fc{tp}{tp+fn}, fpr = \fc{fp}{fp+tn}}.
\]</span> PR (precision-recall) shows <span class="math display">\[
\pa{\text{precision} = \fc{tp}{tp+fp}, \text{recall} = \fc{tp}{tp+fn}}.
\]</span></p>
<ul>
<li>AUROC is prob that a positive example has greater score than negative example. Not great when different base rates.</li>
<li>AUPR (precision-recall)</li>
</ul>
<h3 id="abnormality-detection-with-auxiliary-decoders">Abnormality detection with auxiliary decoders</h3>
<ul>
<li>Train normal classifier and append auxiliary decoder which reconstructs input. (Blue layers)</li>
<li>Train jointly on in-distribution examples. Freeze blue layers. Train red layers (on top) with clean and noised training examples. (Noised are abnormal.)</li>
</ul>
<p>Improves detection.</p>
<h3 id="discussion">Discussion</h3>
<ul>
<li>Baseline beaten by exploiting representations.</li>
<li>Intra-class variance: if distance from example to another is abnormally high, may be out of distribution.</li>
<li>known-unknown vs. unknown-unknown.</li>
</ul>
<h2 id="hg17-early-methods-for-detecting-adversarial-images">[HG17] EARLY METHODS FOR DETECTING ADVERSARIAL IMAGES</h2>
<p>Adversarial images place abnormal emphasis on lower-ranked principal components from PCA.</p>
<p>(Q: can you do this even independent of PCA - just by looking at e.g. wavelet/Fourier coefficients? Also, what if you adversarially keep PCA components low, incorporate weighted norm into adversarial optimization?)</p>
<p>Use variance of PCA coefficients of whitened images to detect. (What is whitening again?)</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2017-04-08</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2017-04-08.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2017-04-08.html</id>
    <published>2017-04-03T00:00:00Z</published>
    <updated>2017-04-03T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2017-04-08</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-04-03 
          , Modified: 2017-04-03 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#adversarial-training-and-anomaly-detection">Adversarial training and anomaly detection</a></li>
 <li><a href="#langevin-monte-carlo">Langevin Monte Carlo</a></li>
 <li><a href="#nn-and-kernels">NN and kernels</a></li>
 <li><a href="#other-topics">Other topics</a></li>
 <li><a href="#rl">RL</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="adversarial-training-and-anomaly-detection">Adversarial training and anomaly detection</h2>
<ul>
<li><a href="../tcs/machine_learning/neural_nets/adversarial_experiments.html">Experiments</a></li>
<li>OOD detection - papers</li>
<li>ICLR paper</li>
<li>Correlation between gradients?</li>
<li>Look at hidden activations.</li>
</ul>
<h2 id="langevin-monte-carlo">Langevin Monte Carlo</h2>
<ul>
<li>Graph algorithms.</li>
<li>Learn background. (OV14, Villani book, Ledoux book.)</li>
<li>Langevin papers.</li>
<li>Extending [AH16].</li>
</ul>
<h2 id="nn-and-kernels">NN and kernels</h2>
<p>See <a href="../tcs/machine_learning/neural_nets/barron_musings.html">musings</a>.</p>
<ul>
<li>Write up summary of NN and kernels.</li>
<li><span class="citation" data-cites="Jason">@Jason</span></li>
<li><span class="citation" data-cites="Manfred">@Manfred</span></li>
</ul>
<h2 id="other-topics">Other topics</h2>
<ul>
<li>Entropy regularizer for SVM</li>
</ul>
<h2 id="rl">RL</h2>
<p>Come up with project. Concrete problems in AI safety?</p>
<ul>
<li>Read CIRL.</li>
<li>Experiment with mixture. (cf. sleeping in NN)</li>
<li>Impact measure ideas/experiment.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>AAML workshop</title>
    <link href="http://holdenlee.github.io/notebook/posts/cs/ai/control/aaml_workshop.html" />
    <id>http://holdenlee.github.io/notebook/posts/cs/ai/control/aaml_workshop.html</id>
    <published>2017-04-01T00:00:00Z</published>
    <updated>2017-04-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>AAML workshop</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-04-01 
          , Modified: 2017-04-01 
	</p>
      
       <p>Tags: <a href="/tags/ai%20safety.html">ai safety</a>, <a href="/tags/machine%20learning.html">machine learning</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#inductive-ambiguity">Inductive ambiguity</a><ul>
 <li><a href="#formalization">Formalization</a></li>
 </ul></li>
 <li><a href="#environment-goals">Environment goals</a><ul>
 <li><a href="#level-1">Level 1</a></li>
 <li><a href="#level-2">Level 2</a></li>
 <li><a href="#philosophy-view">Philosophy view</a></li>
 </ul></li>
 <li><a href="#impact-measure">Impact measure</a></li>
 <li><a href="#learning-utility-function">Learning utility function</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>See</p>
<ul>
<li><a href="alignment_ml.html">Alignment</a></li>
<li><a href="concrete.html">Concrete problems</a></li>
</ul>
<p><a href="https://workflowy.com/#/4296816d1051">Workflowy notes (private)</a></p>
<h2 id="inductive-ambiguity">Inductive ambiguity</h2>
<p>What’s the problem with KWIK?</p>
<ul>
<li>Too restrictive - many things with small VC dimension can take exponentially many samples with KWIK. (In some sense this is necessary, cf. if only one example is 1.)</li>
<li>Requires realizability.</li>
</ul>
<h3 id="formalization">Formalization</h3>
<p>Dimensions to vary</p>
<ul>
<li>Supervised, then test; vs. online/active</li>
<li>How do you measure supervision? KWIK penalized by 1 for each time it asks user. We can e.g. instead be content with decaying rate of asking.</li>
</ul>
<p>Filter class <span class="math inline">\(F\)</span> and hypothesis class <span class="math inline">\(H\)</span>. Suppose there exists <span class="math inline">\(f,h\)</span> such that <span class="math inline">\(fh(x)=y, 1-y, \perp\)</span> with probability <span class="math inline">\(p, 0, 1-p\)</span>. We want to find <span class="math inline">\(\wh f\wh h\)</span> with <span class="math inline">\(\wh f \wh h(x)=y,1-y, \perp\)</span> with probabilities <span class="math inline">\(p+\ep, \ep', 1-p-\ep-\ep'\)</span> with <span class="math inline">\(\ep'\ll \ep\)</span>.</p>
<p>Neural net anomaly detection:</p>
<ul>
<li>RBF</li>
<li>only predict around points seen</li>
<li>[HG17] A BASELINE FOR DETECTING MISCLASSIFIED AND OUT-OF-DISTRIBUTION EXAMPLES IN NEURAL NETWORKS</li>
</ul>
<h2 id="environment-goals">Environment goals</h2>
<p>2-3 levels of problem.</p>
<ol type="1">
<li>Human can evaluate/give accurate feedback.</li>
<li>Human can evaluate, but agent can prevent human from giving feedback.</li>
<li>Human cannot evaluate.</li>
</ol>
<p>(Lump 2 and 3 together.)</p>
<h3 id="level-1">Level 1</h3>
<p>This is easiest for formalize and tackle as a theory problem. Assume there is a distinguishing function (conservative concept?) that excludes all bad outcomes.</p>
<p>Under human distribution of actions, reward given corresponds to value.</p>
<p>But after gaming (ex. realizing human only checks in 1 place to check room is clean), agent leaves human distribution of actions, and inferred/represented reward stops corresponding to value. Either from maintaining multiple hypothesis of reward function, or human feedback, it realizes this.</p>
<p>Going back and forth, can continuously improve. Can it generalize over human pushback?</p>
<p>Alternative: have separate agent/part of agent that acts as predictor - holds model of world, job is to predict, e.g., whether there’s a strawberry.</p>
<p>Question: can’t you just embed the “plagiarism” problem in here?</p>
<p>Maybe the problem considered here is more concrete: There’s a better notion of what a strawberry/plate is than a good story.</p>
<h3 id="level-2">Level 2</h3>
<p>Note that “putting self in simulation” is a relative term. It means “fooling all its sensors.” If it has a world-model, this means the harder task of fooling the world-model. (Think of world-model itself as a sensor system.) (Maybe the world-model asks for proofs?)</p>
<p>Why can’t you just have a world model or adversarial predictor? Problem if there is no good evaluator.</p>
<p>This contains the conservative concept problem and the reward hacking problem. (I think.) Solving the informed oversight problem is sufficient.</p>
<ul>
<li>Conservative concept: a human non-evaluable task means that every hypothesis class we could train with human-curated data, we could not distinguish between the real and fake thing. There is impossibile without access to the agent’s internal state.</li>
<li>Reward hacking: how not just to keep hitting reward button. Related to shutdown problem—preventing human from changing its reward, for instance.</li>
<li>Informed oversight: give human a human-understandable transcript that would help make a decision about whether value achieved (ex. “I put myself in a simulation.”). This is sufficient.</li>
</ul>
<h3 id="philosophy-view">Philosophy view</h3>
<p>It is impossible to refer to the physical world. Our mapping from physical actions to mental representations is many-to-one; many ways of moving our arm all get translated into a mental story of “picking up the strawberry.” There are many ways to execute this task.</p>
<p>We only live in our own conceptual space. This space is highly bound/coupled to actual physics. (There’s no glitch in the universe that if I move my arm in a specific way, Konami code, I shut down the universe.) Any way I move my arm is roughly the same.</p>
<p>The AI solves tasks within its own conceptual space. We can evaluate that the AI is doing the right thing insofar as it is transparent, we can look at its world model, point at the concept of “strawberry” and see that it’s close enough to our own. We can solve “environmental goals” if the intersection of ontologies is nonempty, and the goal is within that intersection.</p>
<h2 id="impact-measure">Impact measure</h2>
<p>Measure 1: get back to what would have happened under null. <span class="math display">\[
I(s) = \min_{\pi} D_{KL}(P_{t_1}^{\phi}(\bullet | s)|| P_{t_2}^\pi(\bullet | s'))
\]</span></p>
<p>Measure 2: stay similar to trusted region <span class="math inline">\(R\)</span>. Let <span class="math inline">\(f:X\to Y\)</span> be mapping to feature space. <span class="math display">\[
I(s) = d(R, f(s)).
\]</span> For example, <span class="math inline">\(R=\{f(s_0)\}\)</span> and <span class="math inline">\(d=\ved_2\)</span>.</p>
<p>Measure 3: train <span class="math inline">\(I\)</span> on examples, conservatively (ex. RBF) on good examples <span class="math inline">\((s_i,0)\)</span>, and bad examples, <span class="math inline">\((s_j,&gt;0)\)</span>. Also can encode prior information, e.g. about things that are neutral.</p>
<h2 id="learning-utility-function">Learning utility function</h2>
<p>Probability of <span class="math inline">\((x_i,y_i)\)</span> under <span class="math inline">\(p_{\te_1}\)</span> and <span class="math inline">\(p_{\te_2}\)</span>, where <span class="math inline">\(p_i=1\)</span> if <span class="math inline">\((x_i,y_i)\sim D_1\)</span> and 0 if <span class="math inline">\(\sim D_2\)</span>. <span class="math display">\[
\sumo in p_i p_{\te_1}(y_i|x_i) + \sumo in (1-p_i) p_{\te_2}p(y_i|x_i).
\]</span> Max log likelihood. Do EM with <span class="math inline">\(p\)</span>’s and <span class="math inline">\((x,y)\)</span>’s.</p>
<p>Version 2: Use IRL: keep track of best guess nets, or sets of valid hypotheses. Keep track of posterior probabilities of each net. (Update in online fashion.) Update posterior probabilities assuming Markovian switching (cf. DP in HMM, sleeping experts), and gradient descent on parameters.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Neural nets as kernel space</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/nn_kernel.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/nn_kernel.html</id>
    <published>2017-03-23T00:00:00Z</published>
    <updated>2017-03-23T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Neural nets as kernel space</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-03-23 
          , Modified: 2017-03-23 
	</p>
      
       <p>Tags: <a href="/tags/neural%20net.html">neural net</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#things-this-could-explain">Things this could explain</a></li>
 <li><a href="#for-neural-nets">For neural nets</a></li>
 <li><a href="#followup">Followup</a></li>
 <li><a href="#thoughts-33017">Thoughts 3/30/17</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>I think that I can place neural networks with non-steep and well-separated sigmoids as low-norm elements of a RKHS, and that this will explain and unify some learning results.</p>
<p>Precise details to be ironed out. First, choice of RKHS:</p>
<ul>
<li>Sobolev space <span class="math inline">\(H^{\fc n2}(\R^n)\)</span>, or</li>
<li><span class="math inline">\(L^2(\R^n)\)</span> or <span class="math inline">\(H^1(\R^n)\)</span> with bandlimited constraint.</li>
<li>Quotient out by constant functions? Allow a sigmoid <span class="math inline">\(\to 1\)</span> at infinity as kernel?</li>
</ul>
<p>For the bandlimiting, there are 2 technical steps, with parameters to vary.</p>
<ul>
<li>Convolve by Gaussian (or ball indicator) in Fourier space, or multiplying by Gaussian (or FT of ball indicator) in ordinary space.</li>
<li>cutting off Fourier spectrum - how does this influence? I.e. what is Fourier decay? Expect exponential - so get <span class="math inline">\(\ln \prc{\ep}\)</span> in exponent.</li>
</ul>
<p>Expect exponential dependence on steepness.</p>
<h2 id="things-this-could-explain">Things this could explain</h2>
<ul>
<li>Kalai’s result on learning smooth NN.</li>
<li>Learning linear separator with margin.</li>
<li>Exponential dependence on dimension or <span class="math inline">\(\rc{\ep}\)</span> (what exactly?) for agnostically learning halfspace, etc.</li>
</ul>
<h2 id="for-neural-nets">For neural nets</h2>
<ul>
<li>Learn 2-NN with linear output, under some conditions (ex. incoherence)</li>
<li>Maybe can learn 2-NN with sigmoid/majority output, by boosting (cf. majority of majorities).</li>
</ul>
<h2 id="followup">Followup</h2>
<ul>
<li>What is relationship to gradient descent?</li>
</ul>
<h2 id="thoughts-33017">Thoughts 3/30/17</h2>
<ul>
<li>How to get something for convolutional neural nets?
<ul>
<li>As an easier question, think about having filters over the entire image, rather than a grid.</li>
<li>Think of periodic case even.</li>
<li>Then this works by Fourier transform over <span class="math inline">\(\R^\N\)</span> (suitably weighted).</li>
<li>Problem: the simplest convnet is more complicated than this, includes maxpool and then fc. How to deal with maxpool? What if you don’t do maxpool? Sigmoid and then average, or weighted average?</li>
<li>Kernel on Fourier transform space. (See how well FT matches…)</li>
<li>See [ZLW16]</li>
</ul></li>
<li>Overcomplete bases
<ul>
<li>Can define RKHS norm by giving norm when written in terms of basis element.</li>
<li>Can’t define norm with overcomplete set of elements.</li>
<li>Can we define some kind of norm and do something kernel-like with overcomplete basis?
<ul>
<li>Project from larger space?</li>
</ul></li>
<li>Cf. wanting symmetries beyond translation</li>
<li>Cf. wavelets offer a natural overcomplete basis respecting symmetries</li>
<li>Perhaps first thing to do is just try wavelet regularization on MNIST.</li>
<li>If you want to use nonconvolutional kernel method on images, you should first convert to Fourier or wavelet basis. Probably wavelet (except that’s not quite a basis). (Multiply by log size.) (This doesn’t give you translation invariance, just resets the norm.)</li>
</ul></li>
<li>Three kernels
<ul>
<li>Fourier-based.</li>
<li><span class="math inline">\(\rc{2-\an{x,y}}\)</span>.</li>
<li>Arccosine.</li>
</ul></li>
<li>I’m super-confused about why toggling just one parameter <span class="math inline">\(n\)</span> changes the number of layers. <span class="math inline">\(K^{(n)}(x,y)=\fc{n-1}n + \fc{1/n}{(n+1) - n\an{x,y}}\)</span>.</li>
<li>Idea to prove NN separation for Lipschitz layers: Show a function that has exponentially higher norm in terms of <span class="math inline">\(l-1\)</span> norm than <span class="math inline">\(l\)</span>. Problem: norm required to express neural net also increases super-exponentially in dimension.</li>
<li>Improper tensor decomp using same method (use case?) cf. Livni’s poly network</li>
<li>Barron functions form a convex set… The reason why it’s intractable is that it’s infinite-dimensional. Hilbert spaces are infinite-dimensional, but the representor theorem saves you.</li>
<li>Can you cut down representation (after using representor theorem) by sampling? Keep norm, but be cruder? (Pick some elements and rescale.)</li>
<li>Using kernel representation at first level of neural network? Do some kind of AM? How to mix nonparametric and parametric? Power and limit of kernel coming from its nonparametricity.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2017-03-18</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2017-03-18.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2017-03-18.html</id>
    <published>2017-03-18T00:00:00Z</published>
    <updated>2017-03-18T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2017-03-18</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-03-18 
          , Modified: 2017-03-18 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#adversarial-training">Adversarial training</a></li>
 <li><a href="#langevin-monte-carlo">Langevin Monte Carlo</a></li>
 <li><a href="#using-barrons-theorem">Using Barron’s Theorem</a></li>
 <li><a href="#entropy-regularizer-for-svm">Entropy regularizer for SVM</a></li>
 <li><a href="#generalization">Generalization</a></li>
 <li><a href="#rl">RL</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="adversarial-training">Adversarial training</h2>
<ul>
<li><a href="../tcs/machine_learning/neural_nets/adversarial_experiments.html">Experiments</a></li>
</ul>
<h2 id="langevin-monte-carlo">Langevin Monte Carlo</h2>
<ul>
<li>Learn background. (OV14, Villani book, Ledoux book.)</li>
<li>Langevin papers.</li>
<li>Extending [AH16].</li>
</ul>
<h2 id="using-barrons-theorem">Using Barron’s Theorem</h2>
<p>See <a href="../tcs/machine_learning/neural_nets/barron_musings.html">musings</a>.</p>
<h2 id="entropy-regularizer-for-svm">Entropy regularizer for SVM</h2>
<h2 id="generalization">Generalization</h2>
<h2 id="rl">RL</h2>
<p>Come up with project. Concrete problems in AI safety?</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Langevin dynamics</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/probabilistic/langevin.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/probabilistic/langevin.html</id>
    <published>2017-03-15T00:00:00Z</published>
    <updated>2017-03-15T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Langevin dynamics</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-03-15 
          , Modified: 2017-03-15 
	</p>
      
       <p>Tags: <a href="/tags/langevin.html">langevin</a>, <a href="/tags/sampling.html">sampling</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#references">References</a><ul>
 <li><a href="#other">Other</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="references">References</h2>
<ul>
<li>Book: Analysis and geometry of Markov diffusion operators</li>
<li>C. Villani. Topics in Optimal Transportation, volume 58 of Graduate Studies in Mathematics</li>
<li>[OV14] GENERALIZATION OF AN INEQUALITY BY TALAGRAND, AND LINKS WITH THE LOGARITHMIC SOBOLEV INEQUALITY</li>
<li>[GM90] Recursive stochastic algorithms for global optimization in R^d</li>
<li>[RT96] Exponential convergence of Langevin distributions and their discrete</li>
<li>[D14] Theoretical guarantees for approximate sampling from smooth and log-concave densities</li>
<li>[ZLC17] A Hitting Time Analysis of Stochastic Gradient Langevin Dynamics</li>
<li>[RRT17] Non-Convex Learning via Stochastic Gradient Langevin Dynamics - A Nonasymptotic Analysis</li>
</ul>
<p>See also: Stochastic calculus, Brownian motion, [AH] on sampling</p>
<h3 id="other">Other</h3>
<ul>
<li>E. Hazan, K. Levi, and S. Shalev-Shwartz. On graduated optimization for stochastic non-convex problems</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Learning to model structures and data</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/complexity/model.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/complexity/model.html</id>
    <published>2017-03-12T00:00:00Z</published>
    <updated>2017-03-12T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Learning to model structures and data</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-03-12 
          , Modified: 2016-03-12 
	</p>
      
       <p>Tags: <a href="/tags/pseudorandomness.html">pseudorandomness</a>, <a href="/tags/structure.html">structure</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#miscellaneous-notes">Miscellaneous notes</a></li>
 <li><a href="#what-does-this-say-for-learning">What does this say for learning?</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="miscellaneous-notes">Miscellaneous notes</h2>
<ul>
<li>Boosting:
<ul>
<li>Generic boosting theorem</li>
<li>Easiest to state when assume access to whole distribution. Otherwise, have to sample and assume something about VC dimension to generalize. (See old 511 notes. Actual argument works with <span class="math inline">\(\Pi_H(m)\)</span>, is there some reason we need to do that?)</li>
<li>Hardcore lemma</li>
<li>Strong version: Either allow real-number predictions, or think of real-number output as flipping a coin with that probability. This can save the vital factor of 2 compared to worst-case.</li>
<li>I don’t get the potential argument.</li>
<li>I do get the multiplicative weights argument of BHK, but there are some things to be careful about.
<ul>
<li>Familiar with MW with divergence applied to it.</li>
<li>How to make non-worst case? Is this already covered?</li>
<li>What about the AdaBoost improvement - algorithmically speaking, it makes sense to adapt weight based on number correct/wrong. Can you not redo the analysis here?</li>
<li>Important: need to keep track of density of <span class="math inline">\(\de\)</span>.</li>
<li>Do we need to cap the weights at 1? What problem is this trying to solve? What happens if we don’t?</li>
</ul></li>
</ul></li>
<li>Dense model theorem
<ul>
<li>Got <span class="math inline">\(\de''\)</span> (<span class="math inline">\(\wh \de\)</span>) part</li>
<li>Lots of algebra/<span class="math inline">\(\ep\)</span>’s and <span class="math inline">\(\de\)</span>’s here. I’m missing an additive factor of <span class="math inline">\(\de^2\)</span> (<span class="math inline">\(\fc{\de}{1+\de}\)</span> instead of <span class="math inline">\(\de\)</span>)…</li>
<li>Todo: Understand example that shows optimality</li>
</ul></li>
<li>Weak regularity
<ul>
<li>Why do we need the condition on no cut having too many edges? Counterexample if this is not satisfied? Is there a statement that’s not “X or Y”?</li>
<li>What subroutines are required for the algorithmic version?</li>
<li>Note that the points are <span class="math inline">\(V\times V\)</span>, or edges in the complete graph. Tests are <span class="math inline">\(A\not\cap B\)</span>. (These are not the points.)</li>
<li>Get directly from algorithmic DMT?</li>
</ul></li>
<li>Other things
<ul>
<li>Low-complexity approximation</li>
<li>Computational entropy</li>
<li>Structure theorem (my understanding is that the dense model theorem gives up if it finds a hot spot, while structure attempts to find all the hot spots because it’s iterative).</li>
</ul></li>
</ul>
<h2 id="what-does-this-say-for-learning">What does this say for learning?</h2>
<ul>
<li>Relax “tests” to “functions”, perhaps <span class="math inline">\([0,1]\)</span>-functions. Does everything still go through?</li>
<li>Does this give a GAN formulation? Note that the dense model/structure theorem breaks into regions of constant density, and you get handle on the probability distribution, while in GAN, you have a generative model, not an assignment of probabilities. Can you make a GAN which knows about its probabilities? Is this a Bayesian NN?</li>
<li>Does this jibe with the greedy algorithmic version of Barron’s Theorem? Is the process of finding the best <span class="math inline">\(h\)</span> (if <span class="math inline">\(H\)</span> is linear thresholds/sigmoids/1-dimensional functions) correspond to finding the best new node? Can we make this convex? What if we just want weak learning here?</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Stochastic calculus</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/probability/stochastic_calculus.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/probability/stochastic_calculus.html</id>
    <published>2017-03-08T00:00:00Z</published>
    <updated>2017-03-08T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Stochastic calculus</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-03-08 
          , Modified: 2017-03-08 
	</p>
      
       <p>Tags: <a href="/tags/stochastic%20calculus.html">stochastic calculus</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#section">4</a><ul>
 <li><a href="#ito-integral">4.2 Ito integral</a></li>
 <li><a href="#some-elementary-properties">4.3 Some elementary properties</a></li>
 <li><a href="#ito-calculus">4.4 Ito calculus</a></li>
 <li><a href="#girsanovs-theorem">4.5 Girsanov’s theorem</a></li>
 <li><a href="#martingale-representation-theorem">4.6 Martingale representation theorem</a></li>
 </ul></li>
 <li><a href="#stochastic-differential-equations">5 Stochastic differential equations</a><ul>
 <li><a href="#sde-existence-and-uniqueness">5.1 SDE existence and uniqueness</a></li>
 <li><a href="#markov-property-and-kolmogorovs-equations">5.2 Markov property and Kolmogorov’s equations</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="section">4</h2>
<p>Stieltjes function can be defined in terms of simple functions first, <span class="math inline">\(I(f_n) = \sum_i f_n(t_i^n) (g(t_{i+1}^n) - g(t_i^n))\)</span>.</p>
<p>What’s wrong with the Stieltjes integral? If <span class="math inline">\(g\)</span> has infinite variation on <span class="math inline">\([0,1]\)</span>, then there exist simple functions <span class="math inline">\(f_n\to f\)</span> uniformly such that <span class="math inline">\(I(f_n)\)</span> diverges.</p>
<p><em>Proof</em>. Take a partition so that <span class="math inline">\(\sum |g({t_{i+1}}) - g(t_i)|\to \iy\)</span>, <span class="math inline">\(h_n(t_i) = \sign(g(t_{i+1})-g(t_i))\)</span>.</p>
<p>But when <span class="math inline">\(g\)</span> is a Wiener process, this example would have to choose <span class="math inline">\(h_n\)</span> adaptively to it. If <span class="math inline">\(h_n\)</span> is nonrandom, this is not a problem. There are certain sample paths of <span class="math inline">\(W_t\)</span> for which the integral diverges, but the set of such sample paths has probability 0! But we would like to integrate random processes.</p>
<p>The lemma cheated by looking into the future!</p>
<ol type="1">
<li>We only define stochastic integrals with repect to <span class="math inline">\(W_t\)</span> of stochastic processes which are <span class="math inline">\(\mathcal F_t\)</span>-adapted.</li>
<li>Even though finite variation of <span class="math inline">\(W_t\)</span> is a.s. infinite, quadratic variation is finite, <span class="math inline">\(\sum_{t_i\in \pi_n} (W_{t_{i+1}}-W_{t_i})^2\to 1\)</span> in probability. Define stochastic integrals as limits in <span class="math inline">\(L^2\)</span>. By independence of increments, <span class="math display">\[
\E [ I(f_n)^2] = \int_0^1 f_n(s)^2\,ds.
\]</span></li>
</ol>
<p>Wiener integral: <span class="math inline">\(f_n\to f\)</span> in <span class="math inline">\(L^2([0,1])\)</span>. Then <span class="math inline">\(\E[(I(f_n)-I(f_m))^2]\to 0\)</span>, so <span class="math inline">\(I(f_n)\)</span> converges to some <span class="math inline">\(I(f)\)</span>. Note: choosing <span class="math inline">\(f_n\)</span> so <span class="math inline">\(\sumo n{\iy} \int_0^1 (f_n(s)-f(s))^2\,ds&lt;\iy\)</span>, can define as a.s. limit (CHECK). Counterexample: take wavelet, divide by <span class="math inline">\(\sqrt{\ln \prc{n}}\)</span>.</p>
<h3 id="ito-integral">4.2 Ito integral</h3>
<p>!! Note the Ito integral is ITSELF a random variable over the probability measure underlying the Wiener process. You can take a <span class="math inline">\(\E\)</span> over Brownian motions.</p>
<ol type="1">
<li>Let <span class="math inline">\(\{X_t^n\}_{t\in [0,T]}\)</span> be a simple, square-integrable, <span class="math inline">\(F_t\)</span>-adapted stochastic process. Define <span class="math inline">\(I(X^n) = \int_0^T X_t^n \,dW_t = \sumz iN X_{t_i}^N (W_{t_{i+1}} - W_{t_i})\)</span>.</li>
<li>Ito isometry: <span class="math inline">\(X_{t_i}^n\)</span> is independent of <span class="math inline">\(W_{t_{i+1}} - W_{t_i}\)</span>, so <span class="math display">\[ \E\ba{\pa{\int_0^T X_t^n \,dW_t}^2} = \E\ba{\int_0^T (X_t^n)^2\,dt}.\]</span> Succinctly, <span class="math display">\[\ve{I(X_\cdot^n)}_{2,\Pj} = \ve{X_\cdot^n}_{2,\mu_T\times \Pj}.\]</span> I.e., <span class="math inline">\(I:L^2(\mu_T\times \Pj)\to L^2(\Pj)\)</span> preserves <span class="math inline">\(L^2\)</span> distance when applied to <span class="math inline">\(F_t\)</span> adapted simple integrands.</li>
<li>Extend to <span class="math inline">\(X_\cdot^n \to X_\cdot\)</span> in <span class="math inline">\(L^2(\mu_T\times \Pj)\)</span>.</li>
<li>Let <span class="math inline">\(X_\cdot\in L^2(\mu_T\times \Pj)\)</span> be <span class="math inline">\(F_t\)</span>-adapted. Then there exists a sequence of <span class="math inline">\(F_t\)</span>_adapted simple <span class="math inline">\(X_\cdot^n\to X\)</span>. <!--DCT-->
<ul>
<li>Continuous and bounded sample paths: uniform continuity, DCT.</li>
<li>Bounded and progressively measurable (?): <span class="math inline">\(X^\ep_\cdot \to X^\ep\)</span>, where <span class="math inline">\(X_t^\ep = \rc\ep\int_{t-\ep}^t X_{\max\{s,0\}}\,ds\)</span>.</li>
<li>Progressively measurable: <span class="math inline">\(X_t I_{|X_t|\le M}\)</span>. DCT.</li>
</ul></li>
</ol>
<p>Ex. <span class="math inline">\(W_T^2 = 2\int_0^T W_t\,dW_t+T\)</span>.</p>
<p>Now what?</p>
<ol type="1">
<li>Consider Ito integral itself as a stochastic process, <span class="math inline">\(t\mapsto \int_0^t X_s\,dW_s\)</span>.
<ul>
<li>For <span class="math inline">\(t\le T\)</span>, define <span class="math inline">\(I_t(X_\cdot^n) = I(X_\cdot^n I_{\le t})\)</span>.</li>
<li><span class="math inline">\(I_t(X_\cdot^n)\)</span> is a <span class="math inline">\(F_t\)</span>-martingale.</li>
<li>Ito integral can be chosen to have continuous sample paths. (Pf. Discontinuous paths live on null set. Use subsequence argument and Borel-Cantelli.) …</li>
</ul></li>
<li>Extend the class of integrable processes, to have nice closure properties. (Product of 2 integrals can be expressed as a integral.)
<ul>
<li>Note we don’t want to define <span class="math inline">\(I_\iy\)</span>, only for every <span class="math inline">\(t\in [0,\iy)\)</span>.</li>
<li>Localization: to define on <span class="math inline">\([0,\iy)\)</span>, define it on every interval <span class="math inline">\([0,T]\)</span>. Require <span class="math inline">\(X_{[0,T]}\in L^2(\mu_T\times \Pj)\)</span> for every <span class="math inline">\(T&lt;\iy\)</span>, i.e. <span class="math inline">\(X\in \bigcap_{T&lt;\iy} L^2(\mu_T\times \Pj)\)</span>.</li>
<li>Check local property: <span class="math inline">\(I_t(X_\cdot)\)</span> does not depend on which <span class="math inline">\(T&gt;t\)</span> we choose.</li>
<li>Behavior under stopping: <span class="math inline">\(X_t\)</span> is <span class="math inline">\(F_t\)</span>-adapted in <span class="math inline">\(\bigcap_{T&lt;\iy} L^2(\mu_T\times \Pj)\)</span>, <span class="math inline">\(\tau\)</span> is <span class="math inline">\(F_t\)</span>-stopping time. Then <span class="math inline">\(I_{\min(t,\tau)}(X_\cdot) = I_t(X_\cdot I_{&lt;\tau})\)</span>. Pf. Let <span class="math inline">\(\tau^n\)</span> be <span class="math inline">\(\tau\)</span> rounded upwards to the earliest jump time.</li>
<li>Localizing sequence for <span class="math inline">\(X_t\)</span>: <span class="math inline">\(F_t\)</span>-stopping times <span class="math inline">\(\tau_n\nearrow \iy\)</span>, <span class="math inline">\(\E\ba{\int_0^{\tau_n} X_t^2\,dt}&lt;\iy\)</span>. “Allow localization intervals to be random.” This is independent of localizing sequence (4.2.10, CHECK).</li>
<li>There is a natural cloass of integrands whose elements admit localizing sequence: <span class="math inline">\(A_T(X_\cdot) = \int_0^T X_t^2&lt;\iy\)</span> a.s. for all <span class="math inline">\(T&lt;\iy\)</span>. Let <span class="math inline">\(\tau_n=\inf\set{t\le n}{A_t(X_{\cdot})\ge n}\)</span>.</li>
</ul></li>
</ol>
<p>Finally: Let <span class="math inline">\(X_t\)</span> be any <span class="math inline">\(F_t\)</span>-adapted stochastic process with <span class="math inline">\(\Pj\ba{\int_0^T X_t^2\,dt&lt;\iy}=1\)</span> for all <span class="math inline">\(T&lt;\iy\)</span>. Then Ito integral <span class="math inline">\(I_t(X_\cdot)\)</span> is uniquely defined by localization and choice of continuous modification as <span class="math inline">\(F_t\)</span>-adapted stochastic process on <span class="math inline">\([0,\iy)\)</span> with continuous sample paths.</p>
<h3 id="some-elementary-properties">4.3 Some elementary properties</h3>
<ol type="1">
<li>Linearity</li>
<li>Stopping time <span class="math inline">\(\int_0^{\min(t,\tau)} X_s\,dW_s = \int_0^t X_s I_{&lt;\tau}\,dW_s\)</span>.</li>
<li>If <span class="math inline">\(X\in \bigcap_{T&lt;\iy} L^2(\mu_T\times \Pj)\)</span> then <span class="math inline">\(\E[]=0\)</span> and <span class="math inline">\(\E[()^2] = \E[\int_0^TX_t^2\,dt]\)</span>.</li>
<li><span class="math inline">\(X^n\to X\)</span> in <span class="math inline">\(L^2(\mu_T\times \Pj)\)</span> means <span class="math inline">\(I_t(X_\cdot^n) \to I_t(X_\cdot)\)</span> in <span class="math inline">\(L^2(\Pj)\)</span>. If convergence fast enough, a.s.</li>
<li><span class="math inline">\(X_t\)</span> is <span class="math inline">\(F_t\)</span>-local martingale if there exists <span class="math inline">\(\tau_n\nearrow \iy\)</span> (reducing sequence), <span class="math inline">\(X_{\min(t,\tau_n)}\)</span> is martingale. Any Ito integral is a local martingale. (Take a localizing sequence.)</li>
</ol>
<h3 id="ito-calculus">4.4 Ito calculus</h3>
<p>Setup</p>
<ul>
<li><span class="math inline">\((\Om, F, \{F_t\}_{t\in [0,\iy)}, \Pj)\)</span></li>
<li><span class="math inline">\(W_t\)</span> is <span class="math inline">\(m\)</span>-dimensional <span class="math inline">\(F_t\)</span>-Wiener process.</li>
<li>Ito process <span class="math inline">\(X_t^i = X_0^i + \int_0^t F_s^i\,ds + \sumo jm \int_0^t G_s^{ij}\,dW_s^j\)</span>.
<ul>
<li><span class="math inline">\(\int_0^t |F_s^i|\,ds&lt;\iy\)</span> a.s.</li>
<li><span class="math inline">\(\int_0^t (G_s^{ij})^2\,ds&lt;\iy\)</span> a.s.</li>
<li>Shorthand: <span class="math inline">\(X_t=X_0+\int_0^t F_s\,ds + \int_0^t G_s\,dW_s\)</span>.</li>
</ul></li>
</ul>
<strong>Theorem</strong> (Ito rule): <span class="math inline">\(u(t,x)\)</span> is <span class="math inline">\(C^1\)</span> in <span class="math inline">\(t\)</span> and <span class="math inline">\(C^2\)</span> in <span class="math inline">\(x\)</span>. Then <span class="math inline">\(u(t,X_t)\)</span> is Ito.
<span class="math display">\[\begin{align}
u(t,X_t) &amp;= u(0,X_0) + \sumo in \sumo km \int_0^t u_i(s,X_s) G_s^{ik} \,dW_s^k\\
&amp;\quad + \int_0^t\ba{u'(s,X_s)+\sumo in u_i(s,X_s)F_s^i + \rc 2 \sum_{i,j=1}^n\sumo km u_{ij}(s,X_s)G_s^{ik}G_s^{jk}}\,ds\\
&amp;=u(0,X_0) + \int_0^t (\nb u)^T G \,dW + \int_0^t u'(s,X_s) + (\nb u)^TF_s + \rc 2 \Tr(G_s^T (\nb^2 u) G_s)\,ds.
\end{align}\]</span>
<p>Alternate notation: <span class="math inline">\(dX_t = F_t\,dt + G_t\,dW_t\)</span>, <span class="math display">\[
du(t,X_t) = u'(t,X_t)\,dt + \pl u(t,X_t)\,dX_t + \rc 2 \Tr(\pl^2 u(t,X_t)dX_t(dX_t)^*),
\]</span> where <span class="math inline">\(dW_t^i \,dW_t^j=\de_{ij}\,dt\)</span> and other derivatives are 0.</p>
<p>First two terms are chain rule. When stochastic integrals are present, we evidently need to take a second-order term into account as well.</p>
<p>Cor. Ito processes form an algebra.</p>
<h3 id="girsanovs-theorem">4.5 Girsanov’s theorem</h3>
<p>What happens to Wiener process under change of measure? We can often simplify a problem by changing to a more convenient probability measure.</p>
<p><strong>Theorem</strong> (Girsanov). Let <span class="math inline">\(W_t\)</span> be <span class="math inline">\(m\)</span>-dimensional <span class="math inline">\(\mathcal F_t\)</span>-Wiener on <span class="math inline">\((\Om, \mathcal F, \{\mathcal F_t\}_{t\in [0,T]}, \Pj)\)</span>, <span class="math inline">\(X_t=\int_0^t F_s\,ds + W_t\)</span> be Ito, <span class="math inline">\(F_t\)</span> Ito integrable, <span class="math display">\[
\La = \exp\pa{-\int_0^T (F_s)^* dW_s - \rc 2\int_0^T \ve{F_s}^2\,ds},
\]</span> Novikov’s condition <span class="math inline">\(\E_{\Pj} \ba{\exp\pa{\rc 2\int_0^T \ve{F_s}^2\,ds}}&lt;\iy\)</span>. THen <span class="math inline">\(\{X_t\}_{t\in [0,T]}\)</span> is <span class="math inline">\(\mathcal F_t\)</span>-Wiener under <span class="math inline">\(\mathbb Q(A) = \E_\Pj (\Ga I_A)\)</span>.</p>
<p>Intuition (discrete case): If <span class="math inline">\(d\Pj\)</span> is the probability measure of standard gaussian, and <span class="math inline">\(d\mathbb Q\)</span> is probability measure where <span class="math inline">\(a_k+\xi_k\)</span> are standard gaussian (<span class="math inline">\(a_k\)</span> is predictable process), write <span class="math inline">\(d\mathbb Q\)</span> wrt <span class="math inline">\(d\Pj\)</span>. <span class="math display">\[
\fc{d\mathbb Q}{d\mathbb P} = \prod\fc{e^{-(\xi_i+a_i)^2/2}}{e^{-\xi_i^2/2}} = \exp\ba{\sumo kn \pa{-a_k \xi_k - \rc 2a_k^2}}.
\]</span></p>
<p>READ PROOF.</p>
<h3 id="martingale-representation-theorem">4.6 Martingale representation theorem</h3>
<p>Gives converse to Ito integral. Every martingale <span class="math inline">\(\{M_t\}_{t\in [0,T]}\)</span> with <span class="math inline">\(M_T\in L^2(\Pj)\)</span> is the Ito integral of a unique process in <span class="math inline">\(L^2(\mu_T\times \Pj)\)</span>.</p>
<p><strong>Theorem</strong>.</p>
<ol type="1">
<li>(Martingale representation) Let <span class="math inline">\(M_t\)</span> be <span class="math inline">\(\mathcal F_t^W = \si\set{W_s}{s\le t}\)</span>-martingale, <span class="math inline">\(M_T\in L^2(\Pj)\)</span>. For a unique <span class="math inline">\(\mathcal F_t^W\)</span>-adapted process <span class="math inline">\(\{H_t\}_{t\in [0,T]}\)</span> in <span class="math inline">\(L^2(\mu_T\times \Pj)\)</span>, <span class="math inline">\(M_t=M_0 + \int_0^t H_s\,dW_s\)</span> a.s.</li>
<li>(Ito representation, more general) Let <span class="math inline">\(X\)</span> be <span class="math inline">\(\mathcal F_T^W\)</span>-measurable rv in <span class="math inline">\(L^2(\Pj)\)</span>. Then for … <span class="math inline">\(X=\E X + \int_0^T H_s\,dW_s\)</span> a.s. <!--can differentiate --></li>
</ol>
<p><em>Proof</em>.</p>
<ol type="1">
<li>Show that any <span class="math inline">\(X\)</span> can be approximated arbitrarily well by Ito integral, <span class="math inline">\(\ve{X-I_T(H_\cdot^\ep)}_2&lt; \ep\)</span>.
<ol type="1">
<li>Identify class of random variables that can approximatate <span class="math inline">\(X\)</span> arbitrarily well. <span class="math inline">\(S=\set{f(W_{t_1},\ldots, W_{t_n})}{n&lt;\iy, t_1,\ldots, t_n\in [0,T], f\in C_0^\iy}\)</span>.
<ul>
<li>Show this holds if allow Borel-measurable <span class="math inline">\(f\)</span>. Filter by slicing into intervals <span class="math inline">\(2^{-n}\)</span>, <span class="math inline">\(X^n = f(W_{2^{-n}T},\ldots, W_T)\)</span>.
<ul>
<li>Levy’s upward theorem: let <span class="math inline">\(X\in L^2(\Pj)\)</span> be <span class="math inline">\(G\)</span>-measurable, <span class="math inline">\(G=\si\{G_n\}\)</span>. Then <span class="math inline">\(\E[X|G_n]\to X\)</span> a.s. and in <span class="math inline">\(L^2(\Pj)\)</span>.</li>
</ul></li>
<li>Any Borel function can be approximated arbitrarily well by <span class="math inline">\(f^n\in C^\iy\)</span>.</li>
</ul></li>
<li>Show any rv in this class can be represented as Ito integral
<ul>
<li>Ito’s rule: <span class="math display">\[g(t,W_t) = g(0,0) + \int_0^t (g_s + \rc2 g_{xx}) (s,W_s)\,ds + \int_0^t g_x (s,W_s)\,dW_s.\]</span> Solve the heat equation for <span class="math inline">\(g\)</span>, <span class="math inline">\(g = \rc{\sqrt{2\pi (t-s)}}\int_{-\iy}^{\iy} f(y) e^{-(x-y)^2/(2(t-s))} \dy\)</span>. Still works for multivariate.</li>
</ul></li>
</ol></li>
<li>Take limits.</li>
</ol>
<h2 id="stochastic-differential-equations">5 Stochastic differential equations</h2>
<p>Existence, uniqueness, Markov property.</p>
<p>Kolmogorov forward (Fokker-Planck) and backward equations.</p>
<h3 id="sde-existence-and-uniqueness">5.1 SDE existence and uniqueness</h3>
<span class="math display">\[\begin{align}
dX_t &amp;= b(t,X_t)dt + \si(t,X_t) dW_t, &amp; X_0=x\\
\iff 
X_t &amp;= x+\int_0^t b(s,X_s) \,ds + \int_0^t \si(s,X_s)\,dW_s.
\end{align}\]</span>
<p>Ex. <span class="math inline">\(dX_t = AX_t dt + B\,dW_t\)</span>, <span class="math inline">\(X_0=x\)</span> has solution <span class="math display">\[
X_t = e^{At}x + \int_0^t e^{A(t-s)}B\,dW_s.
\]</span></p>
<p><strong>Theorem</strong>. Suppose</p>
<ol type="1">
<li><span class="math inline">\(X_0\in L^2(\Pj)\)</span></li>
<li><span class="math inline">\(b,\si\)</span> Lipschitz uniformly on <span class="math inline">\([0,T]\)</span> (in <span class="math inline">\(x\)</span>).</li>
<li><span class="math inline">\(\ve{b(t,0)}, \ve{\si(t,0)}\)</span> bounded on <span class="math inline">\(t\in [0,T]\)</span>.</li>
</ol>
<p>Then there exists solution <span class="math inline">\(X_t\)</span>, and <span class="math inline">\(b(t,X_t),\si(t,X_t)\in L^2(\mu_T\times \Pj)\)</span>, and it is unique a.s.</p>
<h3 id="markov-property-and-kolmogorovs-equations">5.2 Markov property and Kolmogorov’s equations</h3>
<p>A large class of Markov processes with continuous sample paths can be obtained as solution of appropriate SDE.</p>
<p><strong>Theorem</strong>. Suppose conditions hold. Then <span class="math inline">\(X_t\)</span> is <span class="math inline">\(\mathcal F_t\)</span>-Markov process. (Actually it satisfies the strong Markov property, even with random stopping times.)</p>
<p><em>Proof</em>. Calculate <span class="math inline">\(X_t-X_s\)</span>. Note <span class="math inline">\(W_{r+s}-W_s\)</span> is Wiener. <span class="math inline">\(Y_r=X_{r+s}\)</span> satisfies a SDE… ?</p>
<p>Assume <span class="math inline">\(b, \si\)</span> independent of <span class="math inline">\(t\)</span>. Markov property gives <span class="math inline">\(\E[f(X_t)|\mathcal F_s] = g_{t-s}(X_s)\)</span>. This suggests <span class="math inline">\(\ddd t P_t f = \mathcal L P_tf\)</span>.</p>
<p><strong>Theorem</strong> (Kolmogorov backward equation). For <span class="math inline">\(g\in C^2\)</span>, <span class="math display">\[
\mathcal Lg = b^T\nb g + \rc 2 \Tr(\si^T \nb^2 g \si).
\]</span> If <span class="math inline">\(u(t,x)\)</span> is <span class="math inline">\(C^1\)</span> in <span class="math inline">\(t\)</span>, <span class="math inline">\(C^2\)</span> in <span class="math inline">\(x\)</span>, <span class="math inline">\(f\in C^2\)</span> such that <span class="math display">\[
u_t=\mathcal L u, \quad u(0,x)=f(x)
\]</span> then <span class="math inline">\(u(t,x)=P_tf(x)\)</span>.</p>
<p>(Note we can write this backwards as <span class="math inline">\(v_t + \mathcal Lv = 0\)</span>, <span class="math inline">\(v(T,x)=f(x)\)</span>.)</p>
<p>(Note: in principle we would like to define <span class="math inline">\(\E[f(X_t)|\mathcal F_s] =: u(t-s,X_s)\)</span> and show <span class="math inline">\(u\)</span> satisfies the PDE. This is more technical because we need to show smoothness or interpret the PDE in a weak sense.)</p>
<p><em>Proof</em>. Ito’srule on <span class="math inline">\(Y_r=v(r,X_r)\)</span>. (The Ito integral <span class="math inline">\(\int_0^t (\nb v)^T G\,dW_s\)</span> is a “local martingale” here.) Take <span class="math inline">\(\E[\cdot |\mathcal F_s]\)</span> and the martingale part disappears.</p>
<p>Forwards equation: If law of <span class="math inline">\(X_t\)</span> is absolutely continuous, <span class="math inline">\(\E[f(X_t)] = \int_{\R^n} f(y)p_t(y)\dy\)</span> for some <span class="math inline">\(p\)</span>, and more generally, <span class="math display">\[
\E[f(X_t)|\mathcal F_s] = \int_{\R^n} f(y) p_{t-s}(X_s,y)\dy.
\]</span> Can <span class="math inline">\(p\)</span> be obtained as solution to PDE?</p>
<p>“Kolmogorov forward equation is dual of backward equation”: <span class="math inline">\(\int fp_t = \int P_tfp_0\)</span>.</p>
<p><strong>Theorem</strong> (Kolmogorov forward, Fokker-Planck): Assume niceness of the SDE, and <span class="math inline">\(b\in C^1, \si\in C^2\)</span>, <span class="math inline">\(\rh\in C^2\)</span>, <span class="math display">\[
\mathcal L^* \rh = -\sumo in \pd{x^i}(b^i\rh) + \rc2 \suij n \sumo km \pd{{}^2}{x^i\pl x^j} (\si^{ik}\si^{ij}\rh),
\]</span> <span class="math inline">\(p_t\)</span> exists, <span class="math inline">\(C^1\)</span> in <span class="math inline">\(t\)</span>, <span class="math inline">\(C^2\)</span> in <span class="math inline">\(x\)</span>. Then <span class="math display">\[(p_t)_t = \mathcal L^* p_t, \quad t\in [0,T].\]</span></p>
<p><em>Proof</em>.</p>
<ol type="1">
<li>Write Ito’s rule for <span class="math inline">\(f\)</span>.</li>
<li>Take <span class="math inline">\(\E\)</span> so the martingale disappears.</li>
<li>Substitute definition of <span class="math inline">\(p_t(y)\)</span>, integrate by parts to take <span class="math inline">\(\mathcal L\)</span> from <span class="math inline">\(\mathcal L f\)</span> to <span class="math inline">\(\mathcal L^* p_s\)</span>. This holds for all <span class="math inline">\(f\)</span> so remove the <span class="math inline">\(f\)</span>.</li>
<li>Take time derivative.</li>
</ol>
<blockquote>
<p>As a rule of thumb, the backward equation is very well behaved, and will often have a solution provided only that f is sufficiently smooth; the forward equation is much less well behaved and requires stronger conditions on the coefficients</p>
</blockquote>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2017-03-11</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2017-03-11.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2017-03-11.html</id>
    <published>2017-03-07T00:00:00Z</published>
    <updated>2017-03-07T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2017-03-11</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-03-07 
          , Modified: 2017-03-07 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#adversarial-training">Adversarial training</a></li>
 <li><a href="#langevin-monte-carlo">Langevin Monte Carlo</a></li>
 <li><a href="#boosting-etc.">Boosting, etc.</a></li>
 <li><a href="#speculative-thoughts">Speculative thoughts</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="adversarial-training">Adversarial training</h2>
<h2 id="langevin-monte-carlo">Langevin Monte Carlo</h2>
<p>Background</p>
<ul>
<li>Brownian motion (and other probability prerequisites)</li>
<li>Stochastic calculus</li>
<li>High-dimensional probability (LSI, etc.)</li>
<li>Langevin papers</li>
</ul>
<h2 id="boosting-etc.">Boosting, etc.</h2>
<p>Fix notes!</p>
<h2 id="speculative-thoughts">Speculative thoughts</h2>
<ul>
<li>Relationship between Rademacher width and size of <span class="math inline">\(\ep\)</span>-net?</li>
<li>Can we rephrase Frieze-Kannan regularity lemma as follows: there exists a small neural net that approximately computes cuts? Number of hidden nodes independennt of size of graph?
<ul>
<li>FK as: set of graphs is “small” in cut metric, so has a small <span class="math inline">\(\ep\)</span>-net. Rademacher?</li>
</ul></li>
<li>Can we show that certain solutions to problems are approximable by 2-layer neural nets, by showing that their Barron norm is small?
<ul>
<li>Ex. Reinforcment learning value function. Must assume some kind of geometry on state space. (For arbitrary partitions, trivial as all value vectors in <span class="math inline">\(\R^A\)</span>.)</li>
<li>On policy side: it’s clear that graph can be partitioned.</li>
<li>Convolution - what’s the analogue here? It’s very weird because functions form infinite-dimensional space, nonlinear functions on functions is weird.</li>
<li>Something about looking at <span class="math inline">\(\ep^{-\lg |S|}\)</span> nodes, depth <span class="math inline">\(-\lg \ep\)</span>?</li>
<li>Would be nice to study niceness of value, optimal policy in belief space.</li>
<li>Barron is a stronger condition than Lipschitz - says there aren’t many directions of variation.</li>
<li>If e.g. something radial, out of luck!</li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
