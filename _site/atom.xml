<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2016-09-19T00:00:00Z</updated>
    <entry>
    <title>DL generalization</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/DL_generalization.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/DL_generalization.html</id>
    <published>2016-09-19T00:00:00Z</published>
    <updated>2016-09-19T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>DL generalization</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-09-19 
          , Modified: 2016-09-19 
	</p>
      
       <p>Tags: <a href="/tags/dictionary%20learning.html">dictionary learning</a>, <a href="/tags/sparse%20coding.html">sparse coding</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#group-sparsity">Group sparsity</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="group-sparsity">Group sparsity</h2>
<p>Suppose that we have group sparsity with partition <span class="math inline">\(\bigsqcup S_i\)</span>.</p>
<p>In AGMM15, we get instead <span class="math inline">\(\ve{E_1+E_2+E_3}\le O^*\pa{\fc{\mu k^2 \ln^2 n}{m\sqrt n}\max |S_i|^2}\)</span>. For example, if <span class="math inline">\(\max|S_i|^2\)</span> is a constant, the bound is still the same. As long as this is <span class="math inline">\(\le O^*\pf{k}{m\ln m}\)</span>, the bound is still OK.</p>
<p>(Basically we get larger terms in <span class="math inline">\(E_2,E_3\)</span>: In <span class="math inline">\(E_2\)</span> we have <span class="math inline">\(\sum_{i,j\in S_k, i\ne j} q_i \be_i\be_i' A_jA_j^T\)</span>—this is larger because <span class="math inline">\(i,j\)</span> cooccur, we have <span class="math inline">\(q_i\)</span> rather than <span class="math inline">\(q_{i,j}\)</span>, <span class="math inline">\(q_i\)</span> is <span class="math inline">\(\Te\pf km\)</span> not <span class="math inline">\(\Te \pf{k^2}{m^2}\)</span>. Lump these terms into the 1st sum. Ditto for <span class="math inline">\(E_3\)</span>.)</p>
<p>See notebook 15 (end) for details.</p>
<p>Rely on <a href="/posts/math/algebra/linear/matrix_analysis/perturbation.html">modification of Davis-Kahan</a>.</p>
<p>Take <span class="math inline">\(u,v\)</span> and estimate <span class="math inline">\(M_{u,v} = \E \an{u,Ax}\an{v,Ax}(Ax)(Ax)^T\)</span>. Instead of taking the largest singular vector, take the subspace spanned by all singular values of size <span class="math inline">\(\ge C_1 \fc{k}{m}\)</span>. What is the angle between this subspace and the projection of this onto the space of <span class="math inline">\(A_{\cdot i}\)</span> where <span class="math inline">\(i\in \Supp(u)\cap \Supp(v)\)</span>? By generalized Davis-Kahan, <span class="math inline">\(\fc{C_0\fc km - C_1\fc km}{C_2 \fc{k}{m\ln m}} = \fc{C_3}{\ln m}\)</span>.</p>
<p>Now iteratively refine the set of subspaces <span class="math inline">\(\mathcal S\)</span>. Start with a subspace <span class="math inline">\(V\)</span>, and set of atoms <span class="math inline">\(\mathcal A=\phi\)</span>. Let <span class="math inline">\(\ep = \fc{C}{\ln m}\)</span>.</p>
<p>Do a DFS.</p>
<ul>
<li>If the angle between <span class="math inline">\(V\)</span> and any other subspace in <span class="math inline">\(\mathcal S\)</span> is <span class="math inline">\(&gt;\ep\)</span>, not counting subspaces where all canonical angles between <span class="math inline">\(V\)</span> and it are <span class="math inline">\(&lt;\ep\)</span>, then find the orthogonal complement of atoms that are close to being in <span class="math inline">\(V\)</span>, and let that be an atomic subspace. Now go upwards in the tree.</li>
<li>Otherwise, approximately intersect <span class="math inline">\(V\)</span> with a subspace making it smaller, to get <span class="math inline">\(W\)</span>. Draw <span class="math inline">\(V\to W\)</span>. Repeat with <span class="math inline">\(W\)</span>.</li>
</ul>
<p>NO: These subspaces aren’t spanned by the vectors!</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2016-09-17</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-09-17.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-09-17.html</id>
    <published>2016-09-19T00:00:00Z</published>
    <updated>2016-09-19T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2016-09-17</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-09-19 
          , Modified: 2016-09-19 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#threads">Threads</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="2016-09-10.html">Last week</a>.</p>
<h2 id="threads">Threads</h2>
<ul>
<li>Wrote up <a href="/posts/tcs/machine_learning/neural_nets/pmi_images.html">PMI summary</a>.</li>
<li>Conversation with Arora: incorporate 2nd paper into PMI experiments.</li>
<li>Learned <a href="/posts/math/algebra/linear/matrix_analysis/perturbation.html">matrix perturbation theory</a> and applied to <a href="/posts/tcs/machine_learning/matrices/DL_generalization.html">DL assuming group sparsity</a>.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>PCANet: A simple deep learning baseline for image classification?</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/pcanet.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/pcanet.html</id>
    <published>2016-09-10T00:00:00Z</published>
    <updated>2016-09-10T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>PCANet: A simple deep learning baseline for image classification?</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-09-10 
          , Modified: 2016-09-10 
	</p>
      
       <p>Tags: <a href="/tags/pca.html">pca</a>, <a href="/tags/neural%20net.html">neural net</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#pcanet-a-simple-deep-learning-baseline-for-image-classification">PCANet: A simple deep learning baseline for image classification?</a></li>
 <li><a href="#architecture">Architecture</a><ul>
 <li><a href="#variations">Variations</a></li>
 </ul></li>
 <li><a href="#experiments">Experiments</a></li>
 <li><a href="#questions">Questions</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="pcanet-a-simple-deep-learning-baseline-for-image-classification">PCANet: A simple deep learning baseline for image classification?</h2>
<p>Each stage in a CNN consists of 3 layers:</p>
<ul>
<li>convolutional filter bank</li>
<li>nonlinear processing</li>
<li>feature pooling</li>
</ul>
<p>Replace these:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">CNN</th>
<th style="text-align: left;">PCANet</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">convolutional filter bank</td>
<td style="text-align: left;">PCA filter</td>
</tr>
<tr class="even">
<td style="text-align: left;">nonlinear processing</td>
<td style="text-align: left;">binary quantization (hashing)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">feature pooling</td>
<td style="text-align: left;">block-wise histograms of binary codes</td>
</tr>
</tbody>
</table>
<p>Lesson: Deep learning has a lot of hype, but in fact simpler, better theoretically justifiable architectures can do just as well or better! In particular, one weakness of DL is that it depends on parameter tuning expertise and <em>ad hoc</em> tricks.</p>
<p>Examples:</p>
<ul>
<li>ScatNet (wavelet scattering network): filters are wavelet operators, so no learning is needed. (These don’t work so well when there is intra-class variability with illumination change and corruption.)</li>
<li>PCANet cf. OPCA.</li>
</ul>
<p>Motivations:</p>
<ol type="1">
<li>design a simple deep learning network which should be very easy, even trivial, to train and to adapt to different data and tasks.</li>
<li>such a basic network could serve as a good baseline for people to empirically justify the use of more advanced processing components or more sophisticated architectures for their deep learning networks.</li>
</ol>
<h2 id="architecture">Architecture</h2>
<p>Input: <span class="math inline">\(N\)</span> training images <span class="math inline">\(\{I_i\}_{i=1}^N\)</span> of size <span class="math inline">\(m\times n\)</span>.</p>
<ol type="1">
<li><p>Learn PCA: Vectorize all <span class="math inline">\(k_1\times k_2\)</span> patches, mean-center and put them all in a matrix <span class="math inline">\(X\in \R^{k_1k_2\times Nmn}\)</span><a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. Do rank <span class="math inline">\(L_1\)</span> PCA on <span class="math inline">\(X\)</span>. The <span class="math inline">\(l\)</span>th filter (<span class="math inline">\(l\in [L_1]\)</span>) is <span class="math display">\[ W_l^1 := mat_{k_1,k_2}(q_l(XX^T))\]</span> where <span class="math inline">\(mat_{k_1,k_2}\)</span> maps <span class="math inline">\(\R^{k_1k_2}\to \R^{k_1\times k_2}\)</span> and <span class="math inline">\(q_l\)</span> is the <span class="math inline">\(l\)</span> largest eigenvectors.</p>
<p>Now let <span class="math inline">\(I_i^l = I_i * W_l^1\)</span>. (Zero-pad <span class="math inline">\(I_i\)</span> so that <span class="math inline">\(I_i^l\)</span> has the same size.)</p>
You can apply this multiple times. Suppose we apply it twice to get <span class="math inline">\(I_i^{l_1, l_2}\)</span>.</li>
<li>Hashing: Binarize the outputs to get <span class="math display">\[\{b_{l_1,l_2} = H(I_i^{l_1,l_2})\}\]</span> where <span class="math inline">\(H\)</span> is Heaviside function. Let <span class="math display">\[T_i^{l_1} := \sumo{l_2}{L_2} 2^{l_2-1} b_{l_1,l_2}.\]</span> (I.e., treat <span class="math inline">\((b_{l_1,l_2})_{l_2=1}^{L_2}\)</span> categorically.)</li>
<li><p>Histogram: Express histogram with <span class="math inline">\(L_2\)</span> bins as a vector <span class="math inline">\(Bhist(T_i^l)\)</span>. The feature vector of <span class="math inline">\(I_i\)</span> is <span class="math display">\[f_i := \text{map Bhist }[T_i^{1:L_1}]\in \R^{2^{L_2}L_1B}.\]</span></p></li>
</ol>
<p><strong>On many layers</strong>: Note that we DON’T stack by repeating 1-3. Instead, 2-3 happen only once at the end. The stacking happens within 1—doing PCA multiple times.</p>
<p>Note:</p>
<ul>
<li>Nonoverlapping blocks are suitable for faces.</li>
<li>Overlapping blacks are useful for digits, textures, and objects.</li>
<li>Histogram gives some translation invariance (why??).</li>
<li>Model parameters <span class="math inline">\(k_1,k_2,L_1,L_2\)</span>. Ex. <span class="math inline">\(L_1=L_2=8\)</span>.</li>
<li>Two-stage PCANet is good.</li>
<li>PCANet with absolute rectification layer (??) after the first stage doesn’t help.</li>
<li>The overall process is linear. ?? Combining two stages. Two stages works better. Two-stage PCA filters have a low-rank factorization. It has <span class="math inline">\(L_1k_1^2 + L_2k_2^2\)</span> rather than $L_1L_2(k_1+k_2-1)^2 $ variables.</li>
</ul>
<h3 id="variations">Variations</h3>
<ul>
<li>RandNet: use random filters (from standard Gaussian)</li>
<li>LDANet: Use multi-class linear discriminant analysis. (Think: supervised version of PCA.) <a href="../tcs/machine_learning/matrices/lda.html">LDA</a></li>
</ul>
<p>(Does this mean LDANet will fail on things like concentric circles?)</p>
<h2 id="experiments">Experiments</h2>
<p>Face recognition, MNIST.</p>
<p>MNIST: basic (10000-2000-50000), rot, noise bg, image bg, etc.</p>
<h2 id="questions">Questions</h2>
<ul>
<li>What is “block size”? (vs. filter, image size)</li>
<li>How do you classify using PCANet? (ex. do you train a SVM on top?)</li>
<li>How about stacking?
<ul>
<li>An intriguing research direction will then be how to construct a more complicated (say more sophisticated filters possibly with discriminative learning) or deeper (more number of stages) PCANet.</li>
<li>Some preprocessing of pose alignment and scale normalization might be needed for good performance guarantee. The current bottleneck that keeps PCANet from growing deeper (e.g., more than two stages) is that the dimension of the resulted feature would increase exponentially with the number of stages.</li>
</ul></li>
</ul>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Or <span class="math inline">\(N(m-k_1+1)(n-k_1+1)\)</span> if you don’t go past the border.<a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[SV16] The mixing time of the Dikin walk in a polytope - a simple proof</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/SV16.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/SV16.html</id>
    <published>2016-09-07T00:00:00Z</published>
    <updated>2016-09-07T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[SV16] The mixing time of the Dikin walk in a polytope - a simple proof</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-09-07 
          , Modified: 2016-09-07 
	</p>
      
       <p>Tags: <a href="/tags/gradient.html">gradient</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <p>Problem: sample a point from the uniform distribution in a polytope <span class="math inline">\(K\subeq \R^n\)</span>. (This is used in many algorithms.)</p>
<p>Main theorem (informal): From a warm start (i.e., density <span class="math inline">\(\Pj(A)\le C\Pj_{U_K}(A)\)</span> where <span class="math inline">\(U_K\)</span> is the uniform distribution on <span class="math inline">\(K\)</span>), the Dikin walk mixes in time <span class="math inline">\(O(mn)\)</span> for a <span class="math inline">\(n\)</span>-dimensional polytope described with <span class="math inline">\(m\)</span> inequalities.</p>
<h2 id="dikin-walk">Dikin walk</h2>
For <span class="math inline">\(K\)</span> defined by <span class="math inline">\(a_i^Tx\ge b\)</span>, let
\begin{align}
F(x) :&amp;= -\sum_{i\in [m]} \ln (a_i^Tx-b_i)\\
H(x) = \nb^2F(x) &amp;= \sum_{i\in [m]} \rc{(a_i^Tx-b_i)^2}a_ia_i^T\\
\ve{v}_x^2 :&amp;= v^TH(x)v\\
\text{Dikin ellipsoid }\mathcal E_x:&amp;=\set{z}{\ve{z-x}_x\le 1}.
\end{align}
<p>Consider the random variable <span class="math inline">\(g_x\)</span> given by <span class="math display">\[ z = x + \fc{r}{\sqrt n} H(x)^{-\rc 2}g,\quad g\sim N(0,1),\]</span> i.e. it is a Gaussian with covariance matrix <span class="math inline">\(\fc{r^2}{n}H(x)^{-1}\)</span> centered at <span class="math inline">\(x\)</span>.</p>
<p>Let <span class="math inline">\(p_x\)</span> be the <span class="math inline">\(g_x\)</span> with the Metropolis filter applied, <span class="math display">\[ p_x(z) = \min\{g_x(z),g_z(x)\}\]</span> (stay at <span class="math inline">\(x\)</span> with remaining probability).</p>
<h2 id="theorem-and-proof">Theorem and Proof</h2>
<p>Define the cross-ratio distance <span class="math display">\[\si(x,y) = \fc{|\ol{xy}||\ol{pq}|}{|\ol{px}||\ol{qy}|},\]</span> where <span class="math inline">\(xy\)</span> hits the boundary of <span class="math inline">\(K\)</span> at <span class="math inline">\(p,q\)</span> (<span class="math inline">\(pxyq\)</span> are in that order). <span class="math inline">\(\ln (1+\si(x,y))\)</span> is a metric on <span class="math inline">\(K\)</span>.</p>
<p><strong>Theorem 1</strong>: Given a reversible random walk in <span class="math inline">\(K\)</span> with stationary distribution uniform, suppose <span class="math inline">\(\exists \De&gt;0\)</span> such that for all <span class="math inline">\(x,y\in K\)</span> with <span class="math inline">\(\si(x,y)\le \De\)</span>, <span class="math inline">\(\ve{p_x-p_y}_1\le 1-\Om(1)\)</span>. Then after <span class="math inline">\(O(\De^{-2})\)</span>, the lazy version of the walk from a warm start is within <span class="math inline">\(\rc4\)</span> TV distance from the uniform distribution.</p>
<p>(This is saying something about uniform continuity of <span class="math inline">\(x\mapsto p_x\)</span>. There’s a “smoothing out” effect.)</p>
<p>Proof of main theorem.</p>
<ol type="1">
<li><strong>Theorem 2</strong>: For the Gaussian Dikin walk with <span class="math inline">\(r\le \fc{\ep}{400}\pa{\ln \pf{200}{\ep}}^{-\fc 32}\)</span>, for <span class="math inline">\(x,y\in K\)</span> with <span class="math inline">\(\ve{x-y}_x\le \fc{r}{\sqrt n}\)</span>, <span class="math inline">\(\ve{p_x-p_y}\le \ep\)</span>.
<ul>
<li><strong>Lemma 3</strong>: Let <span class="math inline">\(r\le 1, c\ge 0, c\le \min\{r,\rc 3\}\)</span>, <span class="math inline">\(x,y\in K\)</span>. If <span class="math inline">\(\ve{x-y}_x\le \fc{c}{\sqrt n}\)</span>, then <span class="math inline">\(\ve{g_x-g_y}_1\le 3c\)</span>. (Proof: KL divergence between Gaussians, Pinsker’s inequality.)</li>
<li><strong>Lemma 4</strong>: Given <span class="math inline">\(\ep\in [0,\rc 2]\)</span>, for <span class="math inline">\(r\le \fc{\ep}{100}(\ln \fc{50}{\ep})^{-\fc 32}\)</span>, we have <span class="math inline">\(\ve{p_x-g_x}_1\le \ep\)</span>. (Proof: Hypercontractivity)</li>
<li>Use triangle inequality.</li>
</ul></li>
<li>For any polytope <span class="math inline">\(K\)</span> described using <span class="math inline">\(m\)</span> inequalities, <span class="math inline">\(\si(x,y) \ge \rc{\sqrt m}\ve{x-y}_x\)</span>.</li>
<li>Use Theorem 1. <span class="math inline">\(\si(x,y)\le \De\)</span> implies <span class="math inline">\(\ve{x-y}_x\le De\sqrt m\)</span>. Take <span class="math inline">\(\De =O\prc{\sqrt{mn}}\)</span>.</li>
</ol>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>LDA (Linear discriminant analysis)</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/lda.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/lda.html</id>
    <published>2016-09-04T00:00:00Z</published>
    <updated>2016-09-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>LDA (Linear discriminant analysis)</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-09-04 
          , Modified: 2016-09-04 
	</p>
      
       <p>Tags: <a href="/tags/LDA.html">LDA</a>, <a href="/tags/dimensionality%20reduction.html">dimensionality reduction</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    \begin{align}
\Ga_c &amp;= \rc{|S_c|} \sum_{i\in S_c} \ol X_i\\
\Si_c &amp;= \rc{|S_c|} \sum_{i\in S_c} (\ol X_i - \Ga_c)(\ol X_i - \Ga_c)^T\\
\Phi &amp;= \rc{C} \sumo cC  (\Ga_c-\Ga) (\Ga_c-\Ga)^T\\
\max_{V\in \R^{k\times L}, V^TV=I_L} \fc{\Tr(V^T\Phi V)}{\Tr(V^T \sumo cC \si_c v)}
&amp;= L_1\text{ principal eigenvectors of }(\wt \Phi = \pa{\sumo cC \Si_c}^+ \Phi).
\end{align}

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2016-09-10</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-09-10.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-09-10.html</id>
    <published>2016-09-04T00:00:00Z</published>
    <updated>2016-09-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2016-09-10</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-09-04 
          , Modified: 2016-09-04 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#threads">Threads</a></li>
 <li><a href="#conversation-with-arora">Conversation with Arora</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="threads">Threads</h2>
<p>Continue from <a href="2016-09-03.html">last week</a>.</p>
<ul>
<li>Work out optimality conditions for 2-layer net for DL.</li>
</ul>
<p>(Threshold PMI’s first?)</p>
<p><a href="../tcs/machine_learning/neural_nets/pcanet.html">PCANet</a>.</p>
<h2 id="conversation-with-arora">Conversation with Arora</h2>
<ul>
<li>PMI experiments
<ul>
<li>Is PMI the right measure? Are the feature vectors close to sparse? if the average image there is <span class="math inline">\(&gt;20\%\)</span> activation, then no.</li>
<li>Look for discriminative features, those that occur with one label and rarely for others.</li>
<li>Do SVM coefficients have negative weights?</li>
</ul></li>
<li>Things to read
<ul>
<li>[HRS16] Train faster, generalize better: stability of stochastic gradient descent <a href="https://arxiv.org/pdf/1509.01240v2">paper</a>. See also blog post.</li>
<li>Teacher-training net</li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Transductive learning</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/transduction.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/transduction.html</id>
    <published>2016-08-31T00:00:00Z</published>
    <updated>2016-08-31T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Transductive learning</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-31 
          , Modified: 2016-08-31 
	</p>
      
       <p>Tags: <a href="/tags/self-taught%20learning.html">self-taught learning</a>, <a href="/tags/dictionary%20learning.html">dictionary learning</a>, <a href="/tags/sparse%20coding.html">sparse coding</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p>From <a href="https://en.wikipedia.org/wiki/Transduction_(machine_learning)">wikipedia</a></p>
<blockquote>
<p>In logic, statistical inference, and supervised learning, transduction or transductive inference is reasoning from observed, specific (training) cases to specific (test) cases. In contrast, induction is reasoning from observed training cases to general rules, which are then applied to the test cases. The distinction is most interesting in cases where the predictions of the transductive model are not achievable by any inductive model. Note that this is caused by transductive inference on different test sets producing mutually inconsistent predictions.</p>
</blockquote>
<p>Ex. Semi-supervised clustering</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Self-taught learning ([RBLPN07] Self-taught Learning: Transfer Learning from Unlabeled Data)</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/self_taught_learning.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/self_taught_learning.html</id>
    <published>2016-08-31T00:00:00Z</published>
    <updated>2016-08-31T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Self-taught learning ([RBLPN07] Self-taught Learning: Transfer Learning from Unlabeled Data)</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-31 
          , Modified: 2016-08-31 
	</p>
      
       <p>Tags: <a href="/tags/self-taught%20learning.html">self-taught learning</a>, <a href="/tags/dictionary%20learning.html">dictionary learning</a>, <a href="/tags/sparse%20coding.html">sparse coding</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#definition">Definition</a></li>
 <li><a href="#approach">Approach</a></li>
 <li><a href="#discussion">Discussion</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="http://www.andrewng.org/portfolio/self-taught-learning-transfer-learning-from-unlabeled-data/">website</a></p>
<h2 id="definition">Definition</h2>
<p>Unlabeled data (ex. random Internet images, unlimited audio) need not have the same class labels or generative distribution as labeled data. This is different from</p>
<ul>
<li>semi-supervised learning setting (same distribution/classes)</li>
<li>transfer learning (requires labels for both groups)</li>
</ul>
<p>Makes ML easier and cheaper. Much of human learning is believed to be from unlabeled data. (Order of magnitude: <span class="math inline">\(10^{14}\pat{synapses}/10^9s=10^5\)</span>bits/s)</p>
<!-- (? Ex. maybe just overlap in dictionary features?)-->
<h2 id="approach">Approach</h2>
<ol type="1">
<li><p>Use sparse coding to construct higher-level features using unlabeled data. (cf. representation learning)</p>
They use <span class="math inline">\(L^1\)</span> norm regularization. <span class="math inline">\(\min_{A,x, \ve{A_{\cdot j}}\le 1} \sum_i \ve{y^{(i)} - Ax}_2^2 + \be \ve{x}_1\)</span>. Use AM.</li>
<li>For each input, do sparse recovery (same objective, fixing the <span class="math inline">\(A\)</span> now). This is a convex problem.</li>
<li><p>Now apply supervised learning algorithm, e.g. SVM.</p></li>
</ol>
<h2 id="discussion">Discussion</h2>
<p>?? Sparse coding model also suggests a specific specialized similarity function (kernel) for the learned representations. Once the bases b have been learned using unlabeled data, we obtain a complete generative model for the input x. Thus, we can compute the Fisher kernel to measure the similarity between new inputs.</p>
<p>(Disadvantages of PCA: linear and undercomplete)</p>
<p>What about auto-encoders and NMF?</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Nash equilibrium</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/algorithmic_game_theory/nash_equilibrium.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/algorithmic_game_theory/nash_equilibrium.html</id>
    <published>2016-08-31T00:00:00Z</published>
    <updated>2016-08-31T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Nash equilibrium</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-31 
          , Modified: 2016-08-31 
	</p>
      
       <p>Tags: <a href="/tags/game%20theory.html">game theory</a>, <a href="/tags/Nash.html">Nash</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#definitions">Definitions</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="definitions">Definitions</h2>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Tensor decomposition</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/tensors/tensor_decomposition.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/tensors/tensor_decomposition.html</id>
    <published>2016-08-30T00:00:00Z</published>
    <updated>2016-08-30T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Tensor decomposition</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-30 
          , Modified: 2016-08-30 
	</p>
      
       <p>Tags: <a href="/tags/tensors.html">tensors</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#basics">Basics</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="basics">Basics</h2>
<p>See “Tensor methods” in <code>new_thread.pdf</code>.</p>
<p>Review questions:</p>
<ol type="1">
<li>Define tensor and tensor decomposition. When is it unique? Discuss the intractability of tensor problems. Why would we work with them? Give an example where matrix methods fail but tensor methods work because of uniqueness.</li>
<li>What properties of matrix rank fail for tensor rank?</li>
<li>(*) Give Jeinrich’s algorithm. When does it work, and what is its complexity?
<ul>
<li>Suppose <span class="math inline">\(T=\sum^r u_i\ot v_i\ot w_i\)</span>, where <span class="math inline">\(\{u_i\}\)</span> is linearly independent, <span class="math inline">\(\{v_i\}\)</span> is l.i., and every pair in <span class="math inline">\(\{w_i\}\)</span> is l.i. Then this decomposition is unique and there is an efficient algorithm to find it.</li>
<li>For a tensor <span class="math inline">\(T\in V^{\ot 3}\)</span>, view <span class="math inline">\(T_{\bullet}\in V\ot V^*\ot V^*\)</span>, with <span class="math inline">\(\bullet\)</span> corresponding to the last <span class="math inline">\(V^*\)</span>. The slices are sums of rank 1 matrices <span class="math inline">\(u_iv_i^*\)</span> with coefficients given by the projections of <span class="math inline">\(w_i\)</span> onto <span class="math inline">\(a\)</span>.
\begin{align}
T_a &amp;=\sum \an{w_i,a}u_iv_i^* =: UD_aV^*
T_aT_b^+ &amp;= UD_aD_b^+ U^+\\
T_b^+T_a &amp;= VD_b^+D_a V^+
\end{align}
WHP diagonalization recovers <span class="math inline">\(U,V\)</span>.</li>
</ul></li>
<li>Give a perturbation bound for finding eigenvalues and eigenvectors of a matrix. What does the bound depend on, in what way (polynomial?)? Make it quantitative. Use this to give a bound on convergence in tensor decomposition.</li>
<li>What is the Kruskal rank, why is it important?</li>
<li>Describe the phylogenetic tree/HMM setup. How can you solve it with tensor methods?</li>
<li>What about the non-full-rank case? (Give a reduction from noisy parity.)</li>
<li>Give the block stochastic model. When is it information theoretically posible to find the groups (is this exact, or up to some error)? Compare spectral algorithms with tensor methods. (What are the assumptions? Why are tensor methods more flexible?)</li>
<li>(*) Describe and prove the tensor algorithm for the block stochastic model.</li>
<li>(*) Give a tensor algorithm for a pure topic model, and then generalize under latent dirichlet allocation. Why is the Dirichlet distribution particularly nice here? Find the complexity and error in terms of parameters.</li>
<li>(*) Give a tensor algorithm for independent component analysis. There is a dependence on non-Gaussianlity: explain.</li>
<li>Define the cumulants of a distribution. Why are they nice to work with, and how do they help in algorithms?</li>
</ol>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
