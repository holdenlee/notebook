<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2016-05-01T00:00:00Z</updated>
    <entry>
    <title>LDC's: directions</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/coding/directions.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/coding/directions.html</id>
    <published>2016-05-01T00:00:00Z</published>
    <updated>2016-05-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>LDC's: directions</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-05-01 
          , Modified: 2016-05-01 
	</p>
      
       <p>Tags: <a href="/tags/LDC.html">LDC</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#learningreading">Learning/reading</a></li>
 <li><a href="#approaches">Approaches</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="learningreading">Learning/reading</h2>
<ul>
<li>Understand the inequalities from convex geometry.</li>
<li>Approximating the norm with a polynomial.</li>
<li>Understand the relationship with tensor eigenvectors.</li>
<li>Metric geometry.</li>
</ul>
<h2 id="approaches">Approaches</h2>
<ul>
<li>Any lower bound for a <em>perfect</em> LCC.</li>
<li>Some kind of SoS relaxation—understand ARV first.</li>
<li>Zeev’s conjecture on covering the image of a polynomial map of fixed degree with convex sets.</li>
<li>Comparison to convex hull. See <a href="lcc_geometry.html">LCC geometry</a>.
<ul>
<li>The image of the decoding map <span class="math inline">\(D:[-1,1]^n\to [0,1]^n\)</span> contains many points that are far apart, corresponding to the codewords. This gives that the convex hull of the image has large volume or a large <span class="math inline">\(\ep\)</span>-net.</li>
<li>The image is close to convex, so the image is also large.</li>
<li>However, the Jacobian of the map is small, so the volume of the image is small, contradiction.</li>
</ul></li>
<li>Directly bound convex hull. Ex. for 2-query, upper bound uses matrix concentration. More general concentration? (Ex. for tensors)</li>
<li>Use differential geometry. Some notion of curvature?</li>
<li>Phrase in terms of eigenvectors (or almost-eigenvectors) for higher-order tensors.</li>
<li>Vague
<ul>
<li>Reduce to a problem of geometry.</li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 4-30-16</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-04-30.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-04-30.html</id>
    <published>2016-04-30T00:00:00Z</published>
    <updated>2016-04-30T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 4-30-16</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-30 
          , Modified: 2016-04-30 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p>Machine learning</p>
<ul>
<li>Talk with Arora on NMF. Suggestions/directions:
<ul>
<li>Consider adding smoothing (regularizer). Smoothing is widely used in NLP. It would keep gradient updates from blowing up near the edge of <span class="math inline">\(\De_n\)</span>.</li>
</ul></li>
<li>TODO: Read Mike Collins’s notes on NLP.</li>
<li>Nicolaus Boumal sent me references on optimization on manifolds, which I glanced through. TODO: Spend a day, or a few days reading through Optimization on Manifolds.</li>
<li>Started reading Vishnoi’s slime mold paper. TODO: keep going.</li>
<li>Reviewed Ch. 9-10 of Convex Optimization by BV.</li>
<li>TODO: Make sure I have a good understanding of intro convex opt (ex. look at a course), and find more advanced books.</li>
</ul>
<p>Math</p>
<ul>
<li>Convex bodies and isotropicity.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Constrained optimization</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/constrained.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/constrained.html</id>
    <published>2016-04-28T00:00:00Z</published>
    <updated>2016-04-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Constrained optimization</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-28 
          , Modified: 2016-04-28 
	</p>
      
       <p>Tags: <a href="/tags/convex%20optimization.html">convex optimization</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#setup">Setup</a></li>
 <li><a href="#newton">Newton</a><ul>
 <li><a href="#feasible-start">Feasible start</a></li>
 <li><a href="#infeasible-start">Infeasible start</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="setup">Setup</h2>
<p>Consider <span class="math inline">\(\min_{Ax=b}f\)</span>.</p>
<ul>
<li>How can we reduce this problem to one without inequality constraints? Let <span class="math inline">\(x_0+Fu\)</span> parametrize <span class="math inline">\(\set{x}{Ax=b}\)</span>.</li>
<li>Why wouldn’t you want to do this? It can be inefficient—somehow the equality constraint captures more of the problem’s structure.</li>
</ul>
Give the example for quadratic minimization. What do the KKT equations say?
\begin{align}
\min_{Ax=b} \rc 2 x^T P x + q^T x + r &amp; \\
Px^* + q + A^T \nu^*&amp;=0\\
\iff \matt{P}{A^T}{A}0 \coltwo{x^*}{\nu^*} &amp;= \coltwo{-1}{b}.\label{eq:kkt-mat}
\end{align}
<p>This is unbounded below if there exist <span class="math inline">\(v,w\)</span> such that <span class="math inline">\(Pv+A^Tw=0\)</span>, <span class="math inline">\(Av=0\)</span>, <span class="math inline">\(-q^Tv+b^Tw&gt;0\)</span> (left-multiply by <span class="math inline">\((v^T\,w^T)\)</span>) to see that the equation above doesn’t have a solution).</p>
<p>Recall the dual function. Why do we care about it? <span class="math display">\[g(\la,\nu) = \min_x f+\nu^T (Ax-b) = -\nu^Tb - f^*(-\nu^TA), \qquad f^(y)=\max y^Tx - f(x).\]</span> Equality constraints disappear in the dual. If the dual is nice, we can just solve an unconstrained problem <span class="math inline">\(\max_{\la \ge 0} g(\la,\nu)\)</span>.</p>
<h2 id="newton">Newton</h2>
<h3 id="feasible-start">Feasible start</h3>
<p>Describe the Newton method starting with a feasible <span class="math inline">\(x, Ax=b\)</span>. The Newton step is the minimizer for the quadratic approximation under the constraint. (Finding the minimum of a quadratic amounts to solving a linear equation.) Using  <span class="math display">\[\matt{\nb^2 f}{A^T}A0 \coltwo{\De x_{nt}}w = \coltwo{-\nb f}{0}.\]</span> Define <span class="math display">\[\la(x) = (\De x_{nt}^T \nb^2 f \De x_{nt})^{\rc 2}\]</span>.</p>
<p>Note this <em>is</em> normal Newton if you use a parametrization.</p>
<p>(Convexity makes the KKT matrix invertible.)</p>
<h3 id="infeasible-start">Infeasible start</h3>
<p>Describe the infeasible start Newton method. Approximate <span class="math inline">\(f\)</span> by a quadratic using <span class="math inline">\(P=\nb^2 f\)</span> in  and find <span class="math inline">\(\De x_{nt}\)</span> so that <span class="math inline">\(x+\De x_{nt}\)</span> satisfies the KKT conditions. The equation for <span class="math inline">\(\De x_{nt}\)</span> is <span class="math display">\[\matt{\nb^2 f}{A^T}A0 \coltwo{\De x_{nt}}w = \coltwo{-\nb f}{b-Ax}.\]</span></p>
<p>Note here we’re just updating <span class="math inline">\(x\)</span> by solving for <span class="math inline">\(\De x_{nt}\)</span>. Ech time <span class="math inline">\(w\)</span> is treated just as an auxiliary variable. But <span class="math inline">\(w\)</span> comes from the dual solution <span class="math inline">\(\nu\)</span>. Can we look at <span class="math inline">\((x,\nu)\)</span> together as a primal-dual pair and update both of them?</p>
Let the residual be
\begin{align}
r &amp;= (\nb f + A^T \nu, Ax-b).
\end{align}
<p>Instead of minimizing <span class="math inline">\(f\)</span>, we minimize the residual for the KKT conditions. The residual has a component for minimizing <span class="math inline">\(f\)</span>, and a component for trying to satisfy <span class="math inline">\(Ax=b\)</span>.</p>
<p>Here is the algorithm.</p>
<ol type="1">
<li>Start with <span class="math inline">\(x\)</span> (not necessarily satisfying <span class="math inline">\(Ax=b\)</span>).</li>
<li>Calculate <span class="math inline">\(\De x_{nt}\)</span>, keeping track of <span class="math inline">\(\nu\)</span>. Backtrack (by multiplying by <span class="math inline">\(\be\)</span>) until you find <span class="math inline">\(t\)</span> such that <span class="math display">\[\ve{r(x+t\De x_{nt}, \nu + t\De \nu_{nt})}_2\le (1-\al t) \ve{r}_2.\]</span></li>
<li>Repeat until <span class="math inline">\(Ax=b\)</span>, <span class="math inline">\(\ve{r}_2\le \ep\)</span>.</li>
</ol>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Convexity</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/convexity.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/convexity.html</id>
    <published>2016-04-23T00:00:00Z</published>
    <updated>2016-04-23T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Convexity</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-23 
          , Modified: 2016-04-23 
	</p>
      
       <p>Tags: <a href="/tags/convex%20optimization.html">convex optimization</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#questions">Questions</a></li>
 <li><a href="#answers">Answers</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="questions">Questions</h2>
<ul>
<li>Define strongly convex. What estimates can you make using strong convexity? Why is it useful?</li>
<li>Define smoothness. What does it give you?</li>
<li>Define condition number for sets and matrices. What is the relationship? What do bounds on <span class="math inline">\(\nb^2\)</span> give on condition number?</li>
</ul>
<h2 id="answers">Answers</h2>
<ul>
<li>Strongly convex means <span class="math display">\[ f(y) \ge f(x) + \nb f(x)^T (y-x) + \fc m2 \ve{y-x}_2^2. \]</span> For twice-differentiable functions, this is equivalent to <span class="math inline">\(\nb^2 f\succeq mI\)</span>. Strong convexity lower-bounds suboptimality: if the gradient is small, then you are not too far from the minimum, because the gradient changes fast. (Ex. for <span class="math inline">\(c+dx+\fc m2x^2\)</span>, the minimum is at <span class="math inline">\(-\fc dm = -\rc m \nb f\)</span>.) The following inequality hold: (bound the optimal value in terms of the gradient, and bound the distance in terms of the gradient.
\begin{align}
p^* &amp;\ge f(x) - \rc{2m}\ve{\nb f(x)}_2^2\\
\ve{x-x^*}_2 &amp; \le \fc 2m \ve{\nb f}_2^2.
\end{align}</li>
<li>Smoothness means <span class="math display">\[ f(y) \le f(x) + \nb f(x)^T (y-x) + \fc M2 \ve{y-x}_2^2. \]</span> For twice-differentiable functions, this is equivalent to <span class="math inline">\(\nb^2 f\preceq MI\)</span>. Smoothness upper-bounds suboptimality, and ensures that step sizes don’t overshoot the minimum (much)
\begin{align}
p^* &amp;\le f(x) - \rc{2M}\ve{\nb f}_2^2.
\end{align}</li>
<li>The condition number of a convex body <span class="math inline">\(C\)</span> is (<span class="math inline">\(W_{\max},W_{\min}\)</span> are max and min width) <span class="math display">\[ \text{cond} (C) = \fc{W_{\max}^2}{W_{\min}^2}.\]</span> For an ellipsoid defined by <span class="math inline">\(x^TA^{-1}x\le 1\)</span>, <span class="math display">\[ \text{cond} (\mathcal E) = \fc{\la_{\max}(A)}{\la_{\min}(A)} = \ka(A).\]</span> Strong convexity and smoothness together bound the condition number of sublevel sets <span class="math inline">\(C_\al =\set{x}{f(x)\le \al}\)</span>. We have for <span class="math inline">\(p^*&lt;\al\)</span>, <span class="math display">\[B_{\sfc{2(\al-p^*)}{M}}\subeq C_\al \subeq B_{\sfc{2(\al-p^*)}{m}},\]</span> <span class="math inline">\(\text{cond}(C_\al)\le \fc{M}{m}\)</span>. Moreover, <span class="math inline">\(\lim_{\al\to 0^+} \text{cond}(C_\al) = \ka (\nb f(x^*))\)</span>.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Convex problems</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/convex_problems.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/convex_problems.html</id>
    <published>2016-04-23T00:00:00Z</published>
    <updated>2016-04-23T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Convex problems</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-23 
          , Modified: 2016-04-23 
	</p>
      
       <p>Tags: <a href="/tags/convex%20optimization.html">convex optimization</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p>Express the following as convex optimization problems.</p>
<ul>
<li>Quadratic minimization
<ul>
<li>Least squares</li>
</ul></li>
<li>Geometric programming</li>
<li>Analytic center</li>
</ul>
\begin{align}
&amp; \min \rc 2 x^T P x + q^T x + r, &amp;P\in S_+^h\\
&amp; \min \ve{Ax-b}_262, &amp;P=A^TA, q=A^Tb \text{ matching with above}\\
&amp; (\text{optimality conditions: } Px^*+q=0, \quad A^TAx^* = A^Tb\\
&amp; \min \ln \pa{\sum \exp(a_i^Tx+b_i)}\\
&amp; \min - \sum \ln(b_i - a_i^Tx)\\
&amp; \min \ln \det(F(x)^{-1}),&amp; F(x) = F_0+\sum_i x_i F_i.
\end{align}

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Second-order methods</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/second-order.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/second-order.html</id>
    <published>2016-04-22T00:00:00Z</published>
    <updated>2016-04-22T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Second-order methods</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-22 
          , Modified: 2016-04-22 
	</p>
      
       <p>Tags: <a href="/tags/Newton.html">Newton</a>, <a href="/tags/second-order.html">second-order</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#main-points">Main points</a></li>
 <li><a href="#proofs">Proofs</a></li>
 <li><a href="#convergence">Convergence</a></li>
 <li><a href="#more-intuition">More intuition</a></li>
 <li><a href="#implementation-issues">Implementation issues</a></li>
 <li><a href="#scraps">Scraps</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>See <a href="GD.html">gradient descent</a>.</p>
<ul>
<li>What is the general framework?
<ol type="1">
<li>Pick a descent direction <span class="math inline">\(\De x\)</span>.</li>
<li>Choose a step size <span class="math inline">\(\tau&gt;0\)</span>: <span class="math display">\[x^{(t+1)} \leftarrow x+\tau \De x.\]</span></li>
<li>Continue until stop criterion.</li>
</ol></li>
</ul>
<h2 id="main-points">Main points</h2>
<ul>
<li>What’s the shortcoming of gradient descent that we want to fix?
<ul>
<li>It is not invariant under linear transformation.</li>
<li>When the condition number of the Hessian is large, it has bad convergence.</li>
</ul></li>
<li>Steepest descent
<ul>
<li>For a norm <span class="math inline">\(\ved\)</span>, the steepest descent direction is (sd = steepest descent, nsd = normalized steepest descent)
\begin{align}
\De x_{sd} &amp;= \min_{\ve{y}\le 1} \nb f(x)^T y.
\end{align}</li>
</ul></li>
<li>Newton method
<ul>
<li>Let
\begin{align}
\De x_{sd} &amp;= -H^{-1}\nb f\\
\De x_{nsd} &amp;=\fc{H^{-1} \nb f}{\ve{H^{-\rc 2} \nb f}} = \fc{H^{-1} \nb f}{\la(x)^2}\\
\la(x) &amp;= \ve{H^{-\rc 2}\nb f}^{\rc 2} = (\nb f^T H^{-1} \nb f)^{\rc 2}.
\end{align}
Here <span class="math inline">\(\la(x)\)</span> is the Newton decrement.</li>
</ul></li>
<li>Newton for functions with Lipschitz Hessian: The number of steps to reach <span class="math inline">\(\ep\)</span> is <span class="math display">\[\fc{f(x^{(0)}-p^*)}{\ga} + \ln\ln \fc{\ep_0}{\ep}, \ep_0=\fc{2m^3}{L^2}.\]</span></li>
<li><span class="math inline">\(f\)</span> is <strong>self-concordant</strong> if for all <span class="math inline">\(v\)</span>, <span class="math inline">\(\an{\nb^3 f, \De x^{\ot 3}} \le 2 \an{\nb^2 f, \De x^{\ot2}}^{\fc 32}\)</span>. For self-concordant functions, the number of steps to reach <span class="math inline">\(\ep\)</span> is <span class="math display">\[\fc{f(x^{(0)}-p^*)}{\ga} + \log_2\log_2\prc{\ep}\]</span> where <span class="math inline">\(\ga = \fc{\al \be (1-2\al)^2}{20-8\al}\)</span>.</li>
<li>What are the drawbacks of Newton’s method and how to fix them?
<ul>
<li>Naively it requires computing <span class="math inline">\(H^{-1}\)</span> which (naively?) takes <span class="math inline">\(n^3\)</span> time each iteration, where <span class="math inline">\(n\)</span> is the dimension. <em>For large <span class="math inline">\(n\)</span> this is prohibitive.</em></li>
</ul></li>
</ul>
<h2 id="proofs">Proofs</h2>
<ul>
<li>Newton method is steepest descent for <span class="math inline">\(H\)</span>:
<ul>
<li>Let <span class="math inline">\(A\)</span> be a symmetric positive definite matrix. Defining <span class="math inline">\(\ved_A\)</span> as follows, we note that the dual norm is the norm corresponding to the inverse.
\begin{align}
\ve{A} :&amp;= x^TAx = \ve{\sqrt A}_2\\
\ve{x}_{A}^* &amp; = \ve{x}_{A^{-1}}.
\end{align}
<ul>
<li><em>Proof.</em> <span class="math display">\[\ve{x}_A^* = \max_{\ve{y}_A=1} x^Ty \stackrel{z = \sqrt{A}y}{=} \max_{\ve{z}=1} x^T A^{-\rc 2} z = \ve{A^{-\rc 2}x^T}  = \ve{x}_{A^{-1}}.\]</span></li>
<li>This calculation also shows that <span class="math display">\[\amin_{\ve{y}_A=1} v^Ty = \fc{A^{-1} x}{\ve{A^{-\rc 2}x}}.\]</span></li>
<li>Thus,
\begin{align}
\De x_{nsd} &amp;= \fc{H^{-1} \nb f(x)}{\ve{H^{-\rc 2} \nb f}}.
\end{align}</li>
</ul></li>
<li>Why is <span class="math inline">\(\De x_{nsd}\)</span> the right normalization? <span class="math display">\[ f\pa{x - t\fc{H^{-1} \nb f(x)}{\ve{H^{-\rc 2} \nb f}}} \approx f(x) + (-t+\fc{t^2}2) \fc{H^{-1} \nb f(x)}{\ve{H^{-\rc 2} \nb f}}\]</span> and <span class="math inline">\(t=1\)</span> minimizes this.</li>
</ul></li>
</ul>
<h2 id="convergence">Convergence</h2>
<ul>
<li>What is the main idea of Newton descent? Why/how do we get quadratic convergence?
<ul>
<li>Go in the direction of the minimum suggested by the Hessian (second order approximation).</li>
<li>The proof is organized as follows. If <span class="math inline">\(\ve{\nb f(x^{(k)})^2}\)</span> is
<ul>
<li>large (<span class="math inline">\(\ge \eta\)</span>), then make constant progress by <span class="math inline">\(\ga\)</span>. This is the dampled phase; steps are small. Steps are small because the default Newton step <span class="math inline">\(t=1\)</span> is bad, and backtracking will choose a smaller <span class="math inline">\(t\)</span>. (?) (It’s “slow” in the sense it’s not quadratically convergent, but it makes constant progress.)</li>
<li>small (<span class="math inline">\(&lt;\eta\)</span>), then make quadratic progress, <span class="math inline">\(\fc{L}{2m^2}\ve{\nb f(x^{(k+1)})}_2\le \ve{\nb f(x^{(k)})}_2^2\)</span>.</li>
</ul></li>
</ul></li>
<li>What should <span class="math inline">\(\eta\)</span> depend on? <span class="math inline">\(\al\)</span> (the slope for backtracking), <span class="math inline">\(m\)</span> (strong convexity parameter), <span class="math inline">\(L\)</span> (Lipschitz constant on Hessian).</li>
</ul>
<em>Proof (for Lipschitz)</em>. We do the calculations for 1 dimension. The calculations are the same, except we have to use matrices and vectors. Let <span class="math inline">\(\te(y)\)</span> be a quantity in <span class="math inline">\([-y,y]\)</span>. Suppose we are at <span class="math inline">\((0,0)\)</span>. Let <span class="math inline">\(d=f'(0), a=f''(0), \la = \fc{d}{a^{\rc 2}}\)</span>, <span class="math inline">\(\De x_{nt} = \fc{d}{a} = \fc{\la}{a^{\rc 2}}\)</span>.
\begin{align}
f(x) &amp;= dx + \rc 2 ax^2 + \te\pa{\rc 6 Lx^3}\\
f\pa{-\fc da} &amp;= -\rc 2 \fc{d^2}a\\
&amp;\le -\fc{d^2}{a} ]ub{\pa{\rc 2 - \rc 6 L \fc{d}{a^2}}}{\ge \al}.
\end{align}
<p>In order for the quantity to be <span class="math inline">\(\ge \al\)</span>, noting <span class="math inline">\(\fc{d}{a^2} = \fc{d}{a^{\fc 32}}\)</span>, we want <span class="math inline">\(\la \le \fc{3(1-2\al) a^{\fc 32}}L\)</span>; it’s sufficient for <span class="math inline">\(f'\le \fc{3(1-2\al)m^2}{L}\)</span>.</p>
Note that unlike for linear convergence, we don’t prove something like <span class="math inline">\(f(x') - f(x^*)\le \cdots\)</span>. We have to work with the gradients to get quadratic convergence. (Gradients also control the distance to the optimum.) We have
\begin{align}
f'(x) &amp;= d+ ax + \te\pa{\rc L x^2}\\
f'\pa{-\fc da} &amp; \le \rc 2 L\fc{d^2}{a^2} \le \fc{L}{2m^2}f'(0).
\end{align}
<em>Proof (for self-concordant)</em>. Work in 1-D. Instead of integrating <span class="math inline">\(\int f'''=\int (f'')^{\fc 32}\)</span>, we let <span class="math inline">\(F(y)=y^{-\rc 2}\)</span> and integrate <span class="math inline">\(\int (F(f''))'\)</span>.
\begin{align}
|(f'')^{-\rc 2} (t)| &amp;= \ab{\int_0^t ((f'')^{-\rc 2})'} =\int_0^t \ab{-\rc 2 (f'')^{-\fc 32} f'''}\le t\\
\label{eq:f''}
\implies
\rc{(-t + f''(0)^{-\rc 2})^2} &amp;\ge f''(t) \ge \rc{(t+f''(0)^{-\rc 2})^2}\\
\implies
\rc{-t+f''(0)^{-\rc 2}} - f''(0)^{\rc 2} &amp; \ge f'(t)-f'(0) \ge \rc{t+f''(0)^{-\rc 2}} + f''(0)^{\rc 2}\\
\implies f(t) &amp;\le f(0) + (f'(0) - f''(0)^{\rc 2}) t + \ln \pf{f''(0)^{-\rc 2}}{t + f''(0)^{-\rc 2}}\\
\implies f\pa{-\fc{f'}{f''}t} &amp; \le f(0) - \la^2 t + \la t - \ln (1-t\la(x)).
\end{align}
<p>Now consider 2 cases.</p>
<ul>
<li><span class="math inline">\(\la(x^{(k)})&gt;\eta\)</span>. We show <span class="math inline">\(f(x^{(k+1)}) - f(x^{(k)}) \le -\ga\)</span>.
<ul>
<li>Using <span class="math inline">\(-x + \ln (1+x) + \fc{x^2}{2(1+x)}\le 0\)</span>, for <span class="math inline">\(t=\rc{1+\la(x)}\)</span>,
\begin{align}
f(-\fc{f'}{f''} t) &amp; \le f(0) - \al \la(x)^2 t\\
t=\rc{1+\la(x)^2}\implies f(-\fc{f'}{f''} t) &amp; \le f(0) - \al \la(x)^2 t
\end{align}
so backtracking line search chooses <span class="math inline">\(t\ge \fc{\be}{1+\la(x)}\)</span> with <span class="math display">\[f(-\fc{f'}{f''} t)  \le -\al \be \fc{\la^2}{1+\la} \le -\al \be \fc{\eta^2}{1+\eta}.\]</span></li>
</ul></li>
<li><span class="math inline">\(\la(x^{(k)})\le \eta\)</span>. We show <span class="math inline">\(2\la(x^{(k+1)})\le (2\la(x^{(k)}))^2\)</span>. (Note we are not getting a bound on <span class="math inline">\(f'(x_{k+1})\)</span> like before because we don’t have strong convexity, which upper-bounds <span class="math inline">\(f''(x)^{-\rc 2}\)</span>.)
<ul>
<li>We take a unit step because <span class="math display">\[f(-\fc{f'}{f''})   = f(0) - \la^2 + \la - \ln (1-\la(x))\le f(0)-\al \la(x)^2\]</span> when <span class="math inline">\(\la(x) \le \fc{1-2\al}{2}\)</span>.</li>
<li>Self-concordance gives multiplicative bound on how <span class="math inline">\(H\)</span> changes, <span class="math inline">\((1-t\al)^2 H \preceq H(x+tv) \preceq \rc{(1-t\al)^2} H(x)\)</span>, where <span class="math inline">\(\al=\ve{v}_{H^{\rc 2}}\)</span>. Proof:
\begin{align}
\ln f''(t) - \ln f''(0) &amp; = \int_0^t (\ln f'')' \\
&amp; \le \int_0^t (2f'')^{\rc 2}\\
&amp;\le \int_0^t \fc{2}{-x+(f'')^{-\rc 2}} &amp;\eqref{eq:f''}\\
&amp;\le 2\ln (-t (f'')^{\rc 2} + 1).
\end{align}</li>
<li>Now we show <span class="math inline">\(\la(x-\fc{f'}{f''}) \le \fc{\la^2}{(1-\la)^2}\)</span>. Using the multiplicative bound (here <span class="math inline">\(t=1\)</span>, <span class="math inline">\(v=-\fc{f'}{f''}\)</span>, <span class="math inline">\(\al=\la\)</span>.
\begin{align}
f''(x_{k+1})&amp;\in f''(x_k) [1-\la, \rc{1-\la}]\\
f'(x_{k+1}) &amp; \in f'(x_k) + \ba{-\rc{\fc{f'}{f''} + \rc{(f'')^{\rc 2}}} + (f'')^{\rc 2}, \rc{-\fc{f'}{f''}+\rc{(f'')^{\rc2}}} - (f'')^{\rc 2}}\\
&amp;= f'(x_k) \ba{\fc{\la}{-\la + 1}, \fc{\la}{\la+1}}\\
\la(x_{k+1})&amp;\le \fc{\la(x_k)^2}{(1-\la(x_k))^2} \\ &amp; \le 2\la^2
\end{align}
where the last inequality is when <span class="math inline">\(\la\le \rc 4\)</span>.</li>
</ul></li>
<li>Finally, bound the distance to optimum by <span class="math inline">\(\la\)</span>, <span class="math inline">\(f(x^{(l)})-p^* \le \la(x^{(l)})^2\)</span>.</li>
</ul>
<h2 id="more-intuition">More intuition</h2>
<ul>
<li>Why do we use <span class="math inline">\(\ved_H\)</span>?
<ul>
<li>It appears in the second-order Taylor approximation. <span class="math inline">\(f(x) + \nb f^T v + \rc 2 v^T \nb^2 f v\)</span> has a minimum at <span class="math inline">\(\fc{H^{-1}f}{\ve{H^{-\rc 2}f}}\)</span>, not in the gradient direction!</li>
<li>It is invariant to linear transformation: <span class="math inline">\(\De x_{nsd} (f\circ A) = \De x_{nsd} f\)</span>.</li>
</ul></li>
<li>What is the relationship to Newton’s method of finding zeros?
<ul>
<li>The Newton’s method here is the Newton zero-finding method applied to <span class="math inline">\(f'\)</span>.</li>
</ul></li>
<li>Does order <span class="math inline">\(c\)</span> give <span class="math inline">\(2^{-n^c}\)</span> convergence?
<ul>
<li>Probably. But it’s rare to get <span class="math inline">\(&gt;2\)</span> order information.</li>
</ul></li>
<li>Give an example where gradient descent performs poorly compared to Newton. <span class="math inline">\(f = x_1^2 + \ep x_2^2\)</span>, <span class="math inline">\(-\nb f = (-2x_1,-2\ep x_2)\)</span>.</li>
<li>Intuition for self-concordance
<ul>
<li>We relax the requirement that <span class="math inline">\(f\)</span> is strongly convex and has Lipschitz Hessian. We’re allowed to have <span class="math inline">\(f''\to 0\)</span>; the requirement is that when <span class="math inline">\(f''\)</span> is small so is <span class="math inline">\(f'''\)</span>. Note <span class="math inline">\(\fc{(f'')^{\fc 32}}{f'''}\)</span> is dimensionless.</li>
</ul></li>
</ul>
<h2 id="implementation-issues">Implementation issues</h2>
<ol type="1">
<li>Precompute line searches: it can be more efficient to simultaneously compute <span class="math inline">\(f(x+t\De x)\)</span> for many values of <span class="math inline">\(t\)</span>, e.g., if it involves a linear/matrix function.</li>
<li>Computing <span class="math inline">\(\De x_{nt}=H^{-1}\nb f(x)\)</span> is more efficient if <span class="math inline">\(H\)</span> has band structure, sparsity, etc.</li>
</ol>
<h2 id="scraps">Scraps</h2>
<p>? 9.31,</p>
<p>estimating Hessian?</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>NMF algorithm</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/nmf_algorithm.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/nmf_algorithm.html</id>
    <published>2016-04-22T00:00:00Z</published>
    <updated>2016-04-22T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>NMF algorithm</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-22 
          , Modified: 2016-04-22 
	</p>
      
       <p>Tags: <a href="/tags/NMF.html">NMF</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#setup">Setup</a></li>
 <li><a href="#preliminary-calculations">Preliminary calculations</a></li>
 <li><a href="#actual-calculations">Actual calculations</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="setup">Setup</h2>
<p>The setup for alternating minimization is as follows.</p>
<ol type="1">
<li>(Decoding) Let <span class="math inline">\(x \leftarrow \text{decode}_{A^{(t)}} y\)</span>.</li>
<li>(Gradient step) Let <span class="math display">\[A^{(t+1)} = A^{(t)} - \eta \pi(\nb L_x(A))\]</span> where the loss function is the KL-divergence (negative log-likelihood)
\begin{align}
L_x(A) &amp;= \sum y_i \ln \pf{y_i}{Ax_i}\\
\nb L_x(A) &amp;= (-y\odot \rc{Ax}) x^T
\end{align}
and <span class="math inline">\(\pi\)</span> is projection to the probability simplex.</li>
</ol>
<p>The plan is to show this satisfies the conditions needed for approximate gradient descent: each step is correlated with the right direction. We want to find <span class="math inline">\(\al,\be,\ep\)</span> so that <span class="math display">\[ \an{g,A-A^*} \ge \al \ve{A-A^*}^2 + \be \ve{g}^2 - \ep\]</span> (for some norm).</p>
<p>We have to be careful about 2 things.</p>
<ul>
<li>Uniqueness doesn’t hold in general—ex. we can expand the simplex, so there would be a manifold of solutions—but it does hold under separability. We have to use this somehow.</li>
<li>The more complicated the decoding map is, the harder this is to analyze. The MLE estimator is difficult to work with because there is no explicit form. The inverse is probably easier to work with. It gives a biased estimate, but this may be OK if we only want to get close enough (ex. from <span class="math inline">\(\rc{\log n}\)</span> to <span class="math inline">\(\rc{n}\)</span>).</li>
</ul>
<p>We proceed in 2 lemmas.</p>
<ul>
<li>When <span class="math inline">\(A\approx A^*\)</span>, the decodings satisfy <span class="math inline">\(x\approx x^*\)</span>.</li>
<li>When <span class="math inline">\(x\approx x^*\)</span>, the descent direction for <span class="math inline">\(L_x(A)\)</span> is correlated with <span class="math inline">\(A-A^*\)</span>.</li>
</ul>
<p>As a first step, show that <span class="math display">\[\an{\EE_y \pa{-y\odot \pa{\rc{Ax} -\rc{A^*x}}x^T} , A-A^*}&gt;0.\]</span></p>
<h2 id="preliminary-calculations">Preliminary calculations</h2>
First suppose <span class="math inline">\(y=A^*x\)</span> (no noise), <span class="math inline">\(x=x^*\)</span> (perfect decoding). Then letting <span class="math inline">\(D= \diag\prc{(Ax)_i}\)</span>,
\begin{align}
\an{\EE_y \pa{-y\odot \pa{\rc{Ax} -\rc{A^*x}}x^T} , A-A^*}
&amp;= x^T\ub{(A-A^*)^T D (A-A^*)}{=:M^2}x\\
&amp;= \ve{x}_{M}^2.
\end{align}
Now relax <span class="math inline">\(y\approx A^*x\)</span>, We get
\begin{align}
\an{\EE_y \pa{-y\odot \pa{\rc{Ax} -\rc{A^*x}}x^T} , A-A^*}
&amp;= \EE_{y} \ve{x}_M^2 + \cancelto0{(x^TA^{*T} - y^T) D (A-A^*)x}
\end{align}
<p>using <span class="math inline">\(\E y = A^* x\)</span>.</p>
Now relax <span class="math inline">\(x\approx x^*\)</span>. (Warning: <span class="math inline">\(x^*\)</span> simply being close may not imply convergence to <span class="math inline">\(A^*\)</span> without additional assumptions on <span class="math inline">\(A^*\)</span>, because of nonuniqueness.)
\begin{align}
\EE_{x,y} \an{-D yx^T, A-A^*}
&amp;= \EE_{x,y} [-y^T D(A-A^*) x]\\
&amp;= \EE_{x,y} [-y^T DA^*x]-1\\
\end{align}
<p>The dependencies of the random variables are <span class="math inline">\(x^* \to y \to x\)</span> (<span class="math inline">\(\E_y = A^*x^*\)</span>). We can’t simply replace <span class="math inline">\(\E y = A^*x^*\)</span> because we have to average over <span class="math inline">\(x\)</span> first, which depends on the decoding. (If we could replace <span class="math inline">\(y\)</span> like that, we get <span class="math inline">\(\an{x,x-x^*}_M\)</span>.)</p>
<h2 id="actual-calculations">Actual calculations</h2>
<p>Recall that if we’re decoding by multiplying by <span class="math inline">\(B\)</span>, we also have to threshold, <span class="math inline">\(\text{Th}(Bx)\)</span>.</p>
In Theorem 4.1, if <span class="math inline">\(A\)</span> is biased, then we instead obtain a bound <span class="math display">\[ (By)_i - \E x_i = \ub{(By)_i - BA^*x}{\text{w.h.p. }\le 2\la(A) \sfc{\ln r}{n}} + B(A^*-A) x^*. \]</span> For the second term, <span class="math display">\[ \ve{B(A-A^*)x}_{\iy} \le |B|_{\iy} \max_i \ve{A_{\cdot i} - A_{\cdot i}^*}_1 \le \la \ep.\]</span> We want to lower bound
\begin{align}
\an{\nb A, A-A^*}
&amp;= \sum \fc{y_iA_{ij}^*}{(Ax)_i} - 1\\
&amp;= \sum_i \fc{y_i(A^*x)_i}{(Ax)_i} - 1\\
&amp;= \sum \fc{b_i(A^* x)_i}{(Ax)_i} +
\sum_i \fc{(A^*(x^*-x))_i (A^*x)_i}{(Ax)_i} + \sum_{i,j} \pa{(A^*x)_i\sfc{(Ax)_j}{(Ax)_i} - (A^*x)_j\sfc{(Ax)_i}{(Ax)_j}}^2.
\end{align}
<p>We may suppose <span class="math inline">\(\ve{A_{\bullet i} - A_{\bullet i}^*}\le \rc{\poly\log(n)}\)</span>, or something like this.</p>
<p>Try 2: INCORRECT: I mixed up <span class="math inline">\(x,x^*\)</span> here. <span class="math display">\[\an{\nb A, A-A^*} = \an{\pa{y_i \sfc{(Ax)_j}{(Ax)_i} - y_j \sfc{(Ax)_i}{(Ax)_j}}_{ij}, \pa{(A^*x)_i  \sfc{(Ax)_j}{(Ax)_i} - (A^*x)_j  \sfc{(Ax)_i}{(Ax)_j}}_{ij}}.
\]</span> It’s tempting to take <span class="math inline">\(\E_y\)</span> first, but we can’t do that.</p>
<p>If <span class="math inline">\(x=x^*, y= A^*x^*\)</span> then we write this as a sum of squares above. (This is probably the same as writing <span class="math inline">\(\ve{x}_M^2\)</span> from the previous section…) This is Lagrange’s identity.</p>
<p>We want to lower-bound by <span class="math display">\[ \al \ve{A-A^*}_F^2 + \be \ve{\pf{y_i}{(Ax)_i}}_2^2 \ve{x}_2^2 - \ep.
\]</span></p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Convex geometry</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/analysis/metric/convex_geometry.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/analysis/metric/convex_geometry.html</id>
    <published>2016-04-20T00:00:00Z</published>
    <updated>2016-04-20T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Convex geometry</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-20 
          , Modified: 2016-04-20 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <p>Notes from Vershynin’s course <a href="http://www-personal.umich.edu/~romanv/papers/GFA-book/GFA-book.pdf">Geometric functional analysis</a>.</p>
<h2 id="functional-analysis-and-convex-geometry">Functional analysis and convex geometry</h2>
<p><strong>Theorem (Approximate Caratheodory)</strong>: Let <span class="math inline">\(x\in \conv A\)</span>. There exist <span class="math inline">\(N\)</span> points <span class="math inline">\(x_i\in A\)</span> such that <span class="math display">\[\ve{x-\rc N\sumo iN x_i}\le \fc{r(A)}{N}.\]</span> (Given <span class="math inline">\(x\)</span> as a linear combination of elements of <span class="math inline">\(A\)</span>, there is a probabilistic algorithm to find this combination.)</p>
<p>(Here, <span class="math inline">\(r(A)\)</span> is the radius of <span class="math inline">\(A\)</span>.)</p>
<p><em>Proof</em>. If <span class="math inline">\(x=\sum a_i z_i\)</span>, sample by <span class="math inline">\(a_i=\Pj(x=z_i)\)</span>. By Chebyshev, <span class="math display">\[\E\ve{x-\rc N \sumo jN z_j}\le \fc{r(A)^2}{N}.\]</span></p>
<p><em>Remark</em>: This can be <a href="../../../tcs/coding/ldc.html">adapted to <span class="math inline">\(L_p\)</span></a>.</p>
<h2 id="banach-mazur-distance">Banach-Mazur distance</h2>
<p>(Skipped.)</p>
<h2 id="concentration-of-measure-and-euclidean-sections-of-convex-bodies">Concentration of measure and Euclidean sections of convex bodies</h2>
<p>Observation: Convex bodies like spheres tend to have a lot of mass concentrated in the “middle”. This is a very powerful observation: concentration of measure for a set implies concentration for Lipschitz functions on the set.</p>
<p>Johnson-Lindenstrauss says that a random projection approximately preserves distances. A sophisticated way to look at this is the following. We can view this as a concentration result: the norm of the projection of <span class="math inline">\(x\)</span> is a Lipschitz function; it must be concentrated around its mean.</p>
<p>A powerful application of concentration of measure is Dvoretzky’s Theorem (big generalization of JL?), which find large Euclidean-like subspaces in general Banach spaces. (What’s the relationship between the <span class="math inline">\(\log\)</span> here and in Dvoretzky?)</p>
<h3 id="concentration-of-measure">Concentration of measure</h3>
<p>For a set <span class="math inline">\(A\)</span> let <span class="math inline">\(A_\ep\)</span> denote the <span class="math inline">\(\ep\)</span>-neighborhood.</p>
<h4 id="sphere">Sphere</h4>
<ol type="1">
<li>(Isoperimetric inequality) Among all sets with given measure, spherical caps minimize <span class="math inline">\(\si(A_\ep)\)</span>. (Proof?)</li>
<li>Let <span class="math inline">\(\si\)</span> be the measure on the sphere. For any measurable <span class="math inline">\(A\subeq \bS^{n-1}\)</span> with <span class="math inline">\(\si(A)\ge \rc 2\)</span>, <span class="math inline">\(\si(A_\ep)\ge 1-e^{-\fc{n\ep^2}2}\)</span>. Proof:
<ul>
<li>For the equator <span class="math inline">\(E\)</span>, <span class="math inline">\(\si(E_\ep)\ge 1-2e^{-\fc{n\ep^2}{2}}\)</span>. Use isoperimetric inequality.</li>
</ul></li>
<li>Corollary: For <span class="math inline">\(f:\bS^{n-1}\to \R\)</span> 1-Lipschitz, letting <span class="math inline">\(M\)</span> be the median, <span class="math display">\[\si(|f-M|\le \ep) \ge 1-2e^{-\fc{n\ep^2}{2}}.\]</span> This remains true if <span class="math inline">\(M\)</span> is replaced by the mean.</li>
</ol>
<h4 id="gaussians">Gaussians</h4>
<p>Consider <span class="math inline">\(\R^n\)</span> with measure <span class="math inline">\(\ga\)</span> given by <span class="math inline">\(N(0,I)\)</span>.</p>
<ol type="1">
<li>(Isoperimetric inequality) Among all sets with given measure, halfspaces minimize <span class="math inline">\(\si(A_\ep)\)</span>.</li>
<li>If <span class="math inline">\(\ga(A)\ge \rc2\)</span>, then <span class="math inline">\(\ga(A_t)\ge 1-e^{-\fc{t^2}{2}}\)</span>.</li>
<li>Corollary: For <span class="math inline">\(f:\R^{n-1}\to \R\)</span> 1-Lipschitz, letting <span class="math inline">\(M\)</span> be the median, <span class="math display">\[\ge(|f-M|\le \ep) \ge 1-2e^{-\fc{n\ep^2}{2}}.\]</span> This remains true if <span class="math inline">\(M\)</span> is replaced by <span class="math inline">\((\E|X|^p)^{\rc p}\)</span> for any <span class="math inline">\(p\ge 1\)</span>.</li>
</ol>
<h3 id="johnson-lindenstrauss">Johnson-Lindenstrauss</h3>
<p>See <a href="../../../tcs/algorithms/jl.html">JL</a>.</p>
<p><strong>Theorem (Johnson-Lindenstrauss)</strong>: Let <span class="math inline">\(|X|=n\)</span>, <span class="math inline">\(X\)</span> a subset of Hilbert space. For any <span class="math inline">\(\ep&gt;0\)</span>, there exists a <span class="math inline">\((1+\ep)\)</span>-embedding of <span class="math inline">\(X\)</span> into <span class="math inline">\(\ell_2^k\)</span>, <span class="math inline">\(k\ge \fc{C}{\ep^2}\ln n\)</span>.</p>
<p><em>Proof.</em> Project randomly using gaussians. We can directly bound the concentration, but let’s be more sophisticated.</p>
<ol type="1">
<li>Project randomly using gaussians. Let <span class="math inline">\(f:\R^{k\times n}\to \R\)</span> be defined by <span class="math inline">\(G\mapsto \ve{Gx}_2\)</span>. This is 1-Lipschitz. Use concentration of measure for Gaussian space, and then union bound.</li>
<li>Take a uniformly random projection (ise the Grassmannian, or equivalently, take a uniform rotation followed by a fixed projection). Let <span class="math inline">\(f:\bS^{n-1}\to \R\)</span> be defined by <span class="math inline">\(x\mapsto \ve{Tx}_2\)</span>. Use concentration of measure for the sphere.</li>
</ol>
<h3 id="dvoretzkys-theorem">Dvoretzky’s Theorem</h3>
<h4 id="statements">Statements</h4>
<p>Dvoretzky finds large Euclidean subspaces.</p>
<ol type="1">
<li><strong>Theorem (General Dvoretzky)</strong>: Let <span class="math display">\[M(K) = \int_{\bS^{n-1}}\ve{x}\,d\si(x).\]</span> There exists a subspace <span class="math inline">\(E\)</span>, <span class="math inline">\(\dim E=c(\ep) nM^2\)</span> such that <span class="math display">\[ \ve{x}\in [1-\ep,1+\ep] M \ve{x}_2.\]</span> We can take <span class="math inline">\(c(\ep) = C\fc{\ep^2}{\ln \rc{\ep}}\)</span>.</li>
<li><strong>Theorem (Dvoretzky)</strong>: Let <span class="math inline">\(X\)</span> be <span class="math inline">\(n\)</span>-dimensional Banach. Given <span class="math inline">\(\ep&gt;0\)</span> there exists a subspace <span class="math inline">\(E\)</span> of <span class="math inline">\(X\)</span> of dimension <span class="math inline">\(k=k(n,\ep)\to \iy\)</span> as <span class="math inline">\(n\to \iy\)</span>, such that <span class="math inline">\(d(E,\ell_2^k)\le 1+\ep\)</span>.</li>
<li><strong>Theorem (Geometric Dvoretzky)</strong>: Let <span class="math inline">\(K\)</span> be a symmetric convex body in <span class="math inline">\(\R^n\)</span>. Given any <span class="math inline">\(\ep &gt; 0\)</span>, there exists a section <span class="math inline">\(K \cap E\)</span> of <span class="math inline">\(K\)</span> by a subspace <span class="math inline">\(E\)</span> of <span class="math inline">\(\R^n\)</span> of dimension <span class="math inline">\(k = k(n, \ep)\to \iy\)</span> as <span class="math inline">\(n \to \iy\)</span> such that <span class="math inline">\(E \subeq K \subeq (1 + \ep)\mathcal E\)</span> for some ellipsoid <span class="math inline">\(\mathcal E\)</span>.</li>
</ol>
<p>There is an alternative formulation for gaussian space, which is often computationally easier.</p>
<p>Define <span class="math inline">\(\ell_X:=\pa{\int_{\R^n} \ve{x}^2\,d\ga_n(x)}^{\rc 2} = (\E\ve{g}^2)^{\rc 2}\)</span>. This is off from <span class="math inline">\(M\)</span> by a factor of <span class="math inline">\(\sqrt n\)</span>: <span class="math display">\[ \ell_X\sim \sqrt n M_X.\]</span> Thus we can replace <span class="math inline">\(M_X\)</span> by <span class="math inline">\(\fc{\ell_X}{\sqrt n}\)</span> in the bound.</p>
<h4 id="proofs">Proofs</h4>
<ol type="1">
<li>Show it suffices to bound on <span class="math inline">\(\ep\)</span>-nets, and bound the size of the smallest <span class="math inline">\(\ep\)</span>-net.
<ol type="1">
<li>Let <span class="math inline">\(\mathcal{N}_\de\)</span> be a <span class="math inline">\(\de\)</span>-net of <span class="math inline">\(S_X\)</span>. Then
\begin{align}
\ve{T}&amp;\le \rc{1-\de} \sup_{x\in \mathcal{N}_\de}\ve{Tx}\\
\inf_{y\in S_X} &amp;\ge \inf_{x\in N} - \de\ve{T}.
\end{align}
Applying this to the identity map from <span class="math inline">\(\ved_2\)</span> to <span class="math inline">\(\ved\)</span>, obtain: if <span class="math inline">\(\ve{x}\in [1-\ep,1+\ep]M\)</span> for all <span class="math inline">\(x\in \cal N\)</span>, then for all <span class="math inline">\(x\in \bS^{n-1}\)</span>, <span class="math display">\[\ve{x} \in \ba{1-\ep-2\de, \pf{1+\ep}{1-\de}M}.\]</span></li>
<li>There is an <span class="math inline">\(\ep\)</span>-net of size <span class="math inline">\(\pa{1+\fc 2\ep}^n\)</span>.</li>
</ol></li>
<li>General Dvoretzky: Apply concentration of measure to a fixed vector <span class="math inline">\(x\)</span> of the function <span class="math inline">\(\ved\)</span>. Union bound over a <span class="math inline">\(\de\)</span>-net and approximate the sphere by the <span class="math inline">\(\de\)</span>-net using 1.</li>
<li>(Aside) We can calculate <span class="math inline">\(\ell_X\)</span> for many spaces. Standard concentration bounds give
\begin{align}
1\le p\le 2\implies \ell_{\ell_p^n} &amp;= c(\ep) n&amp;\implies k(\ell_p^n) &amp;\ge c(\ep)n\\
q\ge 2\implies \ell_{\ell_q^n} &amp;= c(\ep) q n^{\fc 2q}&amp;\implies k(\ell_q^n) &amp; \ge c(\ep)q n^{\fc 2q}\\
\ell_{\ell_{\iy}^n} &amp; c\sqrt{\ln n} &amp;\implies k(\ell_\iy^n) &amp; \ge c(\ep)\ln n.
\end{align}</li>
<li>We show <span class="math inline">\(k(\ell_\iy^n) \asymp\ln n\)</span>. Spherical caps!</li>
</ol>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Perfect LCCs</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/coding/perfect_lcc.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/coding/perfect_lcc.html</id>
    <published>2016-04-13T00:00:00Z</published>
    <updated>2016-04-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Perfect LCCs</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-13 
          , Modified: 2016-04-13 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#proof-1">Proof 1</a></li>
 <li><a href="#proof-2">Proof 2</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>There are many proofs showing we must have <span class="math inline">\(n=2^k\)</span> for perfect 2 LCCs. Can we extend these methods to give</p>
<ul>
<li>exponential lower bounds for LCCs?</li>
<li>lower bounds for perfect <span class="math inline">\(q\)</span>-LCCs, <span class="math inline">\(q&gt;2\)</span>?</li>
</ul>
<p>A perfect <span class="math inline">\(q\)</span>-LCC of input length <span class="math inline">\(k\)</span> in <span class="math inline">\(n\)</span> dimensions is a set of <span class="math inline">\(2^k\)</span> points in <span class="math inline">\(\{\pm 1\}^n\)</span>, together with <span class="math inline">\(n\)</span> unions of perfect <span class="math inline">\(q\)</span>-matchings <span class="math inline">\(M_i\)</span> (or just matchings) and a sign <span class="math inline">\(s_m\)</span> for each <span class="math inline">\(m\in M_i\)</span>, such that on codewords, the decoding process defined by taking any <span class="math inline">\(m\in M_i\)</span> and taking <span class="math inline">\(s_m \prod_{j\in m}x_i\)</span> recovers <span class="math inline">\(x_i\)</span> with probability 1. In other words, <span class="math display">\[\EE_{m\in M_i} s_m\prod_{j\in m} x_j = 1.\]</span></p>
<h2 id="proof-1">Proof 1</h2>
<p>For <span class="math inline">\(q=2\)</span>, these are quadratic forms <span class="math inline">\(Q_i\)</span>, with associated matrices <span class="math inline">\(A_i\)</span>. The fact that they are matchings means that <span class="math inline">\(A_i = \rc nS_i\)</span> where <span class="math inline">\(S_i\)</span> is (doubly) stochastic. (In fact, we can deal more generally with perfectly smooth LCCs that recover perfectly on codewords.) The codewords are those with <span class="math display">\[Q_i(x)=x \iff \an{x,A_ix}=x_i.\]</span> Now <span class="math inline">\(S\)</span>, being stochastic, satisfies <span class="math inline">\(\ve{S}_{\iy\to \iy} \le 1\)</span>. Now <span class="math inline">\(\ve{x}_{\iy}\le 1\)</span> so <span class="math inline">\(\ve{A_i x}_{\iy}\le \rc{n}\)</span>. We have <span class="math display">\[n=\ve{x}_1 =\sum_i |\an{x,A_ix}|\le \sum_i \ve{A_ix}_{\iy}\ve{x}_1=n,\]</span> so equality holds and <span class="math inline">\(A_ix = x_ix\)</span>.</p>
<p>This means the <span class="math inline">\(x\in C\)</span> are simultaneous eigenvalues for the <span class="math inline">\(A_i\)</span>. The sequences of eigenvectors are different, so <span class="math inline">\(|C|\le n\)</span>, i.e., <span class="math inline">\(2^k\le n\)</span>. Equality is acheived for the Hadamard code.</p>
<p>To extend this: some notion of “approximate eigenvector,” “well-conditioned linear dependency”?</p>
<h2 id="proof-2">Proof 2</h2>
<p>For LDCs whose matching correspond to a group action, every <span class="math inline">\(i\)</span> corresponds to a matching <span class="math inline">\(M_i=\{(y, y+x_i)\}\)</span>. Now for any <span class="math inline">\(\ep\in \{-1,1\}^k\)</span>, there must exist a set <span class="math inline">\(S\)</span>, the support of the codeword <span class="math inline">\(x=C(\ep)\)</span>, for which <span class="math inline">\(M_i\)</span> only has edges in <span class="math inline">\(S\)</span> if <span class="math inline">\(\ep_i=1\)</span>, and <span class="math inline">\(M_i\)</span> only has edges between <span class="math inline">\(S,S^c\)</span> if <span class="math inline">\(\ep_i=-1\)</span>. We must have <span class="math inline">\(|S|=\fc n2\)</span>.</p>
<p>Now if <span class="math inline">\(x_k=\sum_{i=1}^{k-1} a_ix_i\)</span>, consider the set <span class="math inline">\(S\)</span> where <span class="math inline">\(\ep_i=1\)</span> iff <span class="math inline">\(a_i=1\)</span>, but <span class="math inline">\(\ep_{k}=-1\)</span>. Then <span class="math inline">\(S = S+x_i\)</span> for all <span class="math inline">\(i\)</span>, and <span class="math inline">\(S\ne S+x_k\)</span>, contradiction.</p>
<p>This is very much related to the first proof. There, one can use a linear dependency argument to show there can’t be more than <span class="math inline">\(n\)</span> eigenvectors with distinct sequences of eigenvalues. (Make this relationship more explicit?)</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Notes index</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/notes_index.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/notes_index.html</id>
    <published>2016-04-08T00:00:00Z</published>
    <updated>2016-04-08T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Notes index</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-08 
          , Modified: 2016-04-08 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
