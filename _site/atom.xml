<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2017-04-01T00:00:00Z</updated>
    <entry>
    <title>Alignment for advanced machine learning systems</title>
    <link href="http://holdenlee.github.io/notebook/posts/cs/ai/control/aaml_workshop.html" />
    <id>http://holdenlee.github.io/notebook/posts/cs/ai/control/aaml_workshop.html</id>
    <published>2017-04-01T00:00:00Z</published>
    <updated>2017-04-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Alignment for advanced machine learning systems</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-04-01 
          , Modified: 2017-04-01 
	</p>
      
       <p>Tags: <a href="/tags/ai%20safety.html">ai safety</a>, <a href="/tags/machine%20learning.html">machine learning</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#inductive-ambiguity">Inductive ambiguity</a><ul>
 <li><a href="#formalization">Formalization</a></li>
 </ul></li>
 <li><a href="#environment-goals">Environment goals</a><ul>
 <li><a href="#level-1">Level 1</a></li>
 <li><a href="#level-2">Level 2</a></li>
 <li><a href="#philosophy-view">Philosophy view</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <p>See</p>
<ul>
<li><a href="alignment_ml.html">Alignment</a></li>
<li><a href="concrete.html">Concrete problems</a></li>
</ul>
<p><a href="https://workflowy.com/#/4296816d1051">Workflowy notes (private)</a></p>
<h2 id="inductive-ambiguity">Inductive ambiguity</h2>
<p>What’s the problem with KWIK?</p>
<ul>
<li>Too restrictive - many things with small VC dimension can take exponentially many samples with KWIK. (In some sense this is necessary, cf. if only one example is 1.)</li>
<li>Requires realizability.</li>
</ul>
<h3 id="formalization">Formalization</h3>
<p>Dimensions to vary</p>
<ul>
<li>Supervised, then test; vs. online/active</li>
<li>How do you measure supervision? KWIK penalized by 1 for each time it asks user. We can e.g. instead be content with decaying rate of asking.</li>
</ul>
<p>Filter class <span class="math inline">\(F\)</span> and hypothesis class <span class="math inline">\(H\)</span>. Suppose there exists <span class="math inline">\(f,h\)</span> such that <span class="math inline">\(fh(x)=y, 1-y, \perp\)</span> with probability <span class="math inline">\(p, 0, 1-p\)</span>. We want to find <span class="math inline">\(\wh f\wh h\)</span> with <span class="math inline">\(\wh f \wh h(x)=y,1-y, \perp\)</span> with probabilities <span class="math inline">\(p+\ep, \ep', 1-p-\ep-\ep'\)</span> with <span class="math inline">\(\ep'\ll \ep\)</span>.</p>
<p>Neural net anomaly detection:</p>
<ul>
<li>RBF</li>
<li>only predict around points seen</li>
<li>[HG17] A BASELINE FOR DETECTING MISCLASSIFIED AND OUT-OF-DISTRIBUTION EXAMPLES IN NEURAL NETWORKS</li>
</ul>
<h2 id="environment-goals">Environment goals</h2>
<p>2-3 levels of problem.</p>
<ol type="1">
<li>Human can evaluate/give accurate feedback.</li>
<li>Human can evaluate, but agent can prevent human from giving feedback.</li>
<li>Human cannot evaluate.</li>
</ol>
<p>(Lump 2 and 3 together.)</p>
<h3 id="level-1">Level 1</h3>
<p>This is easiest for formalize and tackle as a theory problem. Assume there is a distinguishing function (conservative concept?) that excludes all bad outcomes.</p>
<p>Under human distribution of actions, reward given corresponds to value.</p>
<p>But after gaming (ex. realizing human only checks in 1 place to check room is clean), agent leaves human distribution of actions, and inferred/represented reward stops corresponding to value. Either from maintaining multiple hypothesis of reward function, or human feedback, it realizes this.</p>
<p>Going back and forth, can continuously improve. Can it generalize over human pushback?</p>
<p>Alternative: have separate agent/part of agent that acts as predictor - holds model of world, job is to predict, e.g., whether there’s a strawberry.</p>
<p>Question: can’t you just embed the “plagiarism” problem in here?</p>
<p>Maybe the problem considered here is more concrete: There’s a better notion of what a strawberry/plate is than a good story.</p>
<h3 id="level-2">Level 2</h3>
<p>Note that “putting self in simulation” is a relative term. It means “fooling all its sensors.” If it has a world-model, this means the harder task of fooling the world-model. (Think of world-model itself as a sensor system.) (Maybe the world-model asks for proofs?)</p>
<p>Why can’t you just have a world model or adversarial predictor? Problem if there is no good evaluator.</p>
<p>This contains the conservative concept problem and the reward hacking problem. (I think.) Solving the informed oversight problem is sufficient.</p>
<ul>
<li>Conservative concept: a human non-evaluable task means that every hypothesis class we could train with human-curated data, we could not distinguish between the real and fake thing. There is impossibile without access to the agent’s internal state.</li>
<li>Reward hacking: how not just to keep hitting reward button. Related to shutdown problem—preventing human from changing its reward, for instance.</li>
<li>Informed oversight: give human a human-understandable transcript that would help make a decision about whether value achieved (ex. “I put myself in a simulation.”). This is sufficient.</li>
</ul>
<h3 id="philosophy-view">Philosophy view</h3>
<p>It is impossible to refer to the physical world. Our mapping from physical actions to mental representations is many-to-one; many ways of moving our arm all get translated into a mental story of “picking up the strawberry.” There are many ways to execute this task.</p>
<p>We only live in our own conceptual space. This space is highly bound/coupled to actual physics. (There’s no glitch in the universe that if I move my arm in a specific way, Konami code, I shut down the universe.) Any way I move my arm is roughly the same.</p>
<p>The AI solves tasks within its own conceptual space. We can evaluate that the AI is doing the right thing insofar as it is transparent, we can look at its world model, point at the concept of “strawberry” and see that it’s close enough to our own. We can solve “environmental goals” if the intersection of ontologies is nonempty, and the goal is within that intersection.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Neural nets as kernel space</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/nn_kernel.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/nn_kernel.html</id>
    <published>2017-03-23T00:00:00Z</published>
    <updated>2017-03-23T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Neural nets as kernel space</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-03-23 
          , Modified: 2017-03-23 
	</p>
      
       <p>Tags: <a href="/tags/neural%20net.html">neural net</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#things-this-could-explain">Things this could explain</a></li>
 <li><a href="#for-neural-nets">For neural nets</a></li>
 <li><a href="#followup">Followup</a></li>
 <li><a href="#thoughts-33017">Thoughts 3/30/17</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>I think that I can place neural networks with non-steep and well-separated sigmoids as low-norm elements of a RKHS, and that this will explain and unify some learning results.</p>
<p>Precise details to be ironed out. First, choice of RKHS:</p>
<ul>
<li>Sobolev space <span class="math inline">\(H^{\fc n2}(\R^n)\)</span>, or</li>
<li><span class="math inline">\(L^2(\R^n)\)</span> or <span class="math inline">\(H^1(\R^n)\)</span> with bandlimited constraint.</li>
<li>Quotient out by constant functions? Allow a sigmoid <span class="math inline">\(\to 1\)</span> at infinity as kernel?</li>
</ul>
<p>For the bandlimiting, there are 2 technical steps, with parameters to vary.</p>
<ul>
<li>Convolve by Gaussian (or ball indicator) in Fourier space, or multiplying by Gaussian (or FT of ball indicator) in ordinary space.</li>
<li>cutting off Fourier spectrum - how does this influence? I.e. what is Fourier decay? Expect exponential - so get <span class="math inline">\(\ln \prc{\ep}\)</span> in exponent.</li>
</ul>
<p>Expect exponential dependence on steepness.</p>
<h2 id="things-this-could-explain">Things this could explain</h2>
<ul>
<li>Kalai’s result on learning smooth NN.</li>
<li>Learning linear separator with margin.</li>
<li>Exponential dependence on dimension or <span class="math inline">\(\rc{\ep}\)</span> (what exactly?) for agnostically learning halfspace, etc.</li>
</ul>
<h2 id="for-neural-nets">For neural nets</h2>
<ul>
<li>Learn 2-NN with linear output, under some conditions (ex. incoherence)</li>
<li>Maybe can learn 2-NN with sigmoid/majority output, by boosting (cf. majority of majorities).</li>
</ul>
<h2 id="followup">Followup</h2>
<ul>
<li>What is relationship to gradient descent?</li>
</ul>
<h2 id="thoughts-33017">Thoughts 3/30/17</h2>
<ul>
<li>How to get something for convolutional neural nets?
<ul>
<li>As an easier question, think about having filters over the entire image, rather than a grid.</li>
<li>Think of periodic case even.</li>
<li>Then this works by Fourier transform over <span class="math inline">\(\R^\N\)</span> (suitably weighted).</li>
<li>Problem: the simplest convnet is more complicated than this, includes maxpool and then fc. How to deal with maxpool? What if you don’t do maxpool? Sigmoid and then average, or weighted average?</li>
<li>Kernel on Fourier transform space. (See how well FT matches…)</li>
<li>See [ZLW16]</li>
</ul></li>
<li>Overcomplete bases
<ul>
<li>Can define RKHS norm by giving norm when written in terms of basis element.</li>
<li>Can’t define norm with overcomplete set of elements.</li>
<li>Can we define some kind of norm and do something kernel-like with overcomplete basis?
<ul>
<li>Project from larger space?</li>
</ul></li>
<li>Cf. wanting symmetries beyond translation</li>
<li>Cf. wavelets offer a natural overcomplete basis respecting symmetries</li>
<li>Perhaps first thing to do is just try wavelet regularization on MNIST.</li>
<li>If you want to use nonconvolutional kernel method on images, you should first convert to Fourier or wavelet basis. Probably wavelet (except that’s not quite a basis). (Multiply by log size.) (This doesn’t give you translation invariance, just resets the norm.)</li>
</ul></li>
<li>Three kernels
<ul>
<li>Fourier-based.</li>
<li><span class="math inline">\(\rc{2-\an{x,y}}\)</span>.</li>
<li>Arccosine.</li>
</ul></li>
<li>I’m super-confused about why toggling just one parameter <span class="math inline">\(n\)</span> changes the number of layers. <span class="math inline">\(K^{(n)}(x,y)=\fc{n-1}n + \fc{1/n}{(n+1) - n\an{x,y}}\)</span>.</li>
<li>Idea to prove NN separation for Lipschitz layers: Show a function that has exponentially higher norm in terms of <span class="math inline">\(l-1\)</span> norm than <span class="math inline">\(l\)</span>. Problem: norm required to express neural net also increases super-exponentially in dimension.</li>
<li>Improper tensor decomp using same method (use case?) cf. Livni’s poly network</li>
<li>Barron functions form a convex set… The reason why it’s intractable is that it’s infinite-dimensional. Hilbert spaces are infinite-dimensional, but the representor theorem saves you.</li>
<li>Can you cut down representation (after using representor theorem) by sampling? Keep norm, but be cruder? (Pick some elements and rescale.)</li>
<li>Using kernel representation at first level of neural network? Do some kind of AM? How to mix nonparametric and parametric? Power and limit of kernel coming from its nonparametricity.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Langevin dynamics</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/probabilistic/langevin.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/probabilistic/langevin.html</id>
    <published>2017-03-15T00:00:00Z</published>
    <updated>2017-03-15T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Langevin dynamics</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-03-15 
          , Modified: 2017-03-15 
	</p>
      
       <p>Tags: <a href="/tags/langevin.html">langevin</a>, <a href="/tags/sampling.html">sampling</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#references">References</a><ul>
 <li><a href="#other">Other</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="references">References</h2>
<ul>
<li>Book: Analysis and geometry of Markov diffusion operators</li>
<li>C. Villani. Topics in Optimal Transportation, volume 58 of Graduate Studies in Mathematics</li>
<li>[OV14] GENERALIZATION OF AN INEQUALITY BY TALAGRAND, AND LINKS WITH THE LOGARITHMIC SOBOLEV INEQUALITY</li>
<li>[GM90] Recursive stochastic algorithms for global optimization in R^d</li>
<li>[RT96] Exponential convergence of Langevin distributions and their discrete</li>
<li>[D14] Theoretical guarantees for approximate sampling from smooth and log-concave densities</li>
<li>[ZLC17] A Hitting Time Analysis of Stochastic Gradient Langevin Dynamics</li>
<li>[RRT17] Non-Convex Learning via Stochastic Gradient Langevin Dynamics - A Nonasymptotic Analysis</li>
</ul>
<p>See also: Stochastic calculus, Brownian motion, [AH] on sampling</p>
<h3 id="other">Other</h3>
<ul>
<li>E. Hazan, K. Levi, and S. Shalev-Shwartz. On graduated optimization for stochastic non-convex problems</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Learning to model structures and data</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/complexity/model.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/complexity/model.html</id>
    <published>2017-03-12T00:00:00Z</published>
    <updated>2017-03-12T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Learning to model structures and data</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-03-12 
          , Modified: 2016-03-12 
	</p>
      
       <p>Tags: <a href="/tags/pseudorandomness.html">pseudorandomness</a>, <a href="/tags/structure.html">structure</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#miscellaneous-notes">Miscellaneous notes</a></li>
 <li><a href="#what-does-this-say-for-learning">What does this say for learning?</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="miscellaneous-notes">Miscellaneous notes</h2>
<ul>
<li>Boosting:
<ul>
<li>Generic boosting theorem</li>
<li>Easiest to state when assume access to whole distribution. Otherwise, have to sample and assume something about VC dimension to generalize. (See old 511 notes. Actual argument works with <span class="math inline">\(\Pi_H(m)\)</span>, is there some reason we need to do that?)</li>
<li>Hardcore lemma</li>
<li>Strong version: Either allow real-number predictions, or think of real-number output as flipping a coin with that probability. This can save the vital factor of 2 compared to worst-case.</li>
<li>I don’t get the potential argument.</li>
<li>I do get the multiplicative weights argument of BHK, but there are some things to be careful about.
<ul>
<li>Familiar with MW with divergence applied to it.</li>
<li>How to make non-worst case? Is this already covered?</li>
<li>What about the AdaBoost improvement - algorithmically speaking, it makes sense to adapt weight based on number correct/wrong. Can you not redo the analysis here?</li>
<li>Important: need to keep track of density of <span class="math inline">\(\de\)</span>.</li>
<li>Do we need to cap the weights at 1? What problem is this trying to solve? What happens if we don’t?</li>
</ul></li>
</ul></li>
<li>Dense model theorem
<ul>
<li>Got <span class="math inline">\(\de''\)</span> (<span class="math inline">\(\wh \de\)</span>) part</li>
<li>Lots of algebra/<span class="math inline">\(\ep\)</span>’s and <span class="math inline">\(\de\)</span>’s here. I’m missing an additive factor of <span class="math inline">\(\de^2\)</span> (<span class="math inline">\(\fc{\de}{1+\de}\)</span> instead of <span class="math inline">\(\de\)</span>)…</li>
<li>Todo: Understand example that shows optimality</li>
</ul></li>
<li>Weak regularity
<ul>
<li>Why do we need the condition on no cut having too many edges? Counterexample if this is not satisfied? Is there a statement that’s not “X or Y”?</li>
<li>What subroutines are required for the algorithmic version?</li>
<li>Note that the points are <span class="math inline">\(V\times V\)</span>, or edges in the complete graph. Tests are <span class="math inline">\(A\not\cap B\)</span>. (These are not the points.)</li>
<li>Get directly from algorithmic DMT?</li>
</ul></li>
<li>Other things
<ul>
<li>Low-complexity approximation</li>
<li>Computational entropy</li>
<li>Structure theorem (my understanding is that the dense model theorem gives up if it finds a hot spot, while structure attempts to find all the hot spots because it’s iterative).</li>
</ul></li>
</ul>
<h2 id="what-does-this-say-for-learning">What does this say for learning?</h2>
<ul>
<li>Relax “tests” to “functions”, perhaps <span class="math inline">\([0,1]\)</span>-functions. Does everything still go through?</li>
<li>Does this give a GAN formulation? Note that the dense model/structure theorem breaks into regions of constant density, and you get handle on the probability distribution, while in GAN, you have a generative model, not an assignment of probabilities. Can you make a GAN which knows about its probabilities? Is this a Bayesian NN?</li>
<li>Does this jibe with the greedy algorithmic version of Barron’s Theorem? Is the process of finding the best <span class="math inline">\(h\)</span> (if <span class="math inline">\(H\)</span> is linear thresholds/sigmoids/1-dimensional functions) correspond to finding the best new node? Can we make this convex? What if we just want weak learning here?</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Stochastic calculus</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/probability/stochastic_calculus.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/probability/stochastic_calculus.html</id>
    <published>2017-03-08T00:00:00Z</published>
    <updated>2017-03-08T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Stochastic calculus</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-03-08 
          , Modified: 2017-03-08 
	</p>
      
       <p>Tags: <a href="/tags/stochastic%20calculus.html">stochastic calculus</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#section">4</a><ul>
 <li><a href="#ito-integral">4.2 Ito integral</a></li>
 <li><a href="#some-elementary-properties">4.3 Some elementary properties</a></li>
 <li><a href="#ito-calculus">4.4 Ito calculus</a></li>
 <li><a href="#girsanovs-theorem">4.5 Girsanov’s theorem</a></li>
 <li><a href="#martingale-representation-theorem">4.6 Martingale representation theorem</a></li>
 </ul></li>
 <li><a href="#stochastic-differential-equations">5 Stochastic differential equations</a><ul>
 <li><a href="#sde-existence-and-uniqueness">5.1 SDE existence and uniqueness</a></li>
 <li><a href="#markov-property-and-kolmogorovs-equations">5.2 Markov property and Kolmogorov’s equations</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="section">4</h2>
<p>Stieltjes function can be defined in terms of simple functions first, <span class="math inline">\(I(f_n) = \sum_i f_n(t_i^n) (g(t_{i+1}^n) - g(t_i^n))\)</span>.</p>
<p>What’s wrong with the Stieltjes integral? If <span class="math inline">\(g\)</span> has infinite variation on <span class="math inline">\([0,1]\)</span>, then there exist simple functions <span class="math inline">\(f_n\to f\)</span> uniformly such that <span class="math inline">\(I(f_n)\)</span> diverges.</p>
<p><em>Proof</em>. Take a partition so that <span class="math inline">\(\sum |g({t_{i+1}}) - g(t_i)|\to \iy\)</span>, <span class="math inline">\(h_n(t_i) = \sign(g(t_{i+1})-g(t_i))\)</span>.</p>
<p>But when <span class="math inline">\(g\)</span> is a Wiener process, this example would have to choose <span class="math inline">\(h_n\)</span> adaptively to it. If <span class="math inline">\(h_n\)</span> is nonrandom, this is not a problem. There are certain sample paths of <span class="math inline">\(W_t\)</span> for which the integral diverges, but the set of such sample paths has probability 0! But we would like to integrate random processes.</p>
<p>The lemma cheated by looking into the future!</p>
<ol type="1">
<li>We only define stochastic integrals with repect to <span class="math inline">\(W_t\)</span> of stochastic processes which are <span class="math inline">\(\mathcal F_t\)</span>-adapted.</li>
<li>Even though finite variation of <span class="math inline">\(W_t\)</span> is a.s. infinite, quadratic variation is finite, <span class="math inline">\(\sum_{t_i\in \pi_n} (W_{t_{i+1}}-W_{t_i})^2\to 1\)</span> in probability. Define stochastic integrals as limits in <span class="math inline">\(L^2\)</span>. By independence of increments, <span class="math display">\[
\E [ I(f_n)^2] = \int_0^1 f_n(s)^2\,ds.
\]</span></li>
</ol>
<p>Wiener integral: <span class="math inline">\(f_n\to f\)</span> in <span class="math inline">\(L^2([0,1])\)</span>. Then <span class="math inline">\(\E[(I(f_n)-I(f_m))^2]\to 0\)</span>, so <span class="math inline">\(I(f_n)\)</span> converges to some <span class="math inline">\(I(f)\)</span>. Note: choosing <span class="math inline">\(f_n\)</span> so <span class="math inline">\(\sumo n{\iy} \int_0^1 (f_n(s)-f(s))^2\,ds&lt;\iy\)</span>, can define as a.s. limit (CHECK). Counterexample: take wavelet, divide by <span class="math inline">\(\sqrt{\ln \prc{n}}\)</span>.</p>
<h3 id="ito-integral">4.2 Ito integral</h3>
<p>!! Note the Ito integral is ITSELF a random variable over the probability measure underlying the Wiener process. You can take a <span class="math inline">\(\E\)</span> over Brownian motions.</p>
<ol type="1">
<li>Let <span class="math inline">\(\{X_t^n\}_{t\in [0,T]}\)</span> be a simple, square-integrable, <span class="math inline">\(F_t\)</span>-adapted stochastic process. Define <span class="math inline">\(I(X^n) = \int_0^T X_t^n \,dW_t = \sumz iN X_{t_i}^N (W_{t_{i+1}} - W_{t_i})\)</span>.</li>
<li>Ito isometry: <span class="math inline">\(X_{t_i}^n\)</span> is independent of <span class="math inline">\(W_{t_{i+1}} - W_{t_i}\)</span>, so <span class="math display">\[ \E\ba{\pa{\int_0^T X_t^n \,dW_t}^2} = \E\ba{\int_0^T (X_t^n)^2\,dt}.\]</span> Succinctly, <span class="math display">\[\ve{I(X_\cdot^n)}_{2,\Pj} = \ve{X_\cdot^n}_{2,\mu_T\times \Pj}.\]</span> I.e., <span class="math inline">\(I:L^2(\mu_T\times \Pj)\to L^2(\Pj)\)</span> preserves <span class="math inline">\(L^2\)</span> distance when applied to <span class="math inline">\(F_t\)</span> adapted simple integrands.</li>
<li>Extend to <span class="math inline">\(X_\cdot^n \to X_\cdot\)</span> in <span class="math inline">\(L^2(\mu_T\times \Pj)\)</span>.</li>
<li>Let <span class="math inline">\(X_\cdot\in L^2(\mu_T\times \Pj)\)</span> be <span class="math inline">\(F_t\)</span>-adapted. Then there exists a sequence of <span class="math inline">\(F_t\)</span>_adapted simple <span class="math inline">\(X_\cdot^n\to X\)</span>. <!--DCT-->
<ul>
<li>Continuous and bounded sample paths: uniform continuity, DCT.</li>
<li>Bounded and progressively measurable (?): <span class="math inline">\(X^\ep_\cdot \to X^\ep\)</span>, where <span class="math inline">\(X_t^\ep = \rc\ep\int_{t-\ep}^t X_{\max\{s,0\}}\,ds\)</span>.</li>
<li>Progressively measurable: <span class="math inline">\(X_t I_{|X_t|\le M}\)</span>. DCT.</li>
</ul></li>
</ol>
<p>Ex. <span class="math inline">\(W_T^2 = 2\int_0^T W_t\,dW_t+T\)</span>.</p>
<p>Now what?</p>
<ol type="1">
<li>Consider Ito integral itself as a stochastic process, <span class="math inline">\(t\mapsto \int_0^t X_s\,dW_s\)</span>.
<ul>
<li>For <span class="math inline">\(t\le T\)</span>, define <span class="math inline">\(I_t(X_\cdot^n) = I(X_\cdot^n I_{\le t})\)</span>.</li>
<li><span class="math inline">\(I_t(X_\cdot^n)\)</span> is a <span class="math inline">\(F_t\)</span>-martingale.</li>
<li>Ito integral can be chosen to have continuous sample paths. (Pf. Discontinuous paths live on null set. Use subsequence argument and Borel-Cantelli.) …</li>
</ul></li>
<li>Extend the class of integrable processes, to have nice closure properties. (Product of 2 integrals can be expressed as a integral.)
<ul>
<li>Note we don’t want to define <span class="math inline">\(I_\iy\)</span>, only for every <span class="math inline">\(t\in [0,\iy)\)</span>.</li>
<li>Localization: to define on <span class="math inline">\([0,\iy)\)</span>, define it on every interval <span class="math inline">\([0,T]\)</span>. Require <span class="math inline">\(X_{[0,T]}\in L^2(\mu_T\times \Pj)\)</span> for every <span class="math inline">\(T&lt;\iy\)</span>, i.e. <span class="math inline">\(X\in \bigcap_{T&lt;\iy} L^2(\mu_T\times \Pj)\)</span>.</li>
<li>Check local property: <span class="math inline">\(I_t(X_\cdot)\)</span> does not depend on which <span class="math inline">\(T&gt;t\)</span> we choose.</li>
<li>Behavior under stopping: <span class="math inline">\(X_t\)</span> is <span class="math inline">\(F_t\)</span>-adapted in <span class="math inline">\(\bigcap_{T&lt;\iy} L^2(\mu_T\times \Pj)\)</span>, <span class="math inline">\(\tau\)</span> is <span class="math inline">\(F_t\)</span>-stopping time. Then <span class="math inline">\(I_{\min(t,\tau)}(X_\cdot) = I_t(X_\cdot I_{&lt;\tau})\)</span>. Pf. Let <span class="math inline">\(\tau^n\)</span> be <span class="math inline">\(\tau\)</span> rounded upwards to the earliest jump time.</li>
<li>Localizing sequence for <span class="math inline">\(X_t\)</span>: <span class="math inline">\(F_t\)</span>-stopping times <span class="math inline">\(\tau_n\nearrow \iy\)</span>, <span class="math inline">\(\E\ba{\int_0^{\tau_n} X_t^2\,dt}&lt;\iy\)</span>. “Allow localization intervals to be random.” This is independent of localizing sequence (4.2.10, CHECK).</li>
<li>There is a natural cloass of integrands whose elements admit localizing sequence: <span class="math inline">\(A_T(X_\cdot) = \int_0^T X_t^2&lt;\iy\)</span> a.s. for all <span class="math inline">\(T&lt;\iy\)</span>. Let <span class="math inline">\(\tau_n=\inf\set{t\le n}{A_t(X_{\cdot})\ge n}\)</span>.</li>
</ul></li>
</ol>
<p>Finally: Let <span class="math inline">\(X_t\)</span> be any <span class="math inline">\(F_t\)</span>-adapted stochastic process with <span class="math inline">\(\Pj\ba{\int_0^T X_t^2\,dt&lt;\iy}=1\)</span> for all <span class="math inline">\(T&lt;\iy\)</span>. Then Ito integral <span class="math inline">\(I_t(X_\cdot)\)</span> is uniquely defined by localization and choice of continuous modification as <span class="math inline">\(F_t\)</span>-adapted stochastic process on <span class="math inline">\([0,\iy)\)</span> with continuous sample paths.</p>
<h3 id="some-elementary-properties">4.3 Some elementary properties</h3>
<ol type="1">
<li>Linearity</li>
<li>Stopping time <span class="math inline">\(\int_0^{\min(t,\tau)} X_s\,dW_s = \int_0^t X_s I_{&lt;\tau}\,dW_s\)</span>.</li>
<li>If <span class="math inline">\(X\in \bigcap_{T&lt;\iy} L^2(\mu_T\times \Pj)\)</span> then <span class="math inline">\(\E[]=0\)</span> and <span class="math inline">\(\E[()^2] = \E[\int_0^TX_t^2\,dt]\)</span>.</li>
<li><span class="math inline">\(X^n\to X\)</span> in <span class="math inline">\(L^2(\mu_T\times \Pj)\)</span> means <span class="math inline">\(I_t(X_\cdot^n) \to I_t(X_\cdot)\)</span> in <span class="math inline">\(L^2(\Pj)\)</span>. If convergence fast enough, a.s.</li>
<li><span class="math inline">\(X_t\)</span> is <span class="math inline">\(F_t\)</span>-local martingale if there exists <span class="math inline">\(\tau_n\nearrow \iy\)</span> (reducing sequence), <span class="math inline">\(X_{\min(t,\tau_n)}\)</span> is martingale. Any Ito integral is a local martingale. (Take a localizing sequence.)</li>
</ol>
<h3 id="ito-calculus">4.4 Ito calculus</h3>
<p>Setup</p>
<ul>
<li><span class="math inline">\((\Om, F, \{F_t\}_{t\in [0,\iy)}, \Pj)\)</span></li>
<li><span class="math inline">\(W_t\)</span> is <span class="math inline">\(m\)</span>-dimensional <span class="math inline">\(F_t\)</span>-Wiener process.</li>
<li>Ito process <span class="math inline">\(X_t^i = X_0^i + \int_0^t F_s^i\,ds + \sumo jm \int_0^t G_s^{ij}\,dW_s^j\)</span>.
<ul>
<li><span class="math inline">\(\int_0^t |F_s^i|\,ds&lt;\iy\)</span> a.s.</li>
<li><span class="math inline">\(\int_0^t (G_s^{ij})^2\,ds&lt;\iy\)</span> a.s.</li>
<li>Shorthand: <span class="math inline">\(X_t=X_0+\int_0^t F_s\,ds + \int_0^t G_s\,dW_s\)</span>.</li>
</ul></li>
</ul>
<strong>Theorem</strong> (Ito rule): <span class="math inline">\(u(t,x)\)</span> is <span class="math inline">\(C^1\)</span> in <span class="math inline">\(t\)</span> and <span class="math inline">\(C^2\)</span> in <span class="math inline">\(x\)</span>. Then <span class="math inline">\(u(t,X_t)\)</span> is Ito.
<span class="math display">\[\begin{align}
u(t,X_t) &amp;= u(0,X_0) + \sumo in \sumo km \int_0^t u_i(s,X_s) G_s^{ik} \,dW_s^k\\
&amp;\quad + \int_0^t\ba{u'(s,X_s)+\sumo in u_i(s,X_s)F_s^i + \rc 2 \sum_{i,j=1}^n\sumo km u_{ij}(s,X_s)G_s^{ik}G_s^{jk}}\,ds\\
&amp;=u(0,X_0) + \int_0^t (\nb u)^T G \,dW + \int_0^t u'(s,X_s) + (\nb u)^TF_s + \rc 2 \Tr(G_s^T (\nb^2 u) G_s)\,ds.
\end{align}\]</span>
<p>Alternate notation: <span class="math inline">\(dX_t = F_t\,dt + G_t\,dW_t\)</span>, <span class="math display">\[
du(t,X_t) = u'(t,X_t)\,dt + \pl u(t,X_t)\,dX_t + \rc 2 \Tr(\pl^2 u(t,X_t)dX_t(dX_t)^*),
\]</span> where <span class="math inline">\(dW_t^i \,dW_t^j=\de_{ij}\,dt\)</span> and other derivatives are 0.</p>
<p>First two terms are chain rule. When stochastic integrals are present, we evidently need to take a second-order term into account as well.</p>
<p>Cor. Ito processes form an algebra.</p>
<h3 id="girsanovs-theorem">4.5 Girsanov’s theorem</h3>
<p>What happens to Wiener process under change of measure? We can often simplify a problem by changing to a more convenient probability measure.</p>
<p><strong>Theorem</strong> (Girsanov). Let <span class="math inline">\(W_t\)</span> be <span class="math inline">\(m\)</span>-dimensional <span class="math inline">\(\mathcal F_t\)</span>-Wiener on <span class="math inline">\((\Om, \mathcal F, \{\mathcal F_t\}_{t\in [0,T]}, \Pj)\)</span>, <span class="math inline">\(X_t=\int_0^t F_s\,ds + W_t\)</span> be Ito, <span class="math inline">\(F_t\)</span> Ito integrable, <span class="math display">\[
\La = \exp\pa{-\int_0^T (F_s)^* dW_s - \rc 2\int_0^T \ve{F_s}^2\,ds},
\]</span> Novikov’s condition <span class="math inline">\(\E_{\Pj} \ba{\exp\pa{\rc 2\int_0^T \ve{F_s}^2\,ds}}&lt;\iy\)</span>. THen <span class="math inline">\(\{X_t\}_{t\in [0,T]}\)</span> is <span class="math inline">\(\mathcal F_t\)</span>-Wiener under <span class="math inline">\(\mathbb Q(A) = \E_\Pj (\Ga I_A)\)</span>.</p>
<p>Intuition (discrete case): If <span class="math inline">\(d\Pj\)</span> is the probability measure of standard gaussian, and <span class="math inline">\(d\mathbb Q\)</span> is probability measure where <span class="math inline">\(a_k+\xi_k\)</span> are standard gaussian (<span class="math inline">\(a_k\)</span> is predictable process), write <span class="math inline">\(d\mathbb Q\)</span> wrt <span class="math inline">\(d\Pj\)</span>. <span class="math display">\[
\fc{d\mathbb Q}{d\mathbb P} = \prod\fc{e^{-(\xi_i+a_i)^2/2}}{e^{-\xi_i^2/2}} = \exp\ba{\sumo kn \pa{-a_k \xi_k - \rc 2a_k^2}}.
\]</span></p>
<p>READ PROOF.</p>
<h3 id="martingale-representation-theorem">4.6 Martingale representation theorem</h3>
<p>Gives converse to Ito integral. Every martingale <span class="math inline">\(\{M_t\}_{t\in [0,T]}\)</span> with <span class="math inline">\(M_T\in L^2(\Pj)\)</span> is the Ito integral of a unique process in <span class="math inline">\(L^2(\mu_T\times \Pj)\)</span>.</p>
<p><strong>Theorem</strong>.</p>
<ol type="1">
<li>(Martingale representation) Let <span class="math inline">\(M_t\)</span> be <span class="math inline">\(\mathcal F_t^W = \si\set{W_s}{s\le t}\)</span>-martingale, <span class="math inline">\(M_T\in L^2(\Pj)\)</span>. For a unique <span class="math inline">\(\mathcal F_t^W\)</span>-adapted process <span class="math inline">\(\{H_t\}_{t\in [0,T]}\)</span> in <span class="math inline">\(L^2(\mu_T\times \Pj)\)</span>, <span class="math inline">\(M_t=M_0 + \int_0^t H_s\,dW_s\)</span> a.s.</li>
<li>(Ito representation, more general) Let <span class="math inline">\(X\)</span> be <span class="math inline">\(\mathcal F_T^W\)</span>-measurable rv in <span class="math inline">\(L^2(\Pj)\)</span>. Then for … <span class="math inline">\(X=\E X + \int_0^T H_s\,dW_s\)</span> a.s. <!--can differentiate --></li>
</ol>
<p><em>Proof</em>.</p>
<ol type="1">
<li>Show that any <span class="math inline">\(X\)</span> can be approximated arbitrarily well by Ito integral, <span class="math inline">\(\ve{X-I_T(H_\cdot^\ep)}_2&lt; \ep\)</span>.
<ol type="1">
<li>Identify class of random variables that can approximatate <span class="math inline">\(X\)</span> arbitrarily well. <span class="math inline">\(S=\set{f(W_{t_1},\ldots, W_{t_n})}{n&lt;\iy, t_1,\ldots, t_n\in [0,T], f\in C_0^\iy}\)</span>.
<ul>
<li>Show this holds if allow Borel-measurable <span class="math inline">\(f\)</span>. Filter by slicing into intervals <span class="math inline">\(2^{-n}\)</span>, <span class="math inline">\(X^n = f(W_{2^{-n}T},\ldots, W_T)\)</span>.
<ul>
<li>Levy’s upward theorem: let <span class="math inline">\(X\in L^2(\Pj)\)</span> be <span class="math inline">\(G\)</span>-measurable, <span class="math inline">\(G=\si\{G_n\}\)</span>. Then <span class="math inline">\(\E[X|G_n]\to X\)</span> a.s. and in <span class="math inline">\(L^2(\Pj)\)</span>.</li>
</ul></li>
<li>Any Borel function can be approximated arbitrarily well by <span class="math inline">\(f^n\in C^\iy\)</span>.</li>
</ul></li>
<li>Show any rv in this class can be represented as Ito integral
<ul>
<li>Ito’s rule: <span class="math display">\[g(t,W_t) = g(0,0) + \int_0^t (g_s + \rc2 g_{xx}) (s,W_s)\,ds + \int_0^t g_x (s,W_s)\,dW_s.\]</span> Solve the heat equation for <span class="math inline">\(g\)</span>, <span class="math inline">\(g = \rc{\sqrt{2\pi (t-s)}}\int_{-\iy}^{\iy} f(y) e^{-(x-y)^2/(2(t-s))} \dy\)</span>. Still works for multivariate.</li>
</ul></li>
</ol></li>
<li>Take limits.</li>
</ol>
<h2 id="stochastic-differential-equations">5 Stochastic differential equations</h2>
<p>Existence, uniqueness, Markov property.</p>
<p>Kolmogorov forward (Fokker-Planck) and backward equations.</p>
<h3 id="sde-existence-and-uniqueness">5.1 SDE existence and uniqueness</h3>
<span class="math display">\[\begin{align}
dX_t &amp;= b(t,X_t)dt + \si(t,X_t) dW_t, &amp; X_0=x\\
\iff 
X_t &amp;= x+\int_0^t b(s,X_s) \,ds + \int_0^t \si(s,X_s)\,dW_s.
\end{align}\]</span>
<p>Ex. <span class="math inline">\(dX_t = AX_t dt + B\,dW_t\)</span>, <span class="math inline">\(X_0=x\)</span> has solution <span class="math display">\[
X_t = e^{At}x + \int_0^t e^{A(t-s)}B\,dW_s.
\]</span></p>
<p><strong>Theorem</strong>. Suppose</p>
<ol type="1">
<li><span class="math inline">\(X_0\in L^2(\Pj)\)</span></li>
<li><span class="math inline">\(b,\si\)</span> Lipschitz uniformly on <span class="math inline">\([0,T]\)</span> (in <span class="math inline">\(x\)</span>).</li>
<li><span class="math inline">\(\ve{b(t,0)}, \ve{\si(t,0)}\)</span> bounded on <span class="math inline">\(t\in [0,T]\)</span>.</li>
</ol>
<p>Then there exists solution <span class="math inline">\(X_t\)</span>, and <span class="math inline">\(b(t,X_t),\si(t,X_t)\in L^2(\mu_T\times \Pj)\)</span>, and it is unique a.s.</p>
<h3 id="markov-property-and-kolmogorovs-equations">5.2 Markov property and Kolmogorov’s equations</h3>
<p>A large class of Markov processes with continuous sample paths can be obtained as solution of appropriate SDE.</p>
<p><strong>Theorem</strong>. Suppose conditions hold. Then <span class="math inline">\(X_t\)</span> is <span class="math inline">\(\mathcal F_t\)</span>-Markov process. (Actually it satisfies the strong Markov property, even with random stopping times.)</p>
<p><em>Proof</em>. Calculate <span class="math inline">\(X_t-X_s\)</span>. Note <span class="math inline">\(W_{r+s}-W_s\)</span> is Wiener. <span class="math inline">\(Y_r=X_{r+s}\)</span> satisfies a SDE… ?</p>
<p>Assume <span class="math inline">\(b, \si\)</span> independent of <span class="math inline">\(t\)</span>. Markov property gives <span class="math inline">\(\E[f(X_t)|\mathcal F_s] = g_{t-s}(X_s)\)</span>. This suggests <span class="math inline">\(\ddd t P_t f = \mathcal L P_tf\)</span>.</p>
<p><strong>Theorem</strong> (Kolmogorov backward equation). For <span class="math inline">\(g\in C^2\)</span>, <span class="math display">\[
\mathcal Lg = b^T\nb g + \rc 2 \Tr(\si^T \nb^2 g \si).
\]</span> If <span class="math inline">\(u(t,x)\)</span> is <span class="math inline">\(C^1\)</span> in <span class="math inline">\(t\)</span>, <span class="math inline">\(C^2\)</span> in <span class="math inline">\(x\)</span>, <span class="math inline">\(f\in C^2\)</span> such that <span class="math display">\[
u_t=\mathcal L u, \quad u(0,x)=f(x)
\]</span> then <span class="math inline">\(u(t,x)=P_tf(x)\)</span>.</p>
<p>(Note we can write this backwards as <span class="math inline">\(v_t + \mathcal Lv = 0\)</span>, <span class="math inline">\(v(T,x)=f(x)\)</span>.)</p>
<p>(Note: in principle we would like to define <span class="math inline">\(\E[f(X_t)|\mathcal F_s] =: u(t-s,X_s)\)</span> and show <span class="math inline">\(u\)</span> satisfies the PDE. This is more technical because we need to show smoothness or interpret the PDE in a weak sense.)</p>
<p><em>Proof</em>. Ito’srule on <span class="math inline">\(Y_r=v(r,X_r)\)</span>. (The Ito integral <span class="math inline">\(\int_0^t (\nb v)^T G\,dW_s\)</span> is a “local martingale” here.) Take <span class="math inline">\(\E[\cdot |\mathcal F_s]\)</span> and the martingale part disappears.</p>
<p>Forwards equation: If law of <span class="math inline">\(X_t\)</span> is absolutely continuous, <span class="math inline">\(\E[f(X_t)] = \int_{\R^n} f(y)p_t(y)\dy\)</span> for some <span class="math inline">\(p\)</span>, and more generally, <span class="math display">\[
\E[f(X_t)|\mathcal F_s] = \int_{\R^n} f(y) p_{t-s}(X_s,y)\dy.
\]</span> Can <span class="math inline">\(p\)</span> be obtained as solution to PDE?</p>
<p>“Kolmogorov forward equation is dual of backward equation”: <span class="math inline">\(\int fp_t = \int P_tfp_0\)</span>.</p>
<p><strong>Theorem</strong> (Kolmogorov forward, Fokker-Planck): Assume niceness of the SDE, and <span class="math inline">\(b\in C^1, \si\in C^2\)</span>, <span class="math inline">\(\rh\in C^2\)</span>, <span class="math display">\[
\mathcal L^* \rh = -\sumo in \pd{x^i}(b^i\rh) + \rc2 \suij n \sumo km \pd{{}^2}{x^i\pl x^j} (\si^{ik}\si^{ij}\rh),
\]</span> <span class="math inline">\(p_t\)</span> exists, <span class="math inline">\(C^1\)</span> in <span class="math inline">\(t\)</span>, <span class="math inline">\(C^2\)</span> in <span class="math inline">\(x\)</span>. Then <span class="math display">\[(p_t)_t = \mathcal L^* p_t, \quad t\in [0,T].\]</span></p>
<p><em>Proof</em>.</p>
<ol type="1">
<li>Write Ito’s rule for <span class="math inline">\(f\)</span>.</li>
<li>Take <span class="math inline">\(\E\)</span> so the martingale disappears.</li>
<li>Substitute definition of <span class="math inline">\(p_t(y)\)</span>, integrate by parts to take <span class="math inline">\(\mathcal L\)</span> from <span class="math inline">\(\mathcal L f\)</span> to <span class="math inline">\(\mathcal L^* p_s\)</span>. This holds for all <span class="math inline">\(f\)</span> so remove the <span class="math inline">\(f\)</span>.</li>
<li>Take time derivative.</li>
</ol>
<blockquote>
<p>As a rule of thumb, the backward equation is very well behaved, and will often have a solution provided only that f is sufficiently smooth; the forward equation is much less well behaved and requires stronger conditions on the coefficients</p>
</blockquote>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2017-03-11</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2017-03-18.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2017-03-18.html</id>
    <published>2017-03-07T00:00:00Z</published>
    <updated>2017-03-07T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2017-03-11</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-03-07 
          , Modified: 2017-03-07 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#adversarial-training">Adversarial training</a></li>
 <li><a href="#langevin-monte-carlo">Langevin Monte Carlo</a></li>
 <li><a href="#using-barrons-theorem">Using Barron’s Theorem</a></li>
 <li><a href="#entropy-regularizer-for-svm">Entropy regularizer for SVM</a></li>
 <li><a href="#generalization">Generalization</a></li>
 <li><a href="#rl">RL</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="adversarial-training">Adversarial training</h2>
<ul>
<li><a href="../tcs/machine_learning/neural_nets/adversarial_experiments.html">Experiments</a></li>
</ul>
<h2 id="langevin-monte-carlo">Langevin Monte Carlo</h2>
<ul>
<li>Learn background. (OV14, Villani book, Ledoux book.)</li>
<li>Langevin papers.</li>
<li>Extending [AH16].</li>
</ul>
<h2 id="using-barrons-theorem">Using Barron’s Theorem</h2>
<p>See <a href="../tcs/machine_learning/neural_nets/barron_musings.html">musings</a>.</p>
<h2 id="entropy-regularizer-for-svm">Entropy regularizer for SVM</h2>
<h2 id="generalization">Generalization</h2>
<h2 id="rl">RL</h2>
<p>Come up with project. Concrete problems in AI safety?</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2017-03-11</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2017-03-11.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2017-03-11.html</id>
    <published>2017-03-07T00:00:00Z</published>
    <updated>2017-03-07T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2017-03-11</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-03-07 
          , Modified: 2017-03-07 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#adversarial-training">Adversarial training</a></li>
 <li><a href="#langevin-monte-carlo">Langevin Monte Carlo</a></li>
 <li><a href="#boosting-etc.">Boosting, etc.</a></li>
 <li><a href="#speculative-thoughts">Speculative thoughts</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="adversarial-training">Adversarial training</h2>
<h2 id="langevin-monte-carlo">Langevin Monte Carlo</h2>
<p>Background</p>
<ul>
<li>Brownian motion (and other probability prerequisites)</li>
<li>Stochastic calculus</li>
<li>High-dimensional probability (LSI, etc.)</li>
<li>Langevin papers</li>
</ul>
<h2 id="boosting-etc.">Boosting, etc.</h2>
<p>Fix notes!</p>
<h2 id="speculative-thoughts">Speculative thoughts</h2>
<ul>
<li>Relationship between Rademacher width and size of <span class="math inline">\(\ep\)</span>-net?</li>
<li>Can we rephrase Frieze-Kannan regularity lemma as follows: there exists a small neural net that approximately computes cuts? Number of hidden nodes independennt of size of graph?
<ul>
<li>FK as: set of graphs is “small” in cut metric, so has a small <span class="math inline">\(\ep\)</span>-net. Rademacher?</li>
</ul></li>
<li>Can we show that certain solutions to problems are approximable by 2-layer neural nets, by showing that their Barron norm is small?
<ul>
<li>Ex. Reinforcment learning value function. Must assume some kind of geometry on state space. (For arbitrary partitions, trivial as all value vectors in <span class="math inline">\(\R^A\)</span>.)</li>
<li>On policy side: it’s clear that graph can be partitioned.</li>
<li>Convolution - what’s the analogue here? It’s very weird because functions form infinite-dimensional space, nonlinear functions on functions is weird.</li>
<li>Something about looking at <span class="math inline">\(\ep^{-\lg |S|}\)</span> nodes, depth <span class="math inline">\(-\lg \ep\)</span>?</li>
<li>Would be nice to study niceness of value, optimal policy in belief space.</li>
<li>Barron is a stronger condition than Lipschitz - says there aren’t many directions of variation.</li>
<li>If e.g. something radial, out of luck!</li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Brownian Motion</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/probability/brownian_motion.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/probability/brownian_motion.html</id>
    <published>2017-03-07T00:00:00Z</published>
    <updated>2017-03-07T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Brownian Motion</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-03-07 
          , Modified: 2017-03-07 
	</p>
      
       <p>Tags: <a href="/tags/brownian%20motion.html">brownian motion</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#robert-browns-new-thing">Robert Brown’s new thing</a></li>
 <li><a href="#brownian-motion-as-a-gaussian-process">Brownian motion as a Gaussian process</a><ul>
 <li><a href="#invariance-properties">2.2 Invariance properties</a></li>
 <li><a href="#rd">2.3 <span class="math inline">$\R^d$</span></a></li>
 </ul></li>
 <li><a href="#constructions-of-brownian-motion">3 Constructions of Brownian motion</a><ul>
 <li><a href="#levy-ciesielski">3.1 Levy-Ciesielski</a></li>
 <li><a href="#levys-original-argument">3.2 Levy’s original argument</a></li>
 </ul></li>
 <li><a href="#the-canonical-model">4 The canonical model</a><ul>
 <li><a href="#kolmogorovs-theorem">4.2 Kolmogorov’s Theorem</a></li>
 </ul></li>
 <li><a href="#brownian-motion-as-a-markov-process">6 Brownian motion as a Markov process</a></li>
 <li><a href="#brownian-motion-and-transition-semigroups">7 Brownian motion and transition semigroups</a><ul>
 <li><a href="#the-generator">7.2 The generator</a></li>
 </ul></li>
 <li><a href="#the-pde-connection">8 The PDE connection</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>Notes from Schilling, Partzsch.</p>
<h2 id="robert-browns-new-thing">Robert Brown’s new thing</h2>
<p>A particle</p>
<ul>
<li>starts at <span class="math inline">\(x=0\)</span>.</li>
<li>at times <span class="math inline">\(k\De t\)</span>, move <span class="math inline">\(\De x\)</span> units to L or R with equal probability.</li>
</ul>
<p>Let <span class="math inline">\(X_t\)</span> be position at time <span class="math inline">\(t\in [0,T]\)</span>.</p>
<ul>
<li><span class="math inline">\(\Var(X_t)=\si^2 T\)</span> where <span class="math inline">\(\fc{(\De x)^2}{\De t}=\si^2\)</span>.</li>
<li>CLT: <span class="math inline">\(X_T \to \sqrt T \si N(0,1)\)</span>.</li>
</ul>
<strong>Definition</strong>. A <span class="math inline">\(d\)</span>-dimensional Brownian motion <span class="math inline">\(B=(B_t)_{t\ge 0}\)</span> is a stochastic process indexed by <span class="math inline">\([0,\iy)\)</span>,
<span class="math display">\[\begin{align}
B_0(\om)&amp;=0\\
B_{t_n}-B_{t_{n-1}},\ldots, B_{t_1}-B_{t_0} &amp; \text{ independent}, 0=t_0\le t_1&lt;\cdots &lt; t_n\\
B_t - B_s &amp;\sim B_{t+h} - B_{s+h}\\
B_t &amp;= N(0,t)^{\ot d}\\
t&amp;\mapsto B_t(\om) \text{ continuous.}
\end{align}\]</span>
<p>Note the last property is implied by the previous.</p>
<h2 id="brownian-motion-as-a-gaussian-process">Brownian motion as a Gaussian process</h2>
<p>Characteristic function of Gaussian: <span class="math display">\[
\E e^{i\an{\xi,\Ga}} = e^{i \E\an{\xi, \Ga} - \rc \Var\an{\xi, \Ga}} = e^{i\an{\xi, \Ga}} = e^{i\an{\xi, m} - \rc 2\an{\xi, \Si\xi}}.
\]</span></p>
<ul>
<li><span class="math inline">\(B_t\)</span> is Gaussian with mean 0, variance <span class="math inline">\(t\)</span>. <span class="math inline">\(\E e^{\ze B_t} = e^{t\ze^2/2}\)</span>.</li>
<li><span class="math inline">\(\E (B_t^{2k}) = t^k \fc{2^k \Ga(k+\rc 2)}{\sqrt \pi}\)</span>.</li>
<li><span class="math inline">\(\Cov(B_s,B_t) = \min(s,t)\)</span>.</li>
</ul>
<p><span class="math inline">\((X_t)_{t\ge 0}\)</span> is a Gaussian process if for all <span class="math inline">\(0\le t_1&lt;t_2&lt;\cdots\)</span>, <span class="math inline">\((X_{t_1},\ldots, X_{t_n})\)</span> is a Gaussian random vector.</p>
<p><strong>Theorem</strong>. <span class="math inline">\((B_t)_{t\ge 0}\)</span> is a Gaussian process. The covariance of <span class="math inline">\((B_{t_1},\ldots, B_{t_n})\)</span> is <span class="math inline">\(C = (\min(t_j,t_k))_{j,k}\)</span>.</p>
<p><em>Proof</em>. Linearly transform <span class="math inline">\((t_1-t_0,t_2-t_1,\ldots)\)</span>.</p>
<p>Converse: If the covariance matrices are given by the above and <span class="math inline">\((X_t)_{t\ge 0}\)</span> has continuous sample paths, then <span class="math inline">\((X_t)_{t\ge 0}\)</span> is 1-D Brownian.</p>
<h3 id="invariance-properties">2.2 Invariance properties</h3>
<ul>
<li>Reflection <span class="math inline">\(-B_t\)</span></li>
<li>Renewal <span class="math inline">\(W(t) = B(t+a)-B(a)\)</span>.</li>
<li>Markov property: <span class="math inline">\(B(t):0\le t\le a\)</span> independent to <span class="math inline">\(W(t), t\ge 0\)</span>.</li>
<li>Time inversion <span class="math inline">\(W_t=B_{a-t}-B_a\)</span>.</li>
<li>Scaling <span class="math inline">\(B_{ct}\sim c^{\rc 2}B_t\)</span>.</li>
<li>Projective reflection <span class="math inline">\(W(t) = tB(\rc t)\)</span></li>
</ul>
<h3 id="rd">2.3 <span class="math inline">\(\R^d\)</span></h3>
<p><span class="math inline">\(B_t\)</span> is <span class="math inline">\(BM^d\)</span> iff its coordinates are <span class="math inline">\(BM^1\)</span>.</p>
<p><em>Proof</em>. Forward: Show increments are independent—characteristic function factorizes.</p>
<p><span class="math inline">\(Q\)</span>-Brownian motion: <span class="math inline">\(X_t-X_s\sim N(0,(t-s)Q)\)</span>, <span class="math inline">\(s&lt;t\)</span>.</p>
<h2 id="constructions-of-brownian-motion">3 Constructions of Brownian motion</h2>
<h3 id="levy-ciesielski">3.1 Levy-Ciesielski</h3>
<p>Write paths as random series wrt complete orthonormal system of <span class="math inline">\(L^2([0,1],dt)\)</span>. Let <span class="math inline">\((G_n)_{n\ge 0}\)</span> be sequence of <span class="math inline">\(N(0,1)\)</span> variables. <span class="math display">\[
W_N(t):=\sumz n{N-1} G_n\an{\one_{[0,t)}, \phi_n}_{L^2}.
\]</span></p>
<p><strong>Theorem</strong>. <span class="math inline">\(\lim_{N\to \iy} W_N(t)\)</span> is Brownian motion.</p>
<p><em>Proof</em>.</p>
<ul>
<li>Convergence: <span class="math inline">\(\E [W_N(t)^2] \to t\)</span>.</li>
<li>Covariances correct: <span class="math inline">\(\E[(W(t)-W(s))(W(v)-W(u))] = \an{\one_{[s,t)}, \one_{[u,v)}}\)</span>.</li>
<li>Continuity of <span class="math inline">\(t\mapsto W(t,\om)\)</span>: Proof for a specific system: Haar functions <span class="math inline">\(H_n\)</span>.
<ul>
<li>(? Uniform convergence <span class="math inline">\(\liminf_{n,N\to \iy} \ve{W_{2^N}-W_{2^n}}_{L^{\iy}(t)}=0\)</span>. By Arzela-Ascoli (?) there is convergent subsequence.</li>
</ul></li>
</ul>
<!-- * (Invariance - translation is orthonormal transformation?) (not necessary)-->
<h3 id="levys-original-argument">3.2 Levy’s original argument</h3>
<h2 id="the-canonical-model">4 The canonical model</h2>
<p>Identify <span class="math inline">\(\Om\)</span> (in <span class="math inline">\((\Om, A, \Pj)\)</span>) as a subset of <span class="math inline">\((\R^d)^I\)</span>, actually <span class="math inline">\(C_{(0)}\)</span> consisting of continuous <span class="math inline">\(w:[0,\iy)\to \R^d\)</span>, <span class="math inline">\(w(0)=0\)</span>.</p>
<p>Consider the product <span class="math inline">\(\si\)</span>-algebra <span class="math display">\[
B^I(\R^d) = \si\set{\pi_t^{-1}(B)}{B\in B(\R^d), t\in I} = \si\set{\pi_t}{t\in I}.
\]</span> Consider the intersection with <span class="math inline">\(C_{(0)}\)</span>, <span class="math display">\[
C_{(0)}\cap B^T(\R^d) = \si(\pi_t|_{C_{(0)}}:t\in I).
\]</span> <span class="math inline">\(C_{(0)}\)</span> is complete separable metric space with metric of locally uniform convergence <span class="math inline">\(\rh(w,v)=\sumo n{\iy} (1\wedge \sup_{0\le t\le n} |w(t)-v(t)|)2^{-n}\)</span>.</p>
<p>The finite-dimensional distributions uniquely determine <span class="math inline">\(\mu\)</span> (cylinder sets generate). <span class="math inline">\(\mu\)</span> is <strong>Wiener measure</strong>, the space is the <strong>path space</strong>.</p>
<p><strong>Theorem</strong> <span class="math inline">\((C_{(0)}, B(C_{(0)}), \mu)\)</span>: <span class="math inline">\((\pi_t)_{t\ge 0}\)</span> is Brownian motion (<strong>canonical model of Brownian motion</strong>). <!-- *-stable generator? --></p>
<p>Some properties</p>
<ul>
<li><span class="math inline">\(\Ga\in B^I(\R^d)\)</span> is determined by countably many indices: for every <span class="math inline">\(\Ga\in B^I(\R^d)\)</span> there exists countable <span class="math inline">\(S\sub I\)</span>, such that <span class="math inline">\(f\in (\R^d)^I, w\in \Ga, f|_S=w|_S \implies f\in \Ga\)</span>. <em>Proof</em>. <span class="math inline">\(\set{\Ga \subeq (\R^d)^I}{\pat{holds for some countable }S}\)</span> is a <span class="math inline">\(\si\)</span>-algebra.</li>
<li><span class="math inline">\(C_{(0)}\nin B^{[0,\iy)}\nin B^I (\R^d)\)</span>. Proof: can’t enforce continuity using values at countably many points. (What is the point of this? Saying we can’t work in <span class="math inline">\(B^I(\R^d)\)</span>, have to work in <span class="math inline">\(\cap C_{(0)}\)</span>?)</li>
</ul>
<h3 id="kolmogorovs-theorem">4.2 Kolmogorov’s Theorem</h3>
<p><strong>Theorem</strong>. Let <span class="math inline">\(I\subeq [0,\iy)\)</span>, <span class="math inline">\(p_{t_1,\ldots, t_n}\)</span> be probability measures defined on <span class="math inline">\((\R^d)^n\)</span>. If the family is consistent (<span class="math inline">\(p_t(C) = p_{t_{\si}} (C_\si)\)</span> and <span class="math inline">\(p_{t_{1:n-1},t_n}(C_{1:n-1}\times \R^d) = p_{t_{1:n-1}} (C_{1:n-1})\)</span>), then there exists <span class="math inline">\(\mu\)</span> on <span class="math inline">\(((\R^d)^I, B^I(\R^d))\)</span>, <span class="math inline">\(p_{t_{1:n}}(C) = \mu(\pi_{t_{1:n}}^{-1}(C))\)</span>.</p>
<p>Corollary: can construct canonical process for any family of consistent finite dimensional probability distributions.</p>
<p>(Still not too clear on it sufficing to consider finite-dim projections…)</p>
<p>Can use this theorem to construct BM. Continuity follows from Theorem 4.11.</p>
<h2 id="brownian-motion-as-a-markov-process">6 Brownian motion as a Markov process</h2>
<p>(I’m confused about what more 6.1 says. Is this related to stopping times? <span class="math inline">\(F_t\)</span> is info up to time <span class="math inline">\(t\)</span>?)</p>
<p>Let <span class="math display">\[
\Pj^x(B_{t_i}\in A_i:1\le i\le n) = \Pj(B_{t_i}+x\in A_i:1\le i\le n).
\]</span></p>
<h2 id="brownian-motion-and-transition-semigroups">7 Brownian motion and transition semigroups</h2>
Linear operators: transition semigroup and resolvent
<span class="math display">\[\begin{align}
P_t u(x) &amp;= \E^x u(B_t)\\
U_\al u(x) &amp;= \E^x \ba{\iiy u(B_t)e^{-\al t}\,dt}
\end{align}\]</span>
<p>A semigroup <span class="math inline">\((P_t)_{t\ge 0}\)</span> on a Banach space is family of linear operators <span class="math inline">\(P_t:B\to B, t\ge 0\)</span>, satisfying <span class="math inline">\(P_tP_s=P_{t+s}\)</span>, <span class="math inline">\(P_0=\id\)</span>.</p>
<p>Banach spaces:</p>
<ul>
<li><span class="math inline">\(B_b\)</span>: Borel measurable <span class="math inline">\(\R^d\to \R\)</span> with uniform norm <span class="math inline">\(\ved_{\iy}\)</span>.</li>
<li><span class="math inline">\(C_\iy\)</span>: continuous functions vanishing at infinity with uniform norm <span class="math inline">\(\ved_{\iy}\)</span>.</li>
</ul>
<p>Lemma 7.2. <span class="math inline">\((B_t)_{t\ge 0}\)</span> is uniformly stochastically continuous, <span class="math display">\[ \lim_{t\to 0}\sup_{x\in \R^d} \Pj^x (|B_t-x|&gt;\de)=0\]</span> for all <span class="math inline">\(\de&gt;0\)</span>.</p>
<p>Properties</p>
<ol type="1">
<li>Conservative <span class="math inline">\(P_t1=1\)</span>.</li>
<li>Contraction on <span class="math inline">\(B_b\)</span>: <span class="math inline">\(\ve{P_t u}_\iy \le \ve{u}_{\iy}\)</span>, <span class="math inline">\(u\in B_b\)</span></li>
<li>Positive preserving: <span class="math inline">\(u\ge 0\implies P_tu\ge 0\)</span>, <span class="math inline">\(u\in B_b\)</span>.</li>
<li>Sub-Markovian: <span class="math inline">\(0\le u\le 1\)</span>, <span class="math inline">\(u\in B_b\implies 0\le P_tu\le 1\)</span>.</li>
<li>Feller: <span class="math inline">\(u\in C_\iy\implies P_t u\in C_\iy\)</span>.</li>
<li>Strongly continuous on <span class="math inline">\(C_\iy\)</span>: <span class="math inline">\(u\in C_\iy\implies \lim_{t\to 0}\ve{P_tu-u}_\iy = 0\)</span>.</li>
<li>Strong Feller: <span class="math inline">\(u\in B_b\implies P_tu\in C_b\)</span>.</li>
</ol>
<p>(1-4 is Markov, 2-4 is sub-Markov, 2-6 is Feller, 2-4+7 is strong Feller.)</p>
<p>7.5: <span class="math display">\[\Pj^x(B_{t_i}\in C_i:1\le i\le n) = P_{t_1}[\one_{C_1} P_{t_2-t_1}[\one_{C_2}\cdots ]].\]</span></p>
<p>This gives a way to construct a Markov process from a semigroup of operators. (Apply to indicator functions.) A Feller semigroup has a corresponding Feller process.</p>
<h3 id="the-generator">7.2 The generator</h3>
<p>Motivation: If <span class="math inline">\(\phi:[0,\iy)\to \R\)</span>, additive, <span class="math inline">\(\phi(0)=1\)</span>, then <span class="math inline">\(\phi(t) = e^{at}\)</span>, <span class="math inline">\(a=\ddd t^{+}\phi(t)|_{t=0}\)</span>.</p>
<strong>Definition</strong>. Let <span class="math inline">\((P_t)_{t\ge 0}\)</span> be Feller semigroup on <span class="math inline">\(C_\iy(\R^d)\)</span>. Then
<span class="math display">\[\begin{align}
Au :&amp;= \lim_{t\to 0} \fc{P_t u-u}t\\
D(A) :&amp;= \set{u\in C_\iy(\R^d)}{\exists g\in C_\iy(\R^d), \lim_{t\to 0} \ve{\fc{P_tu-u}t-g}_{\iy}=0}
\end{align}\]</span>
<p>is the infinitesimal generator of <span class="math inline">\((P_t)_{t\ge 0}\)</span>. <span class="math inline">\(A\)</span> is a function <span class="math inline">\(A:D(A)\to C_\iy(\R^d)\)</span>.</p>
<p>Ex. for <span class="math inline">\(BM^d\)</span>, <span class="math inline">\(P_tu(x) = \E^xu(B_t)\)</span>, <span class="math inline">\(A=\rc 2 \De\)</span> on <span class="math inline">\(C_\iy^2(\R^d)\)</span>. Proof: Taylor expansion.</p>
Lemma 7.10. Let <span class="math inline">\(P_t\)</span> be Feller semigroup with generator <span class="math inline">\(A\)</span>. <span class="math inline">\(P_t\)</span> and <span class="math inline">\(\int_0^t P_s\cdot\,ds\)</span> are <span class="math inline">\(D(A)\to D(A)\)</span>.
<span class="math display">\[\begin{align}
\ddd tP_tu &amp;= AP_tu = P_tAu\\
P_t u - u &amp;= A\int_0^t P_su\,ds.
\end{align}\]</span>
<p><em>Proof.</em> Use: <span class="math inline">\(P_t\)</span> is contraction to get <span class="math inline">\(\ddd t^+\)</span>. Use this and strong continuity to get <span class="math inline">\(\ddd t^-\)</span>. Fubini’s Theorem for part 2.</p>
<p>Corollary 7.11. Let <span class="math inline">\((P_t)_{t\ge 0}\)</span> be Feller semigroup with generator <span class="math inline">\(A\)</span>.</p>
<ol type="1">
<li><span class="math inline">\(D(A)\)</span> is dense in <span class="math inline">\(C_\iy(\R^d)\)</span>.</li>
<li><span class="math inline">\(A\)</span> is a closed operator.</li>
<li>If <span class="math inline">\((T_t)_{t\ge 0}\)</span> is also Feller with generator <span class="math inline">\(A\)</span>, <span class="math inline">\(P_t=T_t\)</span>.</li>
</ol>
<p><em>Proof</em>.</p>
<ol type="1">
<li><span class="math inline">\(\ep^{-1}\int_0^\ep P_su\,ds\to u\)</span>. (Decay it a little!)</li>
<li>If <span class="math inline">\(u_n\to u\)</span>, <span class="math inline">\(Au_n\to w\)</span>, then <span class="math inline">\(Au=w\)</span>.</li>
<li><span class="math inline">\(\ddd sP_{t-s} T_su = 0\)</span>.</li>
</ol>
<p><strong>Proposition 7.13</strong>. Let <span class="math inline">\(P_t\)</span> be Feller. Then <span class="math inline">\(\al U_\al\)</span> is conservative, contraction on <span class="math inline">\(B_b\)</span>, positivity preserving, Feller, strongly continuous on <span class="math inline">\(C_\iy\)</span>. Moreover</p>
<ol type="1">
<li><span class="math inline">\((U_\al)_{\al&gt;0}\)</span> is resolvent: <span class="math inline">\(U_\al = (\al \id -A)^{-1}\)</span>.</li>
<li>Resolvent equation <span class="math inline">\(U_\al u - U_\be u = (\be-\al) U_\be U_\al u\)</span>, <span class="math inline">\(u\in B_b\)</span>.</li>
<li>There is 1-to-1 relationship between <span class="math inline">\((P_t)\)</span> and <span class="math inline">\((U_\al)\)</span>.</li>
<li><span class="math inline">\((U_\al)\)</span> is sub-Markovian iff <span class="math inline">\((P_t)\)</span> is sub-Markovian.</li>
</ol>
<p>(Intuition for (1). <span class="math inline">\(\iiy e^{(A-\al I) t}u\,dt = (A-\al I)^{-1}\)</span>.)</p>
<p>Example 7.14. For Brownian motion <span class="math display">\[
U_\al u(x) = \begin{cases}
\int \fc{e^{-\sqrt{2\al}y}{\sqrt{2\al}}u(x+y)\dy,&amp;d=1\\
\int \rc{\pi^{\fc d2}}\pf{\al}{2y^2}^{\fc d4-\rc2} K_{\fc d2-1}(\sqrt{2\al}y) u(x+y)\dy,&amp;d\ge 2.
\end{cases}
\]</span></p>
<p>Determining domain of generator:</p>
<p><strong>Theorem</strong>. If <span class="math inline">\((A', D(A'))\)</span> extends <span class="math inline">\((A,D(A))\)</span>, and for any <span class="math inline">\(u\in D(A)\)</span>, <span class="math inline">\(A'u=u\implies u=0\)</span>&lt; then <span class="math inline">\((A,D(A))=(A', D(A'))\)</span>. (? I don’t get this.)</p>
<h2 id="the-pde-connection">8 The PDE connection</h2>
<blockquote>
<p>For many classical PDE problems probability theory yields concrete representation formulae for the solutions in the form of expected values of a Brownian functional. These formulae can be used to get generalized solutions of PDEs (which require less smoothness of the initial/boundary data or the boundary itself) and they are amenable to Monte-Carlo simulations</p>
</blockquote>
<p><strong>Lemma</strong> 8.1. <span class="math inline">\(f\in D(\De)\)</span>, <span class="math inline">\(u(t,x):=\E^x f(B_t)\)</span>. <span class="math inline">\(u\)</span> is unique bounded solution of heat equation with initial value <span class="math inline">\(f\)</span>.</p>
<p><em>Proof</em>. Laplace transform and IbP. Use uniqueness of resolvent operator.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Tensorflow setup</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/tensorflow.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/tensorflow.html</id>
    <published>2017-03-03T00:00:00Z</published>
    <updated>2017-03-03T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Tensorflow setup</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-03-03 
          , Modified: 2017-03-03 
	</p>
      
       <p>Tags: <a href="/tags/neural%20nets.html">neural nets</a>, <a href="/tags/programming.html">programming</a>, <a href="/tags/tensorflow.html">tensorflow</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#setting-up-tensorflow-project">Setting up tensorflow project</a></li>
 <li><a href="#tensorboard">Tensorboard</a></li>
 <li><a href="#restoring">Restoring</a></li>
 <li><a href="#sessions-and-graphs">Sessions and graphs</a></li>
 <li><a href="#scopes">Scopes</a></li>
 <li><a href="#questions-to-figure-out">Questions to figure out</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="setting-up-tensorflow-project">Setting up tensorflow project</h2>
<pre><code>sudo apt-get install python-pip python-dev python-virtualenv 
virtualenv --system-site-packages ~/tensorflow
source ~/tensorflow/bin/activate # do every time
pip3 install --upgrade tensorflow-gpu</code></pre>
<p>With keras (<a href="http://www.pyimagesearch.com/2016/11/14/installing-keras-with-tensorflow-backend/">tutorial</a>)</p>
<pre><code>virtualenv --system-site-packages ~/keras_tf
source ~/keras_tf/bin/activate # do every time
#mkvirtualenv keras_tf
#workon keras_tf
pip install --upgrade tensorflow-gpu
pip install numpy scipy
pip install scikit-learn
pip install pillow
pip install h5py
pip install keras</code></pre>
<p>To exit, <code>deactivate</code>.</p>
<p>Cleverhans: at <code>holdenl</code></p>
<pre><code>git clone https://github.com/openai/cleverhans
export PYTHONPATH=&quot;/home/optml/holdenl/cleverhans&quot;:$PYTHONPATH # do every time
pip install matplotlib</code></pre>
<p>For cycles, <code>export PYTHONPATH=&quot;/u/holdenl/code/cleverhans&quot;:$PYTHONPATH # do every time</code></p>
<p>Cycles: do every time</p>
<pre><code>export PYTHONPATH=&quot;/u/holdenl/code/cleverhans&quot;:$PYTHONPATH # do every time
source ~/keras_tf/bin/activate # do every time</code></pre>
<h2 id="tensorboard">Tensorboard</h2>
<p>Do <code>ssh</code> linking port 6006:</p>
<pre><code>ssh -t -t -L 6006:localhost:6006 holdenl@portal.cs.princeton.edu &quot;ssh -L 6006:localhost:6006 optml@optml.cs.princeton.edu&quot;</code></pre>
<p>Run tensorboard.</p>
<pre><code>tensorboard --logdir=tf-slim/train/</code></pre>
<h2 id="restoring">Restoring</h2>
<div class="sourceCode"><pre class="sourceCode py"><code class="sourceCode python">restorer <span class="op">=</span> tf.train.Saver()
<span class="cf">with</span> tf.Session() <span class="im">as</span> sess:
	<span class="co"># Restore variables from disk. #tf-slim/</span>
	restorer.restore(sess, <span class="st">&quot;./tf-slim/train/model.ckpt-20000&quot;</span>)
	<span class="bu">print</span>(<span class="st">&quot;Model restored.&quot;</span>)
	<span class="bu">print</span>(<span class="st">&quot;test accuracy </span><span class="sc">%g</span><span class="st">&quot;</span><span class="op">%</span>accuracy.<span class="bu">eval</span>(feed_dict<span class="op">=</span>{
            x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="fl">1.0</span>}, session<span class="op">=</span>sess))</code></pre></div>
<h2 id="sessions-and-graphs">Sessions and graphs</h2>
<p>A graph is just the skeleton. A session is when the graph is actually run.</p>
<p>Two ways to run a session:</p>
<div class="sourceCode"><pre class="sourceCode py"><code class="sourceCode python">sess <span class="op">=</span> tf.Session()
sess.run(tf.global_variables_initializer())
accuracy.<span class="bu">eval</span>(feed_dict<span class="op">=</span>{
	x:batch[<span class="dv">0</span>], y_: batch[<span class="dv">1</span>], keep_prob: <span class="fl">1.0</span>}, session<span class="op">=</span>sess)</code></pre></div>
<p>Second way: (Probably this is preferred because “with” invokes some magic.)</p>
<div class="sourceCode"><pre class="sourceCode py"><code class="sourceCode python">sess <span class="op">=</span> tf.Session()
<span class="cf">with</span> tf.Session() <span class="im">as</span> sess:
	...</code></pre></div>
<h2 id="scopes">Scopes</h2>
<pre><code>with tf.variable_scope(&quot;conv1&quot;):</code></pre>
<h2 id="questions-to-figure-out">Questions to figure out</h2>
<ul>
<li>What CPU/GPU is actually used?</li>
<li>How do virtual environments work?</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Adversarial experiments</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/adversarial_experiments.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/adversarial_experiments.html</id>
    <published>2017-03-02T00:00:00Z</published>
    <updated>2017-03-02T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Adversarial experiments</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-03-02 
          , Modified: 2017-03-02 
	</p>
      
       <p>Tags: <a href="/tags/neural%20nets.html">neural nets</a>, <a href="/tags/uncertainty.html">uncertainty</a>, <a href="/tags/aaml.html">aaml</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#results">Results</a></li>
 <li><a href="#todo">Todo</a><ul>
 <li><a href="#nice-things">Nice things</a></li>
 </ul></li>
 <li><a href="#done">Done</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>See <a href="https://github.com/holdenlee/adversarial">github</a>.</p>
<h2 id="results">Results</h2>
<p>Trained for 100 epochs. Accuracy (out of 1)</p>
<table>
<thead>
<tr class="header">
<th>Mixture\Attack</th>
<th>0.3 fgs</th>
<th>0.5 fgs</th>
<th>1.0 fgs (sanity check)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>?</td>
<td>?</td>
<td>?</td>
</tr>
<tr class="even">
<td>10</td>
<td>?</td>
<td>?</td>
<td>?</td>
</tr>
<tr class="odd">
<td>100</td>
<td>?</td>
<td>?</td>
<td>?</td>
</tr>
</tbody>
</table>
<h2 id="todo">Todo</h2>
<ul>
<li>Look at which samples are misclassified by which networks: are they the same or different?
<ul>
<li>Jaccard similarity</li>
<li>How about if you train an ensemble together on regular examples?</li>
</ul></li>
<li>Baselines
<ul>
<li>How transferable are adversarial examples between the 100 networks?</li>
<li>How well does majority do against adversarial example against the mixture?</li>
</ul></li>
<li>Does clamping help?</li>
<li>Examine activations of adversarial examples in hidden layer. Do they look different for adversarial examples?</li>
<li>What are the weights given to the networks in the mixture? How does this change over time?</li>
<li>Try training with independent updates, multiplicative weights.
<ul>
<li>(Check YZ’s code.)</li>
<li>Try sleeping, etc. - regularize more strongly so that weights don’t become too small/large.</li>
</ul></li>
<li>Hyperparameter search</li>
<li>Autoencoder idea</li>
<li>Regularization/Lipschitz/wavelet idea</li>
<li>Is pretraining necessary?</li>
<li>Compare 100 networks to 1 network with 100x size. Which does better?</li>
</ul>
<h3 id="nice-things">Nice things</h3>
<ul>
<li>Set up tensorboard to show histograms, real and adversarial images, etc.</li>
<li>Run with many different settings.</li>
</ul>
<h2 id="done">Done</h2>
<ul>
<li>Port over to own training loop.</li>
<li>End - save</li>
<li>Output accuracies, etc. over time as list.</li>
<li>Plotting!</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
