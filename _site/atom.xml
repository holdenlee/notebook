<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2016-08-06T00:00:00Z</updated>
    <entry>
    <title>Weekly summary 2016-08-06</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-08-06.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-08-06.html</id>
    <published>2016-08-06T00:00:00Z</published>
    <updated>2016-08-06T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2016-08-06</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-06 
          , Modified: 2016-08-06 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#representation-learning">Representation learning</a></li>
 </ul> </div>

  <div class="blog-main">
    <ul>
<li>Machine learning
<ul>
<li><a href="/tcs/machine_learning/neural_nets/pmi_images.html">PMI images</a>
<ul>
<li>Understood <a href="/tcs/machine_learning/nlp/pmi.html">PMI idea</a> more thoroughly.</li>
<li>Got code from Ben. Having troubles running. See
<ul>
<li>https://www.mathworks.com/matlabcentral/newsreader/view_thread/345876#947272</li>
<li>https://askrc.princeton.edu/question/255/matlab-on-nobel-error-using-mex/</li>
<li>http://stackoverflow.com/questions/38723051/error-using-mex-g-error-no-such-file-or-directory</li>
</ul></li>
<li><a href="/tcs/machine_learning/neural_nets/convnets_ideas.html">Brainstorm on convnets</a>. Skimmed the following. <strong>TODO</strong>: understand more thoroughly.
<ul>
<li>Fastfood - computing Hilbert space expansions in loglinear time
<ul>
<li>Deep-fried convnets</li>
<li>Ailon-Chazelle</li>
</ul></li>
<li><a href="/tcs/machine_learning/nlp/HA16.html">HA16</a> Unsupervised Learning of Word-Sequence Representations from Scratch via Convolutional Tensor Decomposition</li>
<li>[MKHS14] Convolutional Kernel Networks</li>
</ul></li>
</ul></li>
<li>Representation learning problem (see below)</li>
<li><strong>TODO</strong>: Think about AGD, talk with Cyril.</li>
</ul></li>
<li>Probability
<ul>
<li><a href="/math/probability/random_matrices/hi_dim_prob.html">HDP</a> Chapters 2-3</li>
</ul></li>
<li>TT/ATP/PL
<ul>
<li>Reviewed HMTI, read more on type theory. See <a href="cs/type_theory/types_and_pl.html">Types and PL</a>, <a href="cs/type_theory/type_theory.html">Type theory</a>.</li>
</ul></li>
</ul>
<h2 id="representation-learning">Representation learning</h2>
<p>In dictionary learning, we assume we have samples <span class="math inline">\(y = Ax + e\)</span> where <span class="math inline">\(x\)</span> comes from a sparse distribution (ex. <span class="math inline">\(x_i\)</span> independent, <span class="math inline">\(x_i\neq 0\)</span> with probability <span class="math inline">\(s/n\)</span> and then is drawn from some distribution not concentrated at 0) and <span class="math inline">\(e\)</span> is error (ex. Gaussian).</p>
<p>The way we stated our problem is that <span class="math inline">\(x\cdot a_i\)</span> is large for only a few <span class="math inline">\(i\)</span>. This is similar to dictionary learning with <span class="math inline">\((A^+)^T\)</span> where the columns of <span class="math inline">\(A\)</span> are the <span class="math inline">\(a_i\)</span>. (I.e. the <span class="math inline">\(x\)</span>’s here are really the <span class="math inline">\(y\)</span>’s in DL.)</p>
<p>I may be wrong but I think that what’s different is that</p>
<ul>
<li>Dictionary learning on <span class="math inline">\((A^+)^T\)</span> would correspond to when <span class="math inline">\((x\cdot a_i)_i\)</span> is sparse + noise. Our assumption is a bit different that <span class="math inline">\(x\cdot a_i\)</span> is large for only a few <span class="math inline">\(i\)</span>, ex. large negative values don’t count against the assumption.</li>
<li>we’re trying to relax the condition on the noise—ex. instead of saying that the noise in the other coordinates is random, we consider worst-case or make an assumption that they’re random-like in some way.</li>
</ul>
<p>(Actually, I think the undercomplete case when the number of <span class="math inline">\(a_i\)</span> is less than the dimension of <span class="math inline">\(n\)</span> doesn’t quite correspond to DL because the map <span class="math inline">\(x\mapsto (x\cdot a_i)_i\)</span> is not invertible…)</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Neural net experiments</title>
    <link href="http://holdenlee.github.io/notebook/posts/cs/neural_nets/experiments.html" />
    <id>http://holdenlee.github.io/notebook/posts/cs/neural_nets/experiments.html</id>
    <published>2016-08-03T00:00:00Z</published>
    <updated>2016-08-03T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Neural net experiments</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-03 
          , Modified: 2016-08-03 
	</p>
      
       <p>Tags: <a href="/tags/programming.html">programming</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <h2 id="done">Done</h2>
<ul>
<li>MNIST basics</li>
<li>CIFAR-10 basics</li>
</ul>
<h2 id="todo">Todo</h2>
<p>See <a href="/tcs/machine_learning/neural_nets/convnets_ideas.html">Convnets ideas</a>.</p>
<ul>
<li>LSTM: generate text
<ul>
<li>Character by character</li>
<li>Word by word, using word embeddings</li>
<li>With restrictions</li>
<li>With parse trees</li>
</ul></li>
<li>LSTM for algorithms
<ul>
<li>Thresholding</li>
<li>Neural net Turing machines</li>
</ul></li>
<li>Rotational invariance, etc.</li>
<li>Smoothness regularization in convolution kernels</li>
<li>Get familiar with Tensorboard.</li>
<li>Visualizing neural nets.</li>
<li>Sparsification</li>
<li>Reimplementations
<ul>
<li>Deep-fried convnets</li>
<li>Fooling neural nets</li>
<li>Deep reinforcement learning with Atari games</li>
</ul></li>
<li>“Train twice and put together”: independent ways of doing things (cf. boosting)</li>
<li>Check claims of suspiciously good performance (ex. learning to execute…)</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[HA16] Unsupervised Learning of Word-Sequence Representations from Scratch via Convolutional Tensor Decomposition</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/HA16.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/HA16.html</id>
    <published>2016-08-02T00:00:00Z</published>
    <updated>2016-08-02T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[HA16] Unsupervised Learning of Word-Sequence Representations from Scratch via Convolutional Tensor Decomposition</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-02 
          , Modified: 2016-08-02 
	</p>
      
       <p>Tags: <a href="/tags/nlp.html">nlp</a>, <a href="/tags/word%20embeddings.html">word embeddings</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p>See also [HA15] Convolutional Dictionary Learning through Tensor Factorization.</p>
<ol type="1">
<li>Use PCA to reduce dimensionality of 1-hot encodings to <span class="math inline">\(k\)</span> dimensions. (Wouldn’t it be better to use word embeddings?)</li>
<li>For each dimension and each sentence, obtain a vector <span class="math inline">\(x\)</span>. Train on this set. Minimize <span class="math display">\[\min_{f_i,w_i:\ve{f_i}=1} \ve{x-\sum_{i\in [L]} f_i * w_i}^2.
\]</span> (How to avoid SGD or calculating the sum of these? Some tensor trickery?) <span class="math inline">\(f\)</span>’s are filters, <span class="math inline">\(w\)</span>’s are activation maps.</li>
</ol>
<p>Q: Why are we considering each dimension separately? Wouldn’t it make more sense to consider them together, and try to write <span class="math display">\[ X = \sum_{i\in [L]} F_i*w_i\]</span> where <span class="math inline">\(X\)</span> is a matrix corresponding to a sentence, and <span class="math inline">\(w_i\)</span> is in the row direction. In [HA16]’s way of doing it, you’re learning how a generic subject/theme/atom modulates throughout the space (sentence), not how combinations of subjects modulate. Is this what you want? This does need MUCH fewer atoms though, else you can expect <span class="math inline">\(k\)</span> times more.</p>
<p>(T/F? Tensor algorithms are fragile in the sense that they depend on the model being exactly the way it is. Ex. tensor algorithm for NN—if you change the NN a bit it may fail.)</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Convnets ideas</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/convnets_ideas.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/convnets_ideas.html</id>
    <published>2016-08-02T00:00:00Z</published>
    <updated>2016-08-02T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Convnets ideas</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-02 
          , Modified: 2016-08-02 
	</p>
      
       <p>Tags: <a href="/tags/neural%20nets.html">neural nets</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <ul>
<li>In the Fourier basis translation is diagonalized. So can we train better in the Fourier basis?
<ul>
<li>This seems tempting, but it doesn’t seem to lead anywhere. Local convolution with shared parameters commutes with translation. Here, diagonal maps commute with translation (=multiplication by eigenfunctions). So in terms of computing a linear map, it’s easier.</li>
<li>But then we follow by a nonlinearity: pointwise sigmoid, or local pooling. In the Fourier space, this doesn’t correspond to anything nice! Fourier transform scrambles locality, and locality is important here…</li>
</ul></li>
<li>Adding more invariances:
<ul>
<li>Rotational invariance. (See [CW16] Group Equivariant Convolutional Networks)
<ul>
<li>Make local rotated copies of the image (have to be careful with cropping, interpolating) and stack them. Now do a 3-D convolution in <span class="math inline">\((x,y,\te)\)</span> space.</li>
<li>This is less efficient. We should be able to reduce the number of parameters, but by how much?</li>
<li>Beyond translation symmetry, the dimensionality of symmetries becomes greater than the dimensionality of the image (there is more than one symmetry taking one point to another point). This redundancy causes inefficiency.</li>
</ul></li>
<li>Scaling invariance.
<ul>
<li>Run the same convnet on the picture at different scales.</li>
</ul></li>
<li>Coloration (does this cause problems typically?)</li>
<li>There’s some classical result in theoretical computer vision that identifies the dimensionality of the manifold of image alterations… check Amit Singer’s notes.</li>
</ul></li>
<li>Apply the variant on convolutional dictionary learning <a href="../nlp/HA16.html">HA16</a>, except 2-D.</li>
<li>What if we did global convolutions? (This would add many parameters…)</li>
<li>Add regularization to penalize non-smooth/non-simple kernels. For example, write the kernel in a Fourier or wavelet basis, and penalize larger Fourier or wavelet vectors (ex. term based on Fourier expectation).</li>
<li>I “get” max pooling now: it preserves translation invariance. Pool across close-by transformations.
<ul>
<li>Does overlapping impose consistency?</li>
</ul></li>
<li>Could you compute max-pooling across many local transformations (ex. <span class="math inline">\((x,y,\te)\)</span>) more efficiently? Ex. finding the distance from a manifold of transformations of the image.</li>
<li>How to incorporate common sense? Ex. “A thin object in a person’s hand is likely to be a pencil.”</li>
<li>Can we train on the quotient manifold, quotiented out by symmetries? The problem is that the quotient manifold has cusps. Ex. <span class="math inline">\(\R^2/(x,y)\mapsto (y,x)\)</span> has “cusps” at <span class="math inline">\((x,x)\)</span>. Add regularization/change metric to keep it away? cf. keeping things in the simplex.</li>
<li>(General NN) How much can you sparsify a fully connected layer and still have it work? (cf. dropout)</li>
<li>How much can you reduce parameters in a trained NN and still get similar performance? (+ an explanation of why more parameters that needed helps learning) Can you iterate reducing parameters and training?
<ul>
<li>READ: Deep-fried convnets.</li>
</ul></li>
<li>Fooling neural nets…</li>
<li>Train twice independently and put together?</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>PMI for images</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/pmi_images.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/pmi_images.html</id>
    <published>2016-08-01T00:00:00Z</published>
    <updated>2016-08-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>PMI for images</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-01 
          , Modified: 2016-08-01 
	</p>
      
       <p>Tags: <a href="/tags/PMI.html">PMI</a>, <a href="/tags/neural%20nets.html">neural nets</a>, <a href="/tags/vision.html">vision</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#notes">Notes</a></li>
 <li><a href="#theory">Theory</a></li>
 <li><a href="#thoughts">Thoughts</a></li>
 <li><a href="#todo">Todo</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="notes">Notes</h2>
<p>Experiments:</p>
<ul>
<li><code>mnist_testing.m</code>: Prep data.</li>
<li><code>svd_testing.m</code>: How does reducing the number of features through SVD affect classification accuracy?</li>
<li><code>pairwise_mi.m</code>: Tests related to mutual information</li>
<li><code>mi.m</code></li>
<li><code>MI_GG.m</code></li>
<li><code>rsvd.m</code></li>
</ul>
<p>Qs</p>
<ul>
<li>Why is the normalization <code>nrm=mean(sqrt(sum(compTr.^2)))</code>?</li>
</ul>
<pre><code>	         nlayers: 2
             layer: {2x1 cell}
    type_zerolayer: 2
             ndesc: 7200


                         numlayer: 1
                        centering: 0
    median_contrast_normalization: 0
                           npatch: 5
                      subsampling: 2
                             smap: 50
                   type_zerolayer: 2
                            sigma: 0.7459
                                Z: [25x50 double]
                                w: [50x1 double]</code></pre>
<p>In <code>pairwise_mi.m</code> there’s no normalization by scaling (<span class="math inline">\(-2\log\pat{scale}\)</span>).</p>
<p>https://en.wikipedia.org/wiki/Conditional_mutual_information</p>
<h2 id="theory">Theory</h2>
<p>The right PMI measure to use here is <span class="math display">\[
\ln \pf{\an{v,w}}{\an{v,\one}\an{w,\one}}.
\]</span> Because if we assume the activations are like <span class="math inline">\((e^{\chi,v})_\chi\)</span>, then we still get that the expected dot product of these is <span class="math inline">\(\int e^{\an{\chi,v}}e^{\an{\chi,w}}\,d\chi\propto \exp\pa{\fc{\ve{v+w}^2}{2}}\)</span>.</p>
<h2 id="thoughts">Thoughts</h2>
<ul>
<li>Is a picture more like a context or a document? We’re treating it like context (looking at all pairs of features there). But its size makes it seem more like a document. Or better: a sentence, because sentences are big enough to incorporate different features and small enough to still have a vector associated with it.</li>
<li>How to incorporate convolutional ideas? Ex. if features <span class="math inline">\(f_1,f_2\)</span> are in the same relationship (translationally) as <span class="math inline">\(f_1',f_2'\)</span> then we expect PMI to be similar, so we should we really be looking at a <span class="math inline">\(7200\times 7200\)</span> matrix? How about look at PMI of adjacent features? (But they shouldn’t overlap…) Or look at PMI pre-convolution by another layer?</li>
<li>What happens if you apply (convolutional?) DL to the learned features? Then apply SVD to the dimension-reduced vectors?</li>
</ul>
<h2 id="todo">Todo</h2>
<p>Understand loss function for PMI.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>High-dimensional probability</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/probability/random_matrices/hi_dim_prob.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/probability/random_matrices/hi_dim_prob.html</id>
    <published>2016-08-01T00:00:00Z</published>
    <updated>2016-08-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>High-dimensional probability</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-01 
          , Modified: 2016-08-04 
	</p>
      
       <p>Tags: <a href="/tags/random%20matrix.html">random matrix</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#introduction">Introduction</a></li>
 <li><a href="#variance-bounds-and-poincare-inequalities">Variance bounds and Poincare inequalities</a><ul>
 <li><a href="#section">2.1</a></li>
 <li><a href="#markov-semigroups">Markov semigroups</a></li>
 <li><a href="#variance-identities-and-exponential-ergodicity">2.4 Variance identities and exponential ergodicity</a></li>
 <li><a href="#problems">Problems</a></li>
 </ul></li>
 <li><a href="#subgaussian-concentration-and-log-sobolev-inequalities">3 Subgaussian concentration and log-Sobolev inequalities</a><ul>
 <li><a href="#the-martingale-method">3.2 The martingale method</a></li>
 <li><a href="#the-entropy-method">3.3 The entropy method</a></li>
 <li><a href="#log-sobolev-inequalities">3.4 Log-Sobolev inequalities</a></li>
 <li><a href="#problems-1">Problems</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <p>Based on Ramon von Handel’s ORF570 notes.</p>
<h2 id="introduction">Introduction</h2>
<p>Themes:</p>
<ul>
<li>concentration: if <span class="math inline">\(X_{1:n}\)</span> are independent or weakly dependent random variables, and <span class="math inline">\(f\)</span> is not too <em>sensitive</em> to any coordinate, then <span class="math inline">\(f(X_{1:n})\)</span> is <em>close</em> to its mean.</li>
<li>suprema</li>
<li>universality</li>
</ul>
<h2 id="variance-bounds-and-poincare-inequalities">Variance bounds and Poincare inequalities</h2>
<h3 id="section">2.1</h3>
<p>Trivial bound: <span class="math display">\[ \Var[f(X)]\le \rc4 (\sup f - \inf f)^2 \qquad \Var[f(x)] \le \E[(f(X)-\inf f)^2].\]</span></p>
<p>Tensorization gives a bound for functions of independent random variables from bounds for functions of each individual random variable.</p>
<p><strong>Theorem</strong> (Tensorization of variance): <span class="math display">\[\Var[f(X_1,\ldots, X_n)]\le \E\ba{\sumo in \Var_i f(X_1,\ldots, X_n)}\]</span> whenever <span class="math inline">\(X_{1:n}\)</span> are independent.</p>
<p>This is sharp for linear functions.</p>
<em>Proof</em>. Write <span class="math display">\[ f(X_{1:n}) - \E f(X_{1:n}) = \sumo kn \ub{\E[f(X_{1:n}|X_{1:k})] - \E[f(X_{1:n})|X_{1:k-1}]}{\De_k}. \]</span> The <span class="math inline">\(\De_k\)</span> form a martingale. By independence of martingale increments,
\begin{align}
\Var(f) &amp;= \sumo kn \E[\De_k^2] \\
\E[\De_k^2] &amp;= \E[\E[\wt \De_k |X_{1:k}]^2]\\
&amp; \le  \E[(\ub{f - \E[f|X_{1:k-1,k+1:n}]}{\wt \De_k})^2] &amp; \text{Jensen}\\
&amp;= \E\ba{\sumo in \Var_f f(X_1,\ldots, X_n)}.
\end{align}
Define
\begin{align}
D_i f(x) &amp;= (\sup_z-\inf_z)(f(x_{1:i-1},z,x_{i+1:n}))\\
D_i^- f(x) &amp;= f(x) - \inf_z(f(x_{1:i-1},z,x_{i+1:n}))\\
\end{align}
<p><strong>Corollary</strong> (Bounded difference inequality): Tensorization + trivial inequality.</p>
<p><strong>Example</strong>: Consider Bernoulli symmetric matrices. What is the variance of <span class="math inline">\(\la_{\max}(M) = \an{v_{\max}(M), Mv_{\max}(M)}\)</span>? Fix <span class="math inline">\(i,j\)</span>. Let <span class="math display">\[M^- = \amin_{\text{only }M_{ij} \text{ varies}} \la_{\max}(M).\]</span> Then <span class="math display">\[D_{ij}^-\la_{\max}(M) \le \an{v_{\max}(M), (M-M^-) v_{\max}(M)}\le 4|v_{\max}(M)_i||v_{\max}(M)_j|.\]</span> Use the corollary to get <span class="math inline">\(\le 16\)</span>.</p>
<p>This is not sharp. (<span class="math inline">\(\sim n^{-\rc 3}\)</span> is correct.)</p>
<p>Drawbacks to this method:</p>
<ul>
<li>bounds using bounded di↵erence inequalities are typically restricted to situations where the random variables <span class="math inline">\(X_i\)</span> and/or the function <span class="math inline">\(f\)</span> are bounded.</li>
<li>Bounded difference inequalities do not capture any information on the distribution of <span class="math inline">\(X_i\)</span>. In the other direction, the tensorization inequality is too distribution-dependent.</li>
<li>Tensorization depends on independence.</li>
</ul>
<p>Inequalities in this section are roughly of the following form (Poincare inequalities): <span class="math display">\[\Var(f) \le \E[\ve{\nb f}^2].\]</span> “The validity of a Poincar´e inequality for a given distribution is intimately connected the convergence rate of a Markov process that admits that distribution as a stationary measure.”</p>
<h3 id="markov-semigroups">Markov semigroups</h3>
<p>A <strong>Markov process</strong> satisfies: For every bounded measurable <span class="math inline">\(f\)</span> and <span class="math inline">\(s,t\in \R_+\)</span>, here is abounded measurable <span class="math inline">\(P_sf\)</span> such that <span class="math display">\[\E[f(X_{t+s})|\{X_r\}_{r\le t}] = (P_s f)(X_t).\]</span> <span class="math inline">\(\mu\)</span> is <strong>stationary</strong> if <span class="math inline">\(\mu(P_tf)=\mu(f)\)</span> for all <span class="math inline">\(t\in \R_+\)</span>, bounded measurable <span class="math inline">\(f\)</span>.</p>
<p><strong>Lemma 2.7</strong>. <span class="math inline">\(\{P_t\}_{t\in \R_+}\)</span> defines a semigroup of linear operators on <span class="math inline">\(L^p(\mu)\)</span>. It is contractive and conservative (<span class="math inline">\(P_t1=1\)</span> <span class="math inline">\(\mu\)</span>-a.s.).</p>
<p><em>Proof</em>. Jensen.</p>
<p>The semigroup in fact acts on any <span class="math inline">\(f\in L^1(\mu)\)</span>.</p>
<p><strong>Lemma 2.9</strong>. If <span class="math inline">\(\mu\)</span> is stationary, for every <span class="math inline">\(f\)</span>, <span class="math inline">\(\Var_\mu(P_tf)\)</span> is decreasing.</p>
<p><em>Proof</em>. <span class="math inline">\(L^2\)</span> contractivity and semigroup property.</p>
<p>The <strong>generator</strong> is <span class="math display">\[\cal L f = \lim_{t\searrow 0} \fc{P_tf-f}t.\]</span> The set of <span class="math inline">\(f\)</span> where this is defined is the domain; <span class="math inline">\(\cal L:\text{Dom}(\cal L) \to L^2(\mu)\)</span>.</p>
<p>Warning: for Markov processes with continuous sample paths, such as Brownian motion, <span class="math inline">\(Dom(\cL)\sub L^2(\mu)\)</span>. Functional analysis is required for rigor, but results usually extend.</p>
<p><span class="math inline">\(P_t\)</span> is the solution of the Kolmogorov equation <span class="math display">\[ \ddd{t} P_t f = P_t \cL f, \quad P_0f=f.\]</span> The generator and semigroup commute.</p>
<p>A finite-state Markov process with <span class="math display">\[ \Pj[X_{t+\de}=j|X_t=i] = \la_{ij} \de + o(\de), \quad i\ne j\]</span> has generator equal to the transition matrix <span class="math inline">\(\La\)</span>.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<p>Then <span class="math inline">\(P_t=e^{t\La}\)</span>. (In the non-finite case, this makes sense as a power series.)</p>
<p><span class="math inline">\(P_t\)</span> is <strong>reversible</strong> if <span class="math inline">\(P_t\)</span> are self-adjoint on <span class="math inline">\(L^2(\mu)\)</span>: <span class="math display">\[\an{f,P_tg}_\mu = \an{P_tf,g}_\mu.\]</span> Reversibility implies <span class="math display">\[P_tf(x) =\E[f(X_t)|X_0=x] = \E[f(X_0)|X_t=x];\]</span> i.e., the Markov process viewed backwards has the same law. <!-- delta functions? --></p>
<p>For finite state space, this is equivalent to <span class="math display">\[\mu_i \La_{ij} = \mu_j \La_{ji},\]</span> <strong>detailed balance</strong>.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<p><strong>Definition</strong>. A Markov semigroup is <strong>ergodic</strong> if <span class="math inline">\(P_tf \to \mu f\)</span> in <span class="math inline">\(L^2(\mu)\)</span> as <span class="math inline">\(t\to \iy\)</span> for every <span class="math inline">\(f\in L^2(\mu)\)</span>.</p>
<blockquote>
<p>A measure <span class="math inline">\(\mu\)</span> satisfies a Poincare inequality for a certain notion of “gradient” if and only if an ergodic Markov semigroup associated to this “gradient” converges exponentially fast to <span class="math inline">\(\mu\)</span>.</p>
</blockquote>
<p>The <strong>Dirichlet form</strong> is <span class="math display">\[\cal E(f,g) = -\an{f,\cL g}_\mu.\]</span> Note: for complex-valued functions, we take the real part.</p>
<p><strong>Theorem</strong> (Poincare inequality). Let <span class="math inline">\(P_t\)</span> be reversible ergodic Markov semigroup with stationary measure <span class="math inline">\(\mu\)</span>. For <span class="math inline">\(c\ge 0\)</span>, TFAE:</p>
<ol type="1">
<li>(Poincare inequality) <span class="math inline">\(\Var_\mu(f) \le c\cal E(f,f)\)</span></li>
<li><span class="math inline">\(\forall f, t, \ve{P_t f- \mu f}_{L^2(\mu)} \le e^{-\fc tc}\ve{f-\mu f}_{L^2(\mu)}\)</span></li>
<li><span class="math inline">\(\forall f, t, \cal E(P_t f, P_t f) \le e^{-2t/c} \cal E(f,f)\)</span></li>
<li><span class="math inline">\(\forall f \exists \ka(f), \ve{P_t f-\mu f}_{L^2(\mu)} \le \ka(f) e^{-\fc tc}\)</span>.</li>
<li><span class="math inline">\(\forall f \exists \ka(f), \cal E(P_tf,P_tf)\le \ka(f)e^{-2t/c}\)</span>.</li>
</ol>
<p>Note <span class="math inline">\(5\Leftarrow 3\implies 1\Leftrightarrow 2\Rightarrow 4\)</span> doesn’t require reversibility.</p>
<p>Example: Gaussian distribution</p>
<ol type="1">
<li>Define the <strong>Ornstein-Uhlenbeck process</strong> by <span class="math display">\[X_t = e^{-t} X_0 + e^{-t} W_{e^{2t}-1}, \quad X_0\perp W\]</span> where <span class="math inline">\(W_t\)</span> is standard Brownian motion. Note <span class="math inline">\(N(0,1)\)</span> is stationary.</li>
<li>Using Gaussian integration by parts <span class="math inline">\(\E_{N(0,1)} [\xi f(\xi)] = \E_{N(0,1)} [f'(\xi)]\)</span>, show that
<ul>
<li><span class="math inline">\(X_t\)</span> is a Markov process with semigroup <span class="math inline">\(\E[f(e^{-t} x + \sqrt{1-e^{-2t}}\xi)]\)</span>, <span class="math inline">\(\xi\in N(0,1)\)</span>.</li>
<li>The generator is <span class="math inline">\(\cL f(x) = -xf'+f''\)</span>.</li>
<li><span class="math inline">\(\cE (f,g) = \an{f',g'}_\mu\)</span>.</li>
<li>In particular, <span class="math inline">\(\cE(f,f) = \ve{f'}^2_{L^2(\mu)} = \E[f'(\xi)^2]\)</span> is exctly the expected square gradient.</li>
</ul></li>
<li>From the expression for <span class="math inline">\(P_t\)</span> obtain <span class="math inline">\(\ddd{x} P_t f(x) = e^{-t} P_t f'(x)\)</span>. Then <span class="math inline">\(\cE (P_tf,P_tf) \le e^{-2t} \cE(f,f)\)</span>. Hence for <span class="math inline">\(\mu=N(0,1)\)</span>, <span class="math display">\[\Var_\mu(f) \le \ve{f'}_{L^2(\mu)}.\]</span></li>
<li>By tensorization, <span class="math display">\[\Var_\mu(f) \le \E[\ve{\nb f(X_1,\ldots, X_n)}^2].\]</span></li>
</ol>
<p>Note: The O-U process is the solution of the stochastic differential equation <span class="math display">\[dX_t = -X_t \,dt + \sqrt2 \, dB_t.\]</span> Revisit this after I learn stochastic calculus.</p>
<p>Tensorization using Poincare inequality:</p>
<ol type="1">
<li>Construct a random process <span class="math inline">\(X_t=(X_t^1,\ldots, X_t^n)\)</span> by having coordinates re-randomize according to independent Poisson processes.</li>
<li>Then <span class="math display">\[P_tf(x) = \sum_{I\subeq [n]} (1-e^{-t})^{|I|} e^{-t(n-|I|)} \int f(x_1,\ldots, x_n) \prod_{i\in I}\mu_i(dx_i)+o(t).\]</span> (Note the integral is only over the indices in <span class="math inline">\(I\)</span>.) Only the <span class="math inline">\(|I|=1\)</span> terms matter in the limit (makes sense, we’re taking the derivative!),
\begin{align}
\cL f &amp;= -\sumo in \de_i f\\
\de_if(x)&amp;:= f(x) - \int f(x_1,\ldots, x_{i-1}, z, x_{i+1},\ldots, x_n)\,\mu_i(dz).
\end{align}</li>
<li><span class="math inline">\(\int h\de_i g\,d\mu=0\)</span> if <span class="math inline">\(h\)</span> does not depend on <span class="math inline">\(x_i\)</span>. Thus <span class="math inline">\(\cE(f,g) = \sumo in \int \de_i f\de_i g\,d\mu\)</span>. This is symmetric, so the process is reversible.</li>
<li>We have <span class="math inline">\(\cE(f,f) = \sumo in \int \Var_if \,d\mu\)</span>, so the tensorization inequality is exactly <span class="math inline">\(\Var_\mu(f) \le \cE(f,f)\)</span>.</li>
<li>Conclude ergodicity. Conversely, we can prove the tensorization inequality from ergodicity: Note <span class="math display">\[\de_i P_t f=e^{-t} \sum_{I\nin i} (1-e^{-t})^{|I|} e^{-t(n-1-|I|)} \int \de_i f\prod_{i\in I}\mu_i(dx_i)\]</span> so <span class="math inline">\(\cE(P_tf,P_tf) \le \ka(f) e^{-2t}\)</span>.</li>
</ol>
<h3 id="variance-identities-and-exponential-ergodicity">2.4 Variance identities and exponential ergodicity</h3>
<p>We prove the Poincare inequality.</p>
<ol type="1">
<li><strong>Lemma</strong>. <span class="math display">\[\ddd t \Var_\mu(P_t f) = -2\cal E(P_tf,P_tf)\]</span>. <em>Proof</em>. Use <span class="math inline">\(\mu(P_tf)=\mu(f)\)</span>. Both are equal to <span class="math inline">\(\mu(2P_t f\ddd tP_tf)\)</span>.</li>
<li><strong>Corollary</strong>. <span class="math inline">\(\cE(f,f)\ge 0\)</span>.</li>
<li>Integral representation of variance: If the Markov semigroup is ergodic, integrating gives <span class="math inline">\(\Var_\mu (f) = 2\iiy \cE(P_tf,P_tf)\,dt\)</span>.</li>
<li>(<span class="math inline">\(3\implies1\)</span>) Use the integral representation.</li>
<li>(<span class="math inline">\(1\implies2\)</span>) Using <span class="math inline">\(\cal E\propto -\ddd t \Var\)</span>, get a differential inequality that gives exponential decay.</li>
<li>(<span class="math inline">\(2\implies1\)</span>) Write <span class="math inline">\(\cE\)</span> as a limit and apply the inequality to <span class="math inline">\(\Var\)</span>.</li>
<li>If <span class="math inline">\(P_t\)</span> is reversible, then <span class="math inline">\(t\mapsto \log\ve{P_t f}_{L^2(\mu)}^2\)</span>, <span class="math inline">\(\log \cE(P_tf,P_tf)\)</span> are convex. Proof. First derivative is <span class="math inline">\(-\fc{2\an{\cL P_tf, f}}{\ve{P_tf}^2}\)</span>. Differentiate again, use CS.</li>
<li>(<span class="math inline">\(2\implies3\)</span>) The first derivative is increasing. Rearrange to get <span class="math display">\[\fc{\cE(P_tf,P_tf)}{\cE(f,f)}\le \fc{\ve{P_tf}_{L^2(\mu)}^2}{\ve{f}_{L^2(\mu)}^2}\]</span>.</li>
<li>(<span class="math inline">\(4\implies2\)</span>, <span class="math inline">\(5\implies3\)</span>) Use the lemma: if <span class="math inline">\(g\)</span> is convex and <span class="math inline">\(g(t)\le K-\al t\)</span> for <span class="math inline">\(t\ge 0\)</span> then <span class="math inline">\(g(t)\le g(0)-\al t\)</span> for <span class="math inline">\(t\ge 0\)</span>.</li>
</ol>
Intuition: If reversibility holds,
\begin{align}
\cE(f,g) &amp;=\rc2 \sum \mu_i \La_{ij} (f_i-f_j)(g_i-g_j)\\
\cE(f,f) &amp;=\rc2 \sum \mu_i \La_{ij} (f_i-f_j)^2.
\end{align}
<p><a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>In finite dimensions, if <span class="math inline">\(\mu f=0\)</span>, <span class="math display">\[
\cE(f,f) \ge (\ub{\la_1}0-\la_2) \Var_\mu(f).
\]</span> The best constant in the Poincare inequality is the spectral gap. The spectral gap controls the exponential convergence rate. Note it’s essential that <span class="math inline">\(\La\)</span> admits a real spectral decomposition.</p>
<h3 id="problems">Problems</h3>
<ol type="1">
<li>Use <span class="math inline">\(\Var\pa{\ve{\rc n \sumo kn X_k}_B} = \sup_{y\in B^*} \an{\rc n\sumo kn X_k, y}\)</span>. Use the corollary, get <span class="math inline">\(D_k^-\le 2 \sup_{y\in B^*}\an{\rc X_k,y} \le \fc{2C}{n}\)</span>. Now square and sum.</li>
<li>.</li>
<li>.</li>
<li>.</li>
<li>We have <span class="math inline">\((f(x) - f(..., a, ...))^2\le \ve{(b-a)\nb f}^2\)</span>. Now take expectations and sum over different coordinates.</li>
<li>.</li>
<li></li>
<li><ol type="1">
<li><p>Smooth <span class="math inline">\(f\)</span> and use the Gaussian Poincare inequality.</p>
Note we have <span class="math inline">\(\Var[f(x)]\le \E[(f(x)-f(0))^2]\le \E L^2x^2 = L^2\)</span> but this doesn’t help us, because if we sum up derivatives along different coordinates, we overestimate <span class="math inline">\(L\)</span> to <span class="math inline">\(Ln\)</span> instead.</li>
<li>Note <span class="math inline">\((\Si^{\rc Y})_i\)</span> is <span class="math inline">\(\ve{(\Si^{\rc 2})_i}\)</span>-Lipschitz, so <span class="math inline">\(\max((\Si^{\rc 2}Y)_i)\)</span> is <span class="math inline">\(\max \ve{(\Si^{\rc 2})_i}\)</span>-Lipschitz. By (a), <span class="math display">\[ \Var[\max_i X_i] \le \max\ve{(\Si^{\rc 2})_i} = \max \ve{\Si_{ii}} =\max_i \Var(X_i).\]</span></li>
<li>The sum of variances on the LHS is <span class="math inline">\(\sum_j \sum_i F_{x_j}^2 = \ve{\nb F}^2\)</span>. Use the central limit theorem to show that the LHS var approaches the RHS var. There’s a factor of <span class="math inline">\((\max -\min)^2/4=1\)</span>.</li>
</ol></li>
<li></li>
<li><ol type="1">
<li>Let <span class="math inline">\(p_I\)</span> denote the probability of seeing sequence <span class="math inline">\(I\)</span> of jumps in <span class="math inline">\([0,t]\)</span>. (We don’t need to calculate it.) Let <span class="math inline">\(\Pj(x|I)\)</span> be the probability of <span class="math inline">\(x\)</span> after observing jumps in <span class="math inline">\(I\)</span>. We have <span class="math display">\[\E [f(Z_t)] = \sum_I p_I \int \Pj(x|I) f(x)\dx\]</span> which can be written <span class="math inline">\(P_tf\)</span>. Note this converges. (<span class="math inline">\(\sum_I p_I = 1\)</span>.)</li>
<li><p>Only the <span class="math inline">\(|I|=\phi, 1\)</span> terms are significant as <span class="math inline">\(\sum_{|I|=k} p_I = Poisson(n, t, k)\)</span>.</p>
\begin{align}\cL f &amp;= \lim_{t\to 0^+} \pf{e^{-tn} \E f - \E f + \sum (1-e^{-t}) e^{-t (n-1)} \int f\, \mu_i (dx_i|x)}{t}\\
&amp;= \sum_{i=1}^n \ub{\pa{\pa{\int f(x)\mu_i(dx_i|x)} - f(x)}}{=: - \de_if}\\
\cE (f, g) &amp;= -\int f \cL g\,d\mu\\
&amp;= -\int \pa{-\sumo in f\de_i g}\,d\mu\\
&amp;= \sumo in \int \de_if\de_ig\,d\mu
\end{align}
where we used <span class="math inline">\(\int (f-\de_if)\de_ig=0\)</span> because the first term has mean 0 and <span class="math inline">\(\de_ig\)</span> doesn’t depend on <span class="math inline">\(x_i\)</span>.</li>
<li>\begin{align}
\De_i f &amp;= \max_x |f(..., 1_i,...) - f(..., -1_i, ...)|\\
\De_j f \,d\mu_i &amp; = \max_x \ab{\int f(\ldots 1_j\ldots)\,d\mu_i - \int f(\ldots -1_j\ldots)}\\
&amp;\max_x |\Pj(x_i=1|x_{-i, j\leftarrow 1}) f(1_j1_i) - \Pj(x_i=1|x_{-i, j\leftarrow-1}) f(-1_j1_i) + \Pj(x=-1|\cdots)\cdots
\end{align}
The probabilities of <span class="math inline">\(=1|1\)</span> and <span class="math inline">\(=1|-1\)</span> differ by <span class="math inline">\(C_{ij}\)</span> (n.b. typo) so we get
\begin{align}
&amp;\le \max_x |f(x,1_j) - f(x,-1_j)| + C_{ij} \De_i f = \De_j f + \De_i fC_{ij}.
\end{align}</li>
<li>\begin{align}
\De_j\pa{f+\fc tn \cL f} &amp;=
\De_jf + \fc tn \De_j \pa{\pa{\int f(x) \,d\mu_i (dx_i|x)} - f(x)} \\
&amp;\le \pa{1-\fc tn}\De_j f + \fc tn \sumo in \De_i fC_{ij}\\
\De(f+t\cL f/n) &amp;\le \De f(I-t(I-C)/n).
\end{align}</li>
<li>Iterate <span class="math inline">\(n\)</span> times and take <span class="math inline">\(n\to \iy\)</span> to get <span class="math display">\[\De (e^{t\cL}f) = \De P_t f \le \De f e^{-t(I-C)}.\]</span></li>
<li>\begin{align}
\cE(f, f)&amp;=\sumo in \int (f-\int f\,d\mu_i)^2\,d\mu\\
&amp;\le \sumo in |\De_i f|^2\\
\cE(P_tf,P_tf) &amp;\le \sumo in \ve{\De f e^{-2t(I-C)}}^2\\
&amp;\le \ka(f) (\la_{\min}(I-C))^{-1} \\
&amp; = \ka(f) (1-\la_{\max}(C))^{-1}
\end{align}
<p>Use <span class="math inline">\(5\implies 1\)</span> of Poincare.</p></li>
</ol></li>
</ol>
<h2 id="subgaussian-concentration-and-log-sobolev-inequalities">3 Subgaussian concentration and log-Sobolev inequalities</h2>
<strong>Lemma 3.1</strong> (Cheroff bound). Define the <strong>log-moment generating function</strong>
\begin{align}
\psi(\la) :&amp;= \log \E[e^{\la (X-\E X)}]\\
\psi^*(\la)&amp;=\sup_{\la \ge 0} (\la t-\psi(\la)).	
\end{align}
<p>Then <span class="math inline">\(\Pj(X-\E X \ge t) \le e^{-\psi^*(t)}\)</span> for all <span class="math inline">\(t\ge 0\)</span>.</p>
<p><em>Proof</em>. Exponentiate and Markov.</p>
<p>The log-moment generating function is continuous and can be investigated using calculus.</p>
<p><strong>Example</strong>. Gaussian: <span class="math inline">\(\psi(\la) = \fc{\la^2\si^2}2\)</span> and <span class="math inline">\(\psi^*(t) = \fc{t^2}{2\si^2}\)</span> so bound of <span class="math inline">\(e^{-\fc{t^2}{2\si^2}}\)</span>.</p>
<p>A rv is <span class="math inline">\(\si^2\)</span>-<strong>subgaussian</strong> if <span class="math inline">\(\psi(\la)\le \fc{\la^2\si^2}2\)</span>. Then we get tail bounds of <span class="math inline">\(e^{-\fc{t^2}{2\si^2}}\)</span>.</p>
<p><strong>Lemma 3.6</strong> (Hoeffding): If <span class="math inline">\(X\in [a,b]\)</span> a.s., then <span class="math inline">\(X\)</span> is <span class="math inline">\((b-a)^2/4\)</span> subgaussian.</p>
<p><em>Proof</em>. Interpret <span class="math inline">\(\psi''\)</span> as a variance, get <span class="math inline">\(\psi''\le \fc{(b-a)^2}{4}\)</span>, integrate twice.</p>
<h3 id="the-martingale-method">3.2 The martingale method</h3>
<p>We want to show <span class="math inline">\(f\)</span> is subgaussian with variance proxy controlled by a “square gradient” of <span class="math inline">\(f\)</span>.</p>
<p>The subgaussian property does not tensorize.</p>
<p>The proof of subgaussian inequailties can be reduced to a strengthened form of Poincare inequalities, <strong>log-Sobolev</strong> inequalities, that do tensorize.</p>
<p><strong>Lemma</strong> (Azuma): Let <span class="math inline">\(\cF_k\)</span> be a filtration, and 1. (Martingale difference) <span class="math inline">\(\De_k\)</span> is <span class="math inline">\(\cF_k\)</span>-measurable, <span class="math inline">\(\E[\De_k |\cF_{k-1}]=0\)</span>. 2. (Conditional subgaussian) <span class="math inline">\(\E[e^{\la \De_k}|\cF_{k-1}]\le e^{\la^2\si_k^2/2}\)</span>. Then <span class="math inline">\(\sumo kn \De_k\)</span> is subgaussian with variance proxy <span class="math inline">\(\sumo kn \si_k^2\)</span>.</p>
<p><strong>Corollary</strong> (Azuma-Hoeffding): Replace (2) by <span class="math inline">\(A_k\le \De_k\le B_k\)</span> where <span class="math inline">\(A_k,B_k\)</span> are <span class="math inline">\(\cF_{k-1}\)</span>-measurable. The variance proxy is <span class="math inline">\(\rc 4 \sumo kn \ve{B_k-A_k}^2_{\iy}\)</span>. The tail bound is <span class="math inline">\(\exp\pa{-\fc{2t^2}{\sumo kn \ve{B_k-A_k}^2_{\iy}}}\)</span>.</p>
<p><strong>Theorem 3.11</strong> (McDiarmid): For <span class="math inline">\(X_{1:n}\)</span> independent, <span class="math inline">\(f(X)\)</span> is subgaussian with variance proxy <span class="math inline">\(\rc 4\sumo kn \ve{D_kf}_{\iy}^2\)</span> where <span class="math display">\[D_if(x) = (\sup_z-\inf_z)f(x_{1:i-1},z,x_{i+1:n}).\]</span></p>
<p><em>Proof</em>. Use Azuma-Hoeffding on martingale differences <span class="math inline">\(\De_k =\E[f|X_{1:k}] - \E[f|X_{1:k-1}]\)</span>.</p>
<p>This is unsatisfactory because the variance proxy is controlled by a uniform upper bound on square gradient rather than its expectation. Something like <span class="math inline">\(\ve{\sumo kn |D_kf|^2}_{\iy}\)</span> would be better.</p>
<h3 id="the-entropy-method">3.3 The entropy method</h3>
<p>The subgaussian property is equivalent to <span class="math inline">\(\la^{-1}\psi(\la)\precsim \la\)</span>, so it suffices to show <span class="math inline">\(\ddd{\la}(\la^{-1}\psi)\precsim 1\)</span>.</p>
<ul>
<li>Define <span class="math inline">\(\Ent(Z) = \E[Z\ln Z] - (\E Z)(\ln \E Z)\)</span>.</li>
<li>(Entropic formulation of subgaussianity) <span class="math inline">\(\forall \la \ge 0, \Ent(e^{\la X}) \le \fc{\la^2\si^2}{2} \E e^{\la X}\implies \forall \la \ge 0, \psi(\la) \le \fc{\la^2\si^2}2\)</span>.
<ul>
<li><em>Proof</em>. Integrate <span class="math inline">\(\ddd{\la} \fc{\psi(\la)}{\la} = \rc{\la^2} \fc{\Ent(e^{\la x})}{\E (e^{\la x})}\)</span>.</li>
</ul></li>
<li>Variational characterization of entropy: <span class="math inline">\(\Ent(Z) = \sup\set{\E(ZX)}{\E(e^X)=1}\)</span>.
<ul>
<li><em>Proof</em>.
\begin{align}
\Ent(Z) - \E[ZX] &amp;= \Ent_Q (e^{-X}Z)\ge0
\end{align}
with equality when <span class="math inline">\(X=\ln\pf{Z}{\E Z}\)</span>.</li>
</ul></li>
<li>Tensorization: <span class="math inline">\(\Ent(f) \le \E\ba{\sumo in \Ent_i f}\)</span>.
<ul>
<li><em>Proof</em>. Let <span class="math inline">\(Z=f(X)\)</span>.
\begin{align}
U_k :&amp;= \ln \E[Z|X_{1:k}] - \ln \E[Z|X_{1:k-1}]\\
\Ent (Z) &amp;= \sum \E[ZU_k]\\
\E[e^{U_k}|X_{-k}]&amp;=1\implies &amp; \E[ZU_k|X_{-k}]&amp;\le \Ent_k f.
\end{align}</li>
</ul></li>
</ul>
<p>The entropic formulation of subgaussianity and the tensorization inequality tell us that if we prove (for some notion of <span class="math inline">\(\nb\)</span>) <span class="math display">\[ \Ent(e^g) \precsim \E[\ve{\nb g}^2]\]</span> in one dimension, then in any number of dimensions, <span class="math display">\[ \Ent(e^{\la f})\precsim \E[\ve{\nb (\la f)}^2e^{\la f}]\]</span> so <span class="math inline">\(f\)</span> is subgaussian with <span class="math inline">\(\max\ve{\nb f}^2\)</span>.</p>
<ul>
<li>Discrete log-Sobolev: Let <span class="math inline">\(D^-f=f-\inf f\)</span>. Then <span class="math display">\[\Ent[e^f] \le \Cov[f,e^f] \le \E[|D^-f|^2e^f].\]</span>
<ul>
<li><em>Proof</em>. Jensen and convexity.</li>
</ul></li>
<li>On product measure, <span class="math inline">\(f\)</span> is subgaussian with variance proxy <span class="math inline">\(2\ve{\sumo in |D_if|^2}_{\iy}\)</span>. Upper and lower tail bounds with <span class="math inline">\(D_i^-\)</span> and <span class="math inline">\(D_i^+\)</span>.</li>
</ul>
<p><strong>Example</strong> (Random Bernoulli symmetric matrices). Using <span class="math inline">\(D_{ij}^-\la_{\max(M)}\)</span>, get <span class="math display">\[ \Pj(\la_{\max}(M) - \E\la_{\max}(M)\ge t)\le e^{-\fc{t^2}{64}}. \]</span> We can’t use the same technique to look at the lower tail because the bound is in terms of different <span class="math inline">\(M^{(ij)}\)</span>’s.</p>
<h3 id="log-sobolev-inequalities">3.4 Log-Sobolev inequalities</h3>
<p>We have an entropic analogue of just the easy parts of the Poincare inequality equivalence.</p>
<p><strong>Theorem</strong>. 1 and 2 are equivalent. 3 implies 1, 2 if <span class="math inline">\(\Ent_\mu(P_tf)\to 0\)</span> (entropic ergodicity).</p>
<ol type="1">
<li><span class="math inline">\(\Ent_\mu(f)\le c\cE(\ln f, f)\)</span> (log-Sobolev inequality)</li>
<li><span class="math inline">\(\Ent_\mu(P_tf) \le e^{-t/c} \Ent_\mu(f)\)</span> (entropic exponential ergodicity)</li>
<li><span class="math inline">\(\cE(\ln P_tf , P_tf) \le e^{-t/c}\cE(\ln f, f)\)</span>.</li>
</ol>
<p><em>Proof</em>.</p>
<ul>
<li>(<span class="math inline">\(3\implies1\)</span>) Note <span class="math inline">\(\ddd{t} \Ent_\mu(P_tf) = -\cE(\ln P_tf,P_tf)\)</span> using <span class="math inline">\(\mu(\cL P_tf)=0\)</span>. <span class="math inline">\(\Ent_\mu(f) = \lim -\iiy \ddd{\mu} \Ent_\mu(P_tf)\)</span>.</li>
<li>(<span class="math inline">\(1\implies2\)</span>) Inequality for exponential decay</li>
<li>(<span class="math inline">\(2\implies1\)</span>) Take the limit.</li>
</ul>
<strong>Example</strong> (Discrete log-Sobolev inequality). Consider Poisson resampling under <span class="math inline">\(\mu\)</span>. Then
\begin{align}
P_t f&amp;= e^{-t}f + (1-e^{-t}) \mu(f) \\
\cE(f,g)&amp;=\int \de f\de g\,d\mu = \Cov_\mu[f,g]\\
P_tf \ln (P_tf)&amp;\le e^{-t}\ln f + (1-e^{-t}) \mu f\ln \mu f\\
\implies \Ent_\mu[P_t f] &amp;\le e^{-t} \Ent_\mu(f)\\
\implies \Ent_\mu(f) &amp;\le \Cov_\mu(\ln f, f) &amp;(2\implies 1)
\end{align}
<p>The log-Sobolev equivalences cannot reproduce the tensorization inequality for entropy.</p>
<strong>Theorem</strong> (Gaussian log-Sobolev). For independent Gaussian variables,
\begin{align} 
\Ent[f] &amp;\le \rc 2 \E [\nb f \cdot \nb \ln f]&amp; (f\ge 0)\\
\Ent[e^f] &amp; \le \rc 2 \E[\ve{\nb f}^2 e^f].
\end{align}
<p>As a result <span class="math inline">\(f\)</span> is <span class="math inline">\(\si^2 = \ve{\ve{\nb f}^2}_{\iy}\)</span> subgaussian and we get Gaussian concentration, <span class="math display">\[\Pj[f - \E f\ge t] \le e^{-t^2/2\si^2}.\]</span></p>
<em>Proof</em>. Recall <span class="math inline">\(\cE(f,g) = \mu(f'g')\)</span>, <span class="math inline">\((P_tf)' = e^{-t}P_t f'\)</span>. Note <span class="math inline">\(|P_t(fg)|^2 \le P_t(f^2)P_t(g^2)\)</span> by CS (expand out).
\begin{align}
(\ln P_t f)' (P_tf)' &amp;= e^{-2t} \fc{|P_tf|^2}{P_tf}\\
|P_t f'|^2 &amp;\le P_t((\ln f)'f') P_t f&amp;\text{by CS}\\
\implies \cE(\ln (P_tf), P_tf) &amp;\le e^{-2t}\cE(\ln f, f) &amp;\text{by }\int\\
\implies \Ent_\mu(f) &amp;\le \cE(\ln f, f)&amp;(3\implies 1).
\end{align}
Note several different forms of log-Sobolev, equivalent in the Gaussian case (or anytime the chain rule holds for <span class="math inline">\(\cE\)</span>:
\begin{align}
\Ent(f) &amp;\le \rc 2 \E[\nb f \cdot \nb \ln f] = \rc2 \cE (\ln f, f)\\
\Ent(f) &amp;\le \rc 2 \E\pf{\ve{\nb f}^2}{f}\\
\Ent(e^f) &amp;\le \rc 2 \E[\ve{\nb f}^2 e^f]\\
\Ent(f^2) &amp;\le 2\E[\ve{\nb f}^2] = 2\cE(f,f)\\
\E(f^2\ln f)-\E[f^2]\ln \ve{f}_2 &amp;\le c\ve{\nb f}_2^2.
\end{align}
<p>Classical Sobolev inequalities are for <span class="math inline">\(\ved_q\)</span>, <span class="math inline">\(q\ge 2\)</span> and do not tensorize.</p>
<p><strong>Lemma 3.28</strong>: Log-Sobolev <span class="math inline">\(\Ent(f) \le c\cE (\ln f, f)\)</span> implies the Poincare inequality <span class="math inline">\(\Var(f) \le 2c\cE (f,f)\)</span>.</p>
<p><em>Proof</em>. <span class="math display">\[
\ub{\E[\la f e^{\la f}]}{\la^2\cE(f,f) + o(\la^2)} - 
\ub{\E[e^{\la f}] \ln \E[e^{\la f}]}{\la \E f + \la^2(\E[f^2] + \E[f]^2)/2 + o(\la^2)} = 
\ub{\E[\la f e^{\la f}]}{\la \E f + \la^2 \E [f^2] + o(\la^2)}
\]</span></p>
<h3 id="problems-1">Problems</h3>
<p>Equivalent conditions for subgaussianity:</p>
<ol type="1">
<li>The tails are dominated by the tails of the Gaussian: <span class="math display">\[\Pj(|X|\ge t) \le 2\exp(-t^2/K_1^2).\]</span></li>
<li>Moments: For all <span class="math inline">\(p\ge 1\)</span>, <span class="math display">\[
\ve{X}_p = (\E|X|^p)^{\rc p} \le K_2\sqrt p
\]</span></li>
<li>Moment generating function of <span class="math inline">\(X^2\)</span>: <span class="math display">\[\E\exp(X^2/K_3^2)\le 2.\]</span></li>
<li>(If <span class="math inline">\(\E X=0\)</span>,) MGF of <span class="math inline">\(X\)</span>: <span class="math display">\[\E \exp(\la X) \le \exp(\la^2 K_4^2)\]</span> for all <span class="math inline">\(\la\in \R\)</span>.</li>
</ol>
<p>Problems</p>
<ol type="1">
<li><ol type="1">
<li>Expanding <span class="math inline">\(\E(e^{\la (X-\E X)}) \le e^{\la^2\si^2/2}\)</span> gives <span class="math display">\[ 1+\fc{\la^2}2 (X-\E X)^2 \le 1+ \fc{\la^2}2 \si^2.\]</span></li>
<li>Easy.</li>
<li>(<span class="math inline">\(4\implies1\)</span>) This is Chernoff. <span class="math inline">\(\inf_\la \psi(\la)-\la t \le \inf \pa{\fc{\la^2\si^2}2 - \la t} = -\fc{t^2}{2\si^2}\)</span>.</li>
<li>(<span class="math inline">\(1\implies 3\)</span>)
\begin{align}
\E e^{X^2/6\si^2} &amp;= 1+\iiy \fc{t}{3\si^2} e^{\fc{t^2}{6\si^2}} \Pj(|X|\ge t)\,dt\\
&amp; = 1+\iiy \fc{t}{3\si^2} e^{\fc{t^2}{6\si^2}} e^{-\fc{t^2}{2\si^2}}\,dt=2.
\end{align}</li>
<li>? (<span class="math inline">\(3\implies 4\)</span>) <!--Expanding $\E e^{\la X}\le e^{\fc{\la^2\si^2}2 + \la \E X}$ gives--> Weaker: Expanding <span class="math inline">\(\E e^{X^2/6\si^2}\le 2\)</span> gives <span class="math inline">\(\rc{q!}\pf{X^2}{6\si^2}^q\le 1\)</span>, <span class="math inline">\(\E X^{2q}\le (6\si^2)^qq!\)</span>.</li>
<li><span class="math inline">\(\E(e^{X^2/8\si^2}) = \E\pa{\sumz q\iy \rc{q!} \pf{x^2}{8\si^2}^q} \le \E\sumz q\iy\prc{2}^q=2\)</span>.</li>
</ol></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li>Let <span class="math inline">\(Z=\fc{e^{\la X}}{\E e^{\la X}}\)</span>. Then
\begin{align}
\Ent(e^{\la X}) &amp;\le \E(e^{\la X}) \E(Z\ln Z)\\
&amp;\le \E(e^{\la X}) \ln \pf{\E[(e^{\la X})^2]}{(\E e^{\la X})^2}\\
&amp;\le \E(e^{\la X}) (\fc{\la^2\si^2}2 + \ln \pf{e^{2 \la \E X}}{\E(e^{\la X})^2})\\
&amp;\le \E(e^{\la X}) (\fc{\la^2\si^2}2).
\end{align}</li>
<li><ol type="1">
<li>We have
\begin{align}
\Ent Z &amp;= \inf_{t&gt;0} \E[Z\ln Z - Z\ln t - Z+t]\\
\iff (\E Z)(\E \ln Z+1) &amp;=\inf_{t&gt;0} \E(-Z\ln t+t)
\end{align}
Take the derivative; this is minimized at <span class="math inline">\(t=\rc{\E Z}\)</span>.</li>
<li>\begin{align}
\Ent(e^f) &amp;=\inf_{t&gt;0}\E [e^f (f - \ln t - 1) + t]\\
&amp;\le e^f (f-\inf f + e^{\inf f-f} - 1) &amp; t=e^{\inf f}.
\end{align}</li>
<li><span class="math inline">\(\phi(x) = e^{-x}+x-1 \le 1-x+\fc{x^2}2 + x-1 = \fc{x^2}2\)</span>.</li>
<li><span class="math inline">\(\E[\ph(D^-f) e^f] \le \rc{2}\E[|D^-f|^2 e^f]\)</span>. ?? Stuck.</li>
</ol></li>
</ol>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>The kernel is the same as <span class="math inline">\(\La\)</span> except it also records the probbability of staying. <span class="math inline">\(K-I = \La\)</span>.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p><span class="math inline">\(K^*(x,y) = \fc{K(y,x)}{\pi(x)}\pi(y)\)</span>.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>(cf. Laplacian)<a href="#fnref3">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Types and programming languages, Benjamin Pierce</title>
    <link href="http://holdenlee.github.io/notebook/posts/cs/type_theory/types_and_pl.html" />
    <id>http://holdenlee.github.io/notebook/posts/cs/type_theory/types_and_pl.html</id>
    <published>2016-07-31T00:00:00Z</published>
    <updated>2016-07-31T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Types and programming languages, Benjamin Pierce</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-07-31 
          , Modified: 2016-07-31 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#recursive-types">20 Recursive types</a><ul>
 <li><a href="#formalities">20.2 Formalities</a></li>
 <li><a href="#subtyping">20.3 Subtyping</a></li>
 </ul></li>
 <li><a href="#metatheory-of-recursive-types">21 Metatheory of recursive types</a><ul>
 <li><a href="#subtyping-1">21.3 Subtyping</a></li>
 <li><a href="#regular-trees">21.7 Regular trees</a></li>
 <li><a href="#mu-types">21.8 Mu-types</a></li>
 </ul></li>
 <li><a href="#type-reconstruction">22 Type reconstruction</a><ul>
 <li><a href="#let-polymorphism">22.7 Let-polymorphism</a></li>
 </ul></li>
 <li><a href="#universal-types">Universal types</a><ul>
 <li><a href="#system-f">23.3 System F</a></li>
 <li><a href="#section">23.10</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="recursive-types">20 Recursive types</h2>
<p><span class="math inline">\(\mu\)</span> is a recursion operator for types. A definition <span class="math inline">\(T = \mu X. Y\)</span> means: let <span class="math inline">\(T\)</span> be the infinite type satisfying <span class="math inline">\(X=Y\)</span>.</p>
<pre><code>Hungry = \mu A. Nat -&gt; A
Stream = \mu A. Unit -&gt; {Nat, A}
Process = \mu A. Nat -&gt; {Nat, A}
Counter = \mu C. {get: Nat, inc: Unit -&gt; C}</code></pre>
<p>Note: you can’t define Hungry in Haskell because (Then how does printf work? Something with type classes?)</p>
<p>Recursive types well-types the fixed-point combinator. <span class="math display">\[
fix_T = \la f:T\to T.(\la x:(\mu A. A\to T). f (x x)) (\la x:(\mu A. A\to T). f (x x))
\]</span></p>
<p>Every type is inhabited (<span class="math inline">\(\la\_:(). fix_T (\la x:T.x)\)</span>), so systems with recursive types are useless as logics.</p>
<p>[Embed untyped lambda calculus]</p>
<h3 id="formalities">20.2 Formalities</h3>
<p>There are 2 basic approaches to recursive types. What is the relationship between the type and its one-step unfolding?</p>
<ol type="1">
<li>Equi-recursive: They are definitionally equal.</li>
<li>Iso-recursive: They are different but isomorphic. There are functions <code>unfold</code> and <code>fold</code> going both ways. (Ex. Haskell)</li>
</ol>
<p>Note equi-recursive places more demands on the typechecker.</p>
<h3 id="subtyping">20.3 Subtyping</h3>
<h2 id="metatheory-of-recursive-types">21 Metatheory of recursive types</h2>
<p><strong>Theorem</strong> (Knaster-Tarski): Let <span class="math inline">\(X\)</span> be a poset, <span class="math inline">\(f:X\to X\)</span> be order-preserving. Then there exists a fixed point, <span class="math inline">\(\sup\set{x\in X}{x\le f(x)}\)</span>.</p>
<p>Let <span class="math inline">\(\cal U\)</span> be the universal set. Consider <span class="math inline">\((\cal P(\cal U), \subeq)\)</span>. Say <span class="math inline">\(X\)</span> is <span class="math inline">\(F\)</span>-closed/consistent if <span class="math inline">\(F(X)\subeq/\supeq X\)</span>.</p>
<p><em>Corollary</em>. The intersection/union of all <span class="math inline">\(F\)</span>-closed/consistent is the least/greatest fixed point of <span class="math inline">\(F\)</span>, denoted by <span class="math inline">\(\mu F, \nu F\)</span>.</p>
<p>(Principle of induction/coinduction) If <span class="math inline">\(X\)</span> is <span class="math inline">\(F\)</span>-closed, <span class="math inline">\(\mu F\subeq X\)</span>; if <span class="math inline">\(X\)</span> is <span class="math inline">\(F\)</span>-consistent, <span class="math inline">\(X\subeq \nu F\)</span>.</p>
<p>Finite tree types are given by</p>
<pre><code>T = Top | (T, T) | T -&gt; T </code></pre>
<p>Infinite tree types are like this but the tree can be infinite.</p>
<h3 id="subtyping-1">21.3 Subtyping</h3>
<p>Say <span class="math inline">\(T&lt;:Top\)</span>, <span class="math inline">\(S_1&lt;:T_1, S_2&lt;:T_2 \implies (S_1\times S_2)&lt;:(T_1,T_2)\)</span> and similarly for <span class="math inline">\(\to\)</span>. Take the transitive closure to get the subtyping relation.</p>
<h3 id="regular-trees">21.7 Regular trees</h3>
<p>A tree type is regular if subtrees(T) is finite.</p>
<h3 id="mu-types">21.8 Mu-types</h3>
<pre><code>T = X
	| Top
	| T x T
	| T -&gt; T
	| \mu X. T</code></pre>
<p>“Keep substituting” <span class="math inline">\(\mu X. T\)</span> to get the tree type corresponding to the <span class="math inline">\(\mu\)</span>-type, treeof<span class="math inline">\(([X\mapsto \mu X. T]T)(\pi)\)</span>.</p>
<h2 id="type-reconstruction">22 Type reconstruction</h2>
<p>2 questions:</p>
<ol type="1">
<li>Are all substitution instances of t well typed? <span class="math display">\[\forall \si, (\si \Ga \vdash \si t:T)\]</span> Type variables should be held abstract. This leads to <strong>parametric polymorphism</strong>.</li>
<li>Is some substitution instance of <span class="math inline">\(t\)</span> well typed? <span class="math display">\[\exists \si, (\si \Ga \vdash \si t:T)\]</span> Can <span class="math inline">\(t\)</span> be instantiated to a well-typed term by choosing appropriate values? This leads to type reconstruction/inference.</li>
</ol>
<p>Constraint typing: <span class="math inline">\(\Ga \vdash t:T|_{\cal X} C\)</span> means “term <span class="math inline">\(t\)</span> has type <span class="math inline">\(T\)</span> under assumptions <span class="math inline">\(\Ga\)</span> whenever constraints <span class="math inline">\(C\)</span> are satisfied.” <span class="math inline">\(\cal X\)</span> tracks type variables introduced in each subderivation.</p>
<p>(This is a hybrid between the normal deductive system, and the bottom-up constraint generation system.)</p>
<h3 id="let-polymorphism">22.7 Let-polymorphism</h3>
<p>Not allowed: doubleFun:<span class="math inline">\(\forall a . (\forall f : a\to a) \to a \to a\)</span> defined by</p>
<pre class="hs"><code>let doubleFun = \f x -&gt; f (f x)</code></pre>
<p>Reason: a polytype cannot appear inside <code>-&gt;</code>.</p>
<p>T-LetPoly: <span class="math display">\[
\frac{\Ga \vdash [x\mapsto t_1]t_2:T_2 \quad \Ga \vdash t_1:T_1}{\Ga \vdash \text{let }x=t_1\text{ in }t_2:T_2}.
\]</span> Instead of calculating a type for <span class="math inline">\(t_1\)</span>, it substitutes <span class="math inline">\(t_1\)</span> in the body. I.e., perform a step of evaluation before calculating types.</p>
<p>Problem: If the body contains many occurrences, we have to check once for each occurrence. This can take exponential time. See p. 333-4 for solution. Worst-case is still exponential, but in practice it is essentially linear.</p>
<h2 id="universal-types">Universal types</h2>
<p>We need to abstract out a type from a term and instantiate the abstract term with concrete type annotations.</p>
<ul>
<li>Parametric polymorphism: a single piece of code can be typed generically using variables in place of types, and then instantiated. They behave uniformly.
<ul>
<li>Impredicative/first-class</li>
<li>Let-polymorphism (restricted to top-level let-bindings). Functions cannot take polymorphic values as arguments.</li>
</ul></li>
<li>Ad-hoc polymorphism: Exhibit different behaviors when viewed at different types. Overloading: associate single function symbol with many implementations.</li>
<li>Multi-method dispatch</li>
<li>Intensional polymorphism: restricted computation over types at run time.</li>
<li>Subtype polymorphism</li>
</ul>
<h3 id="system-f">23.3 System F</h3>
<p>Equivalent to polymorphic lambda-calculus a.k.a. 2nd-order lambda calculus because it corresponds to 2nd-order intuitionistic logic, which allows quantification over predicates (types) not just terms.</p>
<p>New terms are</p>
<ul>
<li><span class="math inline">\(\la X.t\)</span> (type abstraction)</li>
<li><span class="math inline">\(t [T]\)</span> (type application)</li>
</ul>
<h3 id="section">23.10</h3>
<p>Impredicative: definition involves thing being defined. <span class="math inline">\(T=\forall X.X\to X\)</span> ranges over all types, including <span class="math inline">\(T\)</span> itself.</p>
<p>Predicative/stratified: range is restricted to monotypes.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Type theory</title>
    <link href="http://holdenlee.github.io/notebook/posts/cs/type_theory/type_theory.html" />
    <id>http://holdenlee.github.io/notebook/posts/cs/type_theory/type_theory.html</id>
    <published>2016-07-31T00:00:00Z</published>
    <updated>2016-07-31T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Type theory</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-07-31 
          , Modified: 2016-07-31 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#hindley-milner-type-system">Hindley-Milner type system</a><ul>
 <li><a href="#ingredients">Ingredients</a></li>
 </ul></li>
 <li><a href="#axioms">Axioms</a></li>
 <li><a href="#algorithm-w">Algorithm W</a></li>
 <li><a href="#bottom-up-algorithm-w">Bottom-up Algorithm W</a></li>
 <li><a href="#lambda-cube">Lambda cube</a></li>
 <li><a href="#scratch">Scratch</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="hindley-milner-type-system">Hindley-Milner type system</h2>
<p>The Hindley-Milner type system is a very nice point in the space of possible type systems because there is a reasonable algorithm to deduce the most general type of a program without type annotations (it is sound and complete).</p>
<p>To understand it, we need to understand</p>
<ul>
<li>the ingredients: what are syntactically valid expressions?</li>
<li>the axioms: the rules that allow you to say what types more complicated expressions are, given the types of the building blocks.</li>
<li>the algorithm: an efficient way to find the most general type (ex. <code>Int -&gt; a -&gt; List a</code>) of an expression, given the types of the building blocks. This algorithm can be proved to capture all possible types for the expression.</li>
</ul>
<p>What rules make sense? <span class="math display">\[ x:a,\quad f:a\to b\vdash f x:b\]</span> And we need some kind of specialization <span class="math display">\[ x : \forall a, F(a) \vdash x : F(a').\]</span> We need lambda expressions too.</p>
<h3 id="ingredients">Ingredients</h3>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">newtype</span> <span class="dt">Var</span> <span class="fu">=</span> <span class="dt">Var</span> <span class="dt">String</span>
<span class="kw">newtype</span> <span class="dt">TVar</span> <span class="fu">=</span> <span class="dt">TVar</span> <span class="dt">String</span>

<span class="kw">data</span> <span class="dt">Expr</span> <span class="fu">=</span> <span class="dt">ExprV</span> <span class="dt">Var</span> <span class="co">-- x</span>
	<span class="fu">|</span> <span class="dt">App</span> <span class="dt">Expr</span> <span class="dt">Expr</span> <span class="co">-- e_1 e_2</span>
	<span class="fu">|</span> <span class="dt">Lambda</span> <span class="dt">Var</span> <span class="dt">Expr</span> <span class="co">-- \lambda x. e</span>
	<span class="fu">|</span> <span class="dt">Let</span> <span class="dt">Var</span> <span class="dt">Expr</span> <span class="dt">Expr</span> <span class="co">-- let x = e_1 in e_2</span>

<span class="kw">data</span> <span class="dt">Mono</span> <span class="fu">=</span> <span class="dt">MonoV</span> <span class="dt">TVar</span> <span class="co">-- a</span>
	<span class="fu">|</span> <span class="dt">TApp</span> <span class="dt">Type</span> [<span class="dt">Mono</span>] <span class="co">-- D t_1 ..., here D is a parametric type with some number of arguments</span>

<span class="kw">data</span> <span class="dt">Poly</span> <span class="fu">=</span> <span class="dt">PolyM</span> <span class="dt">Mono</span> <span class="co">-- t</span>
	<span class="fu">|</span> <span class="dt">Forall</span> <span class="dt">TypeVar</span> <span class="dt">Poly</span> <span class="co">-- \forall a . s</span></code></pre></div>
<p>(Note, “-&gt;” is a special case of TApp: TApp fun [a, b].)</p>
<p>Monotypes can only be one type (ex. <code>Int -&gt; [Int]</code>) while polytypes can be many different types (ex. <code>a -&gt; [a]</code>, forall is implicit here).</p>
<p>We need to make a distinction between monotypes and polytypes because <strong>only monotypes can go in the forall</strong>.</p>
<p>We also need the notion of <strong>free variable</strong>. These are variables that have not been captured by a <span class="math inline">\(\forall\)</span>.</p>
<pre><code>import Data.Set as S

freeM :: Mono -&gt; S.Set TVar
freeM = \case
	MonoV t -&gt; S.singleton t
	TApp _ ts -&gt; S.unions (map freeM ts)

freeP :: Poly -&gt; S.Set TVar
freeP = \case
	PolyM m s -&gt; S.delete m (freeP si)</code></pre>
<p>(Warning: in Haskell all type variables are implicitly bound, so free variables do not appear. See Ex. 1 in wikipedia.)</p>
<p>Next we need the notion of a context, which says what expressions are of what type. For example, it can say what types the variables are; in the inside of <code>let</code> we need to know what the context is to do typing.</p>
<pre><code>data Bindings = Bind Var Poly -- x:s

type Context = S.Set Bindings

freeC :: Context -&gt; S.Set Var
freeC ga = S.unions (map (\case Bind v s -&gt; freeP s) (S.elems ga))
	</code></pre>
<!--Is set a monad?-->
<p>The polymorphic types form a partial order <span class="math inline">\(\si\sqsubseteq \si'\)</span>, <span class="math inline">\(\si\)</span> is more special. Ex. <code>Map Int Int</code><span class="math inline">\(\sqsubseteq\)</span><code>Map Int v</code><span class="math inline">\(\sqsubseteq\)</span><code>Map k v</code>.</p>
<!--this requires a bit more work to code...-->
<!--note: need to add deriving...-->
<h2 id="axioms">Axioms</h2>
\begin{align}
\frac{x:\si\in \Ga}{\Ga\vdash x:\si}&amp;&amp; \text{[Var]}\\
\frac{\Ga\vdash e_0:\tau \to \tau'\quad
\Ga\vdash e_1:\tau}{\Ga\vdash e_0 \,e_1:\tau'}&amp;&amp; \text{[App]}\\
\frac{\Ga\cup \{ x:\tau\} \vdash e:\tau'}{\Ga \vdash \lambda x.e:\tau \to \tau'} &amp;&amp;\text{[Abs]}\\
\frac{\Ga \vdash  e_0:\si\quad \Ga\cup \{x:\si\}\vdash e_1:\tau}{\Ga \vdash \text{let }x=e_0\text{ in }e_1:\tau}&amp;&amp; \text{[Let]}\\
\frac{\Ga \vdash e:\si'\quad \si'\sqsubseteq \si}{\Ga \vdash e:\si}&amp;&amp;\text{[Inst]}\\
\frac{\Ga \vdash e:\si\quad \al\nin \text{free}(\Ga)}{\Ga \vdash e:\forall \al.\si}&amp;&amp;\text{[Gen]}.
\end{align}
<p>Abs is abstraction. Inst is instantiation. Note we add to the context when we go inside a lambda or a let. Gen then Inst together help specialize given information in context.</p>
<p>Subtlety: in <code>let</code>, variables enter in polymorphic form and can be specialized. Contrast <span class="math display">\[
\la f. (f \,\text{true}, f \,0)
\]</span> with <span class="math display">\[
\text{let } f = \la x. x\text{ in } (f\text{ true}, f \, 0).
\]</span> This is why <code>let</code> is NOT just syntactic sugar for <span class="math inline">\((\la x.e_2)\,e_1\)</span>; it genuinely adds expressivity.</p>
<h2 id="algorithm-w">Algorithm W</h2>
<p>Algorithm is simple, but there’s a lot of things you have to define first (ex. substitution, instantiation).</p>
<p>First, define a unification algorithm. It takes expressions (AST’s) <span class="math inline">\(\si,\tau\)</span> and returns a substitution (map) <span class="math inline">\(U\)</span>, such that for any substitution <span class="math inline">\(R\)</span> unifying <span class="math inline">\(\si\)</span> and <span class="math inline">\(\tau\)</span>, <span class="math inline">\(R=SU\)</span>. I.e., it gives the most general unification. (Unify by making more specific.)</p>
<p>Algorithm W: Given a context/type environment <span class="math inline">\(\ol p\)</span> (map from strings to polytypes/schemes), and an expression <span class="math inline">\(e\)</span>, return a substitution and a typing for <span class="math inline">\(e\)</span> and all subexpressions. (We will denote such a typing by <span class="math inline">\(\ol{e}_\si\)</span> where <span class="math inline">\(\si\)</span> is the type for <span class="math inline">\(e\)</span>, and <span class="math inline">\(\ol{\bullet}\)</span> means that all subexpressions have been annotated.) If <span class="math inline">\(e\)</span> is…</p>
<ul>
<li>variable <span class="math inline">\(x\)</span>: Lookup <span class="math inline">\(x\)</span> in the type environment. If it’s not there, ERROR. Let <span class="math inline">\(\tau\)</span> be the type. Substitute generic (bound) variables in <span class="math inline">\(\tau\)</span> by new (free) variables. I.e., <code>({}, instantiate(tau))</code>.</li>
<li>application <span class="math inline">\(d\,e\)</span>:
<ul>
<li>Run <span class="math inline">\(W\)</span> on the function: <span class="math inline">\((R,\ol d_\rh) = W(\ol p, d)\)</span>.</li>
<li>Run <span class="math inline">\(W\)</span> on the argument, where we apply the substitution output by the function, <span class="math inline">\((S, \ol e_\si) = W(R\ol p, e)\)</span>.</li>
<li>We’ve now calculated a type <span class="math inline">\(\rh\)</span> for the function, and a type <span class="math inline">\(\si\)</span> for the argument. Now we need to unify these. (Ex. the function is <span class="math inline">\(a\to a\)</span> and the type is <code>Int</code>.) Let <span class="math inline">\(\be\)</span> be a new variable. Unify <span class="math inline">\(S\rh\)</span> and <span class="math inline">\(\si\to \be\)</span>, <span class="math inline">\(U=U(S\rh, \si\to \be)\)</span>.</li>
<li>Return <span class="math inline">\((USR, U(((S\ol d)\ol e)_\be))\)</span>. (Compose the substitutions in the order that we calculated them.) Explanation:
<ul>
<li>We had a typing for <span class="math inline">\(\ol d\)</span>. We update that by <span class="math inline">\(S\)</span>.</li>
<li>The type for <span class="math inline">\(d\, e\)</span> is <span class="math inline">\(\be\)</span> (found in the previous step).</li>
<li>Apply <span class="math inline">\(U\)</span> to get the type for the whole expression.</li>
</ul></li>
<li>Note: if <span class="math inline">\(x\)</span> came from <span class="math inline">\(\la x:\be\)</span>, then <span class="math inline">\(\be\)</span> is a monotype (possibly with free variables), and no substitution is done. If <span class="math inline">\(x\)</span> came from <code>let</code> then <span class="math inline">\(x\)</span> may have bound variables, so we instantiate new variables.</li>
</ul></li>
<li>abstraction <span class="math inline">\(\la x. \,d\)</span>:
<ul>
<li>Let <span class="math inline">\(\be\)</span> be a new type variable.</li>
<li>Add <span class="math inline">\(x:\be\)</span> to the context, <span class="math inline">\(\ol p \cup \{x:\be\}\)</span>.</li>
<li>Run <span class="math inline">\(W\)</span> with this new context, <span class="math inline">\(W(\ol p \cup \{x:\be\}, d)\)</span>.</li>
<li>Return <span class="math inline">\((R, (\la x_{R\be}.\ol{d}_\rh)_{R\be \to \rh})\)</span>.</li>
</ul></li>
<li><code>let x=d in e</code>. This different similar to <span class="math inline">\(\la\)</span> with application (<span class="math inline">\((\la x . d) \, e\)</span>) because there we would apply the substitution to the function <span class="math inline">\(d\)</span> (<span class="math inline">\(S\rh\)</span>) and attempt to unify, but here we keep the bound variables in <span class="math inline">\(d\)</span>.
<ul>
<li>Run <span class="math inline">\(W\)</span> on <span class="math inline">\(d\)</span>: Let <span class="math inline">\((R,\ol d_\rh)=W(\ol p, d)\)</span>.</li>
<li>Run <span class="math inline">\(W\)</span> on <span class="math inline">\(e\)</span> with <span class="math inline">\(\{x:\rh\}\)</span> added: Let <span class="math inline">\((s,\ol e_\si) = W(R\ol p\cup \{x:\rh\}, e)\)</span>.</li>
<li>Return <span class="math inline">\((SR, (\text{let }x_{S\rh} = S\ol d\text{ in }\ol e)_{\si})\)</span>.</li>
<li>!! Should generalize here: abstract (<span class="math inline">\(\forall\)</span>) over all variables free in <span class="math inline">\(d\)</span> but not free in the environment. Ex. <code>let foo = \y -&gt; x</code> in context <code>x:a</code>. <code>\y -&gt; x : b -&gt; a</code> is not yet generalized. Make it <span class="math inline">\(\forall b: b\to a\)</span>.</li>
</ul></li>
</ul>
<p>Note we don’t really need to keep track of the intermediate typings, just the substitutions.</p>
<p>Subtle point I’m still trying to get clear (ex. 1):</p>
<pre><code>let bar [forall a. forall b. a -&gt; (b -&gt; a)] = \x -&gt;
	let foo [forall b. b -&gt; a] = \y -&gt; x
	in foo
in bar</code></pre>
<p>is the same as</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell">\x <span class="ot">-&gt;</span> (\y <span class="ot">-&gt;</span> x)</code></pre></div>
<p>right?</p>
<h2 id="bottom-up-algorithm-w">Bottom-up Algorithm W</h2>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">data</span> <span class="dt">Constraint</span> <span class="fu">=</span> <span class="dt">EqC</span> <span class="dt">Poly</span> <span class="dt">Poly</span>
	<span class="fu">|</span> <span class="dt">InstM</span> <span class="dt">Poly</span> (<span class="dt">S.Set</span> <span class="dt">Mono</span>) <span class="dt">Poly</span>
	<span class="fu">|</span> <span class="dt">GenericInst</span> <span class="dt">Poly</span> <span class="dt">Poly</span></code></pre></div>
<p>Generate the constraint set as follows. For an expression <span class="math inline">\(e\)</span>, if <span class="math inline">\(e\)</span> is</p>
<ul>
<li>variable <span class="math inline">\(x\)</span>: Get fresh <span class="math inline">\(\be\)</span>, note <span class="math inline">\(x:\be\)</span>.</li>
<li>application <span class="math inline">\(e_1\,e_2\)</span>: Recurse on <span class="math inline">\(e_1\)</span> and <span class="math inline">\(e_2\)</span> (take union of constraints and typings). Let <span class="math inline">\(e_1:\tau_1\)</span>, <span class="math inline">\(e_2:\tau_2\)</span>. Note <span class="math inline">\(e_1\, e_2:\be\)</span>, add <span class="math inline">\(\tau_1\equiv \tau_2\to \be\)</span> to the constraint set.</li>
<li>abstraction <span class="math inline">\(\la x. e\)</span>: Recurse on <span class="math inline">\(e\)</span>, suppose <span class="math inline">\(e:\tau\)</span>. Take all typings of the form <span class="math inline">\(x:\tau'\)</span> and make constraints <span class="math inline">\(\tau'\equiv \be\)</span>. Generate fresh <span class="math inline">\(\be\)</span>. Type <span class="math inline">\(\la x.e : (\be \to \tau)\)</span>.</li>
<li><code>let x=e_1 in e_2</code>: Recurse on <span class="math inline">\(e_1:\tau_1\)</span>, <span class="math inline">\(e_2:\tau_2\)</span>, and type as <span class="math inline">\(\tau_2\)</span>. For all typings of the form <span class="math inline">\(x:\tau'\)</span> generated by <span class="math inline">\(e_2\)</span>, add <span class="math inline">\(\tau'\le_M \tau_1\)</span> to the constraint set.</li>
</ul>
<p>Note that for the <span class="math inline">\(\le_M\)</span> constraint, we need to keep a list of monomorphic variables <span class="math inline">\(M\)</span> (corresponding to free—introduced in lambdas) as we recurse down the tree. (Things in lambdas DO NOT generalize, in <span class="math inline">\(\la x. e\)</span>, <span class="math inline">\(x\)</span> can’t have two different types/interpretations in <span class="math inline">\(e\)</span>. Thus within the lambda expression, <span class="math inline">\(x\)</span> is in the monomorphic set—you can’t do <span class="math inline">\(\forall x\)</span>.)</p>
<p>The bottom-up inference rules are different from the usual inference rules:</p>
<ul>
<li>Usual rules keep the context the same; these change the context.</li>
<li>They translate more directly into an algorithm.</li>
<li>They involve the constraints, not the context.</li>
</ul>
<p>See p. 10 for the algorithm.</p>
<h2 id="lambda-cube">Lambda cube</h2>
<p>https://en.wikipedia.org/wiki/Lambda_cube</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/1/19/Lambda_cube.png"></p>
<p>3 dimensions:</p>
<ul>
<li>Polymorphism (bottom/top)</li>
<li>Type operators/types depending on types (front/back)</li>
<li>Types depending on terms, dependent types (left/right)</li>
</ul>
<p>Front:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">*</th>
<th style="text-align: left;">None</th>
<th style="text-align: left;">Dependent types</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Polymorphism</td>
<td style="text-align: left;">F, <span class="math inline">\(\la2\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\la P2\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">None</td>
<td style="text-align: left;"><span class="math inline">\(\la_{\to}\)</span></td>
<td style="text-align: left;">LF, <span class="math inline">\(\la P\)</span></td>
</tr>
</tbody>
</table>
<p>Back: (types depending on types)</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">*</th>
<th style="text-align: left;">None</th>
<th style="text-align: left;">Dependent types</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Polymorphism</td>
<td style="text-align: left;"><span class="math inline">\(F_\om\)</span>, <span class="math inline">\(\la \om\)</span></td>
<td style="text-align: left;">CIC, <span class="math inline">\(\la P\om\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">None</td>
<td style="text-align: left;"><span class="math inline">\(\la_\om\)</span>, <span class="math inline">\(\la\ul{\om}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\la P \ul{\om}\)</span></td>
</tr>
</tbody>
</table>
<p>Hindley-Milner is a subset of System F (in between <span class="math inline">\(\la_{\to}\)</span> and <span class="math inline">\(F=\la 2\)</span>). Haskell contains system F.</p>
<p>References:</p>
<ul>
<li><span class="math inline">\(F_{&lt;:}\)</span> (F with subtyping): Ch. 26, 28</li>
<li><span class="math inline">\(\la_\om\)</span> (types depending on types): Ch. 29</li>
<li><span class="math inline">\(F_\om\)</span> (F with types depending on types): Ch. 30</li>
<li><span class="math inline">\(F_{&lt;:}^\om\)</span>: CH. 31</li>
</ul>
<h2 id="scratch">Scratch</h2>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>LLVM tutorial</title>
    <link href="http://holdenlee.github.io/notebook/posts/cs/PL/llvm.html" />
    <id>http://holdenlee.github.io/notebook/posts/cs/PL/llvm.html</id>
    <published>2016-07-31T00:00:00Z</published>
    <updated>2016-07-31T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>LLVM tutorial</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-07-31 
          , Modified: 2016-07-31 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p><a href="http://www.stephendiehl.com/llvm/">Tutorial</a></p>
<ul>
<li>Source</li>
<li>lexer</li>
<li>parser</li>
<li>checking</li>
<li>codegen</li>
<li>Output</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>PMI and feature vectors</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/pmi.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/pmi.html</id>
    <published>2016-07-29T00:00:00Z</published>
    <updated>2016-07-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>PMI and feature vectors</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-07-29 
          , Modified: 2016-07-29 
	</p>
      
       <p>Tags: <a href="/tags/nlp.html">nlp</a>, <a href="/tags/word%20embeddings.html">word embeddings</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <ul>
<li><a href="randwalk.html">ALLMR16 Randwalk</a></li>
<li><a href="polysemy.html">ALLMR16 Polysemy</a></li>
</ul>
<p>Reduce the dimensionality of words by finding a low-dimensional vector for each word, such that the inner products approximate the log of the co-occurrence matrix (perhaps in a weighted sense).</p>
<p>Why does this work? I.e., why are the low-dimensional vectors useful for NLP tasks? One “task” is analogies. Word embeddings are useful for analogies if addition naturally corresponds to composing their meanings.</p>
<p>What could be a low-dimensional representation of a word? Its PMI with all possible contexts. (Firth: a word’s sense is captured by the distribution of other words around it.) Assume all these PMI vectors live in a low-dimensional space; why does the log of co-occurrence find these vectors?</p>
<p>Here’s a simplified model. Consider words drawn as follows: pick a random context vector, and then take words <span class="math inline">\(w, w'\)</span> with probability <span class="math inline">\(\rc{Z^2} e^{-\an{v_w,c} - \an{v_w',c}}\)</span>. Integrating this gives <span class="math inline">\(\rc{Z^2} \exp\pf{|v_w+v_w'|^2}{2}\)</span>. Then (I’m not being careful with the factor of <span class="math inline">\(d\)</span>) <span class="math display">\[PMI(w,w') = \lg \fc{\Pj(w,w')}{\Pj(w)\Pj(w')} \approx \rc d \an{v_w,v_w'}+o(1).\]</span></p>
<p>If the context vector drifts slowly enough, this analysis still works.</p>
<p>(?) The right optimization problem is <span class="math display">\[\min\sum_{w_1,w_2} \Pj(w_1,w_2) (PMI(w_1,w_2)-\an{v_{w_1},v_{w_2}}.\]</span></p>
<p>Actually, better is <span class="math display">\[
\min_{\{v_w\}, C} \sum_{w,w'} X_{w,w'}(\ln (X_{w,w'}) - \ve{v_w+v_{w'}}_2^2-C)^2.
\]</span></p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
