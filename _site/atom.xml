<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2016-05-11T00:00:00Z</updated>
    <entry>
    <title>Weekly summary 5-14-16</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-05-14.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-05-14.html</id>
    <published>2016-05-11T00:00:00Z</published>
    <updated>2016-05-11T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 5-14-16</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-05-11 
          , Modified: 2016-05-11 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p>Machine learning</p>
<ul>
<li>Presentation on Language Games
<ul>
<li>TODO: Find some theoretical problems.</li>
</ul></li>
<li>NMF
<ul>
<li>Why can we use the regular pseudoinverse? (CONTINUE)</li>
<li>TODO: Search literature, talk to Andrej, Tengyu for ideas</li>
<li>TODO: Look at Freddy’s code and do some experiments.</li>
</ul></li>
<li>NLP learning
<ul>
<li>Read Mike Collins’s notes</li>
<li>Skimmed [CC14] on NMF.</li>
<li>TODO: Read book on NLP.</li>
</ul></li>
<li><a href="../tcs/machine_learning/matrices/streaming_PCA.html">Streaming PCA, Yuanzhi Li</a>.</li>
<li>TODO: Finish reviewing rest of BV</li>
<li>TODO: Read lots, start keeping track of problems and techniques.</li>
</ul>
<!--After NIPS, find collaborations with other ML students.-->
<p>Coding</p>
<ul>
<li>Understood SoS relaxation approach.</li>
<li>Started reading CSP refutation paper.
<ul>
<li>TODO: Read more of it, in particular, understand how to convert it to SoS language.</li>
</ul></li>
<li>Tried “covering with convex sets” but seems difficult.</li>
<li>TODO: try approach with modulus of convexity.</li>
<li>TODO: understand SDP extension approach.</li>
<li>TODO: other approaches?</li>
</ul>
<p>TCS</p>
<ul>
<li>Log-sobolev inequalities</li>
</ul>
<p>TODO</p>
<ul>
<li>Review statistical physics.</li>
<li>Review metric embeddings/geometric inequalities (MAT529) (<span class="citation" data-cites="Kiran">@Kiran</span>).</li>
<li>Things left to summarize:
<ul>
<li>Convex geometry</li>
<li>Optimization (BV)</li>
<li>Optimization on manifolds (AMS)</li>
</ul></li>
<li>Make a summer reading list. (<span class="citation" data-cites="Arora">@Arora</span>)
<ul>
<li>General TCS (randomized algorithms, approximation algorithms)</li>
<li>Probability (finish)</li>
<li>Advanced probability
<ul>
<li>? Martingales</li>
<li>Concentration inequalities</li>
<li>Stochastic processes</li>
<li>Random matrices</li>
</ul></li>
<li>Amit Singer’s course</li>
<li>Optimization (<span class="citation" data-cites="Hazan">@Hazan</span> for recommendations.)</li>
<li>? ATaAP</li>
<li>Topics
<ul>
<li>Variational inference and MCMC
<ul>
<li>Volume estimation??</li>
</ul></li>
<li>Graphical models</li>
</ul></li>
</ul></li>
</ul>
<p>Other</p>
<ul>
<li>Olivier’s generals. TODO: Ask about CertiCoq.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Towards a better understanding of streaming PCA (Yuanzhi Li)</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/streaming_PCA.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/streaming_PCA.html</id>
    <published>2016-05-10T00:00:00Z</published>
    <updated>2016-05-10T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Towards a better understanding of streaming PCA (Yuanzhi Li)</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-05-10 
          , Modified: 2016-05-10 
	</p>
      
       <p>Tags: <a href="/tags/pca.html">pca</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p>Given data <span class="math inline">\(x_i\in \R^d\)</span> sampled from an unknown distribution with <span class="math inline">\(\E[x_ix_i^T]=X\)</span>, where <span class="math inline">\(x\)</span>’s are independent, compute top <span class="math inline">\(k\)</span> eigenvectors of <span class="math inline">\(X\)</span>.</p>
<p>(For the asymmetric <span class="math inline">\(xy^T\)</span> case, finding the top <span class="math inline">\(k\)</span> is harder.)</p>
<p>Normal SVD is not memory efficient; you need to store a <span class="math inline">\(d\times d\)</span> matrix. Goal: compute <span class="math inline">\(k\)</span>-SVD in space <span class="math inline">\(O(kd)\)</span>.</p>
<p>A popular heuristic is to start with a random initial matrix <span class="math inline">\(U_0\in \R^{d\times k}\)</span>. On receiving <span class="math inline">\(x_i\)</span>, update <span class="math inline">\(U_i=(I+\eta_i x_ix_i^T)U_{i-1}\)</span> and let <span class="math inline">\(U_i=QR(U)\)</span> (orthogonalization). Let <span class="math inline">\(\eta_i\sim \rc{i}\)</span>. (Orthogonalization is not a projection. It’s a retraction onto the manifold.)</p>
<p>This works very well in practice but it’s complicated to analyze from a theoretical point of view.</p>
<p>Let <span class="math inline">\(g=\si_k-\si_{k+1}\)</span>. The bound is <span class="math inline">\(\fc{k}{g^3T}\)</span>. We use the measure <span class="math inline">\(\E\ba{\ve{W^TU}_F^2}\)</span> (projection to last <span class="math inline">\(k\)</span> singular values. <span class="math inline">\(U\)</span> is the output, <span class="math inline">\(W\)</span> has the last <span class="math inline">\(d-k\)</span> singular values).</p>
<p>(Rank 1 is just gradient descent; there is no projection.)</p>
<p>Consider the offline case first: we get <span class="math inline">\(X\)</span> directly instead of <span class="math inline">\(x_ix_i^T\)</span>. The algorithm is analogous to the <strong>subspace power method</strong>.</p>
<ul>
<li>Start with random <span class="math inline">\(U_0\in R^{d\times k}\)</span>.</li>
<li><span class="math inline">\(U=(I+\eta_iX)U_{i-1}\)</span>, <span class="math inline">\(U_i = QR(U)\)</span> decomposition. (This is not linear.)</li>
</ul>
<p>We can do the QR decomposition in the last step. It seems nonlinear, how can it commute? In practice you need to do QR decomposition for numerical accuracy. “To TCS people” you can move QR decomposition to the end.</p>
<p>Analysis: Let <span class="math inline">\(U_i = UR, U_{i+1} = U'R'\)</span>. Then <span class="math inline">\(U_{i+1} = (I+\eta_{i+1}X)(I+\eta_iX) U_{i-1}RR'\)</span>.</p>
<p>(For a regret bound, you can’t do this.)</p>
<p>This is how subspace QR is proved: you push everything to the end. <!-- mult update project at end. --></p>
<p>Let <span class="math inline">\(V\)</span> contain the top <span class="math inline">\(k\)</span> eigenvectors and <span class="math inline">\(W\)</span> the rest. Suppose <span class="math inline">\(v_i=e_i\)</span>. Then for <span class="math inline">\(P_t=\prod (I+\eta_{t-i}X)\)</span>, <span class="math inline">\(P_t=\smatt{\text{large}}00{\text{small}}\)</span>. Multiplying by a random initialization gives <span class="math inline">\((\text{large}\,\text{small})^T\)</span>.</p>
<p>In the online setting, let <span class="math inline">\(P_t=(I+\eta_{t-i}x_{t-i}x_{t-i}^T)\)</span>. The offdiagonal part <span class="math inline">\(W_t^TP_tV\)</span> is small compared to the diagonal part. <!--Ohad strts with something close in Frobenius norm. Once you get in neighborhood, it doesn't wander away. --></p>
<ol type="1">
<li>Show the minimal singular value of <span class="math inline">\(V^TP_tV\)</span> is large. This is not easy to bound. (The maximal one can be bounded by trace norm; exchange <span class="math inline">\(\E\)</span> and <span class="math inline">\(\Tr\)</span>.) <!--larger than singular value: martingale concentration-->
<ol type="1">
<li>Show <span class="math inline">\(\Tr[(V^TP_tV)^{-T}(V^TP_tV)^{-1}]\)</span> is small. This requires a lot of computation. (Use Sherman-Morrison formula for inverse for rank 1 update.) But this is suboptimal; lose an <span class="math inline">\(\eta\)</span> factor. <!--use Sherman-Morrison, formula for inverse for rank 1 update--></li>
<li><p>Attempt 2: show concentration. But it doesn’t work because it’s a product, not a sum. It doesn’t even concentrate in the multiplicative scale. <!--The world is against us, lower bounds are so hard --Sanjeev --></p>
<p>Why? Suppose <span class="math inline">\(k=1\)</span>. Think of <span class="math inline">\(X\)</span> as diagonal matrix with first eigenvale large and rest small. <span class="math inline">\((I+\eta e_1e_1^T)e_1e_1^T\)</span>. There is some chance that <span class="math inline">\(x_t\)</span> has some weight in the first and last entry. Then the <span class="math inline">\((1,d)\)</span> entry of the product is <span class="math inline">\(\rc{4}\eta_t\)</span>. <!-- In k=1, do epoching. This is not memory efficient? --></p>
Unlike the offline setting, <span class="math inline">\(x_t\)</span> is not perfectly aligned with the eigenspace of <span class="math inline">\(X\)</span>; it can grab something from the eigenvector.</li>
<li><p>Try induction. Suppose <span class="math inline">\(P_tP_t^T\succ c_tI\)</span>. Then <span class="math inline">\(\E (P_{t+1}P_{t+1}^T) &gt; c_t(I + \eta_{t+1}X)\)</span>. <span class="math inline">\(\si_{\min}\)</span> is not concave; we don’t have <span class="math inline">\(\E[\si_{\min}(A)]&gt;\si_{\min}[\E(A)]\)</span>. (Counterexample: take <span class="math inline">\(A=e_ie_i^T\)</span> w.p. <span class="math inline">\(\rc d\)</span>.) <!-- after taking expected value sigma min is good --> Min singular value is not a good induction hypothesis. <!--hypnosis--> Summarize <span class="math inline">\(P_t\)</span> with a number. Let <span class="math inline">\(f:\)</span> matrix <span class="math inline">\(\to\)</span> value.</p>
<p>When you try to solve a hard problem, just write down the conditions. –Yin-Tat.</p>
“The ideal girlfriend”: <span class="math inline">\(f(P_t)\)</span>
<ul>
<li>is a lower bound on the minimal singular value of <span class="math inline">\(P_t\)</span>,</li>
<li>takes care of all singular values of <span class="math inline">\(P_t\)</span>,</li>
<li>has diminishing margin (larger singular value have smaller effect)</li>
<li>easy to compute and update The magical <span class="math inline">\(\al\)</span>-min-root barrier: unique solution <span class="math inline">\(\la\)</span> of <span class="math inline">\(\sumo in \rc{\si_i - \la} = \al\)</span>, <span class="math inline">\(\la\le \si_i\)</span>. <!-- monotone decreasing. ZAZ: equivalent to regularizer. barrier view. \la is like normalization parameter. s. entropy for q=1. --> It satisfies:</li>
<li>lower bounds the minimum singular value,</li>
<li>when <span class="math inline">\(\al\to \iy\)</span>, the barrier approaches <span class="math inline">\(\si_{\min}\)</span>.</li>
<li>diminishing influence: larger <span class="math inline">\(\si\)</span> have smaller contribution</li>
<li>monotone: <span class="math inline">\(A\succ B\implies f(A)\succ f(B)\)</span>. <!-- solution of crazy function--> <!-- consider distribution of singular values--> Formula: <span class="math inline">\(\sumo in \rc{\si_i-\la} = -\pdd{\la}\log \det(A-\la I)\)</span>. Matrix-determinant lemma (from Sherman-Morrison): <span class="math display">\[\log \det(A-\la I + uv^T) = \pdd{\la} \log \det(A_\la I) + \fc{\pdd{\la}v^T(A-\la I)^{-1}u}{1+v^T(A-\la I)^{-1}u}\]</span> Key observation: If <span class="math inline">\(A=U\Si U^T\)</span>, then <span class="math inline">\(\pdd{\la}v^T (A-\la I)^{-1} u = \sumo in \fc{(Uuv^TU^T)_{ii}}{(\si-\la)^2}\)</span>. <!-- barrier function increases--> <!-- von Neumann entropy more standard. (Elad and Tengyu uses characteric polynomial)--></li>
</ul>
<p><strong>Rank 1 update lemma</strong>: Let <span class="math inline">\(A\)</span> be a symmetric matrix. Then <span class="math inline">\(\E[f(A+uu^T)]&gt;f(A) + \si_{\min}(\E(uu^T))\)</span>.</p>
Apply this on <span class="math inline">\(P_{t+1}\)</span> vs. <span class="math inline">\(P_t\)</span>. Prove the theorem by martingale concentration. <!-- Matrix doesn't concentrate at all, but min singular value improves. --></li>
</ol></li>
<li><p>Bound <span class="math inline">\(W^TP_tW\)</span> term. Using trace? But <span class="math inline">\(W^TP_tW\)</span> is not small at all. <span class="math inline">\(W^TP_tW\)</span> “mixes” with <span class="math inline">\(V^TP_tV\)</span>; it can’t interlace.</p>
<p>Remove influence of <span class="math inline">\(V^TP_tV\)</span> from <span class="math inline">\(W^TP_tW\)</span>.</p>
<p>But they are not the same dimension.</p>
<p>Use random initialization: whp <span class="math inline">\(W^TP_tW(W^TQ)(V^TP_tQ)^{-1}\)</span> is small. <!-- Easiest to explain. PMI, weighted low rank approx, optimal regret for BCO, tight bound for approx convex opt, max-entropy disribution for calc partition, NMF without separability, SVD without being cool.--></p></li>
</ol>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Percy Liang's papers</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/percy_liang.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/percy_liang.html</id>
    <published>2016-05-04T00:00:00Z</published>
    <updated>2016-05-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Percy Liang's papers</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-05-04 
          , Modified: 2016-05-04 
	</p>
      
       <p>Tags: <a href="/tags/nlp.html">nlp</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#l16-learning-executable-semantic-parsers-for-natural-language-understanding">[L16] Learning Executable Semantic Parsers for Natural Language Understanding</a><ul>
 <li><a href="#framework">Framework</a></li>
 <li><a href="#qs">Qs</a></li>
 </ul></li>
 <li><a href="#scraps">Scraps</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="l16-learning-executable-semantic-parsers-for-natural-language-understanding">[L16] Learning Executable Semantic Parsers for Natural Language Understanding</h2>
<p>Goal: Map natural language into logical forms (which can be executed).</p>
<p>We must use <em>logic</em> and <em>statistics/machine learning</em>. Early systems were very rule-based (hand-crafted); we want to use ML to learn without hand-crated rules.</p>
<p>The field is called <strong>statistical semantic parsing</strong>.</p>
<p>Two ideas:</p>
<ul>
<li>Model theory: expressions are symbols which obtain meaning only from execution with respect to a model.</li>
<li>Compositionality: Denotation of an expression is defined recursively in terms of denotations of subexpressions.</li>
</ul>
<p>Recent developments:</p>
<ul>
<li>Previous work relies on having annotated logical forms. Now works rely on weak supervision (just answers).</li>
<li>Scaling up from a specific domain (e.g. geography) to larger domains (with Freebase, etc.).</li>
</ul>
<p>There are linguistic, statistical (generalization), and computational challenges.</p>
<h3 id="framework">Framework</h3>
<p>Example: What is the largest prime less than 10? becomes <span class="math inline">\(\max(\text{primes}\cap (-\iy,10))\)</span>.</p>
<p>5 components</p>
<ul>
<li>Executor—first-order logic with lambda calculus. Use lambda DCS, which eliminates variables, giving a good interface to NL. “A <code>x</code> that satisfies blah” is <span class="math inline">\(\la x. blah(x)\)</span>—lift functions to truth values.</li>
<li>Grammar</li>
<li>Model (distribution over derivations)—loglinear model</li>
<li>Parser (search for high probability derivations)—agenda-based parsing, uses beam search. (Can’t use DP because we need to add constraint that the execution is correct?)</li>
<li>Learner (estimate parameters/rules)</li>
</ul>
<p>Note the grammar can be coarse, and application-specific.</p>
<h3 id="qs">Qs</h3>
<p>Chart parsing? Builds derivations in fixed order, causing parser to waste resources.</p>
<h2 id="scraps">Scraps</h2>
<p>Look at CCG, it’s interesting! Number less than…</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>NLP</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/nlp.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/nlp.html</id>
    <published>2016-05-04T00:00:00Z</published>
    <updated>2016-05-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>NLP</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-05-04 
          , Modified: 2016-05-04 
	</p>
      
       <p>Tags: <a href="/tags/nlp.html">nlp</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#language-models">Language models</a></li>
 <li><a href="#hmms-and-tagging">HMMs and tagging</a></li>
 <li><a href="#pcfgs">PCFGs</a></li>
 <li><a href="#lexicalized-pcfgs">Lexicalized PCFGs</a></li>
 <li><a href="#ibm">IBM</a></li>
 <li><a href="#section">6</a></li>
 <li><a href="#log-linear-models">7 Log-linear models</a></li>
 <li><a href="#memms-maximum-entropy-markov-models-log-linear-tagging-models">MEMMs (Maximum entropy Markov models = Log-linear tagging models)</a></li>
 <li><a href="#crfs">CRFs</a></li>
 <li><a href="#scraps">Scraps</a></li>
 </ul> </div>

  <div class="blog-main">
    <ul>
<li><a href="http://www.cs.columbia.edu/~mcollins/">Notes by Mike Collins</a>.</li>
<li><a href="http://cs224d.stanford.edu/syllabus.html">Deep learning for NLP</a></li>
</ul>
<h2 id="language-models">Language models</h2>
<p>A language model is a probability distribution over sentences consisting of words from a finite set <span class="math inline">\(\mathcal V\)</span>.</p>
<blockquote>
<p>in speech recognition the language model is combined with an acoustic model that models the pronunciation of different words: one way to think about it is that the acoustic model generates a large number of candidate sentences, together with probabilities; the language model is then used to reorder these possibilities based on how likely they are to be a sentence in the language.</p>
</blockquote>
<p>A <span class="math inline">\(t\)</span>th order Markov model (<span class="math inline">\(n+1\)</span>-grams) assumes the probability distribution of the next word depends only on the previous <span class="math inline">\(t\)</span>. To model variable-length sentences, include STOP as a word. Treat the <span class="math inline">\(0,\ldots,-(t-1)\)</span> words as <span class="math inline">\(*\)</span>.</p>
<p>The most naive estimate is to let <span class="math display">\[wh p_{x_{t+1}|x_1\ldots x_t} = \fc{\#(x_1,\ldots, x_{t+1})}{\#(x_1,\ldots, x_t)}\]</span> but this requires lots of computations and makes many counts 0.</p>
<p>Let <span class="math inline">\(x^{(i)}\)</span> be the test sentences. Letting <span class="math inline">\(l=\rc{M} \sumo im \lg p(x^{(i)})\)</span> (average log probability), the <strong>perplexity</strong> is <span class="math inline">\(2^{-l}\)</span> (geometric mean of probabilities).</p>
<ol type="1">
<li><strong>Linear interpolation</strong> is <span class="math display">\[ \wh p(x_{t+1}|x_1\cdots x_t) = \sumo i{t+1} \la_i p(x_i|x_{[1,i-1]}).\]</span> Find <span class="math inline">\(\amax_{\la} L(\la)\)</span> over the development data (separate from training and test data).</li>
<li><strong>Bucketing</strong> is replacing <span class="math inline">\(\la_{t+1} = \fc{\#(x_1,\ldots, x_{t+1})}{#+\ga}\)</span>, <span class="math inline">\(\la_{t} = (1-\la_{t+1}) \fc{\#(x_1,\ldots, x_t)}{\#(x_1,\ldots, x_{t-1})+\ga}\)</span>,…</li>
<li><strong>Discount counts</strong> by <span class="math inline">\(c^(\mathbf x) = c(\mathbf x) - \be\)</span>, <span class="math inline">\(\be\in [0,1]\)</span>, let <span class="math inline">\(\wh p = \fc{c^*(\cdots)}{c(\cdots)}\)</span>. Spread the missing mass evenly over those that have not appeared.</li>
<li><strong>Linear interpolation with bucketing</strong>: Let <span class="math inline">\(\Pi: \mathcal V^{t} \to [K]\)</span> depending on counts seen in the training data. Now make the smoothing parameters depend on <span class="math inline">\(\Pi(u,v)\)</span>.</li>
</ol>
<p>Note that bucketing and discounting only have 1 parameter.</p>
<h2 id="hmms-and-tagging">HMMs and tagging</h2>
<h2 id="pcfgs">PCFGs</h2>
<p>A CFG is <span class="math inline">\(G=(N,\Si,R,S)\)</span> where</p>
<ul>
<li><span class="math inline">\(N\)</span> has non-terminals</li>
<li><span class="math inline">\(\Si\)</span> has terminals</li>
<li><span class="math inline">\(R\)</span> has rules <span class="math inline">\(X\to Y_1\ldots, Y_n\)</span>.</li>
<li><span class="math inline">\(S\in N\)</span> is te start symbol.</li>
</ul>
<p>A leftmost derivation is one where we choose to replace the leftmost nonterminal each time.</p>
<p>Strings can be ambiguous under a CFG.</p>
<p>Notation:</p>
<ul>
<li><span class="math inline">\(\mathcal T_G\)</span> is the set of possible leftmost derivations (parse trees).</li>
<li>For <span class="math inline">\(t\in \mathcal T_G\)</span>, yield<span class="math inline">\((t)\)</span> is the yield.</li>
<li><span class="math inline">\(\mathcal T_G(s)\)</span> is the set of possible parse trees for <span class="math inline">\(s\)</span>.</li>
<li>A sentence is ambiguous/grammatical if <span class="math inline">\(|\mathcal T_G(s)|&gt;1\)</span> and grammatical if <span class="math inline">\(&gt;0\)</span>.</li>
</ul>
<p>We want * define a probability distribution <span class="math inline">\(p\)</span> on parse trees * learn parameters * find <span class="math inline">\(\amax_{t\in \mathcal T_G(s)}p(t)\)</span> (decoding/parsing)</p>
<p>A <strong>PCFG</strong> has probabilities with <span class="math inline">\(\sum_{X\to \be\in R} a(X\to \be)=1\)</span>. The probability is the product of the probabilities of the derivations.</p>
<p>To learn parameters given <em>parse trees</em>, let <span class="math inline">\(\wh p (\al \to \be) = \fc{\#(\al\to \be)}{\#\al}\)</span>.</p>
<p><strong>Chomsky normal form</strong> has rules in the form <span class="math inline">\(X\to Y_1Y_2\)</span> and <span class="math inline">\(X\to Y\)</span> where <span class="math inline">\(Y_1,Y_2\)</span> are nonterminal and <span class="math inline">\(Y\)</span> is terminal.</p>
<ul>
<li>Parsing: The CKY algorithm is a dynamic programming algorithm. For a CFG, just keep track of possibility. For a PCFG, keep track of the max and argmax at each level.</li>
<li>Finding probability: The inside-outside algorithm is a dynamic program that takes the sum at each step.</li>
</ul>
<h2 id="lexicalized-pcfgs">Lexicalized PCFGs</h2>
<p>These have much higher parsing accuracy.</p>
<p>Weaknesses of PCFG’s are</p>
<ol type="1">
<li><p>Lack of sensitivity to lexical information: PCFGs make a strong independence assumption. The identity of each lexical item depends only on the part-of-speech (POS) above that lexical item, but does not depend directly on other information in the tree.</p>
<p>Example: Workers dumped sacks into a bin. What parsing is picked depends only on comparing <span class="math inline">\(q(VP\to VP,PP), q(NP\to NP, PP)\)</span>.</p>
<p><code>Into</code> is 9 times more likely to attach to a VP rather than NP.</p>
Example (coordination ambiguity): Dogs in houses and cats.</li>
<li><p>Lack of sensitivity to structural preferences.</p>
<p>Ex. President of a company in Africa.</p>
<p>President of a (company in Africa) is <em>close attachment</em>. Close attachment is twice as frequent.</p>
<p>When a PP can attach to 2 potential verbs, it is 20 times more likely to attach to the most recent verb.</p></li>
</ol>
<p>To lexicalize a treebank,</p>
<ul>
<li>label terminal words with lexical information (POS).</li>
<li>For each <span class="math inline">\(X\to Y_1\ldots Y_n\)</span> identify <span class="math inline">\(h\in [n]\)</span> the head of the rule, the most important child. (In CNF, <span class="math inline">\(n=2\)</span>.) Do this recursively to get a word associated with the POS. Now label the POS with that word. (See p. 12 for an example.) Write <span class="math inline">\(\to_h\)</span> to specify that the <span class="math inline">\(h\)</span>th child is the head.</li>
<li>Overline the heads.</li>
</ul>
<p>Thus, each node in the parse tree is now POS(word). Note the number of non-terminals is now much larger.</p>
<p>For a rule <span class="math inline">\(X(H) \to_2 Y_1(M) Y_2(H)\)</span> corresponding to <span class="math inline">\(R:X\to_2 Y_1Y_2\)</span>, factor the probability <span class="math display">\[\Pj(X\to_2 Y_1Y_2, M|X, H) = \Pj(X \to_2 Y_1Y_2 |X, H) \Pj(M|S\to_2 Y_1Y_2, X, H)\]</span> and estimate each of the factors with counts.</p>
<ul>
<li>For <span class="math inline">\(\Pj(X \to_2 Y_1Y_2 |X, H)\)</span>, estimate by linearly interpolating <span class="math inline">\(\wh p(X\to_2 Y_1Y_2|X)\)</span> and <span class="math inline">\(\wh p(X\to_2 Y_1Y_2|X,H)\)</span>.</li>
<li>For the other model, linearly interpolate <span class="math inline">\(M|R,H\)</span> and <span class="math inline">\(M|R\)</span>.</li>
</ul>
<p>For DP, include also the head position <span class="math inline">\(h\in [i,j]\)</span>. Complexity goes up to <span class="math inline">\(O(n^4)\)</span>!</p>
<h2 id="ibm">IBM</h2>
<h2 id="section">6</h2>
<h2 id="log-linear-models">7 Log-linear models</h2>
<p>Log-linear models are more flexible; they allow a rich set of features. We can combine a much larger set of estimates with <span class="math inline">\(\la\)</span> parameters. Linear interpolation becomes unwieldy.</p>
<p>We want to model the conditional probability <span class="math inline">\(p(y|x)\)</span>, <span class="math inline">\(x\in \mathcal X\)</span>, <span class="math inline">\(y\in \mathcal Y\)</span>. For example, <span class="math inline">\(\mathcal Y=\mathcal V\)</span>, <span class="math inline">\(\mathcal X = \mathcal Y^{i-1}\)</span>. Or <span class="math inline">\(\mathcal Y=\mathcal T\)</span>, the set of tags.</p>
<p>A loglinear model has <span class="math display">\[\Pj_v(y|x) =\Pj(y|x;v) \propto e^{v\cdot f(x,y)}\]</span> with <span class="math inline">\(f:\mathcal X\times \mathcal Y\to \R^d\)</span>.</p>
<p>Often features are indicator functions. Features can include all bigrams, trigrams… Other features include</p>
<ul>
<li>skip-grams (skip words)</li>
<li>part-of-speech of previous word(s)</li>
<li>suffixes, etc.</li>
</ul>
<p>A key idea is <strong>feature templates</strong>.</p>
<p>For any trigram <em>seen in training data</em>, create an indicator feature. Hash each <span class="math inline">\((u,v,w)\)</span> to a unique integer (for trigrams). We can do the same for suffixes, etc.</p>
<p>Models can have millions of features.</p>
<p>Key observation: for any pair <span class="math inline">\((x,y)\)</span>, the number of values for <span class="math inline">\(k\)</span> in <span class="math inline">\([d]\)</span> such that <span class="math inline">\(f_k(x,y)=1\)</span> is often very small.</p>
<p>Implementation: for any pair <span class="math inline">\((x,y)\)</span>, compute indices of nonzero features (ex. use hash functions).</p>
<p>ML estimates are bad because they overfit—ex. give 0 probability to <span class="math inline">\(n\)</span>-grams that haven’t appeared. Include a regularization term like <span class="math inline">\(\ve{v}^2\)</span>, <span class="math display">\[L(v) = \sumo in \ln p(y^{(i)}|x^{(i)};v) - \fc\la2\ve{v}^2.\]</span> This is convex. Use gradient ascent. LBFGs is a gradient method that makes a more intelligent choice of search direction.</p>
<p>The gradient is <span class="math display">\[\nb L(v) = \sumo in \pa{f(x^{(i)},y^{(i)}) - (p(y|x^{(i)};v))_{y} \cdot (f_k(x^{(i)},y))_{yk}}.\]</span> (Think of the second part as the expected number of times <span class="math inline">\(f_k\)</span> is equal to 1.)</p>
<h2 id="memms-maximum-entropy-markov-models-log-linear-tagging-models">MEMMs (Maximum entropy Markov models = Log-linear tagging models)</h2>
<p>A generative tagging model defines a joint distribution <span class="math inline">\(p(x_1,\ldots, x_n,y_1,\ldots, y_n)\)</span> where <span class="math inline">\(x_i\)</span> is a sentence tagged with <span class="math inline">\(y_i\)</span>. A <strong>conditional tagging model</strong> defines <span class="math inline">\(p(y_1\ldots, y_n|x_1,\ldots, x_n)\)</span>.</p>
<p>Tag by taking the argmax.</p>
<ol type="1">
<li>How to define a conditional tagging model?</li>
<li>How to estimate the parameters?</li>
<li>How to find argmax?</li>
</ol>
<p>Consider trigrams for example. The independence assumption is that conditioned on <span class="math inline">\(X_{[1,n]}\)</span>, the distribution of <span class="math inline">\(Y_i\)</span> depends only on <span class="math inline">\(Y_{i-1},Y_{i-2}\)</span>.</p>
<ol type="1">
<li><p>Let the <span class="math inline">\(i\)</span>th history <span class="math inline">\(h_i=(y_{i-2},y_{i-1},x_{[1,n]}, i)\)</span>. Let <span class="math inline">\(f:H\times \mathcal K\to \R^d\)</span> be the feature-vector representation (<span class="math inline">\(H\)</span> is the space of histories and <span class="math inline">\(\mathcal K\)</span> is the space of tags). Define the probability as a loglinear model on <span class="math inline">\(f\)</span>.</p>
Features may include
<ul>
<li>Word/tag</li>
<li>Prefix, suffix (Ex. -ing is often with VBG, gerunds)</li>
<li><span class="math inline">\(n\)</span>-gram (on <span class="math inline">\(y\)</span>) (Having <span class="math inline">\(i\)</span>-grams for <span class="math inline">\(1\le i\le n\)</span> is useful with regularized approaches to parameter estimation)</li>
<li>Word before/after</li>
<li>Spelling features</li>
<li>Contextual features (<span class="math inline">\(x_{i-2},x_{i+2}\)</span>)</li>
</ul></li>
<li>Use gradient methods.</li>
<li><p>Use the Viterbi algorithm.</p></li>
</ol>
<h2 id="crfs">CRFs</h2>
<blockquote>
<p>There are close connections to support vector machines, where linear models are learned in very high dimensional spaces, with good generalization guarantees hold as long as the margins on training examples are large. Margins are closely related to norms of parameter vectors.</p>
</blockquote>
<h2 id="scraps">Scraps</h2>
<p>Can we extract simple features/algorithms out of neural nets?</p>
<p>Extracting code from a neural net may be NP-hard?</p>
<p>Can a neural net do at least as well as <span class="math inline">\(n\)</span>-gram, and can you crystallize a <span class="math inline">\(n\)</span>-gram from it?</p>
<p>What information is present in memory?</p>
<p>Allow a neural net to revise, or wait before it outputs…</p>
<p>Matrix indexed by Indexable.</p>
<ul>
<li>[KJF14] Deep Fragment Embeddings for bidirectional sentence mapping: &gt; Inspired &gt; by previous work [5, 22] we observe that a dependency tree of a sentence provides a rich set of &gt; typed relationships that can serve this purpose more effectively than individual words or bigrams. &gt; We discard the tree structure in favor of a simpler model and interpret each relation (edge) as an &gt; individual sentence fragment (Figure 2, right shows 5 example dependency relations)</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>LDC's: directions</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/coding/directions.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/coding/directions.html</id>
    <published>2016-05-01T00:00:00Z</published>
    <updated>2016-05-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>LDC's: directions</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-05-01 
          , Modified: 2016-05-01 
	</p>
      
       <p>Tags: <a href="/tags/LDC.html">LDC</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#learningreading">Learning/reading</a></li>
 <li><a href="#approaches">Approaches</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="https://www.sharelatex.com/project/57169ca0b1d6259757ce842d">Notes</a>.</p>
<h2 id="learningreading">Learning/reading</h2>
<ul>
<li>Understand the inequalities from convex geometry.</li>
<li>Approximating the norm with a polynomial.</li>
<li>Understand the relationship with tensor eigenvectors.</li>
<li>Metric geometry.</li>
</ul>
<h2 id="approaches">Approaches</h2>
<ul>
<li>Any lower bound for a <em>perfect</em> LCC.</li>
<li>Some kind of SoS relaxation—understand ARV first.</li>
<li>Zeev’s conjecture on covering the image of a polynomial map of fixed degree with convex sets.</li>
<li>Comparison to convex hull. See <a href="lcc_geometry.html">LCC geometry</a>.
<ul>
<li>The image of the decoding map <span class="math inline">\(D:[-1,1]^n\to [0,1]^n\)</span> contains many points that are far apart, corresponding to the codewords. This gives that the convex hull of the image has large volume or a large <span class="math inline">\(\ep\)</span>-net.</li>
<li>The image is close to convex, so the image is also large.</li>
<li>However, the Jacobian of the map is small, so the volume of the image is small, contradiction.</li>
</ul></li>
<li>Directly bound convex hull. Ex. for 2-query, upper bound uses matrix concentration. More general concentration? (Ex. for tensors)</li>
<li>Use differential geometry. Some notion of curvature?</li>
<li>Phrase in terms of eigenvectors (or almost-eigenvectors) for higher-order tensors.</li>
<li>Vague
<ul>
<li>Reduce to a problem of geometry.</li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 4-30-16</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-04-30.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-04-30.html</id>
    <published>2016-04-30T00:00:00Z</published>
    <updated>2016-04-30T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 4-30-16</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-30 
          , Modified: 2016-04-30 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p>Machine learning</p>
<ul>
<li>Talk with Arora on NMF. Suggestions/directions:
<ul>
<li>Consider adding smoothing (regularizer). Smoothing is widely used in NLP. It would keep gradient updates from blowing up near the edge of <span class="math inline">\(\De_n\)</span>.</li>
</ul></li>
<li>TODO: Read Mike Collins’s notes on NLP.</li>
<li>Nicolaus Boumal sent me references on optimization on manifolds, which I glanced through. TODO: Spend a day, or a few days reading through Optimization on Manifolds.</li>
<li>Started reading Vishnoi’s slime mold paper. TODO: keep going.</li>
<li>Reviewed Ch. 9-10 of Convex Optimization by BV.</li>
<li>TODO: Make sure I have a good understanding of intro convex opt (ex. look at a course), and find more advanced books.</li>
</ul>
<p>Math</p>
<ul>
<li>Convex bodies and isotropicity.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Constrained optimization</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/constrained.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/constrained.html</id>
    <published>2016-04-28T00:00:00Z</published>
    <updated>2016-04-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Constrained optimization</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-28 
          , Modified: 2016-04-28 
	</p>
      
       <p>Tags: <a href="/tags/convex%20optimization.html">convex optimization</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#setup">Setup</a></li>
 <li><a href="#newton">Newton</a><ul>
 <li><a href="#feasible-start">Feasible start</a></li>
 <li><a href="#infeasible-start">Infeasible start</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="setup">Setup</h2>
<p>Consider <span class="math inline">\(\min_{Ax=b}f\)</span>.</p>
<ul>
<li>How can we reduce this problem to one without inequality constraints? Let <span class="math inline">\(x_0+Fu\)</span> parametrize <span class="math inline">\(\set{x}{Ax=b}\)</span>.</li>
<li>Why wouldn’t you want to do this? It can be inefficient—somehow the equality constraint captures more of the problem’s structure.</li>
</ul>
Give the example for quadratic minimization. What do the KKT equations say?
\begin{align}
\min_{Ax=b} \rc 2 x^T P x + q^T x + r &amp; \\
Px^* + q + A^T \nu^*&amp;=0\\
\iff \matt{P}{A^T}{A}0 \coltwo{x^*}{\nu^*} &amp;= \coltwo{-1}{b}.\label{eq:kkt-mat}
\end{align}
<p>This is unbounded below if there exist <span class="math inline">\(v,w\)</span> such that <span class="math inline">\(Pv+A^Tw=0\)</span>, <span class="math inline">\(Av=0\)</span>, <span class="math inline">\(-q^Tv+b^Tw&gt;0\)</span> (left-multiply by <span class="math inline">\((v^T\,w^T)\)</span>) to see that the equation above doesn’t have a solution).</p>
<p>Recall the dual function. Why do we care about it? <span class="math display">\[g(\la,\nu) = \min_x f+\nu^T (Ax-b) = -\nu^Tb - f^*(-\nu^TA), \qquad f^(y)=\max y^Tx - f(x).\]</span> Equality constraints disappear in the dual. If the dual is nice, we can just solve an unconstrained problem <span class="math inline">\(\max_{\la \ge 0} g(\la,\nu)\)</span>.</p>
<h2 id="newton">Newton</h2>
<h3 id="feasible-start">Feasible start</h3>
<p>Describe the Newton method starting with a feasible <span class="math inline">\(x, Ax=b\)</span>. The Newton step is the minimizer for the quadratic approximation under the constraint. (Finding the minimum of a quadratic amounts to solving a linear equation.) Using  <span class="math display">\[\matt{\nb^2 f}{A^T}A0 \coltwo{\De x_{nt}}w = \coltwo{-\nb f}{0}.\]</span> Define <span class="math display">\[\la(x) = (\De x_{nt}^T \nb^2 f \De x_{nt})^{\rc 2}\]</span>.</p>
<p>Note this <em>is</em> normal Newton if you use a parametrization.</p>
<p>(Convexity makes the KKT matrix invertible.)</p>
<h3 id="infeasible-start">Infeasible start</h3>
<p>Describe the infeasible start Newton method. Approximate <span class="math inline">\(f\)</span> by a quadratic using <span class="math inline">\(P=\nb^2 f\)</span> in  and find <span class="math inline">\(\De x_{nt}\)</span> so that <span class="math inline">\(x+\De x_{nt}\)</span> satisfies the KKT conditions. The equation for <span class="math inline">\(\De x_{nt}\)</span> is <span class="math display">\[\matt{\nb^2 f}{A^T}A0 \coltwo{\De x_{nt}}w = \coltwo{-\nb f}{b-Ax}.\]</span></p>
<p>Note here we’re just updating <span class="math inline">\(x\)</span> by solving for <span class="math inline">\(\De x_{nt}\)</span>. Ech time <span class="math inline">\(w\)</span> is treated just as an auxiliary variable. But <span class="math inline">\(w\)</span> comes from the dual solution <span class="math inline">\(\nu\)</span>. Can we look at <span class="math inline">\((x,\nu)\)</span> together as a primal-dual pair and update both of them?</p>
Let the residual be
\begin{align}
r &amp;= (\nb f + A^T \nu, Ax-b).
\end{align}
<p>Instead of minimizing <span class="math inline">\(f\)</span>, we minimize the residual for the KKT conditions. The residual has a component for minimizing <span class="math inline">\(f\)</span>, and a component for trying to satisfy <span class="math inline">\(Ax=b\)</span>.</p>
<p>Here is the algorithm.</p>
<ol type="1">
<li>Start with <span class="math inline">\(x\)</span> (not necessarily satisfying <span class="math inline">\(Ax=b\)</span>).</li>
<li>Calculate <span class="math inline">\(\De x_{nt}\)</span>, keeping track of <span class="math inline">\(\nu\)</span>. Backtrack (by multiplying by <span class="math inline">\(\be\)</span>) until you find <span class="math inline">\(t\)</span> such that <span class="math display">\[\ve{r(x+t\De x_{nt}, \nu + t\De \nu_{nt})}_2\le (1-\al t) \ve{r}_2.\]</span></li>
<li>Repeat until <span class="math inline">\(Ax=b\)</span>, <span class="math inline">\(\ve{r}_2\le \ep\)</span>.</li>
</ol>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Convexity</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/convexity.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/convexity.html</id>
    <published>2016-04-23T00:00:00Z</published>
    <updated>2016-04-23T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Convexity</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-23 
          , Modified: 2016-04-23 
	</p>
      
       <p>Tags: <a href="/tags/convex%20optimization.html">convex optimization</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#questions">Questions</a></li>
 <li><a href="#answers">Answers</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="questions">Questions</h2>
<ul>
<li>Define strongly convex. What estimates can you make using strong convexity? Why is it useful?</li>
<li>Define smoothness. What does it give you?</li>
<li>Define condition number for sets and matrices. What is the relationship? What do bounds on <span class="math inline">\(\nb^2\)</span> give on condition number?</li>
</ul>
<h2 id="answers">Answers</h2>
<ul>
<li>Strongly convex means <span class="math display">\[ f(y) \ge f(x) + \nb f(x)^T (y-x) + \fc m2 \ve{y-x}_2^2. \]</span> For twice-differentiable functions, this is equivalent to <span class="math inline">\(\nb^2 f\succeq mI\)</span>. Strong convexity lower-bounds suboptimality: if the gradient is small, then you are not too far from the minimum, because the gradient changes fast. (Ex. for <span class="math inline">\(c+dx+\fc m2x^2\)</span>, the minimum is at <span class="math inline">\(-\fc dm = -\rc m \nb f\)</span>.) The following inequality hold: (bound the optimal value in terms of the gradient, and bound the distance in terms of the gradient.
\begin{align}
p^* &amp;\ge f(x) - \rc{2m}\ve{\nb f(x)}_2^2\\
\ve{x-x^*}_2 &amp; \le \fc 2m \ve{\nb f}_2^2.
\end{align}</li>
<li>Smoothness means <span class="math display">\[ f(y) \le f(x) + \nb f(x)^T (y-x) + \fc M2 \ve{y-x}_2^2. \]</span> For twice-differentiable functions, this is equivalent to <span class="math inline">\(\nb^2 f\preceq MI\)</span>. Smoothness upper-bounds suboptimality, and ensures that step sizes don’t overshoot the minimum (much)
\begin{align}
p^* &amp;\le f(x) - \rc{2M}\ve{\nb f}_2^2.
\end{align}</li>
<li>The condition number of a convex body <span class="math inline">\(C\)</span> is (<span class="math inline">\(W_{\max},W_{\min}\)</span> are max and min width) <span class="math display">\[ \text{cond} (C) = \fc{W_{\max}^2}{W_{\min}^2}.\]</span> For an ellipsoid defined by <span class="math inline">\(x^TA^{-1}x\le 1\)</span>, <span class="math display">\[ \text{cond} (\mathcal E) = \fc{\la_{\max}(A)}{\la_{\min}(A)} = \ka(A).\]</span> Strong convexity and smoothness together bound the condition number of sublevel sets <span class="math inline">\(C_\al =\set{x}{f(x)\le \al}\)</span>. We have for <span class="math inline">\(p^*&lt;\al\)</span>, <span class="math display">\[B_{\sfc{2(\al-p^*)}{M}}\subeq C_\al \subeq B_{\sfc{2(\al-p^*)}{m}},\]</span> <span class="math inline">\(\text{cond}(C_\al)\le \fc{M}{m}\)</span>. Moreover, <span class="math inline">\(\lim_{\al\to 0^+} \text{cond}(C_\al) = \ka (\nb f(x^*))\)</span>.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Convex problems</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/convex_problems.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/convex_problems.html</id>
    <published>2016-04-23T00:00:00Z</published>
    <updated>2016-04-23T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Convex problems</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-23 
          , Modified: 2016-04-23 
	</p>
      
       <p>Tags: <a href="/tags/convex%20optimization.html">convex optimization</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p>Express the following as convex optimization problems.</p>
<ul>
<li>Quadratic minimization
<ul>
<li>Least squares</li>
</ul></li>
<li>Geometric programming</li>
<li>Analytic center</li>
</ul>
\begin{align}
&amp; \min \rc 2 x^T P x + q^T x + r, &amp;P\in S_+^h\\
&amp; \min \ve{Ax-b}_262, &amp;P=A^TA, q=A^Tb \text{ matching with above}\\
&amp; (\text{optimality conditions: } Px^*+q=0, \quad A^TAx^* = A^Tb\\
&amp; \min \ln \pa{\sum \exp(a_i^Tx+b_i)}\\
&amp; \min - \sum \ln(b_i - a_i^Tx)\\
&amp; \min \ln \det(F(x)^{-1}),&amp; F(x) = F_0+\sum_i x_i F_i.
\end{align}

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Second-order methods</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/second-order.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/second-order.html</id>
    <published>2016-04-22T00:00:00Z</published>
    <updated>2016-04-22T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Second-order methods</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-22 
          , Modified: 2016-04-22 
	</p>
      
       <p>Tags: <a href="/tags/Newton.html">Newton</a>, <a href="/tags/second-order.html">second-order</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#main-points">Main points</a></li>
 <li><a href="#proofs">Proofs</a></li>
 <li><a href="#convergence">Convergence</a></li>
 <li><a href="#more-intuition">More intuition</a></li>
 <li><a href="#implementation-issues">Implementation issues</a></li>
 <li><a href="#scraps">Scraps</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>See <a href="GD.html">gradient descent</a>.</p>
<ul>
<li>What is the general framework?
<ol type="1">
<li>Pick a descent direction <span class="math inline">\(\De x\)</span>.</li>
<li>Choose a step size <span class="math inline">\(\tau&gt;0\)</span>: <span class="math display">\[x^{(t+1)} \leftarrow x+\tau \De x.\]</span></li>
<li>Continue until stop criterion.</li>
</ol></li>
</ul>
<h2 id="main-points">Main points</h2>
<ul>
<li>What’s the shortcoming of gradient descent that we want to fix?
<ul>
<li>It is not invariant under linear transformation.</li>
<li>When the condition number of the Hessian is large, it has bad convergence.</li>
</ul></li>
<li>Steepest descent
<ul>
<li>For a norm <span class="math inline">\(\ved\)</span>, the steepest descent direction is (sd = steepest descent, nsd = normalized steepest descent)
\begin{align}
\De x_{sd} &amp;= \min_{\ve{y}\le 1} \nb f(x)^T y.
\end{align}</li>
</ul></li>
<li>Newton method
<ul>
<li>Let
\begin{align}
\De x_{sd} &amp;= -H^{-1}\nb f\\
\De x_{nsd} &amp;=\fc{H^{-1} \nb f}{\ve{H^{-\rc 2} \nb f}} = \fc{H^{-1} \nb f}{\la(x)^2}\\
\la(x) &amp;= \ve{H^{-\rc 2}\nb f}^{\rc 2} = (\nb f^T H^{-1} \nb f)^{\rc 2}.
\end{align}
Here <span class="math inline">\(\la(x)\)</span> is the Newton decrement.</li>
</ul></li>
<li>Newton for functions with Lipschitz Hessian: The number of steps to reach <span class="math inline">\(\ep\)</span> is <span class="math display">\[\fc{f(x^{(0)}-p^*)}{\ga} + \ln\ln \fc{\ep_0}{\ep}, \ep_0=\fc{2m^3}{L^2}.\]</span></li>
<li><span class="math inline">\(f\)</span> is <strong>self-concordant</strong> if for all <span class="math inline">\(v\)</span>, <span class="math inline">\(\an{\nb^3 f, \De x^{\ot 3}} \le 2 \an{\nb^2 f, \De x^{\ot2}}^{\fc 32}\)</span>. For self-concordant functions, the number of steps to reach <span class="math inline">\(\ep\)</span> is <span class="math display">\[\fc{f(x^{(0)}-p^*)}{\ga} + \log_2\log_2\prc{\ep}\]</span> where <span class="math inline">\(\ga = \fc{\al \be (1-2\al)^2}{20-8\al}\)</span>.</li>
<li>What are the drawbacks of Newton’s method and how to fix them?
<ul>
<li>Naively it requires computing <span class="math inline">\(H^{-1}\)</span> which (naively?) takes <span class="math inline">\(n^3\)</span> time each iteration, where <span class="math inline">\(n\)</span> is the dimension. <em>For large <span class="math inline">\(n\)</span> this is prohibitive.</em></li>
</ul></li>
</ul>
<h2 id="proofs">Proofs</h2>
<ul>
<li>Newton method is steepest descent for <span class="math inline">\(H\)</span>:
<ul>
<li>Let <span class="math inline">\(A\)</span> be a symmetric positive definite matrix. Defining <span class="math inline">\(\ved_A\)</span> as follows, we note that the dual norm is the norm corresponding to the inverse.
\begin{align}
\ve{A} :&amp;= x^TAx = \ve{\sqrt A}_2\\
\ve{x}_{A}^* &amp; = \ve{x}_{A^{-1}}.
\end{align}
<ul>
<li><em>Proof.</em> <span class="math display">\[\ve{x}_A^* = \max_{\ve{y}_A=1} x^Ty \stackrel{z = \sqrt{A}y}{=} \max_{\ve{z}=1} x^T A^{-\rc 2} z = \ve{A^{-\rc 2}x^T}  = \ve{x}_{A^{-1}}.\]</span></li>
<li>This calculation also shows that <span class="math display">\[\amin_{\ve{y}_A=1} v^Ty = \fc{A^{-1} x}{\ve{A^{-\rc 2}x}}.\]</span></li>
<li>Thus,
\begin{align}
\De x_{nsd} &amp;= \fc{H^{-1} \nb f(x)}{\ve{H^{-\rc 2} \nb f}}.
\end{align}</li>
</ul></li>
<li>Why is <span class="math inline">\(\De x_{nsd}\)</span> the right normalization? <span class="math display">\[ f\pa{x - t\fc{H^{-1} \nb f(x)}{\ve{H^{-\rc 2} \nb f}}} \approx f(x) + (-t+\fc{t^2}2) \fc{H^{-1} \nb f(x)}{\ve{H^{-\rc 2} \nb f}}\]</span> and <span class="math inline">\(t=1\)</span> minimizes this.</li>
</ul></li>
</ul>
<h2 id="convergence">Convergence</h2>
<ul>
<li>What is the main idea of Newton descent? Why/how do we get quadratic convergence?
<ul>
<li>Go in the direction of the minimum suggested by the Hessian (second order approximation).</li>
<li>The proof is organized as follows. If <span class="math inline">\(\ve{\nb f(x^{(k)})^2}\)</span> is
<ul>
<li>large (<span class="math inline">\(\ge \eta\)</span>), then make constant progress by <span class="math inline">\(\ga\)</span>. This is the dampled phase; steps are small. Steps are small because the default Newton step <span class="math inline">\(t=1\)</span> is bad, and backtracking will choose a smaller <span class="math inline">\(t\)</span>. (?) (It’s “slow” in the sense it’s not quadratically convergent, but it makes constant progress.)</li>
<li>small (<span class="math inline">\(&lt;\eta\)</span>), then make quadratic progress, <span class="math inline">\(\fc{L}{2m^2}\ve{\nb f(x^{(k+1)})}_2\le \ve{\nb f(x^{(k)})}_2^2\)</span>.</li>
</ul></li>
</ul></li>
<li>What should <span class="math inline">\(\eta\)</span> depend on? <span class="math inline">\(\al\)</span> (the slope for backtracking), <span class="math inline">\(m\)</span> (strong convexity parameter), <span class="math inline">\(L\)</span> (Lipschitz constant on Hessian).</li>
</ul>
<em>Proof (for Lipschitz)</em>. We do the calculations for 1 dimension. The calculations are the same, except we have to use matrices and vectors. Let <span class="math inline">\(\te(y)\)</span> be a quantity in <span class="math inline">\([-y,y]\)</span>. Suppose we are at <span class="math inline">\((0,0)\)</span>. Let <span class="math inline">\(d=f'(0), a=f''(0), \la = \fc{d}{a^{\rc 2}}\)</span>, <span class="math inline">\(\De x_{nt} = \fc{d}{a} = \fc{\la}{a^{\rc 2}}\)</span>.
\begin{align}
f(x) &amp;= dx + \rc 2 ax^2 + \te\pa{\rc 6 Lx^3}\\
f\pa{-\fc da} &amp;= -\rc 2 \fc{d^2}a\\
&amp;\le -\fc{d^2}{a} ]ub{\pa{\rc 2 - \rc 6 L \fc{d}{a^2}}}{\ge \al}.
\end{align}
<p>In order for the quantity to be <span class="math inline">\(\ge \al\)</span>, noting <span class="math inline">\(\fc{d}{a^2} = \fc{d}{a^{\fc 32}}\)</span>, we want <span class="math inline">\(\la \le \fc{3(1-2\al) a^{\fc 32}}L\)</span>; it’s sufficient for <span class="math inline">\(f'\le \fc{3(1-2\al)m^2}{L}\)</span>.</p>
Note that unlike for linear convergence, we don’t prove something like <span class="math inline">\(f(x') - f(x^*)\le \cdots\)</span>. We have to work with the gradients to get quadratic convergence. (Gradients also control the distance to the optimum.) We have
\begin{align}
f'(x) &amp;= d+ ax + \te\pa{\rc L x^2}\\
f'\pa{-\fc da} &amp; \le \rc 2 L\fc{d^2}{a^2} \le \fc{L}{2m^2}f'(0).
\end{align}
<em>Proof (for self-concordant)</em>. Work in 1-D. Instead of integrating <span class="math inline">\(\int f'''=\int (f'')^{\fc 32}\)</span>, we let <span class="math inline">\(F(y)=y^{-\rc 2}\)</span> and integrate <span class="math inline">\(\int (F(f''))'\)</span>.
\begin{align}
|(f'')^{-\rc 2} (t)| &amp;= \ab{\int_0^t ((f'')^{-\rc 2})'} =\int_0^t \ab{-\rc 2 (f'')^{-\fc 32} f'''}\le t\\
\label{eq:f''}
\implies
\rc{(-t + f''(0)^{-\rc 2})^2} &amp;\ge f''(t) \ge \rc{(t+f''(0)^{-\rc 2})^2}\\
\implies
\rc{-t+f''(0)^{-\rc 2}} - f''(0)^{\rc 2} &amp; \ge f'(t)-f'(0) \ge \rc{t+f''(0)^{-\rc 2}} + f''(0)^{\rc 2}\\
\implies f(t) &amp;\le f(0) + (f'(0) - f''(0)^{\rc 2}) t + \ln \pf{f''(0)^{-\rc 2}}{t + f''(0)^{-\rc 2}}\\
\implies f\pa{-\fc{f'}{f''}t} &amp; \le f(0) - \la^2 t + \la t - \ln (1-t\la(x)).
\end{align}
<p>Now consider 2 cases.</p>
<ul>
<li><span class="math inline">\(\la(x^{(k)})&gt;\eta\)</span>. We show <span class="math inline">\(f(x^{(k+1)}) - f(x^{(k)}) \le -\ga\)</span>.
<ul>
<li>Using <span class="math inline">\(-x + \ln (1+x) + \fc{x^2}{2(1+x)}\le 0\)</span>, for <span class="math inline">\(t=\rc{1+\la(x)}\)</span>,
\begin{align}
f(-\fc{f'}{f''} t) &amp; \le f(0) - \al \la(x)^2 t\\
t=\rc{1+\la(x)^2}\implies f(-\fc{f'}{f''} t) &amp; \le f(0) - \al \la(x)^2 t
\end{align}
so backtracking line search chooses <span class="math inline">\(t\ge \fc{\be}{1+\la(x)}\)</span> with <span class="math display">\[f(-\fc{f'}{f''} t)  \le -\al \be \fc{\la^2}{1+\la} \le -\al \be \fc{\eta^2}{1+\eta}.\]</span></li>
</ul></li>
<li><span class="math inline">\(\la(x^{(k)})\le \eta\)</span>. We show <span class="math inline">\(2\la(x^{(k+1)})\le (2\la(x^{(k)}))^2\)</span>. (Note we are not getting a bound on <span class="math inline">\(f'(x_{k+1})\)</span> like before because we don’t have strong convexity, which upper-bounds <span class="math inline">\(f''(x)^{-\rc 2}\)</span>.)
<ul>
<li>We take a unit step because <span class="math display">\[f(-\fc{f'}{f''})   = f(0) - \la^2 + \la - \ln (1-\la(x))\le f(0)-\al \la(x)^2\]</span> when <span class="math inline">\(\la(x) \le \fc{1-2\al}{2}\)</span>.</li>
<li>Self-concordance gives multiplicative bound on how <span class="math inline">\(H\)</span> changes, <span class="math inline">\((1-t\al)^2 H \preceq H(x+tv) \preceq \rc{(1-t\al)^2} H(x)\)</span>, where <span class="math inline">\(\al=\ve{v}_{H^{\rc 2}}\)</span>. Proof:
\begin{align}
\ln f''(t) - \ln f''(0) &amp; = \int_0^t (\ln f'')' \\
&amp; \le \int_0^t (2f'')^{\rc 2}\\
&amp;\le \int_0^t \fc{2}{-x+(f'')^{-\rc 2}} &amp;\eqref{eq:f''}\\
&amp;\le 2\ln (-t (f'')^{\rc 2} + 1).
\end{align}</li>
<li>Now we show <span class="math inline">\(\la(x-\fc{f'}{f''}) \le \fc{\la^2}{(1-\la)^2}\)</span>. Using the multiplicative bound (here <span class="math inline">\(t=1\)</span>, <span class="math inline">\(v=-\fc{f'}{f''}\)</span>, <span class="math inline">\(\al=\la\)</span>.
\begin{align}
f''(x_{k+1})&amp;\in f''(x_k) [1-\la, \rc{1-\la}]\\
f'(x_{k+1}) &amp; \in f'(x_k) + \ba{-\rc{\fc{f'}{f''} + \rc{(f'')^{\rc 2}}} + (f'')^{\rc 2}, \rc{-\fc{f'}{f''}+\rc{(f'')^{\rc2}}} - (f'')^{\rc 2}}\\
&amp;= f'(x_k) \ba{\fc{\la}{-\la + 1}, \fc{\la}{\la+1}}\\
\la(x_{k+1})&amp;\le \fc{\la(x_k)^2}{(1-\la(x_k))^2} \\ &amp; \le 2\la^2
\end{align}
where the last inequality is when <span class="math inline">\(\la\le \rc 4\)</span>.</li>
</ul></li>
<li>Finally, bound the distance to optimum by <span class="math inline">\(\la\)</span>, <span class="math inline">\(f(x^{(l)})-p^* \le \la(x^{(l)})^2\)</span>.</li>
</ul>
<h2 id="more-intuition">More intuition</h2>
<ul>
<li>Why do we use <span class="math inline">\(\ved_H\)</span>?
<ul>
<li>It appears in the second-order Taylor approximation. <span class="math inline">\(f(x) + \nb f^T v + \rc 2 v^T \nb^2 f v\)</span> has a minimum at <span class="math inline">\(\fc{H^{-1}f}{\ve{H^{-\rc 2}f}}\)</span>, not in the gradient direction!</li>
<li>It is invariant to linear transformation: <span class="math inline">\(\De x_{nsd} (f\circ A) = \De x_{nsd} f\)</span>.</li>
</ul></li>
<li>What is the relationship to Newton’s method of finding zeros?
<ul>
<li>The Newton’s method here is the Newton zero-finding method applied to <span class="math inline">\(f'\)</span>.</li>
</ul></li>
<li>Does order <span class="math inline">\(c\)</span> give <span class="math inline">\(2^{-n^c}\)</span> convergence?
<ul>
<li>Probably. But it’s rare to get <span class="math inline">\(&gt;2\)</span> order information.</li>
</ul></li>
<li>Give an example where gradient descent performs poorly compared to Newton. <span class="math inline">\(f = x_1^2 + \ep x_2^2\)</span>, <span class="math inline">\(-\nb f = (-2x_1,-2\ep x_2)\)</span>.</li>
<li>Intuition for self-concordance
<ul>
<li>We relax the requirement that <span class="math inline">\(f\)</span> is strongly convex and has Lipschitz Hessian. We’re allowed to have <span class="math inline">\(f''\to 0\)</span>; the requirement is that when <span class="math inline">\(f''\)</span> is small so is <span class="math inline">\(f'''\)</span>. Note <span class="math inline">\(\fc{(f'')^{\fc 32}}{f'''}\)</span> is dimensionless.</li>
</ul></li>
</ul>
<h2 id="implementation-issues">Implementation issues</h2>
<ol type="1">
<li>Precompute line searches: it can be more efficient to simultaneously compute <span class="math inline">\(f(x+t\De x)\)</span> for many values of <span class="math inline">\(t\)</span>, e.g., if it involves a linear/matrix function.</li>
<li>Computing <span class="math inline">\(\De x_{nt}=H^{-1}\nb f(x)\)</span> is more efficient if <span class="math inline">\(H\)</span> has band structure, sparsity, etc.</li>
</ol>
<h2 id="scraps">Scraps</h2>
<p>? 9.31,</p>
<p>estimating Hessian?</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
