<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2016-10-11T00:00:00Z</updated>
    <entry>
    <title>Hidden Markov Models</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/probabilistic/hmm.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/probabilistic/hmm.html</id>
    <published>2016-10-11T00:00:00Z</published>
    <updated>2016-10-11T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Hidden Markov Models</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-11 
          , Modified: 2016-10-11 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <ul>
<li>[HKZ12] A spectral algorithm for learning Hidden Markov Models</li>
<li>Continuous HMM [LBGS10] [paper](http://repository.cmu.edu/cgi/viewcontent.cgi?article=1057&amp;context=machine_learning)</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Bandit convex optimization</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/bco.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/bco.html</id>
    <published>2016-10-11T00:00:00Z</published>
    <updated>2016-10-11T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Bandit convex optimization</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-11 
          , Modified: 2016-10-11 
	</p>
      
       <p>Tags: <a href="/tags/convex%20optimization.html">convex optimization</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#multi-armed-bandit">Multi-armed bandit</a><ul>
 <li><a href="#de-greedy-algorithm"><span class="math inline">$\de$</span>-greedy algorithm</a></li>
 <li><a href="#exp3">EXP3</a></li>
 <li><a href="#ucb1">UCB1</a></li>
 </ul></li>
 <li><a href="#blo">BLO</a><ul>
 <li><a href="#scrible">SCRIBLE</a></li>
 </ul></li>
 <li><a href="#bco">BCO</a><ul>
 <li><a href="#fkm">FKM</a></li>
 </ul></li>
 <li><a href="#contextual-bandits">Contextual bandits</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>Settings of increasing complexity:</p>
<ul>
<li>Multi-armed bandit</li>
<li>Bandit linear optimization</li>
<li>Bandit convex optimization</li>
</ul>
<h2 id="multi-armed-bandit">Multi-armed bandit</h2>
<ul>
<li><span class="math inline">\(\de\)</span>-greedy algorithm: Balancing exploitation which gives <span class="math inline">\(O(\sqrt T)\)</span> regret and exploration which gives <span class="math inline">\(O(T^{\fc 34} \sqrt n)\)</span> regret.</li>
</ul>
<h3 id="de-greedy-algorithm"><span class="math inline">\(\de\)</span>-greedy algorithm</h3>
<ul>
<li>With probability <span class="math inline">\(\de\)</span>, explore.
<ul>
<li>Play <span class="math inline">\(i_t\sim U_n\)</span>.</li>
<li><span class="math inline">\(\wh f_t(x) = \wh \ell_t^T x\)</span>.</li>
<li><span class="math inline">\(\wh \ell_t = \fc{n}{\de}\ell_t(i_t)e_{i_t}\)</span>.</li>
</ul></li>
<li>With probability <span class="math inline">\(1-\de\)</span>, exploit.
<ul>
<li>Play <span class="math inline">\(i_t\sim x_t\)</span>.</li>
<li><span class="math inline">\(\wh f_t(x) = 0\)</span>. (We can’t use information here because it wasn’t uniformly generated.)</li>
</ul></li>
<li>Update using a bandit algorithm <span class="math inline">\(x_{t+1} = A(\wh f_1,\ldots, \wh f_t)\)</span>.</li>
</ul>
<p>The <span class="math inline">\(\wh \ell_t\)</span> were chosen so <span class="math display">\[ \E \wh \ell_t(i) = \ell_t(i).\]</span> We have <span class="math display">\[\E R_T \le 3 GD\sqrt T + \de T,\]</span> where <span class="math inline">\(G\le \fc n\de\)</span>.</p>
<h3 id="exp3">EXP3</h3>
<p>Adversarial setting.</p>
<p><a href="https://jeremykun.com/2013/11/08/adversarial-bandits-and-the-exp3-algorithm/">Blog post</a>.</p>
<p>We can explore and exploit at the same time if we (a) keep track of a probability distribution and update it, and (b) if we reweight the loss functions to make the expected value correct.</p>
<p>Idea: Use the MWU (hedge) algorithm.</p>
<p>Get <span class="math inline">\(R_T\le O(\sqrt{Tn \ln n})\)</span>.</p>
<h3 id="ucb1">UCB1</h3>
<p>Stochastic setting.</p>
<p>“Optimism in the face of uncertainty.”</p>
<p><a href="https://jeremykun.com/2013/10/28/optimism-in-the-face-of-uncertainty-the-ucb1-algorithm/">Blog post</a>.</p>
<h2 id="blo">BLO</h2>
<h3 id="scrible">SCRIBLE</h3>
<p>Attains <span class="math inline">\(O(\sqrt T\ln T)\)</span> regret.</p>
<h2 id="bco">BCO</h2>
<ul>
<li>[BE16] [paper](https://arxiv.org/pdf/1507.06580v1.pdf) <a href="http://www.jmlr.org/proceedings/papers/v49/bubeck16.pdf">JMLR version</a> - inefficient, <span class="math inline">\(\wt O(\poly(n)\sqrt T)\)</span>-regret algorithm.</li>
<li>[BEL16] [paper](https://arxiv.org/pdf/1607.03084.pdf) - <span class="math inline">\(\wt O(\poly(n)\sqrt T)\)</span>-regret and <span class="math inline">\(\poly(T)\)</span>-time algorithm.</li>
</ul>
<h3 id="fkm">FKM</h3>
<p>Generic reduction from BCO to (first-order) OCO by using gradient estimators. FKM is an instantiation of the algorithm with regret <span class="math inline">\(O(T^{\fc 34})\)</span>.</p>
<h2 id="contextual-bandits">Contextual bandits</h2>
<ul>
<li>EXP4 (?)</li>
<li>[LCLS10] A Contextual-Bandit Approach to Personalized News Article Recommendation
<ul>
<li>Model: stochastic setting, expected payoff of <span class="math inline">\(a\)</span> is linear in feature <span class="math inline">\(x_{t,a}\)</span>, (<span class="math inline">\(\te_a^*\)</span> is the unknown coefficient vector) <span class="math display">\[ \E[r_{t,a}|x_{t,a}] = x_{t,a}^T\te_^*.\]</span></li>
<li>Algorithm: LinUCB attains regret <span class="math inline">\(\wt O(\sqrt{KdT})\)</span>.</li>
</ul></li>
<li>[DHKK] Efficient Optimal Learning for Contextual Bandits</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Generalization of neural nets</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/generalization.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/generalization.html</id>
    <published>2016-10-10T00:00:00Z</published>
    <updated>2016-10-10T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Generalization of neural nets</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-10 
          , Modified: 2016-10-10 
	</p>
      
       <p>Tags: <a href="/tags/neural%20nets.html">neural nets</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#rademacher-complexity">Rademacher complexity</a></li>
 <li><a href="#local-minima-generalization">Local minima generalization</a></li>
 <li><a href="#email">Email</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>How can we show generalization error of neural networks is small?</p>
<h2 id="rademacher-complexity">Rademacher complexity</h2>
<p>The natural way to proceed is to calculate the Rademacher complexity of the neural net.</p>
<p><a href="http://www.jmlr.org/papers/volume3/bartlett02a/bartlett02a.pdf">BM02</a> give generalization bounds for 2-layer NN and say that it generalizes to more layers, but I’m not sure how. (There’s a “trivial” way of doing it with Lipschitzness which seems to grow exponentially with depth; I feel there may be a more natural way to do it with “composition”.)</p>
<p>Given the dimension of each layer, a Lipschitz sigmoid/threshold function, and a bound on the size of the coefficients, what are the bounds we get from Rademacher complexity? How are they unsatisfactory? (Ex. in practical cases, is the sample size too small? Is it, e.g., a difference between a sample of polynomial size and linear size, or something larger?)</p>
<h2 id="local-minima-generalization">Local minima generalization</h2>
<p>I thought a bit about whether local optima would generalize. Do you have any intuitions regarding why? This is meaningful if it generalizes with fewer samples than one would calculate from e.g. Rademacher complexity. (My first thought is that having derivative 0 in all directions (and also nonnegative Hessian) is much less likely to happen by chance than the value being small.)</p>
<p>Also, is the right statement “under xx conditions whp all local optima generalize”? Given that there may be many local optima it seems there’s likely to be some (most) local optima that generalize well and some that don’t; perhaps it takes much fewer samples to have “most” local minima generalize. However, this seems a much more difficult statement to work with (ex. of the type people have been trying to get intuition from statistical physics). Perhaps we also need to tie it to the SGD algorithm in some way.</p>
<h2 id="email">Email</h2>
<p>I thought a bit about your suggestion that local optima would generalize. Do you have any intuitions regarding why or what a statement of this would be like?</p>
<p>Here are some preliminary thoughts/questions I had.</p>
<ul>
<li>Focusing on neural networks: Rademacher complexity would give a baseline for generalization. Given the dimension of each layer, a Lipschitz sigmoid/threshold function, and a bound on the size of the coefficients, what are the bounds we get from Rademacher complexity? How are they unsatisfactory? (Ex. in practical cases, is the sample size too small? Is it, e.g., a difference between a sample of polynomial size and linear size, or something larger?) <a href="http://www.jmlr.org/papers/volume3/bartlett02a/bartlett02a.pdf">BM02</a></li>
<li>Saying that local min generalizes is significant if it generalizes with fewer samples than one would calculate from e.g. Rademacher complexity.</li>
<li>Is the right statement “under xx conditions whp all local optima generalize”? Given that there may be many local optima it seems there’s likely to be some (most) local optima that generalize well and some that don’t; perhaps it takes much fewer samples to have “most” local minima generalize. However, this seems a much more difficult statement to work with (ex. of the type people have been trying to get intuition from statistical physics). Perhaps we need to tie it to the SGD algorithm in some way.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Neural nets learn dictionaries</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/nndl.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/nndl.html</id>
    <published>2016-10-10T00:00:00Z</published>
    <updated>2016-10-10T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Neural nets learn dictionaries</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-10 
          , Modified: 2016-10-10 
	</p>
      
       <p>Tags: <a href="/tags/dictionary%20learning.html">dictionary learning</a>, <a href="/tags/neural%20nets.html">neural nets</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#result">Result</a></li>
 <li><a href="#proof">Proof</a></li>
 <li><a href="#further-directions">Further directions</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="result">Result</h2>
<p>General idea: when each row of <span class="math inline">\(B\)</span> is sufficiently close to the row of <span class="math inline">\(A^T\)</span>, gradient descent converges until <span class="math inline">\(B\)</span> is very close to <span class="math inline">\(A^T\)</span>.</p>
<h2 id="proof">Proof</h2>
<h2 id="further-directions">Further directions</h2>
<ul>
<li>What are the dynamics of the dictionary vectors learned? If we start them randomly, will they converge to the <span class="math inline">\(A_{\bullet i}\)</span> or combinations of the <span class="math inline">\(A_{\bullet i}\)</span>? Will they not be useless? Will they be far apart? (What if you simply do some sampling of <span class="math inline">\(Ah\)</span>?)</li>
<li>Agnostic dictionary learning. Suppose we learn a good dictionary. Can we guarantee that it will do well in classification?</li>
<li>Would a local min for this neural net be good? Generalization follows from Rademacher, so we just care about driving down the cost.</li>
<li>What weird properties does a NP-hard distribution for DL have—is there a simple way to exclude those?</li>
<li>Not overcomplete, but sparse + noise (e.g. componentwise). (If not overcomplete, then a linear classifier works. But thresholding could remove the noise, while averaging wouldn’t (ex. averaging could amplify it, though slowerly than linearly, <span class="math inline">\(\sqrt n\)</span>).)</li>
<li>If don’t get <span class="math inline">\(b^Th\)</span> but only <span class="math inline">\(\sgn(b^Th)\)</span>.</li>
<li>Provable convolutional DL. (The difficulty is that translates that are close by overlap and are not close to orthogonal.)
<ul>
<li>More general (mathematical) problem is DL under various invariances. Perhaps assuming locality of the filter and separation in distance of the nonzero patches (ex. different copies of the same filter don’t overlap).</li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2016-10-15</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-10-15.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-10-15.html</id>
    <published>2016-10-10T00:00:00Z</published>
    <updated>2016-10-10T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2016-10-15</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-10 
          , Modified: 2016-10-10 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#threads">Threads</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="threads">Threads</h2>
<ul>
<li>PMI - get some results!</li>
<li>SoS - lectures 2 and 3</li>
<li>DL: do experiments suggested in <a href="/posts/tcs/machine_learning/matrices/DL_generalization.html">DL generalization</a>
<ul>
<li>(*) NN learns DL. (Finish)</li>
<li>DL generalizations.</li>
</ul></li>
<li>Stability of SGD
<ul>
<li>@Elad: do local minima generalize? (write up a formal question)</li>
</ul></li>
<li>Graphical models reading</li>
<li>Alexa</li>
<li>Papers (finish reading, summarize)</li>
<li>Papers (started reading)
<ul>
<li><a href="http://www.research.rutgers.edu/~lihong/pub/Li10Contextual.pdf">LCLS10 A Contextual-Bandit Approach to Personalized News Article Recommendation</a></li>
<li><a href="http://www.cs.columbia.edu/~djhsu/papers/amo.pdf">DHKK Efficient Optimal Learning for Contextual Bandits</a></li>
<li><a href="http://www.cs.columbia.edu/~djhsu/papers/poshmm-tacl.pdf">SCH Unsupervised Part-Of-Speech Tagging with Anchor Hidden Markov Models</a></li>
<li><a href="http://www.cs.columbia.edu/~djhsu/papers/hmm-jcss-final.pdf">A spectral algorithm for learning Hidden Markov Models</a></li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2016-10-08</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-10-08.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-10-08.html</id>
    <published>2016-10-08T00:00:00Z</published>
    <updated>2016-10-08T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2016-10-08</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-08 
          , Modified: 2016-10-10 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#done">Done</a></li>
 <li><a href="#threads">Threads</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="done">Done</h2>
<ul>
<li>DL
<ul>
<li>(*) NN learns DL. (Did half of calculations - see notebook.)</li>
<li>DL generalizations.</li>
</ul></li>
<li>Papers (started reading)
<ul>
<li><a href="http://www.research.rutgers.edu/~lihong/pub/Li10Contextual.pdf">LCLS10 A Contextual-Bandit Approach to Personalized News Article Recommendation</a></li>
<li><a href="http://www.cs.columbia.edu/~djhsu/papers/amo.pdf">DHKK Efficient Optimal Learning for Contextual Bandits</a></li>
<li><a href="http://www.cs.columbia.edu/~djhsu/papers/poshmm-tacl.pdf">SCH Unsupervised Part-Of-Speech Tagging with Anchor Hidden Markov Models</a></li>
<li><a href="http://www.cs.columbia.edu/~djhsu/papers/hmm-jcss-final.pdf">A spectral algorithm for learning Hidden Markov Models</a></li>
</ul></li>
<li><a href="/posts/tcs/machine_learning/reinforcement_learning/rl.html">RL</a>: read Ch. 7-8 (Fri)</li>
<li>Avi’s birthday conference. Aaronson, Micali’s talks. (Micali has good general advice.)</li>
<li>Alexa meeting.</li>
<li>Thought a little about generalization of NN.</li>
<li>Talked to Kiran on convolutional DL (Mon)</li>
</ul>
<h2 id="threads">Threads</h2>
<ul>
<li>PMI - get some results!</li>
<li>SoS - lectures 2 and 3</li>
<li>DL: do experiments suggested in <a href="/posts/tcs/machine_learning/matrices/DL_generalization.html">DL generalization</a>
<ul>
<li>(*) NN learns DL. (Finish)</li>
<li>DL generalizations.</li>
</ul></li>
<li>Stability of SGD
<ul>
<li>@Elad: do local minima generalize? (write up a formal question)</li>
</ul></li>
<li>Graphical models reading</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[WJ08] Graphical Models, Exponential Families, and Variational Inference</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/probabilistic/graphical_models.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/probabilistic/graphical_models.html</id>
    <published>2016-09-30T00:00:00Z</published>
    <updated>2016-09-30T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[WJ08] Graphical Models, Exponential Families, and Variational Inference</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-09-30 
          , Modified: 2016-09-30 
	</p>
      
       <p>Tags: <a href="/tags/probabilistic%20models.html">probabilistic models</a>, <a href="/tags/graphical%20models.html">graphical models</a>, <a href="/tags/exponential%20families.html">exponential families</a>, <a href="/tags/variational%20inference.html">variational inference</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#introduction">1 Introduction</a></li>
 <li><a href="#graphical-models">2 Graphical models</a></li>
 <li><a href="#exponential-families">3 Exponential families</a></li>
 <li><a href="#bethe-approximation-and-sum-product-algorithm">4 Bethe approximation and sum-product algorithm</a></li>
 <li><a href="#mean-field-methods">5 Mean field methods</a></li>
 <li><a href="#variational-methods-in-parameter-estimation">6 Variational methods in parameter estimation</a></li>
 <li><a href="#variational-methods-based-on-convex-relaxations">7 Variational methods based on convex relaxations</a></li>
 <li><a href="#mode-computation">8 Mode computation</a></li>
 <li><a href="#conic-programming-relaxations">9 Conic programming relaxations</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="introduction">1 Introduction</h2>
<p>Inference: computing marginal probabilities.</p>
<h2 id="graphical-models">2 Graphical models</h2>
<h2 id="exponential-families">3 Exponential families</h2>
<h2 id="bethe-approximation-and-sum-product-algorithm">4 Bethe approximation and sum-product algorithm</h2>
<h2 id="mean-field-methods">5 Mean field methods</h2>
<h2 id="variational-methods-in-parameter-estimation">6 Variational methods in parameter estimation</h2>
<h2 id="variational-methods-based-on-convex-relaxations">7 Variational methods based on convex relaxations</h2>
<h2 id="mode-computation">8 Mode computation</h2>
<h2 id="conic-programming-relaxations">9 Conic programming relaxations</h2>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Reinforcement learning</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/rl.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/rl.html</id>
    <published>2016-09-28T00:00:00Z</published>
    <updated>2016-09-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Reinforcement learning</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-09-28 
          , Modified: 2016-10-07 
	</p>
      
       <p>Tags: <a href="/tags/reinforcement%20learning.html">reinforcement learning</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#references">References</a></li>
 <li><a href="#section">3</a></li>
 <li><a href="#dynamic-programming">4 Dynamic programming</a></li>
 <li><a href="#monte-carlo-methods">5 Monte Carlo methods</a><ul>
 <li><a href="#on-policy-mc-control">5.4 On-policy MC control</a></li>
 <li><a href="#evaluating-one-policywhile-following-an-other-off-policy-policy-evaluation">5.5 Evaluating One PolicyWhile Following An-other (Off-policy Policy Evaluation)</a></li>
 <li><a href="#off-policy-mc-control">5.6 Off-policy MC control</a></li>
 <li><a href="#incremental-implementation">5.7 Incremental implementation</a></li>
 </ul></li>
 <li><a href="#temporal-difference-learning">6 Temporal-difference learning</a><ul>
 <li><a href="#optimality-of-td0">6.3 Optimality of TD(0)</a></li>
 <li><a href="#sarsa-on-policy-td-control">6.4 Sarsa: On-policy TD control</a></li>
 <li><a href="#q-learning-off-policy-td-control">6.5 Q-learning: Off-policy TD control</a></li>
 <li><a href="#games">6.6 Games</a></li>
 </ul></li>
 <li><a href="#eligibility-traces">7 Eligibility traces</a><ul>
 <li><a href="#n-step-td-prediction">7.1 n-step TD prediction</a></li>
 <li><a href="#the-forward-view-of-tdla">7.2 The forward view of TD(<span class="math inline">$\la$</span>)</a></li>
 <li><a href="#the-backward-view-of-tdla">7.3 The backward view of TD(<span class="math inline">$\la$</span>)</a></li>
 <li><a href="#equivalence-of-forward-and-backward-views">7.4 Equivalence of forward and backward views</a></li>
 </ul></li>
 <li><a href="#sarsala">7.5 Sarsa(<span class="math inline">$\la$</span>)</a><ul>
 <li><a href="#qla">7.6 <span class="math inline">$Q(\la)$</span></a><ul>
 <li><a href="#watkins-qla">Watkin’s <span class="math inline">$Q(\la)$</span></a></li>
 <li><a href="#pengs-qla">Peng’s <span class="math inline">$Q(\la)$</span></a></li>
 </ul></li>
 <li><a href="#replacing-traces">7.7 Replacing traces</a></li>
 <li><a href="#implementation-issues">7.8 Implementation issues</a></li>
 <li><a href="#variable-la">7.9 Variable <span class="math inline">$\la$</span></a></li>
 </ul></li>
 <li><a href="#planning-and-learning-with-tabular-methods">8 Planning and learning with tabular methods</a><ul>
 <li><a href="#models-and-planning">8.1 Models and planning</a></li>
 <li><a href="#integrating-planning-acting-and-learning">8.2 Integrating planning, acting, and learning</a></li>
 <li><a href="#when-the-model-is-wrong">8.3 When the model is wrong</a></li>
 <li><a href="#prioritized-sweeping">8.4 Prioritized sweeping</a></li>
 <li><a href="#full-vs.sample-backups">8.5 Full vs. sample backups</a></li>
 <li><a href="#trajectory-sampling">8.6 Trajectory sampling</a></li>
 <li><a href="#heuristic-search">8.7 Heuristic search</a></li>
 </ul></li>
 <li><a href="#deep-rl">Deep RL</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="references">References</h2>
<ul>
<li>Barton, Sutto.</li>
<li>Deep RL tutorial, ICML 2016.</li>
<li>Algorithms course Lecture 8</li>
<li>…</li>
</ul>
<h2 id="section">3</h2>
<p>Bellman optimality equations</p>
\begin{align}
v_*(s) &amp;= \max_\pi v_\pi(s)\\
q_*(s,a) &amp;= \max_\pi q_\pi(s,a)\\
v_*(s)&amp;= \max_{a\in \mathcal A(s)} \sum_{s'} p(s'|s,a) [r(s,a,s') + \ga v_*(s')\\
q_*(s) &amp;= \sum_{s'} p(s'|s,a) [r(s,a,s') +\ga \max q_*(s',a')].
\end{align}
<p>3 assumptions that are rarely all true in practice:</p>
<ol type="1">
<li>Know dynamics of environment</li>
<li>Have enough computational resources</li>
<li>Markov property</li>
</ol>
<p>Many decision-making methods attempt to approximately solve the Bellman optimal equations.</p>
<p>A RL algorithm puts more effort into learning good decisions for frequently encountered states.</p>
<h2 id="dynamic-programming">4 Dynamic programming</h2>
<p>Three classes of methods for solving MDP’s.</p>
<ol type="1">
<li>Dynamic programming
<ul>
<li>Well-developed mathematically</li>
<li>Require complete, accurate model of environment</li>
</ul></li>
<li>Monte Carlo methods
<ul>
<li>Don’t require model, conceptually simple</li>
<li>Not suitable for incremental computation</li>
</ul></li>
<li>Temporal-difference learning
<ul>
<li>Don’t require model, fully incremental</li>
<li>Complex to analyze</li>
</ul></li>
</ol>
<p>Think of other methods as attempts to achieve the same effect as DP with less computation and without a perfect model.</p>
<p>Use value functions to organize the search for good policies.</p>
<p>Iterative <strong>policy evaluation</strong>: (make value function consistent with current policy) <span class="math display">\[
v_{k+1}(s) = \sum_a \pi(a|s) \sum_{s'} p(s'|s,a) [r(s,a,s') + \ga v_k(s')].
\]</span> This is a full backup because we calculate <span class="math inline">\(v_k(s)\)</span> for all <span class="math inline">\(s\)</span> in each stage. (TODO: prove convergence.) Stop when maximum difference <span class="math inline">\(\max_{s\in S}|v_{k-1}(s)-v_k(s)|&lt;\te\)</span>.</p>
<p>(Can also update in-place. Then update order makes a difference.)</p>
<p><strong>Policy improvement theorem</strong>. If <span class="math inline">\(\pi,\pi'\)</span> are deterministic policies such that for all <span class="math inline">\(s\in S\)</span>, <span class="math display">\[q_\pi(s,\pi'(s))\ge v_\pi(s),\]</span> then <span class="math inline">\(\pi'\)</span> is at least as good as <span class="math inline">\(\pi\)</span>, <span class="math inline">\(v_{\pi'}(s) \ge v_\pi(s)\)</span>.</p>
<p><em>Proof</em>. Unfold and note convergence.</p>
<p>This shows that iterative policy improvement can only help: <span class="math display">\[
\pi'(s) = \amax_a \sum_{s'} p(s'|s,a) [r(s,a,s') + \ga v_\pi(s')].
\]</span> If it stops improving, then <span class="math inline">\(\pi'\)</span> is optimal.</p>
<p>Policy iteration: <span class="math inline">\(\pi_k\xra E v_{\pi_k} \xra I \pi_{k+1}\)</span>. Alternately evaluate and improve.</p>
<!-- assuming we know p's-->
<!-- Value iteration is when policy evaluation is stopped after one sweep.-->
<p>Above, to evaluate, we have to keep doing policy iterations until convergence. For speed, we just do one (or a few) step of policy evaluation. Combining the improvement and evaluation steps: <span class="math display">\[v_{k+1} := \max_a \sum_{s'} p(s'|s,a)[r(s,a,s') + \ga v_k(s')].\]</span> (cf. EM/AM?)</p>
<!-- Q: if this stabilizes, does it mean we have converged? p. 100-->
<p>Asynchronous DP: back up values of states in any order, using whatever values of other states are available. Can do with value iteration and policy iteration. Helps intermix computation with interaction (actually experiencing MDP—apply backups as agent visits states).</p>
<p>The time that DP methods take is polynomial in number of states and actions. LP can be used to solve MDP’s but become impractical at smaller number of states than DP.</p>
<!-- can such iterative methods solve LP? -->
<h2 id="monte-carlo-methods">5 Monte Carlo methods</h2>
<p>Don’t assume complete knowledge of the environment. Learn from online and simulated experience. The model only needs to generate sample transitions rather than give a complete probability distribution.</p>
<p>MC assumes experience is divided into episodes.</p>
<p>Each occurrence of a state in an episode is a <em>visit</em>.</p>
<ul>
<li>Every-visit MC: estimate <span class="math inline">\(v_\pi(s)\)</span> as the average of returns following all visits to <span class="math inline">\(s\)</span></li>
<li>First-visit MC: average just returns following first visits (in an episode) to <span class="math inline">\(s\)</span>.</li>
</ul>
<p>DP shows all possible transitions but the MC diagram only shows those sampled on one episode.</p>
<p>(Ex. of finding bubble surface given wire frame. Iterative = compute surface iteratively by averaging at each point. MC = take a random walk from each point; expected height at boundary is approximation of height. This is more efficient if we are interested in a small number of points.)</p>
<p>If the model is not available, it’s mre useful to estimate action than state values. (From the action-value function <span class="math inline">\(q\)</span> we can directly construct the greedy policy.)</p>
<p>Problem: if you follow a deterministic policy, you will only observe one action from each state.</p>
<p>Solution: Explore! Assume each state-action pair has a nonzero probability of being selected at the start of an episode (exploring starts). More realistic: consider policies that are stochastic with nonzero probability of selecting all actions.</p>
<p>This works assuming:</p>
<ol type="1">
<li>episodes have exploring starts</li>
<li>policy evaluation can be done with infinitely many episodes.
<ul>
<li>Instead: approximate and keep track of error bounds. But this requires many episodes.</li>
<li>Forgo policy evaluation: move value function toward <span class="math inline">\(q_{\pi_k}\)</span>.</li>
</ul></li>
</ol>
<p>MC ES cannot converge to any suboptimal policy (if it did, the value function converges to the value function for that policy, and the policy changes). <em>Open question</em>: does it always converge? [Tsitsiklis02]</p>
<h3 id="on-policy-mc-control">5.4 On-policy MC control</h3>
<p>Improve the policy used to make decisions.</p>
<p>Soft policy: <span class="math inline">\(\forall s,a\in \mathcal A(s), \pi(a|s)&gt;0\)</span>. <span class="math inline">\(\ep\)</span>-soft: <span class="math inline">\(\ge \fc{\ep}{|\mathcal A(s)|}\)</span>.</p>
<p>Policy iterations works for <span class="math inline">\(\ep\)</span>-soft policies.</p>
<ol type="1">
<li>The policy improvement theorem shows that for any <span class="math inline">\(\ep\)</span>-soft <span class="math inline">\(\pi\)</span>, the <span class="math inline">\(\ep\)</span>-greedy policy wrt <span class="math inline">\(q_\pi\)</span> is at least as good as <span class="math inline">\(\pi\)</span>.</li>
<li>Equality only when both <span class="math inline">\(\pi,pi'\)</span> are optimal among <span class="math inline">\(\ep\)</span>-soft. Use optimality for all policies, under the modified environment where an action is chosen randomly with probability <span class="math inline">\(\ep\)</span>.</li>
</ol>
<h3 id="evaluating-one-policywhile-following-an-other-off-policy-policy-evaluation">5.5 Evaluating One PolicyWhile Following An-other (Off-policy Policy Evaluation)</h3>
<p>What if we have episodes generated from a different policy?</p>
<ul>
<li><span class="math inline">\(\mu\)</span> behavior policy</li>
<li><span class="math inline">\(\pi\)</span> target policy.</li>
</ul>
Require ratio <span class="math inline">\(\pi/\mu\)</span> not be too large. Weight episodes by this ratio.
\begin{align}
V(s) &amp;= \fc{\sumo i{n_s} \fc{p_i(s)}{p_i'(s)}G_i(s)}{\sumo i{n_s} \fc{p_i(s)}{p_i'(s)}}\\
\fc{p_i(S_t)}{p_i'(S_t)}&amp;=\prod_{k=t}^{T_i(S_t)-1} \fc{\pi(A_k|S_k)}{\mu(A_k|S_k)}.
\end{align}
<h3 id="off-policy-mc-control">5.6 Off-policy MC control</h3>
<p>Ex. estimation policy may be deterministic while behavior policy samples all possible actions.</p>
<p>To do this, after generating an episode, look at the last time where <span class="math inline">\(A_\tau \ne \pi(S_\tau)\)</span>, and update <span class="math inline">\(Q\)</span> using pairs <span class="math inline">\((s,a)\)</span> appearing after that time.</p>
<p>Note: only learns from tails of episodes. Learning is slow if nongreedy actions are frequent.</p>
<h3 id="incremental-implementation">5.7 Incremental implementation</h3>
For <span class="math inline">\(V_n=\fc{\sumo k{n-1}W_kG_k}{\sumo k{n-1}W_k}\)</span> is
\begin{align}
V_{n+1} &amp;=V_n+\fc{W_n}{C_n} (G_n-V_n)\\
C_{n+1} &amp;=C_n+W_{n+1}.
\end{align}
<p>Advantages of MC:</p>
<ol type="1">
<li>learn optimal behavior directly from interaction without model</li>
<li>used with simulation</li>
<li>focus MC on small subset of states</li>
<li>less harmed by violations of Markov property.</li>
</ol>
<p>Maintaining sufficient exploration is an issue.</p>
<p>MC uses experience and does not bootstrap (update value based on other value estimates) (instead MC waits for a bunch of samples), unlike DP.</p>
<p>Next chapter: experience + bootstrap.</p>
<h2 id="temporal-difference-learning">6 Temporal-difference learning</h2>
<p>MC methods can incrementally update <span class="math inline">\(V\)</span>, after waiting to get the actual return, <span class="math display">\[V(S_t) \mapsfrom V(S_t) + \al [G_t-V(S_t)].\]</span> (n.b. <span class="math inline">\(S_t\)</span> is the state at time <span class="math inline">\(t\)</span>, not the state labeled <span class="math inline">\(t\)</span>.) TD methods only wait until the next time step. <span class="math display">\[V(S_t) \mapsfrom V(S_t) + \al [R_{t+1} + \ga V(S_{t+1})-V(S_t)].
\]</span> I.e., it uses the estimate of <span class="math inline">\(v_\pi = \EE_\pi [R_{t+1}+\ga v_\pi(S_{t+1})|S_t=s]\)</span> as a target.</p>
<p>TD samples expected values and uses the current estimate <span class="math inline">\(V\)</span> instead of <span class="math inline">\(v_\pi\)</span>.</p>
<p>Each estimate is shifted towards the estimate that immediately follows it.</p>
<p>p. 133: driving home example.</p>
<p>In practice, TD methods converge faster than constant-<span class="math inline">\(\al\)</span> MC methods on stochastic tasks.</p>
<h3 id="optimality-of-td0">6.3 Optimality of TD(0)</h3>
<p>Finite amount of experience: present it repeatedly until method converges. (cf. SGD?) Do batch updates.</p>
<p>Ex. p.139 example is enlightening.</p>
<p>Batch Monte Carlo methods always find the estimates that minimize mean-squared error on the training set, whereas batch TD(0) always finds the estimates that would be exactly correct for the maximum-likelihood model of the Markov process.</p>
<!-- ? certainty-equivalence estimate-->
<h3 id="sarsa-on-policy-td-control">6.4 Sarsa: On-policy TD control</h3>
<p>For state-action pairs: <span class="math display">\[Q(S_t,A_t) \mapsfrom Q(S_t,A_t) + \al [R_{t+1} + \ga Q(S_{t+1}, A_{t+1})-Q(S_t,A_t)].
\]</span> (<span class="math inline">\(A_{t+1}\)</span> is the action chosen by the (<span class="math inline">\(\ep\)</span>-greedy?<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>) policy on <span class="math inline">\(S_{t+1}\)</span>.) SARSA refers to <span class="math inline">\((S_t,A_t,R_{t+1},S_{t+1},A_{t+1})\)</span>.</p>
<p>(Convergence guarantees: p. 142)</p>
<p>SARSA can learn during the episode!</p>
<h3 id="q-learning-off-policy-td-control">6.5 Q-learning: Off-policy TD control</h3>
<p>One-step Q-learning</p>
<p><span class="math display">\[
Q(S_t,A_t)\leftarrow Q(S_t,A_t) + \al [R_{t+1} + \ga \max_a Q(S_{t+1},a) - Q(S_t,A_t)].
\]</span></p>
<p><span class="math inline">\(Q\)</span> approximates the optimal <span class="math inline">\(q_*\)</span> <em>independently of the policy being followed</em>!</p>
<p>TODO: prove this. [Watkins Dayan 1992] [JJS94] [T94]</p>
<p>Ex. cliff</p>
<ul>
<li>SARSA learns the safe path steering clear of the cliff (because it takes account the <span class="math inline">\(\ep\)</span>-greedy exploration).</li>
<li>Q-learning learns the short path at the edge of the cliff (the optimal).</li>
</ul>
<h3 id="games">6.6 Games</h3>
<p>A conventional state-value function evaluates states in which the agent has the option of selecting an action (arrived at <span class="math inline">\(s'\)</span> where you will select a new <span class="math inline">\(a'\)</span>), but the state-value function in games of perfect information evaluates the board after the agent has mades its move—afterstates.</p>
<p>This is more efficient: many position-move pairs produce the same resulting position.</p>
<p>Often it’s useful to break the environment’s dynamics into</p>
<ul>
<li>immediate effect of action (deterministic) and</li>
<li>unknown random processes.</li>
</ul>
<h2 id="eligibility-traces">7 Eligibility traces</h2>
<p>A general mechanism that can be combined with any TD method.</p>
<p>2 views:</p>
<ol type="1">
<li>(theoretical, forward) bridge from TD to MC. (understand what is computed)</li>
<li>(mechanistic, backward) temporary record of event occurrence. Only “eligible” states/actions are assigned credit/blame. (intuition for algorithms)</li>
</ol>
<h3 id="n-step-td-prediction">7.1 n-step TD prediction</h3>
<p>Perform backup based on an intermediate number of rewards. (MC depends on entire sequence until end of episode, TD depends on 1 next reward.)</p>
Replace the 1-step target by <span class="math inline">\(n\)</span>-step target (corrected <span class="math inline">\(n\)</span>-step truncated return).
\begin{align}
G_t^{(1)} &amp;= R_{t+1} + \ga V(S_{t+1})\\
G_t^{(n)} &amp;= \sumo i{n-1} \ga^{i-1} R_{t+i} + \ga^n V(S_{t+n})\\
\De V_t(S_t) &amp;= \al [G_t^{(n)} - V_t(S_t)].
\end{align}
<p>MC methods are infinite-step returns.</p>
<p>2 ways to update.</p>
<ul>
<li>Online updating: during episode.</li>
<li>Offline updating: accumulated and not used to change estimates until the end.</li>
</ul>
<p>Error-reduction property of <span class="math inline">\(n\)</span>-step returns: (exponential reduction in worst-case error) <span class="math display">\[
\max_s\ab{\EE_\pi\ba{G_t^{(n)} | S_t=s}-v_\pi(s)}
\le \ga^n \max_s |v(s) - v_\pi(s)|.
\]</span></p>
<p>Rarely used because inconvenient to implement; waiting <span class="math inline">\(n\)</span> states is problematic.</p>
<h3 id="the-forward-view-of-tdla">7.2 The forward view of TD(<span class="math inline">\(\la\)</span>)</h3>
<p>Complex backup: Backup can be done toward any average of <span class="math inline">\(n\)</span>-step returns. <!--$G_t^{avg}= \rc2 G_t^{(2)} + \rc2 G_t^{(4)}$.--></p>
<p><span class="math inline">\(TD(\la)\)</span> is averaging <span class="math inline">\(n\)</span>-step backups <span class="math inline">\(\propto \la^{n-1}\)</span>, i.e., with weights <span class="math inline">\((1-\la)\la^{n-1}\)</span>. The <span class="math inline">\(\la\)</span>-return is <span class="math display">\[
G_t^\la = (1-\la) \sumo n\iy \la^{n-1}G_t^{(n)}.
\]</span></p>
<ul>
<li><span class="math inline">\(\la=1\)</span> reduces to MC.</li>
<li><span class="math inline">\(\la=0\)</span> reduces to <span class="math inline">\(G_t^{(1)}\)</span>, one-step return.</li>
</ul>
<p>Forward view: look forward to all future rewards and decide how to combine them.</p>
<!-- don't you have to wait until end of episode?-->
<h3 id="the-backward-view-of-tdla">7.3 The backward view of TD(<span class="math inline">\(\la\)</span>)</h3>
<p>The <strong>eligibility trace</strong> is a memory variable associated with each state. It is a <em>random variable</em> <span class="math inline">\(Z_t(s)\in \R^+\)</span>, <span class="math display">\[
Z_t(s) = \begin{cases} 
\ga \la Z_{t-1}(s), &amp;\text{if }s\ne S_t\\
\ga \la Z_{t-1}(s) + 1, &amp;\text{if }s=S_t.
\end{cases}
\]</span> <span class="math inline">\(\la\)</span> is the trace-decy parameter. Traces indicate the degree to which each state is eligible for undergoing learning changes. <span class="math display">\[\De V_t(s) = \al \ub{(R_{t+1}+\ga V_tS_{t+1} - V_t(S_t))}{\de_t} Z_t(s).\]</span> <!-- Q: \ga\la (Z+1) or \ga\la Z + 1? latter one. They update before multiplying by $\ga\la$--></p>
<p>Note TD(1) is MC but it can also be applied to discounted continuing tasks, and can be applied incrementally and online.</p>
<h3 id="equivalence-of-forward-and-backward-views">7.4 Equivalence of forward and backward views</h3>
<!-- what if also influence actions/policy? -->
<p>For offline updating, <span class="math display">\[\sumz t{T-1} \De V_t^{TD}(s) = 
\sumz t{T-1} \al \de_t \sumz kt (\ga \la)^{t-k} I_{sS_k} =
\sumz t{T-1} V_t^\la(S_t)I_{sS_t}.\]</span> (Online updating is different because <span class="math inline">\(V_t\)</span> changes as <span class="math inline">\(t\)</span>.)</p>
<p>Online algorithm generally works better over a broader range of parameters.</p>
<p>To maintain equivalence in online case, <span class="math inline">\(\de_t= R_{t+1} + \ga V_t(S_{t+1})-V_{t-1}(S_t)\)</span>, <span class="math inline">\(R_t^{(n)} = \sumo in \ga^{i-1}R_{t+i} + \ga^n V_{t+n-1}(S_{t+n})\)</span>.</p>
<h2 id="sarsala">7.5 Sarsa(<span class="math inline">\(\la\)</span>)</h2>
How to use eligibility traces for control? Learn <span class="math inline">\(q\)</span> rather than <span class="math inline">\(V\)</span>. Keep a trace for each <span class="math inline">\((s,a)\)</span>.
\begin{align}
Q_{t+1}(s,a) &amp;= Q_t(s,a) + \al \de_t Z_t(s,a)\\
\de_t &amp;= R_{t+1} + \ga Q_t(S_{t+1}, A_{t+1}) - Q_t(S_t,A_t)\\
Z_t(s,a) &amp;= \begin{cases}
\ga \la Z_{t-1} (s,a) + 1, &amp;\text{if }s=S_t, a=A_t\\
\ga \la Z_{t-1} (s,a), &amp;\text{otherwise.}
\end{cases}
\end{align}
<h3 id="qla">7.6 <span class="math inline">\(Q(\la)\)</span></h3>
<h4 id="watkins-qla">Watkin’s <span class="math inline">\(Q(\la)\)</span></h4>
<p>We can use subsequent experience only as long as the greedy policy is being followed! Look ahead as far as the next exploratory action.</p>
<p>If <span class="math inline">\(A_{t+n}\)</span> is first exploratory action, the longest backup is toward <span class="math display">\[
 \sumo in \ga^{i-1}R_{t+i} + \ga^n \max_a Q_t(s_{t+n},a).
\]</span> Q: why 1 step past?</p>
<p>Set eligibility traces to 0 whenever an exploratory action is taken.</p>
<p>But cutting off traces every time an exploratory action is taken loses the advantage of eligibility traces.</p>
<h4 id="pengs-qla">Peng’s <span class="math inline">\(Q(\la)\)</span></h4>
<p>Hybrid of Sarsa and Watkin’s <span class="math inline">\(Q(\la)\)</span>.</p>
<p>Converges to some hybrid of <span class="math inline">\(q_\pi, q_*\)</span>. If made gradually more greedy, may still converge to <span class="math inline">\(q_*\)</span>.</p>
<p>Cannot be implemented as simply.</p>
<p>Naive <span class="math inline">\(Q(\la)\)</span>: traces not set to 0.</p>
<!-- formula? -->
<h3 id="replacing-traces">7.7 Replacing traces</h3>
<p><span class="math display">\[Z_t (s) = \begin{cases}
\ga \la Z_{t-1}(s), &amp; s\ne S_t\\
1, &amp; s=S_t.
\end{cases}\]</span> (Max out at 1.)</p>
<p>Can produce significant improvement in learning rate.</p>
<p>Ex. when this helps: reward at rightmost. Sequence WWWWWWWWWWRRRR (wrong stays, right goes right) reinforces W more times.</p>
<p>Another solution: set the traces of all other actions from the revisited state to 0.</p>
<!-- check 7.12-->
<h3 id="implementation-issues">7.8 Implementation issues</h3>
<p>Larger computational cost.</p>
<h3 id="variable-la">7.9 Variable <span class="math inline">\(\la\)</span></h3>
<p>Never been used practically, but interesting/useful theoretically.</p>
<p>Ex. Vary as function of state: depend on confidence</p>
<p><span class="math display">\[G_t^\la = \sum_{k=t+1}^{T-1} G_t^{(k-t)} (1-\la_k) \prod_{i=t+1}^{k-1} \la_i + G_t\prod_{i=t+1}^{T-1}\la_i.\]</span></p>
<h2 id="planning-and-learning-with-tabular-methods">8 Planning and learning with tabular methods</h2>
<p>Planning requires a model of the environment; learning methods don’t.</p>
<h3 id="models-and-planning">8.1 Models and planning</h3>
<p>A model is anything an agent cn use to predict how the environment responds to its actions.</p>
<ul>
<li>Distribution model (e.g. needed in DP)</li>
<li>Sample model</li>
</ul>
<p>Models can simulate experience.</p>
<p>Planning takes a model as input and produces or improves a policy.</p>
<p>Approaches:</p>
<ul>
<li>State-space planning: search through state-space for optimal policy or path to goal.</li>
<li>Plan-space planning: search through space of plans.
<ul>
<li>Evolutionary methods</li>
<li>Partial-order planning</li>
</ul></li>
</ul>
<p>State-space planning methods share common structure.</p>
<ol type="1">
<li>Involve computing value functions</li>
<li>Compte value functions by backup applied to simulated experience.</li>
</ol>
<p>Go from… to…</p>
<ol type="1">
<li>model</li>
<li>simulated experience</li>
<li>values</li>
<li>policy</li>
</ol>
<p>Common structure means many ideas/algorithms can be transferred between planning and learning. (They differ only in source of experience.)</p>
<p>Planning in small incremental steps helps intermix plannig with acting/learnig.</p>
<h3 id="integrating-planning-acting-and-learning">8.2 Integrating planning, acting, and learning</h3>
<ul>
<li>New information from interaction may change the model and interact with planning.</li>
<li>Customize planning to the states/decisions under consideration or expect in the near future.</li>
</ul>
<p>Two roles for experience:</p>
<ol type="1">
<li>improve model (model-learning) - make fuller use of limited experience</li>
<li>directly improve value function (direct reinforcement learning) - simpler, not affect by model biases</li>
</ol>
<p>Dyna-Q: repeat</p>
<ol type="1">
<li>Run (<span class="math inline">\(\ep\)</span>-greedy) Q-learning for 1 step.</li>
<li>Update model (assume deterministic environment)</li>
<li>Repeat <span class="math inline">\(n\)</span> times Q-learning using simulated experience. (Take a random <span class="math inline">\((S,A)\)</span> pair that has been observed.)</li>
</ol>
<!-- what if instead of greedy, use temperature? -->
<p>Q: why is only 1 step added without planning? A: because rewards only go back 1 step.</p>
<h3 id="when-the-model-is-wrong">8.3 When the model is wrong</h3>
<ul>
<li>Sometimes the suboptimal policy computed leads to discovery/correction of the modeling error. Happens when model is optimistic.</li>
<li>Difficulties arise when the environment changes to become better and the formerly correct policy does not reveal the improvement.</li>
</ul>
<p>Dyna-Q+: This agent keeps track for each state{action pair of how many time steps have elapsed since the pair was last tried in a real interaction with the environment. The more time that has elapsed, the greater (we might presume) the chance that the dynamics of this pair has changed and that the model of it is incorrect. Bonus reward for long-untried actions, <span class="math inline">\(R+\ka \sqrt \tau\)</span> if not tried for <span class="math inline">\(\tau\)</span> time steps.</p>
<h3 id="prioritized-sweeping">8.4 Prioritized sweeping</h3>
<p>Planning can be more efficient if simulated transitions and backups are focused on particular state-action pairs.</p>
<p>Work backwards from goal state/reward?</p>
<p>Ex. change estimated value of 1 state. The only useful 1-state backups are those of actions leading directly into that state. Propagate.</p>
<p>Prioritize backups according to urgency and perform in order of priority.</p>
<ol type="1">
<li>Execute 1 step of Q-learning</li>
<li>If the update is <span class="math inline">\(&gt;\te\)</span>, insert <span class="math inline">\((S,A)\)</span> into the priority queue with priority |update|.</li>
<li>Repeat <span class="math inline">\(n\)</span> steps: take <span class="math inline">\((S,A)\)</span> from the top of the queue. Update it. Then for all <span class="math inline">\((\ol S, \ol A)\)</span> predicted to lead to <span class="math inline">\(S\)</span>, update. If difference is <span class="math inline">\(&gt;\te\)</span>, insert into queue.</li>
</ol>
<p>Problem: algorithms rely on the assumption of discrete states.</p>
<p>For stochastic environments: keep counts. Backup each pair with a full backup.</p>
<h3 id="full-vs.sample-backups">8.5 Full vs. sample backups</h3>
<p>1-step backups vary in 3 ways.</p>
<ol type="1">
<li>State vs. action values</li>
<li>Estimate value for optimal policy vs. arbitrary policy.</li>
<li>Full vs. sample backups.</li>
</ol>
<p>Sample backups are TD(0), Sarsa, and Q-learning.</p>
<p>Full backup requires <span class="math inline">\(b\)</span> times as much computation, <span class="math inline">\(b\)</span> the branching factor.</p>
<p>Sample backups have the advantage that the values backed up from successor states will be more accurate.</p>
<h3 id="trajectory-sampling">8.6 Trajectory sampling</h3>
<p>2 ways of distributing backups</p>
<ol type="1">
<li>(classical, DP, exhaustive) sweep through entire state space, backing up each state (or state-action pair) once per sweep.</li>
<li>sample from state/state-action space according to distribution, ex. according to on-policy distribution. Simulate trajectories and perform backups at those encountered. (This ignores large, uninteresting parts of the space.)</li>
</ol>
<p>Experimentally: sampling according to on-policy results in faster planning initially and retarded planning in long run. (In the long run, commonly occurring states already have correct values.)</p>
<h3 id="heuristic-search">8.7 Heuristic search</h3>
<p>Heuristic search is concerned with policy computation, i.e., making improved action selections given the current value function.</p>
<p>Unlike in usual heuristic search, it’s natural to consider allowing the value function to be improved.</p>
<p>When we have a perfect model and imperfect <span class="math inline">\(Q\)</span>, deeper search usually yields better policies.</p>
<p>Grow the search tree selectively. How to use this idea for backups?</p>
<p>Focus on the states and actions that might immediately follow the current state.</p>
<h2 id="deep-rl">Deep RL</h2>
<p>“AI = RL + DL”: RL defines objective, DL gives mechanism.</p>
<!-- value, policy, model-based -->
<p>Use deep NN to represent value function, policy, and model.</p>
<ul>
<li><p>Value: <span class="math inline">\(Q\)</span>-network <span class="math inline">\(Q(s,a,w)\)</span>. (either accept <span class="math inline">\(a\)</span> as input, or list out <span class="math inline">\(Q\)</span> for all values of <span class="math inline">\(a\)</span>)</p>
Tread <span class="math inline">\(r+\ga \max_{a'} Q(s',a',w)\)</span> as target. Minimize <span class="math display">\[I = (r+\ga \max_a Q(s',a',w) - Q(s,a,w))^2.\]</span>
<ul>
<li>Converges using lookup table representation, but diverges using NN due to
<ul>
<li>correlations between samples
<ul>
<li>To remove correlations, build data-set from agent’s own experience</li>
</ul></li>
<li>non-stationary targets
<ul>
<li>Hold target parameters <span class="math inline">\(w^-\)</span> fixed <span class="math display">\[I = (r+\ga \max_a Q(s',a',w^-) - Q(s,a,w))^2.\]</span></li>
</ul></li>
</ul></li>
<li>Ex. Atari: Stack of 4 previous frames <span class="math inline">\(4\times 84\times 84\)</span>, conv layers. Output for 18 joystick positions. Reward is change in score.</li>
<li>Double DQN: Remove upward bias caused by <span class="math inline">\(\max_a Q(s,a,w)\)</span> (?). Use current network to select actions and older network <span class="math inline">\(w^-\)</span> to evaluate actions. <span class="math display">\[ I = \pa{ r + \ga Q(s', \amax_{a'} Q(s', a', w), w^-) - Q(s,a,w)}^2.\]</span></li>
<li>Prioritized replay: weight experience according to surprise. Priority queue according to <span class="math display">\[ |r + \ga \max_{a'} Q(s',a', w^-) - Q(s,a,w)|.\]</span></li>
<li>Duelling network: Split <span class="math inline">\(Q\)</span>-network into 2 channels, <span class="math inline">\(Q(s,a) = V(s,v) + A(s,a,w)\)</span>.
<ul>
<li>Action-independent value function <span class="math inline">\(V(s,v)\)</span></li>
<li>Action-dependent advantage function <span class="math inline">\(A(s,a,w)\)</span>.</li>
</ul></li>
<li>Asynchronous: execute instances in parallel.</li>
</ul></li>
<li>Policy: stochastic <span class="math inline">\(\pi(a|s,u)\)</span>, deterministic <span class="math inline">\(a=\pi(s,u)\)</span>, objective <span class="math display">\[ L(u) = \E\ba{\sumo n{\iy} \ga^{n-1}r_n|\pi(\cdot, u)}.\]</span>
<ul>
<li>Actor-critic: Estimate <span class="math inline">\(Q(s,a,w)\)</span> and update <span class="math inline">\(u\)</span> by SG ascent. (What more does this gain from just having Q-network? See below.)</li>
<li>A3C, Asynchronous Advantage Actor-Critic: Estimate <span class="math inline">\(V\)</span>, <span class="math inline">\(Q\)</span> value estimated from <span class="math inline">\(V\)</span>, actor updated towards target, critic updated.</li>
<li>High-dimensional continuous action spaces: Can’t compute <span class="math inline">\(\max_a Q(s,a)\)</span>, learn without max. <span class="math inline">\(Q\)</span> differentiable wrt <span class="math inline">\(a\)</span>.
<ul>
<li>DPG <span class="math display">\[I_w = \pa{r + \ga Q(s', \pi(s', u^-), w^-) - Q(s,a,w)}^2.\]</span> Actor improves <span class="math inline">\(Q\)</span>. Critic provides loss function for actor.</li>
</ul></li>
</ul></li>
<li><p>Model - ???</p></li>
</ul>
<p>Notes:</p>
<ul>
<li>Can use LSTM (how?)</li>
<li>Simulated physics.</li>
<li>Fictitious self-play. Nash equilibria in multi-agent games?</li>
</ul>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Why not greedy?<a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[PMDH16] Convolutional Patch Representations for Image Retrieval - an Unsupervised Approach</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/PMDH16.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/PMDH16.html</id>
    <published>2016-09-26T00:00:00Z</published>
    <updated>2016-09-26T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[PMDH16] Convolutional Patch Representations for Image Retrieval - an Unsupervised Approach</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-09-26 
          , Modified: 2016-09-26 
	</p>
      
       <p>Tags: <a href="/tags/neural%20net.html">neural net</a>, <a href="/tags/vision.html">vision</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#background">Background</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>We write things in the continuous limit. In actuality we have to discretize.</p>
Define the kernel <span class="math inline">\(K:(\Om \to \R)\times (\Om\to \R) \to \R\)</span> by
\begin{align}
K(M,M') = \sum_{z,z'\in \Om} e^{-\rc{2\be^2}\ve{z-z'}^2} \ka (P_z, P_{z'}')\\
\ka(P_z,P_{z'}') &amp; = \ve{P_z}\ve{P_{z'}'} e^{-\rc{2\al^2} \ve{\wt P_z - \wt P_{z'}^2}^2
\end{align}
<p>where <span class="math inline">\(P_z =M_{z+[0,e)\times [0,e)\times [0,d)}\)</span>.</p>
<h2 id="background">Background</h2>
<p>A <a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space">reproducing kernel Hilbert space (RKHS)</a> is a Hilbert space of functions <span class="math inline">\(X\to \R\)</span> in which point evaluation <span class="math inline">\(L_x[f] = f(x)\)</span> is a continuous linear functional. (Equivalently, it is bounded, <span class="math inline">\(f(x)\le M_x\ve{f}_H\)</span>.</p>
<ul>
<li><span class="math inline">\(L^2\)</span> is not a RKHS—it consists of equivalence classes of functions.</li>
<li><span class="math inline">\(H=\set{f\in L_2(\R)}{\Supp(\phi)\subeq [-a,a]}\)</span> is RKHS. Here <span class="math inline">\(K_x(y) = \fc{\sin(a(y-x))}{\pi(y-x)}\)</span> (bandlimited Dirac delta).</li>
</ul>
<p><strong>Theorem</strong> A Hilbert space of functions is a RKHS iff for every <span class="math inline">\(x\in X\)</span>, there exists <span class="math inline">\(K_x\)</span> such that <span class="math inline">\(g(x) = \an{g, K_x}\)</span>.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<p><em>Proof</em>. <span class="math inline">\(\Leftarrow\)</span>: inner product is continuous. <span class="math inline">\(\Rightarrow\)</span>: Riesz representation.</p>
<p>Note <span class="math inline">\(K_x(y) = \an{K_x,K_y}=:K(x,y)\)</span>. <span class="math inline">\(K\)</span> is symmetric and positive definite. (Think of <span class="math inline">\(K\)</span> as <span class="math inline">\(X_0^\R\times X_0^\R\to \R\)</span> where <span class="math inline">\(X_0^\R\)</span> is the set of functions <span class="math inline">\(X\to \R\)</span> nonzero only at a finite number of <span class="math inline">\(x\)</span>’s. I.e. formal span of <span class="math inline">\(X\)</span>.)</p>
<p><a href="https://en.wikipedia.org/wiki/Representer_theorem">Representer theorem</a> states that every function in an RKHS that minimises an empirical risk function can be written as a linear combination of the kernel function evaluated at the training points</p>
<p><strong>Theorem (Moore-Aronszajn)</strong>. Suppose <span class="math inline">\(K\)</span> is a symmetric, positive definite kernel on a set <span class="math inline">\(X\)</span>. Then there is a unique Hilbert space of functions on <span class="math inline">\(X\)</span> for which <span class="math inline">\(K\)</span> is a reproducing kernel.</p>
<p><em>Proof</em>. Extend <span class="math inline">\(K\)</span> by bilinearity to <span class="math inline">\(\spn(X)\)</span>. Take the completion. (Extend to functions <span class="math inline">\(f=\sumo i{\iy} a_i K_{x_i}(x)\)</span> where <span class="math inline">\(\sumo i{\iy} a_i^2 K(x_i,x_i)\)</span>.</p>
<p>(Question: how to realize this inner product as an integral?)</p>
<p>See also: Mercer’s theorem</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Letting <span class="math inline">\(f_x=\delta_x\)</span> is cheating—we want actual functions, not distributions.<a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2016-10-01</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-10-01.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-10-01.html</id>
    <published>2016-09-26T00:00:00Z</published>
    <updated>2016-09-26T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2016-10-01</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-09-26 
          , Modified: 2016-09-26 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#threads">Threads</a></li>
 <li><a href="#meeting-with-arora">Meeting with Arora</a></li>
 <li><a href="#thoughts-about-pmi">Thoughts about PMI</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="threads">Threads</h2>
<ul>
<li>PMI - get some results!</li>
<li>CKN - understood theory, RKHS (9/26)</li>
<li>SoS - lectures 2 and 3</li>
<li>DL: do experiments suggested in <a href="/posts/tcs/machine_learning/matrices/DL_generalization.html">DL generalization</a>
<ul>
<li>NN learns DL? Preliminary calculations.</li>
</ul></li>
<li>Stability of SGD
<ul>
<li>@Elad: do local minima generalize?</li>
</ul></li>
<li>RL and cousins: see below. <a href="/posts/tcs/machine_learning/reinforcement_learning/rl.html">Notes</a></li>
</ul>
<h2 id="meeting-with-arora">Meeting with Arora</h2>
<p>Directions</p>
<ul>
<li>Learning noisy or-nets with 3-tensors (<span class="math inline">\(n^4\)</span> time) (<span class="citation" data-cites="Andrej">@Andrej</span>, <span class="citation" data-cites="Tengyu">@Tengyu</span>)</li>
<li>Matrix factorization assuming separability</li>
<li>Mike Collins NLP. Words as features.
<ul>
<li>Logliear/NN</li>
<li>For bigram classifiers, it’s fine to use word embeddings. (<span class="citation" data-cites="Tengyu">@Tengyu</span>)</li>
</ul></li>
<li>Systems with memory
<ul>
<li>Learning HMM’s with tensor methods</li>
<li>Reinforcement learning (MDP’s)
<ul>
<li>Theoretical algorithms are polynomial in state size and mixing time.</li>
<li>What if we have a succinct reprsentation? <span class="citation" data-cites="Mengdi">@Mengdi</span> Wang. Vector in <span class="math inline">\(\R^n\)</span>.
<ul>
<li>Stochastic Primal-Dual Methods and Sample Complexity of Markov Decision Process. (recent results on learning in MDPs via a primal-dual approach, and new ideas on finding compact representations of MDPs via low-rank approximation.)</li>
</ul></li>
</ul></li>
<li>cf. contextual bandits (<span class="citation" data-cites="Elad">@Elad</span>, <span class="citation" data-cites="Karan">@Karan</span>)
<ul>
<li><a href="http://www.research.rutgers.edu/~lihong/pub/Li10Contextual.pdf">LCLS10 A Contextual-Bandit Approach to Personalized News Article Recommendation</a></li>
<li><a href="http://www.cs.columbia.edu/~djhsu/papers/amo.pdf">DHKK Efficient Optimal Learning for Contextual Bandits</a></li>
<li><a href="http://www.cs.columbia.edu/~djhsu/papers/poshmm-tacl.pdf">SCH Unsupervised Part-Of-Speech Tagging with Anchor Hidden Markov Models</a></li>
<li><a href="http://www.cs.columbia.edu/~djhsu/papers/hmm-jcss-final.pdf">A spectral algorithm for learning Hidden Markov Models</a></li>
</ul></li>
</ul></li>
<li>Representation learning
<ul>
<li>Is PCA/SVD “complete” for classification? <a href="https://www.quora.com/How-can-PCA-be-used-as-a-pre-processing-step-for-classification">PCA as preprocessing for classification</a></li>
</ul></li>
<li>PMI for images
<ul>
<li>How is it NMF? (<span class="citation" data-cites="Tengyu">@Tengyu</span>)</li>
</ul></li>
<li>Dictionary learning
<ul>
<li>Relax incoherence. Only correlated with <span class="math inline">\(n^\de\)</span> others.</li>
<li>@Elad: Training on top of improperly learned dictionary.</li>
</ul></li>
</ul>
<h2 id="thoughts-about-pmi">Thoughts about PMI</h2>
<p>(9/28)</p>
<p>In the CKN, given that one layer is <span class="math inline">\(x\)</span>, the next layer (before pooling) is computed as <span class="math inline">\(y_i=(e^{v_i^x+b_i})_i\)</span> for some <span class="math inline">\(v_i,b_i\)</span>. We have that the dimension of <span class="math inline">\(y\)</span> is larger than the dimension of <span class="math inline">\(x\)</span>.</p>
<p>This looks very much like in PMI for word vectors, where the probability of word with vector <span class="math inline">\(v\)</span> given context <span class="math inline">\(x\)</span> is <span class="math inline">\(e^{-v^Tx}\)</span>, and the low-rank approximation to the PMI matrix recovers the <span class="math inline">\(v\)</span>’s.</p>
<p>But does that mean applying weighted SVD for PMI for the CKN feature vectors is somehow just trying to recover the <span class="math inline">\(v_i\)</span>? In that case the dimension reduction would just be going from <span class="math inline">\(y\)</span> back to <span class="math inline">\(x\)</span>, which doesn’t help classification.</p>
<p>(This doesn’t take into account the Gaussian pooling though.)</p>
<p>What would be the “test” for interpretability? For word embeddings, the test was analogy completion.</p>
<p>If the PMI matrix is low-rank, then what do we get beyond the fact that the feature vectors (7200-dim) came from a lower-dimensional (28x28) space? (In what sense would we expect the dimension-reduced feature vectors to be more interpretable than the original image?)</p>
<p>TODO:</p>
<ul>
<li>Try thresholding + WSVD + SVM.</li>
<li>Do pipeline (data exploration) with other MNIST and CIFAR architectures.</li>
<li>Think about the interpretability test above.</li>
<li><span class="citation" data-cites="Tengyu">@Tengyu</span> on interpreting CKN as NMF.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
