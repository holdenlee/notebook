<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2016-02-29T00:00:00Z</updated>
    <entry>
    <title>[SD15] Minimax rates for memory-bounded sparse linear regression</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/SD15.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/SD15.html</id>
    <published>2016-02-29T00:00:00Z</published>
    <updated>2016-02-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[SD15] Minimax rates for memory-bounded sparse linear regression</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-02-29 
          , Modified: 2016-02-29 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Scraps</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/complexity/scraps.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/complexity/scraps.html</id>
    <published>2016-02-29T00:00:00Z</published>
    <updated>2016-02-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Scraps</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-02-29 
          , Modified: 2016-02-29 
	</p>
      
       <p>Tags: <a href="/tags/scratch.html">scratch</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <p>Known that there is <span class="math inline">\(A\)</span>, <span class="math inline">\(BQP^A\nsubeq MA^A\)</span>, but not for <span class="math inline">\(AM\)</span>. AM, MA, NP?</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[Rem16] The Hilbert Function, Algebraic Extractors, and Recursive Fourier Sampling</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/complexity/Rem16.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/complexity/Rem16.html</id>
    <published>2016-02-29T00:00:00Z</published>
    <updated>2016-02-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[Rem16] The Hilbert Function, Algebraic Extractors, and Recursive Fourier Sampling</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-02-29 
          , Modified: 2016-02-29 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#recursive-fourier-sampling">Recursive Fourier sampling</a></li>
 <li><a href="#vc-dimension">VC dimension</a></li>
 <li><a href="#algebraic-geometry">Algebraic geometry</a></li>
 <li><a href="#versatile-functions">Versatile functions</a></li>
 <li><a href="#rfs">RFS</a></li>
 <li><a href="#todo">Todo</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>Summary: Define an algebraic measure of randomness (<span class="math inline">\(\de\)</span>-versatility for <span class="math inline">\(U\)</span>), such that a versatile function is an algebraic extractor. RFS is versatile, with applications to <span class="math inline">\(BQP^A\stackrel{?}{\subeq} ?^A\)</span>. #Extractors</p>
<p>Problem: Find extractors for algebraic sets of degree <span class="math inline">\(d\)</span> and density <span class="math inline">\(\ge \rh\)</span>, i.e., for sources uniform over sets of the form <span class="math inline">\(V(f_1,\ldots, f_t)\)</span> (common zeros), <span class="math inline">\(\deg(f_i)\le d\)</span>.</p>
<p>Previous results: [Dvi12] gives explicit extractors for</p>
<ul>
<li><span class="math inline">\(|\F|=\poly(d)\)</span>, <span class="math inline">\(\rh=2^{-\fc n2}\)</span>,</li>
<li><span class="math inline">\(|\F|=d^{\Om(n^2)}\)</span>, small density.</li>
<li>[CT13] degree 2, at most <span class="math inline">\((\ln \ln n)^{\rc{2e}}\)</span> polynomials; disperser for <span class="math inline">\(t\)</span> polys of degree <span class="math inline">\(\le (1-o(1))\fc{\ln\pf nt}{\ln^{0.9}n}\)</span>.</li>
</ul>
<p><strong>Theorem 1</strong>: Any <span class="math inline">\(\de\)</span>-versatile function is an extractor for algebraic sets. Parameters: <span class="math inline">\(\de\ge \fc n2-n^{\ga}\)</span>, <span class="math inline">\(d\le n^\al\)</span>, <span class="math inline">\(\rh\ge 2^{-n^{\be}}\)</span>, bias <span class="math inline">\(O\pf{n^\ga+d\ln \pf{\sqrt n}{\rh}}{\sqrt n}\)</span>. Proof exploits structure of sets of zeros.</p>
<h2 id="recursive-fourier-sampling">Recursive Fourier sampling</h2>
<p>Used to find <span class="math inline">\(A\)</span>, <span class="math inline">\(BQP^A\nsubeq \NP^A\)</span>.</p>
<p>Problem: Find larger class <span class="math inline">\(C\)</span> for which <span class="math inline">\(BQP^A\nsubeq C^A\)</span>.</p>
<p>Technique: connection between relativized separations from the polynomial hierarchy and lower bounds against constant depth circuits [FSS84],[Yao85]. “Here, the key idea is to reinterpret the 9 and 8 quantifiers of a PH machine as OR and AND gates.”</p>
<p>?: Don’t understand the part on poly approx.</p>
<p>Problem: What is the lowest degree polynomial <span class="math inline">\(/\F_2\)</span> representing <a href="RFS.html">recursive Fourier sampling</a>?</p>
<p><strong>Theorem 2</strong>: <span class="math inline">\(n=2^k-1\)</span>. No poly of degree <span class="math inline">\(&lt;\pf{n+1}2^h\)</span> can nontrivally one-sided agree (soundness) with <span class="math inline">\(RFS_{n,h}^{\Maj}\)</span>. (Similar statement for GIP.)</p>
<h2 id="vc-dimension">VC dimension</h2>
<p>Interpolation degree <span class="math inline">\(\text{reg}(C)\)</span> is min <span class="math inline">\(d\)</span> such that every <span class="math inline">\(f\)</span> is a multilinear poly of degree <span class="math inline">\(\le d\)</span>.</p>
<p>Problem: Characterize sets with interpolation degree <span class="math inline">\(r\)</span>.</p>
<p><strong>Theorem 4</strong>: <span class="math inline">\(\text{reg}(C)=r\)</span> iff <span class="math inline">\(r\)</span> is smallest so <span class="math inline">\(\rank \mathcal M(C, \binom{[n]}{\le r})=|C|\)</span>.</p>
<h2 id="algebraic-geometry">Algebraic geometry</h2>
<ul>
<li>Affine Hilbert function <span class="math inline">\(h^a(R,d) = \dim(R_{\le d})\)</span> where <span class="math inline">\(R_{\le d}=\F[\mx]_{\le d}/I(V)_{\le d}\)</span>.</li>
<li>Leading monomials of an ideal. Standard monomials are the complement. (Don’t know why they’re named like this. “Missing monomials” makes more sense.)
<ul>
<li>Hilbert function is a sum of sizes of SM’s: $h^a(V,d)=_{i=0}^d |SM(V,i)|.</li>
<li>Regularity is maximum degree of standard monomial of <span class="math inline">\(V\)</span>.</li>
</ul></li>
<li><span class="math inline">\(a(I)\)</span> is the min degree of <span class="math inline">\(g\in I\)</span> such that <span class="math inline">\(g\)</span> consists only of monomials from <span class="math inline">\(SM(V)\)</span>. (If says <span class="math inline">\(SM(\F^n)\)</span>, but this doesn’t make sense.)
<ul>
<li><span class="math inline">\(V\subeq \F^n\)</span> a nonempty zero-dimensional algebraic set. (What does zero-dim mean here?) Then <span class="math inline">\(a(\ol V)+\text{reg}(V)=n\)</span>.</li>
</ul></li>
<li>How to calculate Hilbert function? Inclusion matrix <span class="math inline">\(M(\mathcal F,\mathcal G)\)</span> has <span class="math inline">\(M_{F,G} = 1_{F\subeq G}\)</span>. Calculate Hilbert function by <span class="math inline">\(h^a(V,d) =\rank M\pa{V,\binom{[n]}{\le d}}\)</span>.</li>
</ul>
<h2 id="versatile-functions">Versatile functions</h2>
<ul>
<li>Versatile: <span class="math inline">\(\forall g, \exists \deg(u),\deg(v)\le \fc n2\)</span> and <span class="math inline">\(g=uf+v\)</span>.
<ul>
<li>Question: isn’t this always true for <span class="math inline">\(\deg g=\fc n2\)</span> by division? Yes, but there are other functions that are versatile too!</li>
<li>Observation: when <span class="math inline">\(f(x)=0, g=v\)</span>. When <span class="math inline">\(f(x)=1\)</span>, <span class="math inline">\(g=u+v\)</span>. Let <span class="math inline">\(U_i=\set{x}{f(x)=i}\)</span>. (Lemma 4) This shows that any poly can be collapsed to one of degree <span class="math inline">\(\le \fc n2\)</span> on <span class="math inline">\(U_0\)</span> and <span class="math inline">\(U_1\)</span>, i.e. <span class="math inline">\(\text{reg}(U_i)\le \fc n2\)</span>. Converse also holds. (How is regularity related to max degree of defining function?)</li>
</ul></li>
</ul>
<p>Example: the standard monomials (those missed by leading monomials) of Maj are those with weight <span class="math inline">\(&lt;\fc n2\)</span>. (Proof: any product of <span class="math inline">\(\ge \fc n2\)</span> variables vanishes on Maj.)</p>
<p>Generalize: <span class="math inline">\(\de\)</span>-versatile on <span class="math inline">\(U\)</span> if <span class="math inline">\(\de \le \text{reg}(U)-\text{reg}(U_i)\)</span>. (A versatile function is <span class="math inline">\(\fc n2\)</span>-versatile on <span class="math inline">\(\F_2^n\)</span>.)</p>
<p>Lemmas:</p>
<ul>
<li>6: A <span class="math inline">\(\de\)</span>-versatile function on <span class="math inline">\(U\)</span> cannot be represented on <span class="math inline">\(U\)</span> by degree <span class="math inline">\(&lt;\de\)</span>. (Note how we changed the degree and the set!)</li>
<li>7: If <span class="math inline">\(q\)</span> has degree <span class="math inline">\(&lt;\de\)</span> and <span class="math inline">\(q\)</span> vanishes on <span class="math inline">\(U_i\)</span>, then it vanishes on <span class="math inline">\(U_{1-i}\)</span>. (This gives failure of one-sided computation.) Necessary is that <span class="math inline">\(U\)</span> be critical algebraic, <span class="math inline">\(a(\ol V)=\deg(1_U)\)</span>. I don’t understand this condition.</li>
<li>8: The LM’s for <span class="math inline">\(U\cap G,U_i\cap G\)</span> for <span class="math inline">\(G\)</span> a union of hypersurfaces of degree <span class="math inline">\(&lt;d\)</span>, are the same up to degree <span class="math inline">\(\le \de -d\)</span>.</li>
<li>9: Bound the difference between 2 evaluations of the Hilbert function at around <span class="math inline">\(\text{reg}(V)\)</span>. Q: Why do we expect this to be small, and why do we care?</li>
</ul>
<h2 id="rfs">RFS</h2>
<h2 id="todo">Todo</h2>
<ul>
<li>Go through proof of Theorem 1.</li>
<li>Understand recursive Fourier sampling. Read another source for this.</li>
</ul>
<p>Questions</p>
<ul>
<li>How to construct a versatile set?</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Recursive Fourier Sampling</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/complexity/RFS.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/complexity/RFS.html</id>
    <published>2016-02-29T00:00:00Z</published>
    <updated>2016-02-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Recursive Fourier Sampling</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-02-29 
          , Modified: 2016-02-29 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <h2 id="definition">Definition</h2>
<h3 id="fs">FS</h3>
<p><span class="math inline">\(FS_n:B^{2^{n+1}} \to \{0,1,*\}\)</span> is defined by <span class="math display">\[
FS_n(f,g)=\begin{cases}
g(s),&amp;\text{if }\exists s\in B^n, f(x)=x\cdot s\\
*,&amp;\text{else.}
\end{cases}
\]</span> <span class="math inline">\(FS_n^g\)</span> is where <span class="math inline">\(g\)</span> is fixed.</p>
<h3 id="rfs">RFS</h3>
<p>Define by recurrence</p>
\begin{align}
RFS_{n,1}(s,g)&amp;=g(s)\\
RFS_{n,h}&amp;: B^{n2^{n(h-1)} +\sumo j{h-1}} \to \{0,1,*\}\\
RFS_{n,h}(R_0,\ldots, R_{2^n-1},g) &amp;=\begin{cases}
g(s),&amp;\exists s\in B^n, \forall \si\in B^n, RFS_{n,h-1}(R_\si)=\si \cdot s\\
*,&amp;\text{else}.
\end{cases}\\
\pat{Example}\; RFS_{n,2}(R_0,\ldots, R_{2^n-1},g) &amp;=\begin{cases}
g(s),&amp;\exists s\in B^n, \forall \si\in B^n, g(R_\si) = \si \cdot s\\
*,&amp;\text{else}.
\end{cases}
\end{align}

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>MAT529 notes</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/analysis/metric/MAT529.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/analysis/metric/MAT529.html</id>
    <published>2016-02-29T00:00:00Z</published>
    <updated>2016-02-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>MAT529 notes</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-02-29 
          , Modified: 2016-02-29 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <h2 id="questions">Questions</h2>
<h2 id="research-questions">Research questions</h2>
<h2 id="scraps">Scraps</h2>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Censored block model</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/community/cbm.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/community/cbm.html</id>
    <published>2016-02-28T00:00:00Z</published>
    <updated>2016-02-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Censored block model</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-02-28 
          , Modified: 2016-02-28 
	</p>
      
       <p>Tags: <a href="/tags/research.html">research</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <p>Model: Graph <span class="math inline">\(G\)</span> is given. Each node is iid red/blue with probability <span class="math inline">\(\rc2\)</span>. An edge is labeled <span class="math display">\[\begin{cases}+&amp;\text{nodes same color}\\-&amp;\text{nodes different color}\end{cases}.\]</span> Each edge is flipped independently with probability <span class="math inline">\(\rc2\)</span>.</p>
<p>Question: Under what conditions of <span class="math inline">\(G\)</span> can the communities be recovered (up to sign)</p>
<ul>
<li>exactly with high (<span class="math inline">\(1-o(1)\)</span>) probability?</li>
<li>with correlation tending to 1? (whp, the number of correctly classified nodes is <span class="math inline">\(1-o(1)\)</span>)</li>
<li>weakly (with <span class="math inline">\(\rc2+\Om(1)\)</span> correlation)?</li>
</ul>
<p>Past work:</p>
<ul>
<li>[ABSS12] For exact recovery
<ul>
<li>Necessary: <span class="math inline">\(C_\ep \ln n\)</span> average degree.</li>
<li>For random graphs <span class="math inline">\(G(n, p)\)</span>, <span class="math inline">\(p\ge \fc{2C_\ep\ln n}{n}\)</span> is sufficient. (Factor of 2 gap, later closed by [??])</li>
<li>Algorithm is SDP relaxation. Sufficient criteria for it to work is that there is a constant spectral gap in the graph. Up to constants, this is the same as having a large Cheeger constant.</li>
<li>Spectral criteria are not necessary: for example, for 2 complete graphs with a <span class="math inline">\(\ln n\)</span> sized cut, recovery is possible.</li>
</ul></li>
<li>Strong recovery - ?</li>
<li>Weak recovery - ?</li>
</ul>
<p>Suggestion: Let <span class="math inline">\(X\)</span> be the node colors and <span class="math inline">\(Y_\ep\)</span> be the observed edges. Consider the entropy <span class="math inline">\(H(X|Y_\ep)\)</span>. This is like a measure of connectivity “on average.” Ex. When <span class="math inline">\(\ep=0\)</span>, this measures the number of components. Think of it as a average-case Cheeger: rather than look at the proportionally minimal cut, we’re looking at an average cut size. (Related is the probability of success for ML decoding, which is <span class="math inline">\(\E_{X,Y_\ep}P(X|Y)\)</span>.)</p>
<p>Thoughts: Any nontrivial partition <span class="math inline">\(A\sqcup B= V\)</span> of the vertices is associated with a cut. For a set <span class="math inline">\(S\)</span>, let <span class="math inline">\(X_S\)</span> be <span class="math inline">\(X\)</span> with the colors of vertices in <span class="math inline">\(S\)</span> flipped. Given observed <span class="math inline">\(Y_\ep\)</span>, <span class="math inline">\(P(X_S|Y_\ep)\ge P(X|Y_\ep)\)</span> iff most of the edges of the cut <span class="math inline">\((A,V\bs A)\)</span> are flipped. Thus, the ML decoding is the <span class="math inline">\(X\)</span> such that in no cut is the majority of the edges wrong.</p>
Let <span class="math inline">\(G_C\)</span> be the (good) event that the majority of edges of cut <span class="math inline">\(C\)</span> are correct. Letting <span class="math inline">\(|C|\)</span> be the number of edges in the cut,
\begin{align}
P\pa{\bigwedge_C G_C} &amp; \ge \prod P(G_C) &amp; \text{Correlation inequality} \\
&amp; = \prod_C P(\Binom(|C|, 1-\ep) &gt; \fc{|C|}2)\\
&amp; = \prod_C (1-e^{-K_\ep|C|})\\
&amp; \ge 1-\sum_C e^{-K_\ep|C|}.
\end{align}
<p>(By Chernoff, we can take <span class="math inline">\(K_\ep = \fc{(\ep-\rc2)^2}{2\ep(1-\ep)}\)</span>.)</p>
<p>Question: is there an efficient algorithm to approximate <span class="math inline">\(\sum_C e^{-K|C|}\)</span>? By Karger’s algorithm, we can estimate the number of minimum cuts. By sampling, we can estimate the number of cuts of some size if it’s <span class="math inline">\(&gt;\rc{\poly}\)</span> proportion. But what about all cuts in between?</p>
<p>Note: The inequalities are not close to sharp because the events are very correlated when the cuts overlap a lot. Somehow it should be more affected by small cuts which <em>don’t</em> share a lot of edges in common.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[MNS12]</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/MNS12.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/MNS12.html</id>
    <published>2016-02-28T00:00:00Z</published>
    <updated>2016-02-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[MNS12]</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-02-28 
          , Modified: 2016-02-28 
	</p>
      
       <p>Tags: <a href="/tags/abbe.html">abbe</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#summary">Summary</a></li>
 <li><a href="#proofs">Proofs</a><ul>
 <li><a href="#estimation">Estimation</a></li>
 <li><a href="#non-recovery">Non-recovery</a></li>
 <li><a href="#non-estimation">Non-estimation</a></li>
 </ul></li>
 <li><a href="#questions">Questions</a><ul>
 <li><a href="#minor">Minor</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="summary">Summary</h2>
<p>Mossel, Elchanan, Joe Neeman, and Allan Sly. “Stochastic block models and reconstruction.” arXiv preprint arXiv:1202.1499 (2012).</p>
<p>Model: Given a stochastic block model <span class="math inline">\(G(n, \fc an, \fc bn)\)</span>, recover the communities and estimate <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</p>
<p>They prove 3/4 of the problem. The conjectured threshold is <span class="math inline">\((a-b)^2&gt;2(a+b)\)</span>.</p>
<ol type="1">
<li>When $(a-b)^2&gt;2(a+b),
<ol type="1">
<li>Recovery: can we recover the communities efficiently? (Still open.)</li>
<li>Estimate <span class="math inline">\(a,b\)</span> (w.h.p. get <span class="math inline">\(a(1+o(1))\)</span> as <span class="math inline">\(n\to \iy\)</span>). (Theorem 2.5)</li>
</ol></li>
<li>When <span class="math inline">\((a-b)^2\le 2(a+b)\)</span>,
<ol type="1">
<li>Non-recovery: we can recover communities exactly (with probability <span class="math inline">\(1-o(1)\)</span>). (Theorem 2.1)</li>
<li>Non-estimation: we cannot estimate <span class="math inline">\(a,b\)</span>. (Theorem 2.4)</li>
</ol></li>
</ol>
<p>Note that recovery seems stronger than estimation (is this true formally?).</p>
<p>Details of the theorems: Let <span class="math inline">\(\Pj_n=\cal G(n,\fc an, \fc bn)\)</span>, <span class="math inline">\(\Pj_n' = \cal G(n,\fc{a+b}{2n})\)</span>.</p>
<ul>
<li>2.1: Show something stronger: for fixed vertices, <span class="math inline">\(\Pj_n(\si_u=+|G,\si_v=+)\to \rc2\)</span> a.a.s. This means no algorithm can tell whether 2 vertices have the same label.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></li>
<li>2.4: Show that <span class="math inline">\(\Pj_n,\Pj_n'\)</span> are mutually continuous.
<ul>
<li>Mutually contiguous means that <span class="math inline">\(\Pj_n(A_n)\to 0\iff \Pj_n'(A_n)\to 0\)</span>. This is weaker than being statistically indistinguishable.</li>
<li>Mutual continiguity is a transitive relation, so it’s indistinguishable from all <span class="math inline">\(\cal G(n,\fc{a'+b'}{2n})\)</span> also satisfying the same inequality, with the same sum.</li>
</ul></li>
<li>2.5: There are consistent estimators for <span class="math inline">\(a,b\)</span> depending on the number of <span class="math inline">\(k_n\)</span>-cycles (<span class="math inline">\(k_n=\fl{\ln^{\rc 4}n}\)</span>).
<ul>
<li><span class="math inline">\(\Pj_n,\Pj_n'\)</span> are asymptotically orthogonal, i.e., there is <span class="math inline">\(A_n\)</span>, <span class="math inline">\(\Pj_n(A_n)\to 1, \Pj_n'(A_n)\to 0\)</span>.</li>
</ul></li>
</ul>
<h2 id="proofs">Proofs</h2>
<h3 id="estimation">Estimation</h3>
<p>Idea: The number of cycles for <span class="math inline">\(\Pj_n,\Pj_n'\)</span> follow a Poisson distribution. They are spaced farther apart than their standard deviation exactly when <span class="math inline">\((a-b)^2&gt;2(a+b)\)</span>.</p>
<ol type="1">
<li>Calculation of number of <span class="math inline">\(k\)</span>-cycles: <span class="math display">\[ X_{k,n}\xra{d} \Pois\pa{\rc{k2^{k+1}}((a+b)^k + (a-b)^k)}.\]</span> (For the Erdos-Renyi random graph <span class="math inline">\(a'=b'=\fc{a+b}{2}\)</span>, there is no 2nd term.) To calculate this,
<ol type="1">
<li>Expected value: Use linearity of <span class="math inline">\(\E\)</span> over all <span class="math inline">\(\binom nk\)</span> cycles. The probability of the cycle depends on the number of sign changes. Get <span class="math inline">\(n^{-k}2^{-k+1}\sum_{m\text{ even}} \binom km a^{k-m}b^m\)</span>.</li>
<li>Higher moments: We’re counting number of <span class="math inline">\(m\)</span>-cycles. It suffices to show the expected number of non-vertex disjoint <span class="math inline">\(m\)</span>-types converges to 0.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> Then it’s Poisson.</li>
</ol></li>
<li>Parameters. <span class="math inline">\(a+b\)</span> can be estimated from average degree. Estimate <span class="math inline">\(a-b\)</span> using the estimate for <span class="math inline">\(a+b\)</span> and <span class="math inline">\(X_{k_n}\)</span>.</li>
<li>Algorithm. This is Proposition 3.2 which I don’t understand!<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></li>
</ol>
<h3 id="non-recovery">Non-recovery</h3>
<p>First consider a problem on trees.</p>
<p>Model: A Galton-Watson tree has <span class="math inline">\(\Pois(d)\)</span> offspring. An offspring is flipped with probability <span class="math inline">\(\ep\)</span>. Can you deduce the sign of the root from the sign of the depth-<span class="math inline">\(R\)</span> signs?</p>
<p>Answer: There is threshold.</p>
<p>Idea of non-recovery: On neighborhoods, the distribution of signs is close to that of the model. The posterior distribution of the signs given the graph is approximately a Markov.</p>
<ol type="1">
<li><strong>Theorem 4.1</strong>: <span class="math inline">\(d(1-2\ep)^2\le 1 \iff \lim_{R\to \iy} \Pj(\tau_p=+|\tau_{\pl T_R})=\rc2\)</span> a.s.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a></li>
<li><strong>Pr. 4.2</strong>: The distribution of a small neighborhood of a given vertex is statistically close the the distribution for the tree model. Take the radius to be <span class="math inline">\(\rc{C} \log_{\fc{a+b}2} n\)</span>, so that the expected number of nodes is <span class="math inline">\(O(n^{\rc C})\)</span>. Think of this as a coupling argument. Do an inductive argument on the depth, the distance between distributions grows a little each time. Bound the probability of the bad event of having too many children—if this doesn’t happen, there are still approximately <span class="math inline">\(\fc n2\)</span> <span class="math inline">\(+\)</span>’s and <span class="math inline">\(-\)</span>’s left, and the appromate number of children that switch/don’t switch will be close to <span class="math inline">\(\fc a2, \fc b2\)</span>. (See lemmas 4.3-6.)</li>
<li>Consider <span class="math inline">\(\Pj(\si|G)\)</span>. This is not a Markov field because the probability (multiplying factor) of non-edge is different for if the vertices are same/different. But the ratio is <span class="math inline">\(\fc{1-\fc an}{1-\fc bn}\approx 1\)</span>, so it shouldn’t have much effect. We show we still have approximate independence in the sense of <strong>Lemma 4.7</strong>: <span class="math inline">\(\Pj(\si_A|\si_{B\cup C,G} = (1+o(1))\Pj(\si_A|\si_B,G)\)</span> for a.a.e. <span class="math inline">\(G,\si\)</span>,<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> when <span class="math inline">\(A,B,C\)</span> is a partition with <span class="math inline">\(|A\cup B|=o(\sqrt n)\)</span>. (This condition is necessary to make sure we’re not multiplying too many <span class="math inline">\((1-\fc an)\)</span> and <span class="math inline">\((1-\fc bn)\)</span>’s.) Take <span class="math inline">\(A\)</span> to be <span class="math inline">\(B_{R-1}(v)\)</span>, <span class="math inline">\(B\)</span> to be <span class="math inline">\(\pl G_R\)</span>, and <span class="math inline">\(C\)</span> to be the rest. This gives that <span class="math inline">\(\si_v,\si_\rh\)</span> are conditionally independent given the boundary.</li>
<li>Use 1 with <span class="math inline">\(\ep = \fc{b}{a+b}, d=\fc{a+b}{2}\)</span> (proportion of edges corresponding to flipping). The variance approaches the variance without conditioning on <span class="math inline">\(\si_v\)</span>. The variance without conditioning <span class="math inline">\(\Var(\si_\rh|G_,\si_v)\)</span> is close to tht for the tree model, which is 1 (the nonrecovery regime) when <span class="math inline">\(d(1-2\ep)^2\le 1\)</span>, which is exactly the condition. From the variance going to 1, the expectation goes to 0;probability goes to <span class="math inline">\(\rc2\)</span>.</li>
</ol>
<h3 id="non-estimation">Non-estimation</h3>
<p>(Unfinished)</p>
<p>Define <span class="math inline">\(\Pj_n(\si|G)\)</span> to be the same as <span class="math inline">\(\Pj_n'(\si|G)\)</span>. The joint distribution is not the same because the marginal distribution over the graphs is different.</p>
<p>Use a criteria for contiguity, <strong>Theorem 5.1</strong><a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a>. See this as a black box. Calculate moments, etc. of <span class="math inline">\(Y_n=\fc{\Pj_n}{\Pj_n'}\)</span>. Using independence of edges given <span class="math inline">\(\si\)</span>, you can decompose this as a product nicely.</p>
<h2 id="questions">Questions</h2>
<h3 id="minor">Minor</h3>
<ul>
<li>What is a.a.s.?</li>
</ul>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Note that the “a.s.” is with respect to <span class="math inline">\((\si,G)\)</span>. Two neighboring vertices will have a lot of information on each other, but two vertices will be neighboring with low probability with respect to the distribution over <span class="math inline">\(G\)</span>.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>? (How? Reference given is Bollobas, Ch. 4. Need <span class="math inline">\(k=O(\ln^{\rc 4}n)\)</span>.)<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>?<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>It would be good to understand this proof for <span class="math inline">\(d\)</span>-ary trees (without the GW complication).<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>I don’t understand what it means by random partition.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>I don’t understand the motivation/theory behind this. Reference is [35], Wormald, Models of random regular graphs.<a href="#fnref6">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Test</title>
    <link href="http://holdenlee.github.io/notebook/posts/test/test.html" />
    <id>http://holdenlee.github.io/notebook/posts/test/test.html</id>
    <published>2016-02-27T00:00:00Z</published>
    <updated>2016-02-27T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Test</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-02-27 
          
	</p>
      
       <p>Tags: </p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <p>Math equations.</p>
<p><span class="math inline">\(1\in \R, \infty\nin \R\)</span></p>
<p>This is math:</p>
<p><span class="math display">\[ \int_1^x \fc{1}{u}\,du = \ln x.\]</span></p>
<p>More math:</p>
<p><span class="math display">\[
\pi = 4\sum_{n=0}^{\iy} (-1)^n \fc{1}{2n+1}
\]</span></p>
\begin{align}
\label{eq:1}
\rc{2} &amp;= \fc 24\\
&amp;=\fc 48
\end{align}
<p>Equation <span class="math inline">\(\eqref{eq:1}\)</span> above.</p>
<p><span class="math display">\[
\sA \Pj  \mfp \matt 1001
\]</span></p>
<p>Things that don’t work:</p>
<ul>
<li><code>\mathbbm{1}</code></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Real channels</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/information_theory/real_channels.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/information_theory/real_channels.html</id>
    <published>2016-02-27T00:00:00Z</published>
    <updated>2016-02-27T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Real channels</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-02-27 
          , Modified: 2016-02-27 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#reading">Reading</a><ul>
 <li><a href="#mckay">McKay</a><ul>
 <li><a href="#exercises">Exercises</a></li>
 </ul></li>
 <li><a href="#cover-thomas">Cover, Thomas</a><ul>
 <li><a href="#ch.-8">Ch. 8</a></li>
 <li><a href="#ch.-9">Ch. 9</a></li>
 </ul></li>
 </ul></li>
 <li><a href="#scraps">Scraps</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>References:</p>
<ul>
<li>McKay Ch. 11</li>
<li>Cover, Thomas Ch. 9</li>
</ul>
<h2 id="reading">Reading</h2>
<h3 id="mckay">McKay</h3>
<p>What’s the model?</p>
<ul>
<li>Transmit a signal <span class="math inline">\(x(t)\)</span> in a channel with error at every moment in time Gaussian with variance <span class="math inline">\(\si^2\)</span>.</li>
<li>You are limited to using <span class="math inline">\(v\)</span> power, i.e., <span class="math inline">\(\int_0^T \fc{x(t)^2}{T}\,dt\le v\)</span>.</li>
<li>You are bandwidth-limited: <span class="math inline">\(x\)</span> must be a sum <span class="math inline">\(x=\sum_{n=1}^{N} x_n\phi_n\)</span> where <span class="math inline">\(\phi_n\)</span> are orthonormal basis. (i.e., <span class="math inline">\(\sum x_n^2=v\)</span>.) Take the functions to be <span class="math inline">\(\cos\pf{2\pi kx}{T}\)</span> and <span class="math inline">\(\sin\)</span> for <span class="math inline">\(1\le k\le TW\)</span>. We have <span class="math inline">\(N=2TW\)</span> where <span class="math inline">\(W\)</span> is the max allowable frequency.</li>
</ul>
<p>At what (limiting) rate can you transmit information?</p>
<p>Note: A sum is like an integral. Take exps instead of sin/cos for simplicity. Then for <span class="math inline">\(x\)</span> a sum of exponentials <span class="math inline">\(\phi_n\)</span>, <span class="math display">\[
\int_0^1 x(t)\phi_n(t) = a_n = \EE_{t=0}^{N-1}x\pf{t}{N} \phi_n\pf{t}{N}.
\]</span></p>
<p>Useful: <span class="math inline">\(N(y;x,\si^2)N(x;0,v) = N\pa{x;\fc{v}{v+\si^2}y, \pa{\rc{v}+\rc{\si^2}}^{-1}}\)</span>. Mean is means harmonically weighted by variances (i.e. weighted by precisions). “Whenever two independent sources contribute information, via Gaussian distributions, about an unknown variable, the precisions add.”</p>
<p>“Real continuous channel with <span class="math inline">\(W\)</span> and noise <span class="math inline">\(W_0\)</span> is <span class="math inline">\(\fc{N}{T}=2W\)</span> uses per second of Gaussian with <span class="math inline">\(\si^2=N_0/2, \ol{x_n^2}\le \fc{P}{2W}\)</span>.</p>
<p>(How do you think of discrete bits as encoding a point in real space? Or are you transmitting analog information?)</p>
<p>Bandwidth is more powerful than low noise.</p>
<p>(Q: Why does M say that entropy can’t be defined for continuous variables? I guess this should be taken to mean that the definition of <span class="math inline">\(h\)</span> wouldn’t be invariant under change of coordinates.)</p>
<h4 id="exercises">Exercises</h4>
<ol type="1">
<li><p>(Solution p. 189/201) Use Lagrange multipliers with <em>functions</em> (calculus of variations). Use <span class="math inline">\(\fc{\de F}{\de P^*}\int P(x) f(x,P) \dx = f(x^*,P) + \int P(x)\fc{\de f(x,P)}{\de P(x)}\)</span> (cf. normal product rule). Note <span class="math inline">\(P(y|x)\)</span> is fixed (normal), but <span class="math inline">\(P(y)\)</span> depends on <span class="math inline">\(P(x)\)</span>, it is really a function of <span class="math inline">\(P\)</span>, <span class="math inline">\(f(P)\)</span>. Simplify, and then substitute <span class="math inline">\(P(y|x)\)</span>. Match coefficients in Taylor expansion.</p>
<p>Question: Why does the constraint <span class="math inline">\(\int P\dx=1\)</span> become <span class="math inline">\(\mu\pa{\int P\dx}\)</span> rather than <span class="math inline">\(\mu(\bullet-1)\)</span>? They disappear after differentiation!</p>
<p>TAKEAWAY: Calculus of variations. Gaussian distributions is best. (Why not 2 points as far as possible?)</p></li>
<li><p>Just calculate the integral. Mutual info is <span class="math inline">\(\rc2\ln \pa{1+\fc{v}{\si^2}}\)</span>. This is the capacity (explained on p. 182) (?). The more power, the more capacity, scaling by log.</p></li>
</ol>
<h3 id="cover-thomas">Cover, Thomas</h3>
<h4 id="ch.-8">Ch. 8</h4>
<ul>
<li><strong>Differential entropy</strong> <span class="math inline">\(h=-\int f\lg f\dx\)</span>. “differential entropy is the logarithm of the equivalent side length of the smallest set that contains most of the probability.”
<ul>
<li>How differential entropy and entropy are related: Let <span class="math inline">\(X^{\De}\)</span> be <span class="math inline">\(X\)</span> quantized to <span class="math inline">\(\De\)</span>. Then <span class="math inline">\(\lim_{\De\to 0}H(X^{\De})+\lg \De = h(X)\)</span>.</li>
<li>Note mutual info is limit of quantized mutual info (the <span class="math inline">\(\lg \De\)</span>’s cancel).</li>
<li><strong>KL divergence</strong>: <span class="math inline">\(D(f||g)=\int f \lg \fc{f}{g}\ge 0\)</span>.</li>
</ul></li>
<li>Basic calculations
<ul>
<li><span class="math inline">\(h(N(0,\si^2)) = \rc2 \lg (2\pi e\si^2)\)</span></li>
<li><span class="math inline">\(\rh\)</span>-correlated Gaussians: <span class="math inline">\(I(X:Y) = -\rc2 \lg(1-\rh^2)\)</span>.</li>
</ul></li>
<li>Basic inequalities
<ul>
<li>By LoLN, AEP holds for continuous random variables (<span class="math inline">\(\log f\)</span> needs to be <span class="math inline">\(L^1\)</span>?).</li>
<li>(Hadamard) <span class="math inline">\(\det(K)\le \prod K_{ii}\)</span> by subadditivity of entropy.</li>
<li><span class="math inline">\(h(AX)=h(X)+\lg |\det(A)|\)</span>.</li>
</ul></li>
<li>Normal maximizes entropy over distributions with same variance. Proof: <span class="math inline">\(0\le D(g||\phi_K) = -h(g)+h(\phi_K)\)</span> where the second term follows from the fact that <span class="math inline">\(g,\phi_K\)</span> have the same second moments and <span class="math inline">\(\log \phi_K\)</span> is a quadratic form (write it out!).</li>
<li>Estimation version of Fano: <span class="math inline">\(\E[(X-\hat X)^2]\ge \rc{2\pi e}e^{2h(X)}\)</span>, equality only if <span class="math inline">\(X\)</span> Gaussian and <span class="math inline">\(\hat X = \E X\)</span>. Proof: use that Gaussian distribution has max entropy for given variance.
<ul>
<li>Cor: <span class="math inline">\(\E[(X-\hat X(Y))^2] \ge \rc{2\pi e} e^{2h(X|Y)}\)</span>.</li>
</ul></li>
</ul>
<p>Nice summary on p. 282.</p>
<h4 id="ch.-9">Ch. 9</h4>
<ul>
<li>A Gaussian channel with power <span class="math inline">\(P\)</span> can be converted to a binary symmetric channel with error <span class="math inline">\(1-\Phi\pa{\sfc{P}{\si^2}}\)</span>.</li>
<li>9.1: A more conceptual way to upper-bound the mutual information: <span class="math inline">\(Y=X+Z\)</span> where <span class="math inline">\(Z\)</span> is noise.
<ul>
<li>Theorem 8.6.5: normal maximizes the entropy for a given variance. (cf. entropy-maximizing distributions. Use CoV here?) Then <span class="math inline">\(I(X:Y)=h(Y)-h(Z)\le \rc2 \log \pa{1+\fc PN}\)</span>, equality only when <span class="math inline">\(X\)</span> is Gaussian.</li>
</ul></li>
<li>Model of Gaussian channel: Encoding, Gaussian noise, decoding, <span class="math inline">\([M]\xra{x}\mathcal{X}^n \xra{N}\mathcal{Y}^n \xra{g} [M]\)</span>.</li>
<li><strong>Theorem 9.1.1</strong>: Capacity of Gaussian channel is <span class="math inline">\(\rc 2 \lg\pa{1+\fc{P}N}\)</span>.
<ul>
<li>Heuristic proof: Pack spheres of radius <span class="math inline">\(\sqrt{nN}\)</span> in <span class="math inline">\(\sqrt{n(P+N)}\)</span>.</li>
<li>Think of mutual info as a limit achievable in infinite dimensions. It doesn’t make sense to encode a discrete set with a probability distribution, but in large dimensions you can approximate it!</li>
<li>Proof of achievability: (1) generate codewords iid (2) search for jointly typical. (3) It’s rare that either it’s far from the codeword, or it’s close to another one.</li>
<li>Proof of necessity: Take uniform distribution over inputs. Use Fano’s inequality to relate probability of error and mutual info.
<ul>
<li><strong>Fano’s inequality</strong> says that for <span class="math inline">\(X\to Y\to \wh X\)</span>, <span class="math inline">\(P_e=\Pj(X\ne \wh X)\)</span>, <span class="math inline">\(H(P_e)+P_e\lg |\mathcal X| \ge H(X|\wh X) \ge H(X|Y)\)</span>.</li>
<li>Applied here, <span class="math inline">\(H(W|\hat W)\le H(P_e) + P_e\lg |\mathcal X|\le 1 + (nR) P_e^{(n)}=o(n)\)</span>. Since <span class="math inline">\(nR=H(W)\)</span>, we get that <span class="math inline">\(I(W:\hat W) \ge \asymp nR\)</span> and hence <span class="math inline">\(\sum I(X_i:Y_i) \ge \asymp nR\)</span>. But this sum can be bounded by the power (use Jensen on the <span class="math inline">\(P_i\)</span>).</li>
</ul></li>
</ul></li>
<li>6.3 Bandlimited channels: “Rate <span class="math inline">\(\rc{2W}\)</span> is sufficient to reconstruct the signal because it cannot change substantially in <span class="math inline">\(&lt;\rc2\)</span> a cycle.” Proof: Note that the Fourier transform is 0 outside <span class="math inline">\([-W,W]\)</span>. <span class="math inline">\(f\pf{n}{2W}\)</span> are the Fourier coefficients of <span class="math inline">\(F(\om)\)</span>. (“a bandlimited function has only 2W degrees of freedom per second”)<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>
<ul>
<li>Note band-limitation can be expressed as convolution by <span class="math inline">\(\mathcal F^{-1} 1_{\le W}\)</span>.</li>
<li>The characteristic-function basis for sampling at <span class="math inline">\(\fc{n}{2W}\)</span> are the translates of <span class="math inline">\(\text{sinc}(Wt) = \sin(Wt)/(Wt)\)</span>.</li>
</ul></li>
<li>Relating the 2 models.
<ul>
<li>In the Gaussian noise model, <span class="math inline">\(C=\rc 2 \log\pa{1+\fc{P_G}{N_G}}\)</span> where <span class="math inline">\(P\)</span> is power (average magnitude) and <span class="math inline">\(N\)</span> is variance.</li>
<li>In the band-limited model, suppose <span class="math inline">\(W\)</span> is bandwidth, <span class="math inline">\(P\)</span> is power, and <span class="math inline">\(\fc{N_0}2\)</span> is variance (power spectral density - I don’t understand this).</li>
<li>Match up by <span class="math inline">\(P_G=\fc{P}{2W}\)</span> (divide by sampling rate) and <span class="math inline">\(N_G=N_0/2\)</span> to get <span class="math inline">\(\rc2\lg\pa{1+\fc{P}{N_0W}}\)</span> per sample. Multiply by <span class="math inline">\(2W\)</span>. (<span class="math inline">\(\fc{P}{N_0W}\)</span> is SNR.)
\begin{equation} C= W\lg\pa{1+\fc{P}{N_0W}}.\end{equation}
With infinite bandwidth, <span class="math inline">\(C=\fc{P}{N_0}\lg e\)</span>.</li>
</ul></li>
<li>9.4 Parallel Gaussian channels: set of Gaussian channels in parallel with common (total) power constraint <span class="math inline">\(P\)</span>. How to distribute.
<ul>
<li>Maximize <span class="math inline">\(\sum \rc2\lg\pa{1+\fc{P_i}{N_i}}\)</span> subject to <span class="math inline">\(\sum P_i=P\)</span>. By Lagrange multipliers, the best solution is water-filling, p. 277.</li>
</ul></li>
<li>9.5 Channels with colored (correlated) Gaussian noise (SKIP)</li>
<li>9.6 Gaussian channels with feedback: for channels with memory, where the noise is correlated from time instant to time instant, feedback does increase capacity. (SKIP)</li>
</ul>
<h5 id="exercises-1">Exercises</h5>
<p>9.10 looks interesting.</p>
<h2 id="scraps">Scraps</h2>
<ul>
<li>In the presence of noise, doesn’t it help to sample <span class="math inline">\(&gt;2W\)</span> times per second? Can’t you approach the true average with more samples?</li>
<li></li>
</ul>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I thought that this meant the <span class="math inline">\(f\)</span> live in a finite-dimensional space. No? This confuses me. Are we taking infinitely many samples spaced <span class="math inline">\(\rc{2W}\)</span> apart? Because <span class="math inline">\(F\)</span> is not in a finite dimensional space.<a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Calculus formulas</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/calculus/formulas.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/calculus/formulas.html</id>
    <published>2016-02-27T00:00:00Z</published>
    <updated>2016-02-27T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Calculus formulas</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-02-27 
          , Modified: 2016-02-27 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    \begin{align}
\int_{-\iy}^{\iy} e^{-(ax^2+bx+c)}\dx &amp;= e^{\fc{b^2}{4a}-c}\sfc{\pi}a.
\end{align}

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
