<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2016-03-19T00:00:00Z</updated>
    <entry>
    <title>LSTM Programming</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/lstm_code.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/lstm_code.html</id>
    <published>2016-03-19T00:00:00Z</published>
    <updated>2016-03-19T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>LSTM Programming</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-19 
          , Modified: 2016-03-19 
	</p>
      
       <p>Tags: <a href="/tags/programming.html">programming</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <h2 id="math">Math</h2>
<p>Here are the equations for LSTM.</p>
\begin{align}
f_t&amp;=\si(W_f \coltwo{h_{t-1}}{x_t} + b_f)\\
i_t&amp;=\si(W_i \coltwo{h_{t-1}}{x_t} + b_i)\\
\wt{C}_t &amp;= \tanh (W_C\coltwo{h_{t-1}}{x_t}+b_C)\\
C_t &amp;= f_t \odot C_{t-1} + i_t \odot \wt{C}_t\\
o_t &amp;= \si(W_o\coltwo{h_{t-1}}{x_t} + b_o)\\
h_t &amp;= o_t\odot \tanh(C_t)\\
\wh y &amp;= \text{softmax}(Wh_t + b).
\end{align}
<p>References:</p>
<ul>
<li>http://colah.github.io/posts/2015-08-Understanding-LSTMs/</li>
<li>http://colah.github.io/posts/2015-09-NN-Types-FP/</li>
</ul>
<h2 id="lstm-layer">LSTM layer</h2>
<p>We define functions</p>
<ul>
<li><code>step_lstm</code> :: <span class="math inline">\(\R^n\times \R^m\times \R^m \to \R^m\times \R^m\)</span> sending <span class="math display">\[(i_t, C_{t-1}, h_{t-1}) \mapsto (C_t, h_t).\]</span></li>
<li><code>sequence_lstm</code> :: <span class="math inline">\((\R^n)^s \times \R^m\times \R^m \to (\R^m)^s\)</span> sending <span class="math display">\[((i_t)_{t=1}^T, C_0, h_0)\mapsto (h_t)_{t=1}^T.\]</span> (This is essentially “scanl” of step_lstm.)</li>
<li><code>step_multiple_lstm</code> :: <span class="math inline">\((\R^n)^k\times (\R^m)^k \times (\R^m)^k \to (\R^m)^k \times (\R^m)^k\)</span>. The mapped vrsion of step_lstm. This we can implement efficiently as a matrix multiplication.</li>
<li><code>sequence_multiple_lstm</code> :: <span class="math inline">\(((\R^n)^s)^k\times (\R^m)^k \times (\R^m)^k \to (\R^m)^k \times (\R^m)^k \to ((\R^m)^s)^k\)</span>. There are two ways to write this:
<ul>
<li>As the mapped version of <code>sequence_lstm</code> (i.e., scan, then map).</li>
<li>As the scanned version of <code>step_multiple</code> (i.e., map, then scan). This is more efficient since we can implement the “map” as a matrix multiplication.</li>
</ul></li>
</ul>
<p>(Actually these functions will involve the parameters as well, which we omitted here.)</p>
<h3 id="implementation">Implementation</h3>
<p>Define <code>step_lstm1</code> by</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> step_lstm1(x, C, h, Wf, bf, Wi, bi, WC, bC, Wo, bo):
    hx <span class="op">=</span> T.concatenate([h,x]) <span class="co">#dimension m+n</span>
    f <span class="op">=</span> sigmoid(T.dot(hx, Wf) <span class="op">+</span> bf) <span class="co">#dimension m</span>
    i <span class="op">=</span> sigmoid(T.dot(hx, Wi) <span class="op">+</span> bi) <span class="co">#dimension m</span>
    C_add <span class="op">=</span> T.tanh(T.dot(hx, WC) <span class="op">+</span> bC) <span class="co">#dimension m</span>
    C1 <span class="op">=</span> f <span class="op">*</span> C <span class="op">+</span> i <span class="op">*</span> C_add <span class="co">#dimension m</span>
    o <span class="op">=</span> sigmoid(T.dot(hx, Wo) <span class="op">+</span> bo) <span class="co">#dimension m</span>
    h1 <span class="op">=</span> o <span class="op">*</span> T.tanh(C1) <span class="co">#dimension m</span>
    <span class="cf">return</span> [C1, h1] <span class="co">#dimension 2m</span></code></pre></div>
<p>Now define <code>step_lstm</code> as the version with parameters grouped together.</p>
<pre class="py"><code>def step_lstm(x, C, h, tparams): 
    Wf, bf, Wi, bi, WC, bC, Wo, bo = unpack_params(tparams, [&quot;Wf&quot;, &quot;bf&quot;, &quot;Wi&quot;, &quot;bi&quot;, &quot;WC&quot;, &quot;bC&quot;, &quot;Wo&quot;, &quot;bo&quot;])
    return step_lstm1(x, C, h, Wf, bf, Wi, bi, WC, bC, Wo, bo)</code></pre>
<p>To define <code>sequence_lstm</code> we use Theano’s can function. The arguments are</p>
<ul>
<li><code>fn</code> is the function</li>
<li><code>outputs_info</code> are the initial values in the recursion</li>
<li><code>non_sequences</code> are fixed values that are not involved in the recursion.</li>
</ul>
<p>Thus to create a scanned function like so</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">scan' ::</span> ((a,b,c) <span class="ot">-&gt;</span> b) <span class="ot">-&gt;</span> [a] <span class="ot">-&gt;</span> b <span class="ot">-&gt;</span> c <span class="ot">-&gt;</span> [b]
scan' f a's init fixed <span class="fu">=</span></code></pre></div>
<p>we call</p>
<pre class="py"><code>theano.scan(fn=f, sequences=a's, outputs_info=init, non_sequences=fixed)</code></pre>
<p>Note here a, b, c can encompass multiple arguments, in which case you pass a list to <code>sequences</code>, <code>outputs_info</code>, and <code>non_sequences</code>. However, a, b, c must appear in that order.</p>
<pre class="py"><code>def sequence_lstm(C0, h0, xs, tparams):.
    Wf, bf, Wi, bi, WC, bC, Wo, bo = unpack_params(tparams, [&quot;Wf&quot;, &quot;bf&quot;, &quot;Wi&quot;, &quot;bi&quot;, &quot;WC&quot;, &quot;bC&quot;, &quot;Wo&quot;, &quot;bo&quot;])
    #the function fn should have arguments in the following order:
    #sequences, outputs_info (accumulators), non_sequences
    #(x, C, h, Wf, bf, Wi, bi, WC, bC, Wo, bo)
    ([C_vals, h_vals], updates) = theano.scan(fn=step_lstm1,
                                          sequences = xs, 
                                          outputs_info=[C0, h0], #initial values of the memory/accumulator
                                          non_sequences=[Wf, bf, Wi, bi, WC, bC, Wo, bo], #fixed parameters
                                          strict=True)
    return [C_vals, h_vals]</code></pre>
<p>Note this will map automatically; to define <code>sequence_multiple_lstm</code> all we have to do is swap two axes.</p>
<p>(Note on Theano list in scan.)</p>
<h2 id="neural-net-functions">Neural net functions</h2>
<p>A vanilla neural net layer is</p>
<pre class="py"><code>def nn_layer1(x, W, b):
    return x * W + b

def nn_layer(x, tparams):
    W, b = unpack_params(tparams, [&quot;W&quot;, &quot;b&quot;])
    return nn_layer1(x, W, b)</code></pre>
<p>We define functions</p>
<ul>
<li><code>nn_layer</code> :: <span class="math inline">\(\R^n\times \R^n\)</span></li>
<li><code>logloss</code> :: <span class="math inline">\(\R^n\times \R^n\)</span> given by <span class="math display">\[\text{logloss}(x,y) = -\sum_i x_i \ln' (y_i)\]</span> where we use <code>corrected_log</code>, <span class="math inline">\(\ln'(y) = \ln(\max(10^{-6}, x))\)</span> to avoid blowup at small probabilities.</li>
</ul>
<p>Now we can combine these with our LSTM to make the evaluation, prediction, and loss function. Evaluation will give the probabilities of each output, prediction will give the output with max probability, and loss is the logloss on the expected and actual outcomes. We also include a accuracy function that outputs 1 if the prediction is correct and 0 otherwise.</p>
<p>Note <code>fns_lstm</code> returns a list of Theano variables (depending on the input lists/parameters) representing the activations, predictions, losses and accuracy. We haven’t compiled these variables into a function yet.</p>
<!--
We include a flag saying if we just want the output for the last in the sequence, or every time step. We also want versions that are mapped over sequences (to do them in batch).
-->
<p>(Add code here)</p>
<p>Some other functions:</p>
<ul>
<li><code>init_params_with_f_lstm(n,m,f,g)</code></li>
<li><code>train_lstm</code></li>
<li><code>weight_decay</code> :: <span class="math inline">\(\R\)</span> -&gt; Dict String TheanoVars -&gt; [String] -&gt; <span class="math inline">\(\R\)</span>. For the parameters in the list, sum the squares of their norms and multiply by the decay constant.</li>
</ul>
<p>(A further speedup is to concatenate the matrices.)</p>
<h2 id="data-processing-functions">Data processing functions</h2>
<p>We’ll keep parameters in a dictionary, and unpack them as needed.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> unpack_params(tparams, li):
    <span class="cf">return</span> [tparams[name] <span class="cf">for</span> name <span class="op">in</span> li]</code></pre></div>
<ul>
<li><code>wrap_theano_dict</code> and <code>unwrap_theano_dict</code>.</li>
<li><code>get_minibatches_idx</code> (::Int -&gt; Int -&gt; Bool -&gt; [(Int, [Int])]) will give an enumerated list of minibatch indices, given <code>n</code>, the size of the list, and <code>minibatch_size</code>. It will make a minibatch out of the remainder.</li>
<li><code>oneHot(choices, n)</code> gives a way to encode one-hot vectors within Theano.</li>
</ul>
<h2 id="optimization-functions">Optimization functions</h2>
<p>These are taken from…</p>
<p>The arguments of each are</p>
<ul>
<li>lr: learning rate</li>
<li>tparams: dictionary of parameters (not Theano variables)</li>
<li>grads: gradient of function to optimize</li>
<li>cost:</li>
<li>args: args to cost function (e.g., neural net inputs)</li>
</ul>
<p>Retrns</p>
<ul>
<li><code>f_grad_shared</code></li>
<li><code>f_update</code></li>
</ul>
<p>What does the train function need?</p>
<ul>
<li>Epochs: An epoch is going through all the data once. Stop after <code>patience</code> number of epochs have passed without progress, or after <code>max_epochs</code>.</li>
<li>Optimizer
<ul>
<li>Cost function to optimize
<ul>
<li>Arguments to cost function</li>
</ul></li>
</ul></li>
<li>Batch:
<ul>
<li>batch size during training</li>
<li>batch size for validation</li>
</ul></li>
<li>Initial parameters</li>
<li>Frequency (after how many updates do you…)
<ul>
<li>validate?</li>
<li>save data? (to where?)</li>
</ul></li>
<li>Data (train, validation, test): What’s the difference between validation and test?</li>
<li>Batch-maker: Given the data, make a list of batches. One epoch consists of going through all the batches.</li>
</ul>
<!-- Scraps
while I wait for someone to write a frontend in haskell...
-->

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[ALLMR16] RAND-WALK: A latent variable model approach to word embeddings</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/randwalk.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/randwalk.html</id>
    <published>2016-03-15T00:00:00Z</published>
    <updated>2016-03-15T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[ALLMR16] RAND-WALK: A latent variable model approach to word embeddings</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-15 
          , Modified: 2016-03-15 
	</p>
      
       <p>Tags: <a href="/tags/nlp.html">nlp</a>, <a href="/tags/word%20embeddings.html">word embeddings</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#task">Task</a></li>
 <li><a href="#previous-work-observed-phenomena">Previous work, observed phenomena</a><ul>
 <li><a href="#mysteries">Mysteries</a></li>
 </ul></li>
 <li><a href="#model">Model</a></li>
 <li><a href="#explanation">Explanation</a></li>
 <li><a href="#algorithm">Algorithm</a></li>
 <li><a href="#followup">Followup</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="task">Task</h2>
<p>Given a corpus (a long sequence of words, e.g. the text of Wikipedia) to learn from, answer analogy questions such as ?:queen::man:woman.</p>
<h2 id="previous-work-observed-phenomena">Previous work, observed phenomena</h2>
<p>The usual approach is to learn “word vectors.”</p>
<p>Define a context of a word <span class="math inline">\(w\)</span> in the corpus to be the two words (say) on either side of <span class="math inline">\(w\)</span>. (Thus, for a context <span class="math inline">\(\chi=(w_{-2},w_{-1},w_1,w_2)\)</span>, <span class="math inline">\(\Pj(\chi|w)\)</span> means the probability of observing the words of <span class="math inline">\(\chi\)</span> in a window of length 2, given that the middle word is <span class="math inline">\(w\)</span>.) (Mikolov)</p>
<p>Pennington et al. and Levy and Goldberg posit the following approach.</p>
<ul>
<li>Model: If <span class="math inline">\(a:b::c:d\)</span> is an analogy, then <span class="math display">\[ \fc{\Pj(\chi|a)}{\Pj(\chi|b)} \approx \fc{\Pj(\chi|a)}{\Pj(\chi|b)}.\]</span></li>
<li>Thus, the solution to the analogy <span class="math inline">\(?:b::c:d\)</span>$ is <span class="math display">\[ \amin_w \sum_\chi \pa{ \ln \pf{\Pj(\chi|w)}{\Pj(\chi|b)} - \ln \pf{\Pj(\chi|c)}{\Pj(\chi|d)} }^2.\]</span></li>
<li>Define probability-mutual-information <span class="math display">\[ PMI(w,\chi) = \ln \pf{\Pj(w,\chi)}{\Pj(w)\Pj(\chi)} = \ln \pf{\Pj(\chi|w)}{\Pj(\chi)}.\]</span> Let <span class="math inline">\(C\)</span> be the set of possible contexts. For a word <span class="math inline">\(w\)</span>, let <span class="math inline">\(v_w\)</span> be the vector indexed by contexts <span class="math inline">\(\chi \in C\)</span>, <!--containing the empirical estimate of PMI from the corpus,
    $$ v_w(\chi)= \wh{PMI}(w,\chi) = \ln \pf{\wh \Pj(w,\chi)}{\wh \Pj(w)\wh \Pj(\chi)} = \ln \pf{\wh \Pj(\chi|w)}{\wh \Pj(\chi)}.$$--> <span class="math display">\[ v_w(\chi)= {PMI}(w,\chi) = \ln \pf{ \Pj(w,\chi)}{\Pj(w) \Pj(\chi)} = \ln \pf{ \Pj(\chi|w)}{\Pj(\chi)}.\]</span> Under this embedding, the summand equals <span class="math inline">\(v_a-v_b-v_c+v_d\)</span>. To find <span class="math inline">\(a\)</span>, solve <span class="math display">\[ \amin_a \ve{v_a-v_b-v_c+v_d}^2.\]</span></li>
<li>Algorithm: GloVe (global vector) method (Pennington) Let <span class="math inline">\(X_{w,w'}\)</span> be the co-occurrence for words <span class="math inline">\(w,w'\)</span>. Find low-dimensional <span class="math inline">\(\wt v_w, \wt v_{w'}, \wt b_w, \wt b_{w'}\)</span> to minimize <span class="math display">\[ \sum_{w,w'} f(X_{w,w'}) (\an{\wt v_w, \wt v_{w'}} - \wt b_w- \wt b_{w'} - \ln X_{w,w'})^2\]</span> for some function <span class="math inline">\(f\)</span>. They choose <span class="math inline">\(f(x) = \min\bc{\pf{x}{x_{\max}}^{.75}, 1}, x_{\max}=100\)</span> from experiments.</li>
</ul>
<p>(How to optimize this?)</p>
<h3 id="mysteries">Mysteries</h3>
<ol type="1">
<li>There is a disconnect between the definition of <span class="math inline">\(v_w\)</span> and the estimate <span class="math inline">\(\wt v_w\)</span>. Namely, the <span class="math inline">\(v_w\)</span> are vectors giving the PMI with all <em>contexts</em> while <span class="math inline">\(\wt v_w\)</span> are the vectors such that <span class="math inline">\(\an{\wt v_w,\wt v_{w'}}\)</span> give the <em>word-word co-occurences</em>. Why do the learned <span class="math inline">\(\wt v_w\)</span> help in solving analogies? Why is the optimization problem in the algorithm a good proxy?</li>
<li>Why is this method stable to noise? <!--Why would we suspect noise is bad?--></li>
</ol>
<h2 id="model">Model</h2>
<p>Let the dimension of the underlying space be <span class="math inline">\(d\)</span>.</p>
<ul>
<li>There are <span class="math inline">\(n\)</span> words <span class="math inline">\(w\)</span> in the dictionary <span class="math inline">\(W\)</span>, and they are associated with vectors <span class="math inline">\(v_w\in \R^d\)</span>.
<ul>
<li>The vectors <span class="math inline">\(v_w\)</span> are iid generated by <span class="math inline">\(v=s\cdot \wh v\)</span> where <!--$\wh v$ is uniform on the sphere and -->
<ul>
<li><span class="math inline">\(\wh v\sim N(0,I_d)\)</span>,</li>
<li><span class="math inline">\(s\)</span> is a random scalar with <span class="math inline">\(\si^2\le d\)</span> and <span class="math inline">\(|s|\le \ka \sqrt d\)</span> for some constant <span class="math inline">\(\ka\)</span>.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></li>
</ul></li>
</ul></li>
<li>At each time <span class="math inline">\(t\in \N\)</span>, there is a context vector <span class="math inline">\(c_t\in \R^d\)</span>.
<ul>
<li>The vectors <span class="math inline">\(c_t\)</span> follow a random walk, satisfying the following:
<ul>
<li>(Uniform stationary distribution) The stationary distribution is <span class="math inline">\([-\rc{\sqrt d},\rc{\sqrt d}]^d\)</span>.</li>
<li>(Small drift) The drift in the context vector is <span class="math inline">\(\ve{c_{t+1}-c_t}_1\le \rc{\ln n}\)</span>. (There are more complicated general conditions under which the theorems work; take this for simplicity.) <!--  * (small drift) $\ve{\De_t}_1\le \rc{\ln n}$.
*--></li>
</ul></li>
</ul></li>
<li>At each time <span class="math inline">\(t\)</span>, word <span class="math inline">\(w\)</span> is emitted with probability
\begin{align}
\Pj[w|c_t] &amp;= \rc{Z_{c_t}} \exp(\an{v_w,c_t})\\
\text{where} Z_{c}:&amp;= \sum_w \exp(\an{v_w,c}).
\end{align}</li>
</ul>
<h2 id="explanation">Explanation</h2>
<p>Let <span class="math inline">\(\Pj(w,w')\)</span> be the probability that <span class="math inline">\(w,w'\)</span> appear consecutively at time <span class="math inline">\(t,t+1\)</span> when <span class="math inline">\(c_t\sim U_{[-\rc{\sqrt d},\rc{\sqrt d}]^d}\)</span> is drawn from the stationary distribution. Then the following hold.</p>
<strong>Theorem 1</strong>: With high probability over choice of <span class="math inline">\(v_w\)</span>’s,
\begin{align}
\forall &amp;w,w', &amp;
\ln \Pj (w,w')&amp;\approx \rc{2d}|v_w+v_w'|^2 - 2\lg Z - o(1)\\
\forall &amp; w,&amp;
\lg \Pj(w) &amp;\approx \rc{2d} |v_w|^2 - \lg Z -o(1)\\
\therefore &amp;&amp; \lg \fc{\Pj[w,w']}{\Pj[w]\Pj[w']} &amp;\approx \rc d \an{v_w,v_w'}\pm o(1).
\end{align}
<p>This is exactly the PMI, so the theorem “explains” Mystery 1.</p>
<p><em>Proof idea</em>:</p>
<p>Let <span class="math inline">\(c\)</span> be the context vector at time <span class="math inline">\(t\)</span> and <span class="math inline">\(c'\)</span> be the vector at time <span class="math inline">\(t+1\)</span>.</p>
<ol type="1">
<li>The main difficulty is that <span class="math inline">\(Z_c\)</span> is intractable to compute. However, because the <span class="math inline">\(v_w\)</span> are random (or isotropic), so <span class="math inline">\(Z_c\)</span> concentrates around its mean, and we can approximate it by a constant <span class="math inline">\(Z\)</span> (Theorem 2 in the paper).</li>
<li>Because drift is small, we can make the approximation <span class="math inline">\(c'\approx c\)</span>. Then
\begin{align}
\Pj(w,w') &amp;= \int_{c,c'} \Pj(w|c)\Pj(w'|c_{t+1})\Pj(c,c')\,dc\,dc'\\
&amp;\sim \rc{Z^2} \EE_{c} \exp(\an{v_w+v_w',c})\\
&amp;\sim \rc{Z^2}\exp\pf{|v_w+v_{w'}|^2}{2}
\end{align}
using some calculus in the last step (exercise). <!--check this--></li>
</ol>
<p>The calculation for <span class="math inline">\(\Pj(w)\)</span> is even simpler.</p>
<h2 id="algorithm">Algorithm</h2>
<p>What algorithm does the theory suggest to estimate the <span class="math inline">\(v_w\)</span>’s?</p>
<p>It suggests minimizing an objective as in GloVe. The weights <span class="math inline">\(f_{w,w'}\)</span> re selected to compensate noise in <span class="math inline">\(X_{w,w'}\)</span>; when <span class="math inline">\(X_{w,w'}\)</span> is on the average larger, it has lower variance, and the weight <span class="math inline">\(f_{w,w'}\)</span> is larger.</p>
<p>(What improvement does it suggest?)</p>
<h2 id="followup">Followup</h2>
<ul>
<li>Polysemy</li>
<li>Weighted SVD (Yuanzhi Li)</li>
</ul>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>The results in the paper are stated as “with high probability over the choice of <span class="math inline">\(v_w\)</span>”. This can probably be relaxed to “For all <span class="math inline">\(v_w\)</span> that are isotropic”, where <strong>isotropic</strong> means <span class="math inline">\(\EE_w [v_wv_w^T]\)</span> has all eigenvalues in <span class="math inline">\([1,1+\de]\)</span>.<a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Thoughts on LDC's</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/coding/3-21-16.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/coding/3-21-16.html</id>
    <published>2016-03-14T00:00:00Z</published>
    <updated>2016-03-14T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Thoughts on LDC's</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-14 
          , Modified: 2016-03-14 
	</p>
      
       <p>Tags: <a href="/tags/LDC.html">LDC</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#zeevs-thoughts">Zeev’s thoughts</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="zeevs-thoughts">Zeev’s thoughts</h2>
<p>I was thinking a bit about the 2 query case (proving lower bounds). If we define the decoding map for 2-LCC we get a quadratic map Q from R^n to R^n sending the codewords (in +-1) to themselves. Now, the Jacobian of this map can be written (I think) as a combination of permutation matrices with coefficients that are the x_i. Using standard matrix chernoff/Bernstein bounds (see here: http://users.cms.caltech.edu/~jtropp/books/Tro14-Introduction-Matrix-FnTML-rev.pdf) I think you can show that the Jacobian has tiny norm almost everywhere. Which means that, to contain all the codewords, the image of Q must look very `spiky’ (it has small volume but contains a code). But then I started thinking that maybe we can use the fact that it is a low degree mapping to show that it cannot be too spiky. A quick google search came up with this paper:</p>
<p>http://www.math.lsa.umich.edu/~barvinok/product.pdf</p>
<p>Which is not exactly what we need, but seems in the right direction (and uses tools we already discussed before). If it works, this has some chance to generalize to degree 3 maps as well (not using the same tools but maybe something more sophisticated from real AG).</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Thoughts on LDC's</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/coding/3-14-16.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/coding/3-14-16.html</id>
    <published>2016-03-14T00:00:00Z</published>
    <updated>2016-03-14T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Thoughts on LDC's</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-14 
          , Modified: 2016-03-14 
	</p>
      
       <p>Tags: <a href="/tags/LDC.html">LDC</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#section">3-14-16</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="section">3-14-16</h2>
<ul>
<li>Why the lower bound for 3-query LCC’s doesn’t extend to LDC’s.</li>
<li>We aren’t using anything about the form of <span class="math inline">\(\phi\)</span> other than small dual Gowers uniformity…</li>
<li>An interpretation of the AP differences <span class="math inline">\(y\)</span>: it fails to be an LDC if you can’t arbitrarily specify the (relative) density of AP’s in those directions.</li>
<li>A MVC is linear on <span class="math inline">\(\F_p\)</span>, not <span class="math inline">\(\Z/m\)</span>. The coefficients and evaluations are in <span class="math inline">\(\F_p\)</span> even though the vectors aren’t. (Could we do better with codes on <span class="math inline">\(\Z/m\)</span>?) A linear code on <span class="math inline">\(\F_p\)</span> can be converted to a code on <span class="math inline">\(\F_2\)</span> but it loses linearity.
<ul>
<li>However, a MVC is NOT captured by the “query at an AP formalism.” Although the codeword index space is <span class="math inline">\(\F_p^N\)</span>, we’re not using that group structure for querying. Rather, we’re using the group structure of <span class="math inline">\(\F_p^{\times N}\)</span>. So it could still be possible to prove a lower bound querying AP’s.</li>
<li>That is false, it IS captured. The group can be different! There is a group, and there is a field. Two different things!</li>
</ul></li>
<li>Q4.14 in “APs and LDCs”: When <span class="math inline">\(N\)</span> grows large, there doesn’t even exist a degree 1 polynomial for which this holds. This doesn’t seem promising; the only interesting case is for <span class="math inline">\(n\)</span> constant or small.</li>
<li>Variations on the poly method:
<ul>
<li>How to use it for small finite fields? (In Kakeya, <span class="math inline">\(p\to \iy\)</span>.)</li>
<li>How about considering polynomials with multiple outputs, ex. <span class="math inline">\(\F_p^n\to \F_p^{n-2}\)</span> to capture vanishing on a plane?</li>
</ul></li>
<li>To study: Guth’s course. Does the idea for the plane/regulus theorem help in thinking about <span class="math inline">\(\de\)</span>-SG configurations and LDC’s?</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Type and Cotype</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/analysis/metric/type.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/analysis/metric/type.html</id>
    <published>2016-03-14T00:00:00Z</published>
    <updated>2016-03-14T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Type and Cotype</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-14 
          , Modified: 2016-03-14 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <ol type="1">
<li>Say that <span class="math inline">\(X\)</span> has <strong>type</strong> <span class="math inline">\(p\)</span> if there exists <span class="math inline">\(C&gt;0\)</span> such that for every <span class="math inline">\(n, y_1,\ldots, y_n\in X\)</span>, [ _{{1}^n} _XC^{p}. ]
<ul>
<li>This is always true for <span class="math inline">\(p=1\)</span> by the triangle inquality.</li>
<li>The RHS decreases as <span class="math inline">\(p\)</span> increases.</li>
<li>Let <span class="math inline">\(T_p(X)\)</span> be the infimum of valid <span class="math inline">\(T\)</span>.</li>
<li><span class="math inline">\(X\)</span> has <strong>nontrivial type</strong> if it has type <span class="math inline">\(&gt;1\)</span>.</li>
</ul></li>
<li>Say that <span class="math inline">\(X\)</span> has <strong>cotype</strong> <span class="math inline">\(r\)</span> if there exists <span class="math inline">\(C&gt;0\)</span> such that for every <span class="math inline">\(n, x_1,\ldots, x_n\in Y\)</span>, [ _{{1}^n} _YC^{p}. ]
<ul>
<li>This is always true for <span class="math inline">\(p=\iy\)</span> by Jensen.</li>
<li>Let <span class="math inline">\(C_r(X)\)</span> be the infimum of valid <span class="math inline">\(C\)</span>.</li>
<li><span class="math inline">\(X\)</span> has <strong>finite cotype</strong> if it has type <span class="math inline">\(&lt;\iy\)</span>.</li>
</ul></li>
</ol>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Neural nets basics</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/basics.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/basics.html</id>
    <published>2016-03-13T00:00:00Z</published>
    <updated>2016-03-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Neural nets basics</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-13 
          , Modified: 2016-03-13 
	</p>
      
       <p>Tags: <a href="/tags/neural%20net.html">neural net</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <h2 id="types">Types</h2>
<ul>
<li>neural net (vanilla)</li>
<li>convolutional neural net</li>
<li>recurrent neural nets
<ul>
<li>LSTM</li>
</ul></li>
<li>A <strong>Boltzmann machine</strong> has joint distribution of two adjacent layers to be <span class="math inline">\(\exp(x^TAh)\)</span>. If it has only two layers it is reversible, i.e., <span class="math inline">\(\Pj(x|h),\Pj(h|x)\)</span> are both easy to calculate. (A regularized version would be <span class="math inline">\(\exp(x^TAh - hh^T)\)</span>. (?))
<ul>
<li>A <strong>DBM</strong> stacks these units (so that the probability of a configuration is now <span class="math inline">\(\exp\pa{\sum x_i^TA_ix^{i+1}}\)</span>). It loses reversibility.</li>
<li>This is a graphical model. It’s a probability distribution rather than a deterministic function as in a vanilla neural net.</li>
</ul></li>
</ul>
<h2 id="functions">Functions</h2>
<ul>
<li>RELU <span class="math inline">\((x\ge 0) x\)</span>.</li>
</ul>
<h2 id="features">Features</h2>
<ul>
<li>Dropout
<ul>
<li>On nodes: zero out each node in a layer with probability <span class="math inline">\(1-\rh\)</span>.</li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[ALM16] WHY ARE DEEP NETS REVERSIBLE: A SIMPLE THEORY, WITH IMPLICATIONS FOR TRAINING</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/ALM16.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/ALM16.html</id>
    <published>2016-03-13T00:00:00Z</published>
    <updated>2016-03-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[ALM16] WHY ARE DEEP NETS REVERSIBLE: A SIMPLE THEORY, WITH IMPLICATIONS FOR TRAINING</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-13 
          , Modified: 2016-03-13 
	</p>
      
       <p>Tags: <a href="/tags/neural%20net.html">neural net</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#summary">Summary</a></li>
 <li><a href="#model">Model</a><ul>
 <li><a href="#boltzmann-machine">Boltzmann machine</a></li>
 <li><a href="#layer-neural-net">1-layer neural net</a></li>
 <li><a href="#multi-layer-neural-net">Multi-layer neural net</a></li>
 </ul></li>
 <li><a href="#theorems-and-proof-sketches">Theorems and proof sketches</a><ul>
 <li><a href="#baby-theorem">Baby theorem</a></li>
 <li><a href="#reversibility-of-one-layer-nets-with-dropout-and-relu">Reversibility of one-layer nets with dropout and ReLU</a></li>
 <li><a href="#reversibility-for-2-layers">Reversibility for 2+ layers</a></li>
 </ul></li>
 <li><a href="#experiments">Experiments</a></li>
 <li><a href="#questions">Questions</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>Arora, Sanjeev, Yingyu Liang, and Tengyu Ma. “Why are deep nets reversible: A simple theory, with implications for training.” arXiv preprint arXiv:1511.05653 (2015).</p>
<p>http://arxiv.org/pdf/1511.05653</p>
<h2 id="summary">Summary</h2>
<p>Consider a feedforward net with input <span class="math inline">\(x\)</span> and output <span class="math inline">\(h\)</span>. ALM give a model under which a neural net can be said to be predicting the output distribution. This also gives a theoretical explanation of why it’s possible to use a neural net to do the following: given <span class="math inline">\(h\)</span>, generate some <span class="math inline">\(x\)</span> that could have given rise to that <span class="math inline">\(h\)</span> (cf. neural net dreams).</p>
<p>Their theoretical explanation has two important ingredients for theoretical explanations:</p>
<ol type="1">
<li>It specifies a joint distribution of <span class="math inline">\(x,h\)</span>. (Specifying <span class="math inline">\(x|h\)</span> may also be good enough).</li>
<li>It gives a proof that the deep net does compute the most likely <span class="math inline">\(h\)</span> given <span class="math inline">\(x\)</span> (in some sense, up to some error).</li>
</ol>
<h2 id="model">Model</h2>
<p>(See <a href="basics.html">neural net basics</a>.)</p>
<p>We want to model (input, output) by a probability distribution, and prove that a neural net is predicting the output given the input, with respect to that probability distribution.</p>
<h3 id="boltzmann-machine">Boltzmann machine</h3>
<p>A <strong>Boltzmann machine</strong> is a joint distribution of random variables <span class="math inline">\(x,h \in \R^{m\times n}\)</span> given by <span class="math display">\[\Pj(x,h) \approx \exp(-x^TAh + \pat{regularization}).\]</span> It is reversible because <span class="math inline">\(\Pj(x|h),\Pj(h|x)\)</span> are both easy to calculate. (Deep Boltzmann machines are much less nice—they lose reversibility.)</p>
<p>Compare this to a 2-layer neural net. Note a Boltzmann machine is a probabilistic model, not a neural net. A neural net and a Boltzmann machine both model the relationship between an input vector and an output vector, but do it differently: A neural net deterministically computes an output as a function of its input, while a Boltzmann machine gives a probability distribution on (input, output). A Boltzmann machine is trivially reversible; our hope is that a neural net is also reversible. <!-- How are these models related? A Boltzmann machine is one of the most natural generative models for neural nets; one hope is that a neural net is predicting a distribution given by a Boltzmann machine.
Note by this reversibility, there is not much to prove --></p>
<p>Think of <span class="math inline">\(x\)</span> as the observed layer and <span class="math inline">\(h\)</span> as the hidden layer. (For example, think of it as the middle layer of an autoencoder. Because we’re not considering any layers put on top of <span class="math inline">\(h\)</span>, we also think of it as the output.)</p>
<p>We don’t worry about learning (ex. gradient descent) here. We just consider a neural net statically.</p>
<!-- Note we're *not* actually using the Boltzmann machine model.-->
<h3 id="layer-neural-net">1-layer neural net</h3>
<p>The model is the following. Key aspects of the model are modeling weights by random matrices<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>, hypothesizing/enforcing sparsity, and allowing dropout (this is a standard training technique, so we want the theoretical results to hold in this setting).<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<p>Here <span class="math inline">\(x\in \R^n,h\in \R^m\)</span> and <span class="math inline">\(m&lt;n\)</span>. The hidden layer has fewer nodes, so the forward map <span class="math inline">\(x\mapsto h\)</span> is many-to-one. This is necessary if we want to be able to reconstruct <span class="math inline">\(h\)</span> from <span class="math inline">\(x\)</span> with high probability.</p>
<ul>
<li>Generate <span class="math inline">\(h\sim D_h\)</span> where <span class="math inline">\(D_h\)</span> is any distribution on <span class="math inline">\(\R^n_{\ge 0}\)</span> satisfying the following: With <span class="math inline">\(1-\text{neg}(n)\)</span> probability,
<ul>
<li>(Sparsity) <span class="math inline">\(\ve{h}_0\le k\)</span>.</li>
<li>(No coordinate much larger than the average) <span class="math inline">\(\ve{h}_{\iy}\le \be \ve{h}\)</span> where <span class="math inline">\(\be = O\sfc{\ln k}{k}\)</span>.</li>
</ul></li>
<li>Let <span class="math inline">\(W\in \R^{n\times m}\)</span> be a matrix with random <span class="math inline">\(N(0,1)\)</span> entries.</li>
<li>Generate the observed variable by <span class="math display">\[ x \sim r(\al W h)\odot n_{\rh}, \]</span> where
<ul>
<li><span class="math inline">\(n_{\rh}\)</span> is a vector of iid draws from Bernoulli<span class="math inline">\((\rh)\)</span>, and <span class="math inline">\(\odot\)</span> is componentwise multiplication, i.e., entries are zeroed out with probability <span class="math inline">\(1-\rh\)</span>.</li>
<li><span class="math inline">\(\al=\fc{2}{\rh n}\)</span>, so that it normalizes the effect of dropout. (Why factor of 2?)</li>
</ul></li>
</ul>
<p>The feedforward neural net does the following operation. For some <span class="math inline">\(b\)</span>,</p>
<ul>
<li>Given <span class="math inline">\(x\)</span>, compute <span class="math inline">\(\hat h = r(W^Tx+b)\)</span>, where <span class="math inline">\(r(x)=\sgn(x)\cdot (x\ge 0)\)</span><a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> is the <strong>ReLU (rectified linear unit)</strong> function.
<ul>
<li>If we’re using dropout (with parameter <span class="math inline">\(\rc2\)</span>, say), replace <span class="math inline">\(x\)</span> by <span class="math inline">\(x \odot n_{\rc 2}\)</span> before doing this.</li>
</ul></li>
</ul>
<p>The hope is that <span class="math inline">\(\hat h \approx h\)</span>. Why do we take the matrix in the proposed inverse map to be <span class="math inline">\(W^T\)</span>? For random matrices, the transpose is like an inverse, because <span class="math inline">\(\ve{W^TW-I}_2\)</span> will be small.</p>
<h3 id="multi-layer-neural-net">Multi-layer neural net</h3>
<p>Let <span class="math inline">\(W_t,\ldots, W_1\)</span> be random matrices. Basically just iterate the construction <span class="math inline">\(t\)</span> times in both the generative model and the feedforward net (from <span class="math inline">\(h^{(t)}\)</span> get <span class="math inline">\(h^{(t-1)},\ldots, h^{(0)}=x\)</span> and from <span class="math inline">\(x\)</span> generate <span class="math inline">\(\wh h^{(1)}, \ldots , \wh h^{(t)}\)</span>). In the dropout case, do dropout on each layer.</p>
<h2 id="theorems-and-proof-sketches">Theorems and proof sketches</h2>
<h3 id="baby-theorem">Baby theorem</h3>
<p>To get an idea of why reversibility might hold, let’s consider a random one-layer neural net without nonlinearities (or bias), which is just multiplying by a random matrix. In this case <span class="math inline">\(x=\rc nWh\)</span>, <span class="math inline">\(\wh h = W^Tx=\rc n W^TWh\)</span>.</p>
<p><strong>Theorem (Baby version)</strong>: Let <span class="math inline">\(h\in \{0,1\}^m\)</span> be fixed, <span class="math inline">\(m&lt;n\)</span>. Suppose <span class="math inline">\(W\in \R^{n\times m}\)</span> has <span class="math inline">\(N(0,1)\)</span> entries. Then for any polynomial <span class="math inline">\(p(n)\)</span> there is <span class="math inline">\(C\)</span> so that <span class="math display">\[\Pj_{W}\ba{\ve{W^TWh - h}_{\iy}\le C\ln(mn)\sfc{m}{n}}\ge 1-\rc{p(n)}.\]</span></p>
<em>Proof</em>: We have
\begin{align}
(W^TWh)_i &amp;= (w_{i\bullet})^TW h\\
&amp;=\rc n (w_{i\bullet})^T w_{i\bullet}h + \ub{\rc n\sum_{j\ne i} (w_{i\bullet})^T w_{j\bullet} h}{\text{noise}}\\
&amp;=\rc n (w_{i\bullet})^T w_{i\bullet} h +  \rc n\sum_{k=1}^m w_{ik} \sum_{j\ne i} W_{jk} h_k.
\end{align}
<p>By Chernoff, <span class="math inline">\(\rc n w_{i\bullet}^T w_{i\bullet}\)</span>, as a sum of <span class="math inline">\(n\)</span> variables distributed as <span class="math inline">\(\chi_1^2\)</span>, is <span class="math inline">\(\rc{\sqrt{n}}\)</span>-concentrated around 1. There are <span class="math inline">\(&lt;mn\)</span> terms in the noise term, so it is <span class="math inline">\(\fc{\sqrt{mn}}{n}\)</span> concentrated around 0 (a little work is needed here—details left to the reader). Thus we get <span class="math inline">\(\sfc{m}{n}\)</span>-concentration around 0.</p>
<p>If we bound by <span class="math inline">\(\ln(mn)\)</span> times this quantity, then we can use the union bound to finish. <span class="math inline">\(\square\)</span></p>
<p>Before moving on, we note two things.</p>
<ol type="1">
<li>To get a good bound here, we need <span class="math inline">\(n\gg m\)</span>, i.e., the hidden layer needs to be much smaller. Often this is not true in practice.</li>
<li>We get a <span class="math inline">\(L^{\iy}\)</span> bound which naively doesn’t give a good <span class="math inline">\(L^2\)</span> bound.</li>
</ol>
<p>The key next step is to assume that <span class="math inline">\(h\)</span> is sparse. It turns out then having a sigmoid function (ReLU) can be naturally interpreted as picking out the nonzero coordinates, ``recovering&quot; the sparse <span class="math inline">\(h\)</span>. Thus, <em>thresholding has a denoising effect</em>. This allows better recovery (in the <span class="math inline">\(L^2\)</span> norm).</p>
<h3 id="reversibility-of-one-layer-nets-with-dropout-and-relu">Reversibility of one-layer nets with dropout and ReLU</h3>
<p><strong>Theorem 2.3</strong>: (Formulation in appendix A.) Let <span class="math inline">\(W,h,x\)</span> be generated as in the model. Suppose <span class="math inline">\(k\)</span> is such that <span class="math inline">\(k&lt;\rh n&lt;k^2\)</span> (the number of non-dropped entries greater than the minimum sparsity, but also not too much). Then <span class="math display">\[
\ab{\Pj_{x,h,W}\ba{\ve{r(W^T x+b) - h}^2\le \wt O\pf{k}{\rh n}\ve{h}^2}} \ge 1-\rc{\poly(n)}.
\]</span></p>
<p><em>Proof (Theorem 2.3)</em>:</p>
<ol type="1">
<li><p><strong>Lemma 2.5</strong>: For <span class="math inline">\(\de=\wt O\prc{\sqrt m}\)</span>, <span class="math display">\[\Pj_{W,h,x}\ba{\ve{W^Tx - h}_\iy\le \de\ve{h}^2} \ge 1-\rc{\poly(n)}.\]</span></p>
<em>Proof of (2.5)<span class="math inline">\(\implies\)</span>(2.3):</em> If we used the naive <span class="math inline">\(L^\iy-L^2\)</span> bound, we get that w.h.p. the <span class="math inline">\(L^2\)</span> norm is at most <span class="math inline">\(m(\de\ve{h})^2 = \wt O\pf{m\ve{h}^2}{t}\)</span>, which is too large. We need to use sparsity to get a good bound. The idea is to zero out the entries with <span class="math inline">\(h_i=0\)</span> by adding a bias term <span class="math inline">\(b\)</span> and thresholding using <span class="math inline">\(r\)</span>. The <span class="math inline">\(L^{\iy}\)</span> bound in Lemma 2.3 tells us the offset necessary to zero out the entries where <span class="math inline">\(h_i=0\)</span>, <span class="math inline">\(b=-\de \ve{h}_1\)</span>. With this value of <span class="math inline">\(b\)</span>,
\begin{align}
h_i&amp;=0 &amp;\implies ((W^Tx)_i+b_i)&amp;\in [-2\de \ve{h},0]&amp;\implies \hat h_i&amp;=r((W^T)x_i + b_i) &amp;= h_i = 0\\
h_i&amp;\ne 0 &amp; \implies |\wt h_i-h_i| &amp;\le 2\de\ve{h}.
\end{align}
Square and multiply by <span class="math inline">\(k\)</span>.</li>
<li><em>Proof of Lemma 2.5:</em> We have
\begin{align}
x_j &amp;= r\pa{\al \sumo im W_{ji} h_i} (n_\rh)_j\\
\hat h_i &amp;= \sumo jn (W^T)_{ij} x_j\\
&amp;= \sumo jn \ub{W_{ji} r\pa{\al \sumo km W_{jk} h_k}}{=:Z_j} (n_\rh)_j.
\end{align}
As before, think of the sum inside <span class="math inline">\(r\)</span> as a main term plus noise: <span class="math display">\[\sumo km W_{jk} h_k = W_{ji} h_i + \sum_{k\ne i} W_{jk} h_k.\]</span> We want to compute <span class="math inline">\(\E[\hat h_i|h]\)</span>. To do this we need to understand the distribution of expressions that look like <span class="math inline">\(Z_j\)</span>.
<ul>
<li><strong>Lemma (Technical)</strong>: Let <span class="math inline">\(w\sim N(0,1), \xi \sim N(0,\si^2), \si=\Om(1), 0\le h\le \ln \si\)</span>. Then
\begin{align}
\EE_{w,\xi} [w\cdot r(wh+\xi)] &amp; = \fc h2 \pm \wt O\prc{\si^3}\\
\EE_{w,\xi} [w^2\cdot r(wh+\xi)] &amp; \le 3h^2 + \si^2.
\end{align}
<em>Proof:</em> To calculate the expectation, take the integral over <span class="math inline">\(\xi\)</span> first and then <span class="math inline">\(w\)</span>. <span class="math inline">\(w,\xi\)</span> are Gaussians, so we can evaluate the expectation <span class="math display">\[ \E[\E[w\cdot r(wh+\xi)|w]] = \fc{h}2+\E[G(w)]\]</span> for some calculable error <span class="math inline">\(G\)</span>. Estimate <span class="math inline">\(G\)</span> with its Taylor expansion (of degree 4). Now calculate <span class="math inline">\(\E[G(w)]\)</span> by estimating it for values below and above a cutoff. Using the lemma on the terms and noting <span class="math inline">\(\E\ve{n_j}_0=\rh n\)</span>, and the normalizing constant <span class="math inline">\(\al=\fc{2}{\rh n}\)</span>, we have <span class="math display">\[ \E[\wh h_i|h] = h_i \pm \wt O\prc{k^{\fc 32}}.\]</span></li>
<li>Show that <span class="math inline">\(\wh h_i|h_i\)</span> concentrates with high probability. The <span class="math inline">\(Z_j\)</span> are independent so we can use a version of Matrix Bernstein. Technically it seems we have to use a version for subexponential random variables, in terms of Orlicz norms, and check that <span class="math inline">\(Z_j\)</span> hve small norm in expectation. See Theorem D1.</li>
</ul></li>
</ol>
<!--Write
\begin{align}
x_j &= r(W_{ji} h_i + \ub{\sum_{l=1}^m W_{jl} h_l}{=:\eta_j}) (j\in T),\\
h_i &= \sum_{j=1}^n W_{ji}x_j
\end{align}
where $T$ is the set of non-zeroed out coordinates. We have $h_i = $. 
We want to show this is close to $h_j$ with high probability. 
-->
<h3 id="reversibility-for-2-layers">Reversibility for 2+ layers</h3>
<p><strong>Theorem </strong>: Similar theorems hold for 2 and 3 layers, in a weaker sense.</p>
<p>(I haven’t gone through this.)</p>
<h2 id="experiments">Experiments</h2>
<p>The theory gives a way to improve training. Take a labeled data point <span class="math inline">\(x\)</span>, use the current feedforward net to compute the label <span class="math inline">\(h\)</span>, and use the <strong>shadow distribution</strong> <span class="math inline">\(p(x|h)\)</span> to create a synthetic data point <span class="math inline">\(\wt x\)</span>, and use <span class="math inline">\((\wt x,z)\)</span> as a training pair.</p>
<h2 id="questions">Questions</h2>
<ul>
<li>How realistic is the sparsity assumption? How realistic is the model?</li>
<li>Can we use Boltzmann machines as the model instead?</li>
<li>What complications come up for 2+ layers? Is there a proof for any constant number of layers without loss?</li>
</ul>
<!---
Show that feedforward deep nets compute z|x in this model. (You can define this by
MLE, MAP, etc.)
-->
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Because we use properties such as eigenvalue concentration, this suggests that the theorem will still hold for “random-like” matrices, i.e., matrices having these properties.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Dropout encourages all of the nodes to learn useful features, because the neural net will essentially rely on a subset of them to make a prediction. (Think of nodes as taking a “majority vote” over inputs; dropout makes sure this still works even if you only take a subset.)<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>We use the notation <span class="math inline">\((P)=\begin{cases} 1,&amp;P\text{ true}\\ 0,&amp;P\text{ false}. \end{cases}\)</span><a href="#fnref3">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Interests</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/interests.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/interests.html</id>
    <published>2016-03-13T00:00:00Z</published>
    <updated>2016-03-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Interests</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-13 
          , Modified: 2016-03-13 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#standard-topics">“Standard” topics</a><ul>
 <li><a href="#links">Links</a></li>
 </ul></li>
 <li><a href="#other-topics">Other topics</a></li>
 <li><a href="#sanjeev-aroras-group">Sanjeev Arora’s group</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="standard-topics">“Standard” topics</h2>
<ol type="1">
<li>How can we give theoretical guarantees for machine learning? <a href="http://unsupervised.cs.princeton.edu/index.html">Sanjeev Arora’s page</a>.
<ul>
<li>Model the input distribution (e.g. sparsity).</li>
<li>Explain why standard approaches (e.g. neural nets) work (e.g., with respect to the input distribution). <a href="neural_nets/nn_modeling.html">Neural net modeling</a></li>
<li>Use the theoretical understanding to obtain better algorithms.</li>
</ul></li>
<li>Apply math (analysis, geometry, probability) to machine learning problems.
<ul>
<li>For example, understand threshold phenomenon in community detection and other graph problems.</li>
<li>Many problems in convex optimization can be understood in terms of choosing a right kernel, doing random walks, convex geometry, simulated annealing (cf. statistical physics), etc. “Manifold learning,” etc.</li>
<li>Understand the “random” case of objects like graphs and neural nets and analyze using ideas from statistical physics, random matrix theory, etc.</li>
<li>Questions in data science can involve topology, differential geometry, etc.</li>
</ul></li>
</ol>
<h3 id="links">Links</h3>
<ul>
<li><a href="http://www.cs.princeton.edu/~ehazan/">Elad Hazan</a></li>
<li><a href="http://www.scrible.com/contentview/page/64G21C041IKS82NH20O741000I60MGAV:206493171/index.html?utm_source=tb_permalink&amp;utm_medium=permalink&amp;utm_campaign=tb_buttons&amp;_sti=2847545">“Deep stuff about deep learning”</a></li>
</ul>
<h2 id="other-topics">Other topics</h2>
<ol type="1">
<li>How can we obtain theoretical guarantees for behavior of an AI (e.g. on the level of logic)? See <a href="http://cstheory.stackexchange.com/questions/32445/program-reasoning-about-own-source-code">my question</a>. Formulate the question of AI control mathematically and prove theorems. <a href="https://medium.com/ai-control">Paul Christiano</a>.</li>
<li>How can we combine ML/data and logical approaches in domains that require modeling with logic, ex. automated theorem proving, NLP (ex. combine CFG-based approaches with neural net approaches), reasoning about series of events with causality, etc.? What are the structure of these problems that allow tractable inference (ex. sentences in natural language are “mostly unambiguous” if you understand them semantically, unlike worst-case).</li>
<li>Build modular AI systems. Ex. how to make AlphaGo something that anyone can implement a baby version of?</li>
<li>Explore creativity. Ex. make a program that writes poetry or stories, or generates maps for a RPG. cf. Neural net dreams. cf. Hofstadter’s FARG.</li>
<li>How do humans reason? What are the learning problems that humans face, and how are our neural algorithms optimized or not for those problems? How does “bounded computation” come into play? cf. Jacob Steinhardt. Ref: Rationally Speaking #154, Tom Griffiths.</li>
</ol>
<h2 id="sanjeev-aroras-group">Sanjeev Arora’s group</h2>
<ul>
<li><a href="http://www.cs.princeton.edu/~yingyul/">Yingyu Liang</a></li>
<li><a href="http://www.cs.princeton.edu/~tengyu/">Tengyu Ma</a></li>
<li><a href="http://www.cs.princeton.edu/~risteski/">Andrej Risteski</a></li>
</ul>
<p>Summaries/thoughts on the papers.</p>
<!---
Composite of these smaller questions
-->

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Matrix concentration</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/probability/random_matrices/concentration.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/probability/random_matrices/concentration.html</id>
    <published>2016-03-13T00:00:00Z</published>
    <updated>2016-03-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Matrix concentration</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-13 
          , Modified: 2016-03-13 
	</p>
      
       <p>Tags: <a href="/tags/random%20matrix.html">random matrix</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <p>See <a href="https://en.wikipedia.org/wiki/Matrix_Chernoff_bound">wikipedia</a>.</p>
<p><strong>Theorem (Matrix Bernstein)</strong>: Let <span class="math inline">\(\{X_k\}_{k=1}^n\)</span> be a sequence of independent random <span class="math inline">\(d\times d\)</span> matrices with <span class="math display">\[ \E X_k = 0, \quad \la_{\max}(X_k)\le R\text{ a.s.}\]</span> Then for all <span class="math inline">\(t\ge 0\)</span>, <span class="math display">\[ \Pj \ba{\la_{\max}\pa{\sumo kn X_k}} \le de^{-\fc{t^2}{2\si^2+\fc 23 Rt}}\]</span> where <span class="math inline">\(\si^2 = \ve{\sumo kn \E(X_k^2)}\)</span>.</p>
<p>(I don’t understand what a.s. means here. The second inequality is for a finite sum—so we need to quantify the probability that <span class="math inline">\(\la_{\max}(X_i)&gt;R\)</span>. a.s. means with probability 1 so we have to truncate the probability distribution of the <span class="math inline">\(X_i\)</span>? But we can’t do this when there are many <span class="math inline">\(X_i\)</span>.)</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Spectral Bounds for Stochastic Diffusion Model in Networks</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/networks/pacm-3-8-16.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/networks/pacm-3-8-16.html</id>
    <published>2016-03-08T00:00:00Z</published>
    <updated>2016-03-08T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Spectral Bounds for Stochastic Diffusion Model in Networks</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-08 
          , Modified: 2016-03-08 
	</p>
      
       <p>Tags: <a href="/tags/pacm.html">pacm</a>, <a href="/tags/GSS.html">GSS</a>, <a href="/tags/networks.html">networks</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p>Eun Jun Lee</p>
<p>This work studies stochastic diffusion model where influence propagates in networks from seed-nodes along edges with independent probabilities. Specifically, we propose spectral bounds for the expected number of nodes that are influenced at the end of propagation. The proposed bounds show significant improvements over the existing bounds in the presence of sensitive edges such as bottlenecks, seed adjacent, and high probability edges.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
