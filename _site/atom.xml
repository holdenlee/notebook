<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2017-02-01T00:00:00Z</updated>
    <entry>
    <title>Logical induction</title>
    <link href="http://holdenlee.github.io/notebook/posts/cs/ai/control/logical_induction.html" />
    <id>http://holdenlee.github.io/notebook/posts/cs/ai/control/logical_induction.html</id>
    <published>2017-02-01T00:00:00Z</published>
    <updated>2017-02-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Logical induction</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-02-01 
          , Modified: 2017-02-01 
	</p>
      
       <p>Tags: <a href="/tags/ai%20safety.html">ai safety</a>, <a href="/tags/logic.html">logic</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#definitions">Definitions</a></li>
 <li><a href="#theorem">Theorem</a></li>
 <li><a href="#properties">Properties</a><ul>
 <li><a href="#limit-properties">Limit properties</a></li>
 <li><a href="#pattern-recognition">Pattern recognition</a><ul>
 <li><a href="#definitions-1">Definitions</a></li>
 <li><a href="#properties-1">Properties</a></li>
 </ul></li>
 <li><a href="#self-knowledge">Self-knowledge</a><ul>
 <li><a href="#definitions-2">Definitions</a></li>
 <li><a href="#results">Results</a></li>
 </ul></li>
 </ul></li>
 <li><a href="#misc">Misc</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="definitions">Definitions</h2>
<ul>
<li>Let <span class="math inline">\(L\)</span> be a language of propositional logic<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>, and <span class="math inline">\(S\)</span> be sentences in <span class="math inline">\(L\)</span>.</li>
<li>A <strong>valuation</strong> is <span class="math inline">\(\mathbb V:S\to [0,1]\)</span>.
<ul>
<li>A <strong>pricing</strong> is <span class="math inline">\(\Pj: S\to \Q \cap [0,1]\)</span>.</li>
<li>A <strong>belief state</strong> is a pricing with finite support. (Syntactically. Semantically, think of it as probabilities.)</li>
</ul></li>
<li><strong>Valuation sequence</strong> <span class="math inline">\(\mathbb N\to (S\to [0,1])\)</span>.
<ul>
<li>A <strong>market</strong> is a computable sequence of pricings <span class="math inline">\(\Pj_i:S\to \Q\cap [0,1]\)</span>. (i.e., <span class="math inline">\(\N \to (S\to \Q\cap [0,1])\)</span>).</li>
<li>A <strong>computable belief sequence</strong> is a market with each <span class="math inline">\(\Pj_i\)</span> having finite support. (Syntactically. Semantically, think of it as probabilities.)</li>
</ul></li>
<li>A <strong>deductive process</strong> <span class="math inline">\(\ol D:\N^+\to \text{Fin}(S)\)</span> (finite subsets of <span class="math inline">\(S\)</span>) is a computable nested sequence <span class="math inline">\(D_1\subeq D_2\subeq\cdots\)</span> of sentences. Let <span class="math inline">\(D_\iy:=\bigcup_n D_n\)</span>.</li>
<li>A <strong>world</strong> is a truth assignment <span class="math inline">\(\mathbb W: S\to \mathbb B\)</span>. True/false in <span class="math inline">\(\mathbb W\)</span> means <span class="math inline">\(\mathbb W(\phi)=0,1\)</span>.
<ul>
<li><span class="math inline">\(\mathbb W\)</span> is <strong>propositionally consistent (p.c.)</strong> if it satisfies
<span class="math display">\[\begin{align}
\mathbb W(\phi\wedge \psi)&amp;= \mathbb W(\phi) \wedge \mathbb W(\psi)\\
\mathbb W(\phi\vee \psi)&amp;= \mathbb W(\phi) \vee\mathbb W(\psi)\\
\mathbb W(\neg \phi) &amp;= 1-\mathbb W(\phi)
\end{align}\]</span>
<p>(cf. Christiano. Can define more complicated equivalences, but becomes more computationally difficult to verify consistency; you can’t make it intractable.)</p>
<span class="math inline">\(PC(D)\)</span> is the set of worlds where <span class="math inline">\(\mathbb W(\phi)=1\)</span> for <span class="math inline">\(\phi\in D\)</span>. <span class="math inline">\(PC(D)\)</span> is the set of worlds propositionally consistent with <span class="math inline">\(D\)</span>.</li>
<li><span class="math inline">\(\mathbb W\)</span> is <strong>consistent</strong> with <span class="math inline">\(\Ga\)</span>, <span class="math inline">\(\mathbb \in C(\Ga)\)</span>, if you can’t prove contradiction from <span class="math inline">\(\mathbb W\)</span> and <span class="math inline">\(\Ga\)</span>.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> <span class="math display">\[
\Ga \cup \set{\phi}{\mathbb W(\phi)=1}\cup \set{\neg \phi}{\mathbb W (\phi)=0}\not\vdash \perp.
\]</span></li>
</ul></li>
<li><strong>Efficiently computable</strong> <span class="math inline">\((x_n)_{n=1}^{\iy}\)</span> means <span class="math inline">\(x_n\)</span> computable in time <span class="math inline">\(\poly(n)\)</span>.</li>
<li>A valuation <strong>feature</strong> <span class="math display">\[
\al:\ub{[0,1]^{S\times \N^+}}{\text{valuation sequences}}\to \R
\]</span> is a continuous function such that <span class="math inline">\(\al(\ol{\mathbb V})\)</span> depends only on <span class="math inline">\(\mathbb V_{\le n}\)</span> for some <span class="math inline">\(n\in \N^+\)</span> called the rank.
<ul>
<li><strong>Price feature</strong> (cf. evaluation functional) <span class="math display">\[\psi^{*n}(\ol{\mathbb V}):=\mathbb V_n(\phi)\]</span></li>
<li><strong>Expressible feature</strong> is built up from price features, <span class="math inline">\(\Q, +, \times, \max, \max(\cdot, 1)^{-1}\)</span>. <span class="math inline">\(\mathcal{EF}\)</span> is the set of expressible features, <span class="math inline">\(\mathcal{EF}_n\)</span> of rank <span class="math inline">\(\le n\)</span> (commutative ring).</li>
</ul></li>
<li>A <strong>trading strategy</strong> is an affine combination <span class="math display">\[T = \pa{-\sum_i \xi_i \phi_i^{*n}} + \sum_i \xi_i \phi_i^{*n}\]</span> where <span class="math inline">\(\phi_i\in S\)</span>, <span class="math inline">\(\xi_i\)</span> are expressible features of rank <span class="math inline">\(\le n\)</span>. (I.e., it is an element of <span class="math inline">\(\ker(\ph)\)</span> where <span class="math display">\[\ph:\mathcal{EF}_n^{\opl S} \opl \mathcal{EF_n}
= \mathcal{EF}_n^{\opl S\cup \{1\}} \mapsto \mathcal{EF_n}\]</span> given by <span class="math inline">\(\ph:x\opl y\mapsto x^{*n}+y\)</span>.) Let <span class="math inline">\(T[\phi]\)</span> be the coefficient of <span class="math inline">\(\phi\)</span>, <span class="math inline">\(T[1]\)</span> be the constant term.
<ul>
<li>A <strong>trader</strong> <span class="math inline">\(\ol T\)</span> is a sequence <span class="math inline">\((T_1,T_2,\ldots)\)</span> where each <span class="math inline">\(T_n\)</span> is a trading strategy for day <span class="math inline">\(n\)</span>.</li>
<li>Think of <span class="math inline">\(\phi-\phi^{*n}\)</span> as meaning “buy share of <span class="math inline">\(\phi_i\)</span> at prevailing price.”</li>
<li>Example of trading strategy: Arbitrage <span class="math inline">\(\psi\)</span> against <span class="math inline">\(\neg \neg \psi\)</span>. <span class="math display">\[
[(\neg \neg \phi)^{*n} - \phi^{*n}](\phi-\phi^{*n}) + [\phi^{*n}-(\neg\neg\phi)^{*5}](\neg \neg \phi - (\neg\neg \phi)^{*5}).
\]</span></li>
<li>Define evaluation of a value function on a <span class="math inline">\(\mathcal F\)</span>-combination, <span class="math inline">\((S\to [0,1]) \times (\mathcal F^{\opl(S\cup \{1\})}) \to \mathcal F\)</span> in the natural way, sending <span class="math inline">\(\phi\)</span> to <span class="math inline">\(\mathbb V(\phi)\)</span> and extending by linearity. (<span class="math inline">\(\mathbb V(1)=1\)</span>.)</li>
</ul></li>
<li><span class="math inline">\(\ol T\)</span> <strong>exploits</strong> <span class="math inline">\(\mathbb V\)</span> relative to a deductive process <span class="math inline">\(\ol D\)</span> if <span class="math display">\[
\set{\mathbb W\pa{\sumz in \mathbb V(_i(T_i))}}{n\in \N^+, \mathbb W\in PC(D_n)}
\]</span> is bounded below, but not bounded above.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> This set is the set of <strong>plausible assessments</strong> of net worth. “The trader can make unbounded returns with bounded risk.”
<ul>
<li>Ex. if PA proves <span class="math inline">\(\phi\vee \psi\)</span>, then a trader who buys <span class="math inline">\(\phi,\psi\)</span> at combined price <span class="math inline">\(&lt;1\)</span> exploits the market.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a></li>
</ul></li>
<li><span class="math inline">\(\ol{\Pj}\)</span> satisfies the <strong>logical induction criterion</strong> relative to deductive process <span class="math inline">\(\ol D\)</span> if there is no efficiently computable trader <span class="math inline">\(\ol T\)</span> that exploits <span class="math inline">\(\ol\Pj\)</span> relative to <span class="math inline">\(\ol D\)</span>. <span class="math inline">\(\ol \Pj\)</span> is a <strong>logical inductor</strong>.
<ul>
<li>A logical inductor over a <span class="math inline">\(\Ga\)</span>-complete deductive process <span class="math inline">\(\ol D\)</span> is a <strong>logical inductor</strong> over <span class="math inline">\(\Ga\)</span>.</li>
</ul></li>
<li><span class="math inline">\(\ol\Pj\)</span> assigns <span class="math inline">\(\ol p\)</span> to <span class="math inline">\(\ol\phi\)</span> in a timely manner if <span class="math inline">\(\Pj_n(\phi_n) \simeq_n p_n\)</span>.</li>
</ul>
<h2 id="theorem">Theorem</h2>
<p>For any deductive process <span class="math inline">\(\ol D\)</span>, there exists a computable belief sequence <span class="math inline">\(\ol \Pj\)</span> satisfying the logical induction criterion relative to <span class="math inline">\(\ol D\)</span>.</p>
<p>For any recursively axiomatizable <span class="math inline">\(\Ga\)</span>, there exists a computable belief sequence that is a logical inductor over <span class="math inline">\(\Ga\)</span>.</p>
<p>Intuition: Consider any polynomial-time method for efficiently identifying patterns in logic. If the market prices don’t learn to reflect that pattern, a clever trader can use it to exploit the market. For example, if there is a polynomial-time method for identifying theorems that are always underpriced by the market, a clever trader could use that pattern to buy those theorems low, exploiting the market. To avoid exploitation, logical inductors must learn to identify many different types of patterns in logical statements.</p>
<p>(Note: I expected that the logical inductor would be the trader, but no, it’s the market! The traders are people trying to take advantage of the market. But I’m thinking of the AI against nature, and nature throwing catastrophes in the sense of trying to find things the AI would reason badly about.)</p>
<h2 id="properties">Properties</h2>
<h3 id="limit-properties">Limit properties</h3>
<ul>
<li>Convergence: <span class="math inline">\(\Pj_\iy(\phi) :=\limn \Pj_n(\phi)\)</span> exists.</li>
<li>Limit coherence: <span class="math inline">\(\Pj_\iy\)</span> defines a probability measure on the set of worlds consistent with <span class="math inline">\(\Ga\)</span>. <span class="math display">\[
\Pj(\mathbb W(\phi)=1):=\Pj_\iy(\phi).
\]</span></li>
<li>Occam bounds: There exists <span class="math inline">\(C&gt;0\)</span> such that, letting <span class="math inline">\(\ka(\phi)\)</span> be <a href="http://www.scholarpedia.org/article/Algorithmic_complexity#Prefix_complexity">prefix complexity</a> of <span class="math inline">\(\phi\)</span>,<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a>
<span class="math display">\[\begin{align}
\Pj_\iy(\phi) &amp;\ge C 2^{-\ka(\phi)}\\
\Pj_\iy(\phi) &amp;\le 1-C2^{-\ka(\phi)}.
\end{align}\]</span></li>
<li>Domination of universal semimeasure<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> <span class="math inline">\(M\)</span>: Let <span class="math inline">\(\ol{\si}_n\)</span> be the statement: the first <span class="math inline">\(n\)</span> bits of the observed sequence is <span class="math inline">\(\si_{\le n}\)</span>. There is universal <span class="math inline">\(C\)</span> such that <span class="math inline">\(\Pj_\iy(\ol \si_{\le n}) \ge CM(\ol\si_{\le n})\)</span>.</li>
</ul>
<h3 id="pattern-recognition">Pattern recognition</h3>
<h4 id="definitions-1">Definitions</h4>
<ul>
<li>Write <span class="math inline">\(x_n\simeq_n y_n\)</span> for <span class="math inline">\(\limn x_n-y_n=0\)</span>. <span class="math inline">\(\gtrsim\)</span> and <span class="math inline">\(\lesssim\)</span> for <span class="math inline">\(\liminf\ge 0\)</span>, <span class="math inline">\(\limsup\le 0\)</span>.</li>
<li>A divergent weighting is <span class="math inline">\(\ol w\in [0,1]^{\N^+}\)</span> with <span class="math inline">\(\su w_n=\iy\)</span>. <span class="math inline">\(\ol q\)</span> is <strong>generable</strong> from <span class="math inline">\(\ol\Pj\)</span> if there exists e.c. <span class="math inline">\(\mathcal{EF}\)</span>-progression <span class="math inline">\(\ol{q^{\dagger}}(\ol\Pj) =q_n\)</span>. (<span class="math inline">\(q_n^{\dagger}(\ol\Pj) = \ol\Pj_n(q_n^{\dagger})\)</span>.)
<ul>
<li>“Divergent weightings generable from <span class="math inline">\(\ol\Pj\)</span> are the pattern detectors that logical inductors can use.”</li>
</ul></li>
<li>Elements of <span class="math inline">\(\R^{\opl (S\cup \{1\})}\)</span> are constraints.</li>
<li><strong>Bounded combination sequences</strong> <span class="math inline">\(BCS(\ol \Pj)\)</span>: <span class="math inline">\(\ol\Pj\)</span>-generable <span class="math inline">\(\ol A\)</span> (<span class="math inline">\(A_n\in \R^{\opl (S\cup \{1\})}\)</span>) that are bounded (<span class="math inline">\(\exists b, \forall n,|A_n[1]|\le b\)</span>).</li>
</ul>
<h4 id="properties-1">Properties</h4>
<ul>
<li>Provability induction: Let <span class="math inline">\(\ol{\phi}\)</span> be e.c. sequence of theorems. Then <span class="math display">\[
\Pj_n(\phi_n) \simeq_n 1
\]</span> For disprovable sentences, <span class="math inline">\(\simeq_n0\)</span>. (Analogy: Ramanujan and Hardy)</li>
<li>Preemptive learning: For e.c. sequence <span class="math inline">\(\ol\phi\)</span>, <span class="math inline">\(\liminf_{n\to \iy}\Pj_n(\phi_n) = \liminf_{n\to \iy} \sup_{m\ge n} \Pj_m(\phi_n).\)</span> and similarly for inf/sup switched.</li>
<li>Learning pseudorandom frequencies. <span class="math inline">\(\ol{\phi}\)</span> of decidable sentences is <strong>pseudorandom with frequency p</strong> wrt set of divergent weightings <span class="math inline">\(S\)</span> if for all <span class="math inline">\(\ol w\in S\)</span>, <span class="math inline">\(\limn \fc{\sumo in w_i \one_{\phi_i \text{ is theorem in }\Ga}}{\sumo in w_i}=p\)</span>. For <span class="math inline">\(\ol\phi\)</span> e.c., if <span class="math inline">\(\ol\phi\)</span> is pseudorandom over <em>all <span class="math inline">\(\ol\Pj\)</span>-generable divergent weightings</em>, <span class="math inline">\(\Pj_n(\phi_n) \simeq_n p\)</span>.
<ul>
<li>Think of <span class="math inline">\(\ol{\Pj}\)</span>-generable as meaning: trading strategies you could come up with looking at past history of beliefs. This still doesn’t fit my picture of traders as adversaries, rather than unkind nature.</li>
</ul></li>
<li>Affine coherence: Let <span class="math inline">\(\ol A\in BCS(\ol\Pj)\)</span>. <span class="math display">\[
\liminf_{n\to \iy} \inf_{\mathbb W\in C(\Ga)} \mathbb W(A_n) \le \liminf_{n\to \iy} \Pj_\iy (A_n) \le \lim_{n\to \iy} \Pj_n (A_n).
\]</span> Reverse inequalities for sup.
<ul>
<li>“Tie ground truth on <span class="math inline">\(\ol A\)</span> to value of <span class="math inline">\(\ol A\)</span> on main diagonal.”</li>
<li>“learns in a timely manner to respect all linear inequalities that actually hold between sentences, so long as those relationships can be enumerated in polynomial time.” Ex. if exactly one of <span class="math inline">\(A_n,B_n,C_n\)</span> is true, will get that.<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a></li>
<li>What is each inequality saying?</li>
</ul></li>
</ul>
<p>I’m confused: Consider a sequence <span class="math inline">\(y_n=f(x_n)\)</span>, <span class="math inline">\(f:B^{2n}\to B^n\)</span> which is hard-to-invert. Theorem <span class="math inline">\(n\)</span> is <span class="math inline">\(y_n\in \im f\)</span>. How can we hope to assign probabilities <span class="math inline">\(\to 1\)</span>?</p>
<p>Can we add true randomness? Randomness in <span class="math inline">\(\Ga\)</span> is not allowed because axioms are recursively computable. (Footnote 3 in paper says we can add randomness to market.) How about allowing traders to be random?</p>
<h3 id="self-knowledge">Self-knowledge</h3>
<h4 id="definitions-2">Definitions</h4>
<ul>
<li><strong>Logically uncertain variable (LUV)</strong>: formula <span class="math inline">\(X\)</span> free in one variable that defines a unique value via <span class="math inline">\(\Ga\)</span>: (<span class="math inline">\(\exists!\)</span>) <span class="math display">\[
\Ga \perp \exists x: \forall x': X(x')\to x'=x.
\]</span> Value of <span class="math inline">\(X\)</span> in <span class="math inline">\(\mathbb W\in C(\Ga)\)</span> is <span class="math display">\[
\mathbb W(X):= \sup \set{x\in [0,1]}{\mathbb W(&quot;X\ge x&quot;)=1}.
\]</span> Because it may not be clear that <span class="math inline">\(X\)</span> has a unique value, “<span class="math inline">\(X\ge p\)</span>” is shorthand for <span class="math inline">\(\forall x: X(x)\to x&lt;p\)</span>.
<ul>
<li>Ex. “<span class="math inline">\(\nu\)</span> is 1/0 if Goldbach’s conjecture is true/false.”</li>
<li>Problem with a defining <span class="math inline">\(\E\)</span> the normal way: <span class="math inline">\(\ol\Pj\)</span> may not have figured out that <span class="math inline">\(X\)</span> takes a unique value!</li>
<li><strong>Indicator LUV</strong> <span class="math inline">\(\one(\phi) := &quot;(\phi \wedge (\nu = 1))\vee (\neg \psi \wedge (\nu=0))&quot;\)</span>.</li>
<li><strong>Approximate expectation operator</strong> with precision <span class="math inline">\(k\)</span>: For <span class="math inline">\(X\)</span> a <span class="math inline">\([0,1]\)</span>-LUV, <span class="math display">\[
\E_k^{\mathbb V}(X):=\sumz i{k-1}\rc k \mathbb V(&quot;X&gt;i/k&quot;).
\]</span> Let <span class="math inline">\(\E_n:=\E_n^{\Pj_n}\)</span>.</li>
</ul></li>
<li><span class="math inline">\(f:\N^+\to \N^+\)</span> is <strong>deferral function</strong> if <span class="math inline">\(f(n)&gt;n\)</span> for all <span class="math inline">\(n\)</span>, <span class="math inline">\(f(n)\)</span> can be computed in time <span class="math inline">\(\poly(f(n))\)</span>. Say <span class="math inline">\(f\)</span> defers <span class="math inline">\(n\)</span> to <span class="math inline">\(f(n)\)</span>.</li>
<li>Continuous threshold indicator <span class="math inline">\(\Ind_\de(x&gt;y)\)</span> interpolates between 0 and 1 on <span class="math inline">\([y,y+\de]\)</span>.</li>
</ul>
<h4 id="results">Results</h4>
<p>You can use the logic to encode the computation of the market prices, i.e. <span class="math inline">\(\ul{\Pj}_{\ul{n}}(\ul{\phi_n})\)</span>. (I’m omitting underlines in the following theorem statements.)</p>
<ul>
<li>Introspection: Let <span class="math inline">\(\ol \phi\)</span> be e.c. sequence of sentences, <span class="math inline">\(\ol a,\ol b\)</span> be e.c. sequences of probabilities expressible from <span class="math inline">\(\ol \Pj\)</span>. For any e.c. sequence of <span class="math inline">\(\Q^+\)</span> with <span class="math inline">\(\ol \de\to 0\)</span>, there exists e.c. sequence of <span class="math inline">\(\Q^+\)</span> with <span class="math inline">\(\ol\ep\to 0\)</span>, s.t. for all <span class="math inline">\(n\)</span>,
<span class="math display">\[\begin{align}
\Pj_n(\phi_n)&amp;\in (a_n+\de_n,b_n-\de_n)&amp;\implies \Pj_n(&quot;a_n&lt;\Pj_n(\phi_n)&lt;b_n&quot;)&amp;&gt;1-\ep_n\\
\Pj_n(\phi_n)&amp;\nin (a_n-\de_n,b_n+\de_n)&amp;\implies \Pj_n(&quot;a_n&lt;\Pj_n(\phi_n)&lt;b_n&quot;)&amp;&lt;\ep_n
\end{align}\]</span>
<!--(Asymptotically faster than any e.c. sequence!)-->
<ul>
<li>“If there is e.c. pattern of the form your probabilities on <span class="math inline">\(\ol{\phi}\)</span> will be in <span class="math inline">\((a,b)\)</span> then <span class="math inline">\(\ol{\Pj}\)</span> learns to believe that pattern iff it is true subject to finite precision.”</li>
</ul></li>
<li>Paradox resistance: Define paradoxical sentences <span class="math inline">\(\ol{\chi^p}\)</span> <span class="math display">\[
\Ga \vdash \ul{\chi_n^p} \lra (\Pj_n(\chi_n^p)&lt;p).
\]</span> Then <span class="math inline">\(\lim_{n\to \iy} \Pj_n(\chi_n^p) = p\)</span>.<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a>
<ul>
<li>p. 16: Brain scanner.</li>
</ul></li>
<li>Expectations
<ul>
<li>Convergence: <span class="math inline">\(\E_\iy\)</span> exists.</li>
<li><strong>Linearity of expectation</strong>. <span class="math inline">\(\ol \al,\ol\be\)</span> e.c. sequences of <span class="math inline">\(\Q\)</span> expressible from <span class="math inline">\(\ol\Pj\)</span>, <span class="math inline">\(\ol X, \ol Y, \ol Z\)</span> e.c. of <span class="math inline">\([0,1]\)</span>-LUV. <span class="math display">\[
\forall n, \Ga\vdash Z_n = \al_nX_n  +\be_nY_n\implies \al_n\E_n(X_n) + \be_n\E_n(Y_n) \simeq_n \E_n(Z_n).
\]</span></li>
<li><strong>Expectation coherence</strong>. $B<span class="math inline">\(BLCS(\)</span>$) set of bounded LUV-combination sequences. <span class="math display">\[
\liminf_{n\to \iy} \inf_{\mathbb W\in C(\Ga)} \mathbb W(B_n) \le \liminf_{n\to \iy} \E_\iy (B_n) \le \lim_{n\to \iy} \E_n (B_n)
\]</span> and reverse for sup.</li>
</ul></li>
<li>No expected net update: <span class="math inline">\(f\)</span> deferral, <span class="math inline">\(\ol\phi\)</span> e.c. sequence of sentences. <span class="math display">\[\Pj(\phi_n) \simeq_n \E_n(&quot;\Pj_{f(n)} (\phi_n)&quot;).\]</span>
<ul>
<li>If <span class="math inline">\(\ol\Pj\)</span> on day <span class="math inline">\(n\)</span> believes on day <span class="math inline">\(f(n)\)</span> it will believe <span class="math inline">\(\phi_n\)</span> whp, then it already believes <span class="math inline">\(\phi_n\)</span> whp today. “In other words, logical inductors learn to adopt their predicted future beliefs as their current beliefs in a timely manner.”</li>
</ul></li>
<li>Self-trust: <span class="math inline">\(f\)</span> deferral, <span class="math inline">\(\ol\phi\)</span> e.c., <span class="math inline">\(\ol\de\in (\Q^+)^n\)</span> e.c., <span class="math inline">\(\ol p\)</span> e.c. sequence of rational probs expressible from <span class="math inline">\(\ol\Pj\)</span>. Then <span class="math display">\[
\E_n (&quot;\one(\phi_n)\Ind_{\de_n} (\Pj_{f(n)}(\phi_n)&gt;p_n)&quot;) \gtrsim_n p_n \E_n (&quot;\Ind_{\de_n}(\Pj_{f(n)}(\phi_n)&gt;p_n)&quot;).
\]</span>
<ul>
<li>“Trust that if beliefs change, they must have changed for good reason”</li>
<li>Roughly: If we ask <span class="math inline">\(\ol\Pj\)</span> what it believes about <span class="math inline">\(\phi\)</span> now if it learned it was going to believe <span class="math inline">\(\phi\)</span> wp <span class="math inline">\(\ge p\)</span> in the future, it will answer with probability <span class="math inline">\(\ge p\)</span>. (Some subtlety with continuous indicators, paradoxical sentences.)</li>
</ul></li>
</ul>
<h2 id="misc">Misc</h2>
<p>“No dutch book” in expected utility theory, Bayesian probability theory</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>What does this mean? Surely not <span class="math inline">\(L\)</span> is only propositional logic. <span class="math inline">\(L\)</span> is any logic that’s universal (in the sense that you can express anything in arithmetic), right?<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Checking propositional consistency is tractable if you restrict to a finite number of sentences. Otherwise (given oracle to <span class="math inline">\(\mathbb W\)</span>), of course not because there’s infinitely many things to check.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Why this definition? (Note: in the paper this is written as <span class="math inline">\(T_i(\ol{\mathbb{V}})\)</span>. I changed this to be more consistent in notation.)<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>At any finite time a trader can earn arbitrarily large amount of money, but this is a statement about unbounded, not arbitrarily large.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>??? Prefix complexity has to be with respect to something.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>What is this?<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>What about transformations? Ex. if <span class="math inline">\(P\)</span> is a polytime transformation <span class="math inline">\(S\to \R^{\opl (S\cup \{1\})}\)</span>, such that <span class="math inline">\(P(s)=0\)</span> in all worlds ((or just is provable) and this is provable within <span class="math inline">\(\Ga\)</span>?), then eventually it becomes 0? (I mean on worst case <span class="math inline">\(s\)</span> too, not just specific <span class="math inline">\(s\)</span>.) Also, do there exist computable sequences <span class="math inline">\(A_n\)</span> of theorems such that for each <span class="math inline">\(n\)</span>, <span class="math inline">\(A_n\)</span> is provable, but <span class="math inline">\(\forall n, A_n\)</span> is not provable? (Here we don’t have the generators for <span class="math inline">\(A_n\)</span>. Could we do more with the generators? The <span class="math inline">\(A_n\)</span> are just given to us by some adversary.)<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>How do polytime traders get access to prices if they are real numbers? Do they only get <span class="math inline">\(\rc{\poly(n)}\)</span> precision?<a href="#fnref8">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Concrete problems in AI safety</title>
    <link href="http://holdenlee.github.io/notebook/posts/cs/ai/control/concrete.html" />
    <id>http://holdenlee.github.io/notebook/posts/cs/ai/control/concrete.html</id>
    <published>2017-02-01T00:00:00Z</published>
    <updated>2017-02-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Concrete problems in AI safety</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-02-01 
          , Modified: 2017-02-01 
	</p>
      
       <p>Tags: <a href="/tags/ai%20safety.html">ai safety</a>, <a href="/tags/machine%20learning.html">machine learning</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <blockquote>
<p>Focus is on the empirical study of practical safety problems in modern machine learning systems, which we believe is likely to be robustly useful across a broad variety of potential risks, both short-and long-term</p>
</blockquote>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Alignment for advanced machine learning systems</title>
    <link href="http://holdenlee.github.io/notebook/posts/cs/ai/control/alignment_ml.html" />
    <id>http://holdenlee.github.io/notebook/posts/cs/ai/control/alignment_ml.html</id>
    <published>2017-02-01T00:00:00Z</published>
    <updated>2017-02-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Alignment for advanced machine learning systems</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-02-01 
          , Modified: 2017-02-01 
	</p>
      
       <p>Tags: <a href="/tags/ai%20safety.html">ai safety</a>, <a href="/tags/machine%20learning.html">machine learning</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#increasing-safety-by-reducing-autonomy">Increasing safety by reducing autonomy</a></li>
 <li><a href="#increasing-safety-without-reducing-autonomy">Increasing safety without reducing autonomy</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="https://intelligence.org/2016/07/27/alignment-machine-learning/">Article</a></p>
<blockquote>
<p>new agenda is intended to help more in scenarios where advanced AI is relatively near and relatively directly descended from contemporary ML techniques, while our agent foundations agenda is more agnostic about when and how advanced AI will be developed.</p>
</blockquote>
<blockquote>
<p>Where the agent foundations agenda can be said to follow the principle “start with the least well-understood long-term AI safety problems, since those seem likely to require the most work and are the likeliest to seriously alter our understanding of the overall problem space,” the concrete problems agenda follows the principle “start with the long-term AI safety problems that are most applicable to systems today, since those problems are the easiest to connect to existing work by the AI research community.”</p>
</blockquote>
<h2 id="increasing-safety-by-reducing-autonomy">Increasing safety by reducing autonomy</h2>
<ol type="1">
<li>Inductive ambiguity identification: How can we train ML systems to detect and notify us of cases where the classification of test data is highly under-determined from the training data?</li>
<li>Robust human imitation: How can we design and train ML systems to effectively imitate humans who are engaged in complex and difficult tasks?</li>
<li>Informed oversight: How can we train a reinforcement learning system to take actions that aid an intelligent overseer, such as a human, in accurately assessing the system’s performance?</li>
</ol>
<p>Ambiguity identification helps:</p>
<blockquote>
<p>We could reduce risk somewhat by building systems that are still reasonably smart and autonomous, but will pause to consult operators in cases where their actions are especially high-risk. Ambiguity identification is one approach to fleshing out which scenarios are “high-risk”</p>
</blockquote>
<p>vs. human imitation.</p>
<blockquote>
<p>In practice, however, ambiguity identification is probably too mild a restriction on its own, and strict human imitation probably isn’t efficiently implementable. Informed oversight considers more moderate approaches to keeping humans in the loop.</p>
</blockquote>
<h2 id="increasing-safety-without-reducing-autonomy">Increasing safety without reducing autonomy</h2>
<ol start="4" type="1">
<li>Generalizable environmental goals: How can we create systems that robustly pursue goals defined in terms of the state of the environment, rather than defined directly in terms of their sensory data?</li>
<li>Conservative concepts: How can a classifier be trained to develop useful concepts that exclude highly atypical examples and edge cases?</li>
<li>Impact measures: What sorts of regularizers incentivize a system to pursue its goals with minimal side effects?</li>
<li>Mild optimization: How can we design systems that pursue their goals “without trying too hard”-stopping when the goal has been pretty well achieved, as opposed to expending further resources searching for ways to achieve the absolute optimum expected score?</li>
<li>Averting instrumental incentives: How can we design and train systems such that they robustly lack default incentives to manipulate and deceive their operators, compete for scarce resources, etc.?</li>
</ol>
<blockquote>
<p>ambiguity-identifying learners are designed to predict potential ways they might run into edge cases and defer to human operators in those cases, conservative learners are designed to err in a safe direction in edge cases.</p>
</blockquote>
<p>Ex. cooking the cat.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Linear convex regulator 2</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/lcr2.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/lcr2.html</id>
    <published>2017-01-10T00:00:00Z</published>
    <updated>2017-01-10T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Linear convex regulator 2</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-01-10 
          , Modified: 2017-01-10 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#convex-optimization">Convex optimization</a><ul>
 <li><a href="#full-information">Full information</a></li>
 <li><a href="#see-only-costs-of-sampled-trajectories">See only costs of sampled trajectories</a></li>
 <li><a href="#unknown-dynamics">Unknown dynamics</a></li>
 <li><a href="#notes">Notes</a></li>
 </ul></li>
 <li><a href="#noisy-dynamics">Noisy dynamics</a></li>
 <li><a href="#references">References</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="lcr.html">Part 1</a></p>
<p>Fix <span class="math inline">\(x_1\)</span>. Let the cost function (jointly convex in <span class="math inline">\(x,a\)</span>) be <span class="math inline">\(c(x,a)\)</span> (for example, <span class="math inline">\(g(x) + \ve{a}^2\)</span>) the actions be <span class="math inline">\(a_1,a_2,...\)</span> and the recurrence describing the dynamics be <span class="math inline">\(x_{n+1} =Ax_n+Ba_n\)</span>, we want</p>
<span class="math display">\[\begin{align}
&amp;\quad
\min_{a_1} [c(x_1,a_1) + \min_{a_2} [c(Ax_1+Ba_1,a_2) + ...]] \\
&amp;=
\min_{a_1,...,a_T} c(x_1,a_1) + c(Ax_1+Ba_1,a_2) + ...
\end{align}\]</span>
<p>This objective is jointly convex because we are assuming <span class="math inline">\(c\)</span> is convex, and a convex function composed with a linear function is convex. Letting <span class="math inline">\(f_a(x) = Ax + Ba\)</span> be the function describing the dynamics and <span class="math inline">\(f_{a_1\cdots a_n} (x) = f_{a_n}\circ \cdots \circ f_{a_1}(x)\)</span>, we can write this as <span class="math display">\[
\min_{a_1,...,a_T} \ub{c(x_1,a_1) + c(f_{a_1}(x_1),a_2) + c(f_{a_1a_2} (x_1),a_3)+\cdots}{F(a_1,\ldots, a_T)}
\]</span></p>
<h2 id="convex-optimization">Convex optimization</h2>
<p>What guarantees do we get from convex optimization? We can consider different settings:</p>
<ul>
<li>We know the function <span class="math inline">\(c\)</span> (full information) vs. we don’t know <span class="math inline">\(c\)</span>, only get access to <span class="math inline">\(c\)</span> through the trajectories that we sample, i.e., for each episode we choose a sequence of actions <span class="math inline">\((a_1,\ldots, a_T)\)</span> and observe <span class="math inline">\(c(x_1,a_1), c(f_{a_1}(x_1),a_2),\ldots\)</span>. (Then we can care about regret bounds, etc.)</li>
<li>If we don’t know <span class="math inline">\(c\)</span>: <span class="math inline">\(c\)</span> can be deterministic or stochastic (so we care about <span class="math inline">\(\min_{a_1,...,a_T} \E c(x_1,a_1) + c(f_{a_1}(x_1),a_2) + c(f_{a_1a_2} (x_1),a_3)+\cdots\)</span>). (Stochasticity not incorporated below, but easy to include.)</li>
<li>Whether we know the dynamics. Otherwise we would have to learn the dynamics. (<a href="../optimization/HMR16.html">HMR16</a> applies if there is noise in the observation.)</li>
</ul>
<h3 id="full-information">Full information</h3>
<p>This is just gradient descent. The dimension is <span class="math inline">\(Tn\)</span>, the gradient is a sum of gradients of <span class="math inline">\(c\)</span>’s, see below. If <span class="math inline">\(c\)</span> is stochastic, then this is SGD.</p>
<h3 id="see-only-costs-of-sampled-trajectories">See only costs of sampled trajectories</h3>
<p><strong>Theorem 6.6 in OCO</strong>. Online gradient descent without a gradient attains regret bound <span class="math display">\[
\E R_T \le 9nDGT^{\fc 34}
\]</span> where</p>
<ul>
<li><span class="math inline">\(n\)</span> is the dimension.</li>
<li><span class="math inline">\(D=\diam K\)</span>, where <span class="math inline">\(K\)</span> is the convex set we’re optimizing over.</li>
<li><span class="math inline">\(G=\sup_{x\in K} \ve{\nb f(x)}\)</span>.</li>
<li><span class="math inline">\(T\)</span> is the number of steps.</li>
</ul>
<p>Applying this here, we have</p>
<ul>
<li>The dimension is <span class="math inline">\(Tn\)</span>.</li>
<li>Suppose each action is in <span class="math inline">\(K\)</span>. Then <span class="math inline">\(D=\sqrt n \diam(B)\)</span>.</li>
<li>Suppose <span class="math inline">\(f_a(x)\)</span> is a function <span class="math inline">\(K\times C \to C\)</span> and <span class="math inline">\(x_1\in C\)</span>. Let
<span class="math display">\[\begin{align}
m &amp;= \max_{x\in C, a\in K^T}\nb_x c(x,a)\\
L &amp;= \max_{x\in C, a\in K^T}\nb_a c(x,a).
\end{align}\]</span>
Let <span class="math inline">\(\ga\)</span> be the max eigenvalue of <span class="math inline">\(B\)</span>. Then
<span class="math display">\[\begin{align}
\ve{\nb_{a_i}F} &amp;= \ve{
\nb_{a_i} c(f_{a_{1:i-1}}(x_1),a_i)
+\nb_{a_i} c(f_{a_{1:i}}(x_1), a_{i+1})
+\cdots \nb_{a_i} c(f_{a_{1:T-1}}(x_1),a_T)}\\
&amp;\le L + \be (1+\ga +\cdots + \ga^{T-1-i})m\\
&amp;\le L + \fc{\be}{1-\ga}m\\
\ve{\nb_{a_{1:T}}F} &amp; \le \sqrt{T} (L + \fc{\be}{1-\ga}m).
\end{align}\]</span></li>
</ul>
<h3 id="unknown-dynamics">Unknown dynamics</h3>
<p><strong>Exploration/exploitation involved in balancing learning the dynamics with choosing the best action given known information.</strong> Think about each linearly independent <span class="math inline">\(a\)</span> as giving more information.</p>
<h3 id="notes">Notes</h3>
<ul>
<li>This does not use any kind of function approximation, so the optimization only gives us information about the optimal action at <span class="math inline">\(x_1\)</span>. <strong>If we choose a different starting point, we have to run the optimization procedure with that new starting point.</strong></li>
<li><strong>We ignore the structure of each point <span class="math inline">\(x\)</span> in the space being a tuple of actions</strong> <span class="math inline">\((a_1,\ldots, a_T)\)</span>. Is there a way to use the structure of the cost function (as a sum of costs) to get better complexity?</li>
<li>Complexity scales as lookahead time <span class="math inline">\(T\)</span>. We can do better by realizing that later actions are less important - so e.g. to estimate the gradient we can do ellipsoid sampling instead of sphere sampling.</li>
</ul>
<h2 id="noisy-dynamics">Noisy dynamics</h2>
<p>The problem changes when there is noise in the dynamics. <span class="math display">\[
x_{n+1} = Ax_n+Ba_n + \ep_n.
\]</span></p>
<ul>
<li>The convex optimization problem solves for the best actions if all actions <span class="math inline">\(a_{1:T}\)</span> have to be chosen ahead of time - you don’t get to choose your action <span class="math inline">\(a_n\)</span> online after observing <span class="math inline">\(x_n\)</span>. This may be very suboptimal.
<ul>
<li>For general MDP’s, there can be a significant gap between what you can achieve with and without an online strategy (not sure how significant the gap is for convex <span class="math inline">\(c\)</span>, but I expect it will still be there).</li>
</ul></li>
<li>(If the noise is nice, ex. Gaussian, then because we are basically convolving with the noise, convexity is preserved.)</li>
<li>If we want an online strategy (which makes much more sense), we can’t unroll like this and get a convex optimization problem. <strong>What to do?</strong></li>
</ul>
<h2 id="references">References</h2>
<p>Might be useful.</p>
<ul>
<li>Kernel methods solve bandit convex optimization: [BEL16] Kernel-based methods for bandit convex optimization (Can something like this work in the RL setting?)</li>
<li>Relationship between sampling and interior point methods: [AH15] Faster Convex Optimization - Simulated Annealing with an Efficient Universal Barrier</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Arrows</title>
    <link href="http://holdenlee.github.io/notebook/posts/cs/functional_programming/arrows.html" />
    <id>http://holdenlee.github.io/notebook/posts/cs/functional_programming/arrows.html</id>
    <published>2016-12-31T00:00:00Z</published>
    <updated>2016-12-31T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Arrows</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-12-31 
          , Modified: 2016-12-31 
	</p>
      
       <p>Tags: <a href="/tags/haskell.html">haskell</a>, <a href="/tags/functional%20programming.html">functional programming</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p>References</p>
<ul>
<li>Wikibooks
<ul>
<li><a href="https://en.m.wikibooks.org/wiki/Haskell/Understanding_arrows">Understanding</a></li>
<li><a href="https://en.m.wikibooks.org/wiki/Haskell/Arrow_tutorial">Tutorial</a></li>
</ul></li>
<li><a href="http://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#arrow-notation">GHC</a></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Transfer learning</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/transfer.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/transfer.html</id>
    <published>2016-12-29T00:00:00Z</published>
    <updated>2016-12-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Transfer learning</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-12-29 
          , Modified: 2016-12-29 
	</p>
      
       <p>Tags: <a href="/tags/transfer%20learning.html">transfer learning</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#changes-at-test-time">Changes at test time</a></li>
 <li><a href="#domain-adaptationmulti-task-learning">Domain adaptation/multi-task learning</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="changes-at-test-time">Changes at test time</h2>
<p>Ex. Google Flu Trends</p>
<p>Training <span class="math inline">\(x\sim p^*\)</span>, test <span class="math inline">\(x\sim q^*\)</span>.</p>
<p>Instance weighting: upweight examples underrepresented at test time <span class="math display">\[
\wh L(\te) = \rc n \sumo in \wh w(x) \ell((x,y);\te).
\]</span></p>
<p>Problems: have to estimate <span class="math inline">\(\wh w\)</span>, have to assume <span class="math inline">\(q^*\)</span> is absolutely continuous wrt <span class="math inline">\(p^*\)</span>.</p>
<h2 id="domain-adaptationmulti-task-learning">Domain adaptation/multi-task learning</h2>
<p>Even <span class="math inline">\(p^*(y|x)\)</span> can be different.</p>
<p>Solve joint ERM problem assuming weight vectors are close.</p>
<p>Regularize by e.g. <span class="math inline">\(\sum_{i\ne j}\ve{w_i-w_j}^2\)</span>, <span class="math inline">\(\ve{W}_*\)</span>, or <span class="math inline">\(\ve{W}_{2,1}\)</span> (sum of <span class="math inline">\(L^2\)</span> norms of rows - for sparse set of features).</p>
<p>NN: share same hidden layer.</p>
<p>Deep NN non-robust: perturbation <span class="math display">\[
\min_{r\in \R^d} (f(x+r) - y_{\text{wrong}})^2
+ \la\ve{r}^2.
\]</span></p>
<p>Robust optimization: <span class="math inline">\(K\)</span> features zeroed out at test time.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Kernels</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/kernels.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/kernels.html</id>
    <published>2016-12-29T00:00:00Z</published>
    <updated>2016-12-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Kernels</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-12-29 
          , Modified: 2016-12-29 
	</p>
      
       <p>Tags: <a href="/tags/kernels.html">kernels</a>, <a href="/tags/RKHS.html">RKHS</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#kernels">Kernels</a></li>
 <li><a href="#equivalences">Equivalences</a></li>
 <li><a href="#rkhs">RKHS</a></li>
 <li><a href="#learning">Learning</a></li>
 <li><a href="#fourier-properties">Fourier properties</a></li>
 <li><a href="#efficient-computation">Efficient computation</a><ul>
 <li><a href="#random-features">Random features</a></li>
 <li><a href="#nystrom">Nystrom</a></li>
 </ul></li>
 <li><a href="#universality">Universality</a></li>
 <li><a href="#rkhs-embedding-of-probability-distributions">RKHS embedding of probability distributions</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>Mostly from Percy Liang’s 229T notes.</p>
<h2 id="kernels">Kernels</h2>
<p>If <span class="math inline">\(w=\sumo in \al_i \phi(x^{(i)})\)</span> then <span class="math inline">\(\an{w,\phi(x)} = \sumo in \al_i \an{\phi(x^{(i)}), \phi(x)}\)</span>.</p>
<h2 id="equivalences">Equivalences</h2>
<p>The following are equivalent (although multiple feature maps can correspond to the same kernel/RKHS):</p>
<ul>
<li>Feature map <span class="math inline">\(\phi:X\to H\)</span>.</li>
<li>Symmetric, positive (semi)definite kernel <span class="math inline">\(k:X\times X\to \R\)</span>. (for all <span class="math inline">\(S\)</span>, <span class="math inline">\((k(x_i,x_j))_{i,j\in S}\)</span> is PSD.)</li>
<li>RKHS <span class="math inline">\(\{f:X\to \R\}, \ved_H\)</span>.</li>
</ul>
<p>Proof:</p>
<ul>
<li>Feature map to kernel: <span class="math inline">\(k(x,x') = \an{\phi(x), \phi(x')}\)</span>.</li>
<li>RKHS to kernel: Riesz representation (see below). <span class="math inline">\(L_x(f) = \an{R_x,f}\)</span>.</li>
<li>Kernel to RKHS: Moore-Aronszajn. <span class="math inline">\(f=\sumo in \al_i k(x_i,x)\)</span>, <span class="math inline">\(\an{f,g} - \sumo in \sumo j{n'} \al_i\be_j k(x_i,x_j')\)</span>. (See below.)</li>
</ul>
<h2 id="rkhs">RKHS</h2>
<p>A <a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space">reproducing kernel Hilbert space (RKHS)</a> is a Hilbert space of functions <span class="math inline">\(X\to \R\)</span> in which point evaluation <span class="math inline">\(L_x[f] = f(x)\)</span> is a continuous linear functional. (Equivalently, it is bounded: <span class="math inline">\(f(x)\le M_x\ve{f}_H\)</span>.)</p>
<ul>
<li><span class="math inline">\(L^2\)</span> is not a RKHS—it consists of equivalence classes of functions.</li>
<li><span class="math inline">\(H=\set{f\in L_2(\R)}{\Supp(\wh f)\subeq [-a,a]}\)</span> is RKHS. Here <span class="math inline">\(K_x(y) = \fc{\sin(a(y-x))}{\pi(y-x)}\)</span> (bandlimited Dirac delta).</li>
</ul>
<p><strong>Theorem</strong> A Hilbert space of functions is a RKHS iff for every <span class="math inline">\(x\in X\)</span>, there exists <span class="math inline">\(K_x\)</span> such that <span class="math inline">\(g(x) = \an{g, K_x}\)</span>.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<p><em>Proof</em>. <span class="math inline">\(\Leftarrow\)</span>: inner product is continuous. <span class="math inline">\(\Rightarrow\)</span>: Riesz representation.</p>
<p>Note <span class="math inline">\(K_x(y) = \an{K_x,K_y}=:K(x,y)\)</span>. <span class="math inline">\(K\)</span> is symmetric and positive definite. (Think of <span class="math inline">\(K\)</span> as <span class="math inline">\(X_0^\R\times X_0^\R\to \R\)</span> where <span class="math inline">\(X_0^\R\)</span> is the set of functions <span class="math inline">\(X\to \R\)</span> nonzero only at a finite number of <span class="math inline">\(x\)</span>’s. I.e. formal span of <span class="math inline">\(X\)</span>.)</p>
<p><strong>Theorem (Moore-Aronszajn)</strong>. Suppose <span class="math inline">\(K\)</span> is a symmetric, positive definite kernel on a set <span class="math inline">\(X\)</span>. Then there is a unique Hilbert space of functions on <span class="math inline">\(X\)</span> for which <span class="math inline">\(K\)</span> is a reproducing kernel.</p>
<p><em>Proof</em>. Extend <span class="math inline">\(K\)</span> by bilinearity to <span class="math inline">\(\spn(X)\)</span>. Take the completion. (Extend to functions <span class="math inline">\(f=\sumo i{\iy} a_i K_{x_i}(x)\)</span> where <span class="math inline">\(\sumo i{\iy} a_i^2 K(x_i,x_i)\)</span>.)</p>
<p>(Question: how to realize this inner product as an integral?)</p>
<p>See also: Mercer’s theorem</p>
<h2 id="learning">Learning</h2>
<p><a href="https://en.wikipedia.org/wiki/Representer_theorem">Representer theorem</a> states that every function in an RKHS that minimises an empirical risk function can be written as a linear combination of the kernel function evaluated at the training points.</p>
<p>If <span class="math display">\[
f^*\in \amin_{f\in H} L(\{(x_i,y_i,f(x_i))\}) + Q(\ve{f}_H^2)
\]</span> where <span class="math inline">\(Q:[0,\iy)\to \R\)</span> strictly increasing is a regularizer, then <span class="math display">\[
f^*\in \spn(\set{k(x_i, \cdot)}{i\in [n]}).
\]</span></p>
<p>Example: kernel ridge regression <span class="math inline">\(\al = (K+\la I)^{-1}Y\)</span>.</p>
<h2 id="fourier-properties">Fourier properties</h2>
<p><span class="math inline">\(k:X\times X\to \R\)</span>, <span class="math inline">\(X\subeq \R^b\)</span> is shift-invariant if <span class="math inline">\(k(x,x') = h(x-x')\)</span>. Let <span class="math inline">\(t=x-x'\)</span>.</p>
<p><strong>Bochner’s theorem</strong>. Let <span class="math inline">\(k(x,x') = h(x-x')\)</span> be a continuous shift-invariant kernel (<span class="math inline">\(x\in \R^b\)</span>). There exists a unique finite <em>nonnegative</em> measure <span class="math inline">\(\mu\)</span> (spectral measure) such that <span class="math display">\[
h(t) = \int e^{-i\an{t,\om}}\mu(d\om).
\]</span> (<span class="math inline">\(h\)</span> is the characteristic function of <span class="math inline">\(\mu\)</span>. For <span class="math inline">\(\mu(d\om)=s(\om)d\om\)</span>, call <span class="math inline">\(s\)</span> the spectral density.) (See probability notes.)</p>
<h2 id="efficient-computation">Efficient computation</h2>
<ul>
<li>Random features: We will write the kernel function as an integral, and using Monte Carlo approximations of this integral. These approximations are of the kernel function and are data-independent.</li>
<li>Nystrom method: We will sample a subset of the n points and use these points to approximate the kernel matrix. These approximations are of the kernel matrix and are data-dependent.</li>
</ul>
<p>Ex. when points are clustered into <span class="math inline">\(m\)</span> clusters, kernel matrix looks like block diagonal with <span class="math inline">\(m\)</span> blocks <span class="math inline">\(J\)</span>, so rank <span class="math inline">\(m\)</span> approximation is effective.</p>
<h3 id="random-features">Random features</h3>
<p>Draw <span class="math inline">\(\om_i\sim \mu\)</span>, and estimate <span class="math display">\[
\wh k(x,x') = \rc m \sumo im \phi_{\om_i}(x)\ol{\phi_{\om_i}(x)}.
\]</span></p>
<strong>Theorem</strong>. Let
<span class="math display">\[\begin{align}
\mathcal F &amp;= \set{x\mapsto \int \al(\om)\phi_\om(x)\mu(d\om)}{\forall \om, |\al(\om)|\le C}\\
\wh{\mathcal F} &amp;= \set{x\mapsto \rc m\sumo im \al(\om_i)\phi_{\om_i}(x)\mu(d\om)}{\forall \om, |\al(\om)|\le C}
\end{align}\]</span>
<p>where <span class="math inline">\(\om_i\sim \mu\)</span>. Let <span class="math inline">\(\an{f,g}=\EE_{x\sim p^*}[fg]\)</span>. Then <span class="math display">\[
\Pj\pa{
\exists \wh f\in \wh{\mathcal F},
\ve{\wh f- f^*}\le \fc{C}{\sqrt m} 
\pa{1+\sqrt{2\ln(1/\de)}}
}.
\]</span></p>
<p><em>Proof</em>. McDiarmid and Jensen. (p. 133-5, omitted.)</p>
<p>(Q: what about generalization guarantee? See p. 134)</p>
<p>For other kernels, note <span class="math display">\[\an{x,x'}=\E[\an{\om, x}\an{\om,x'}].\]</span> For <span class="math inline">\(\an{x,x'}^p\)</span>, take <span class="math inline">\(p\)</span> independent draws and multiply together. For general <span class="math inline">\(f\)</span>, expand in Taylor series.</p>
<p>Comparison to neural nets: “the random features view of kernels defines a function class where <span class="math inline">\(\om_{1:m}\)</span> is chosen randomly rather than optimized, while <span class="math inline">\(\alpha_{1:m}\)</span> are optimized. The fact that random features approximates a kernel suggests that even the random initialization is quite sensible starting point.”</p>
For <span class="math inline">\(\phi_\om(x) = \one_{\om^Tx\ge 0}(\om^Tx)^q\)</span>, as <span class="math inline">\(m\to \iy\)</span>, get the kernel
<span class="math display">\[\begin{align}
k(x,x') &amp;= 2\int\phi_\om(x)\phi_\om(x')p(\om)d\om\\
&amp;= \rc\pi \ve{x}^q\ve{x'}^q J_q\ub{\pf{\cos^{-1}\pf{x^Tx'}}{\ve{x}\ve{x'}}}{\te}\\
J_0(\te) &amp;= \pi - \te\\
J_1(\te) &amp;= \sin\te + (\pi - \te)\cos\te.
\end{align}\]</span>
<p>(Bessel function?) Decouples magnitude from angle. (Cho/Saul 2009)</p>
<h3 id="nystrom">Nystrom</h3>
<p>$$</p>
<p><span class="math display">\[K \approx \matt{K_{II}}{K_{IJ}}{K_{JI}}{K_{JI}K_{II}^{+}K_{IJ}}.\]</span> <span class="math inline">\(K_{JJ} - K_{JI}K_{II}^{+}K_{IJ}\)</span> is the Schur complement.</p>
<p>Drineas/Mahoney: Choose <span class="math inline">\(I\)</span> by sampling <span class="math inline">\(i\)</span> with probability <span class="math inline">\(\fc{K_{ii}}{\sumo jn K_{jj}}\)</span>. (p. 138)</p>
<h2 id="universality">Universality</h2>
<p>Let <span class="math inline">\(X\)</span> be locally compact Hausdorff. <span class="math inline">\(k\)</span> is a <span class="math inline">\(c_0\)</span>-universal kernel if <span class="math inline">\(H\)</span> with reproducing kernel <span class="math inline">\(k\)</span> is dense in <span class="math inline">\(C_0(X)\)</span> (continuous bounded functions wrt uniform (<span class="math inline">\(L^{\iy}\)</span>) norm).</p>
<p>Carmeli2010: Let <span class="math inline">\(k\)</span> be shift-invariant with spectral measure <span class="math inline">\(\mu\)</span>. If <span class="math inline">\(\Supp(\mu) = \R^b\)</span>, then <span class="math inline">\(k\)</span> is universal.</p>
<h2 id="rkhs-embedding-of-probability-distributions">RKHS embedding of probability distributions</h2>
<p>Kernels can be used to represent and answer questions about probability distributions without having to explicitly estimate them.</p>
<p><strong>Maximum mean discrepancy</strong> <span class="math display">\[
D(P,Q,F) := \sup_{f\in\mathcal F}
\pa{
\EE_{x\sim P}[f(x)] - \EE_{x\sim Q}[f(x)]
}.
\]</span></p>
<p>If <span class="math inline">\(\mathcal F=C_0(X)\)</span>, then <span class="math inline">\(D(P,Q,\mathcal F)=0\implies P=Q\)</span>. Better: Can let <span class="math inline">\(\mathcal F = \set{f\in H}{\ve{f}_H\le 1}\)</span> where <span class="math inline">\(k\)</span> is universal. (Check proof p. 140)</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Letting <span class="math inline">\(f_x=\delta_x\)</span> is cheating—we want actual functions, not distributions.<a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>(R16) How to calculate partition functions using convex programming hierarchies - provable bounds for variational methods</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/probabilistic/R16.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/probabilistic/R16.html</id>
    <published>2016-12-28T00:00:00Z</published>
    <updated>2016-12-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>(R16) How to calculate partition functions using convex programming hierarchies - provable bounds for variational methods</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-12-28 
          , Modified: 2016-12-28 
	</p>
      
       <p>Tags: <a href="/tags/dynamical%20systems.html">dynamical systems</a>, <a href="/tags/quasi-convexity.html">quasi-convexity</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#introduction">Introduction</a></li>
 <li><a href="#definitions-and-setting">Definitions and setting</a></li>
 <li><a href="#result">Result</a></li>
 <li><a href="#method">Method</a><ul>
 <li><a href="#dense-graphs-sa">Dense graphs, SA</a></li>
 <li><a href="#low-threshold-rank-lasserre">Low threshold rank, Lasserre</a></li>
 </ul></li>
 <li><a href="#interpretation">Interpretation</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="introduction">Introduction</h2>
<p>Two approaches for calculating partition functions</p>
<ol type="1">
<li>Markov chains to sample
<ul>
<li>(Jerrum04, JerrumSinclair1993) Certain Markov chains mix rapidly; used to approximate permanent with nonnegative entries and partition function for ferromagnetic Ising.</li>
</ul></li>
<li>Variational methods: partition function is the solution of a (intractable) optimization problem over the polytope of valid distributions.
<ul>
<li>Popular, faster, easier to parallelize</li>
<li>Little theoretical understanding</li>
<li>Another algorithm: Belief propagation (solving non-convex relaxation)
<ul>
<li>Regime of correlation decay and locally tree-like graphs.</li>
</ul></li>
</ul></li>
</ol>
<p>Use Sherali-Adams and Lasserre convex programming hierarchies to get the first provable, convex variational methods. They work <em>because</em> local correlations propagate to global correlations, the opposite of correlation decay.</p>
<p>(See <a href="../optimization/hierarchies.html">hierarchies</a>.)</p>
<h2 id="definitions-and-setting">Definitions and setting</h2>
<p>Ising model <span class="math inline">\(p(x)\propto \exp\pa{\sum_{i,j} J_{i,j}x_ix_j}\)</span>, <span class="math inline">\(x\in \{-1,1\}^n\)</span> is <span class="math inline">\(\De\)</span>-dense if letting <span class="math inline">\(J_T=\sum_{i,j}|J_{i,j}|\)</span>, <span class="math display">\[
\forall i,j\in [n], \De |J_{i,j}|\le \fc{J_T}{n^2},
\]</span> (Ex. if the <span class="math inline">\(J_{i,j}\)</span> is 1 for an edge and 0 for a non-edge, and the graph has density <span class="math inline">\(cn^2\)</span>, then <span class="math inline">\(\De = \rc{c}\)</span>.)</p>
<p><span class="math inline">\(p\)</span> is <strong>regular</strong> if <span class="math inline">\(\sum_j |J_{i,j}|=J'\)</span>. The adjacency matrix is <span class="math inline">\(A_{i,j} = \fc{|J_{i,j}|}{J'}\)</span>. Let <span class="math inline">\(\rank_\tau(A)\)</span> be the number of eigenvalues with <span class="math inline">\(\ad\ge\tau\)</span>.</p>
<p>Why dense Ising models?</p>
<ul>
<li>There are PTAS for CSPs for dense constraint graphs.</li>
<li>Mean-field ferromagnetic Ising model: each spin interacts with every other spin.</li>
</ul>
<h2 id="result">Result</h2>
<ul>
<li>Algorithm based on SA hierarchy achieving additive approximation of <span class="math inline">\(\ep J_T\)</span> to <span class="math inline">\(\ln Z\)</span> in time <span class="math inline">\(n^{O\prc{\De \ep^2}}\)</span>.</li>
<li>Algorithm based on Lasserre hierarchy achieving additive approximation of <span class="math inline">\(\ep n J'\)</span> to <span class="math inline">\(\ln Z\)</span> in time <span class="math inline">\(n^{\rank_{\Om(\ep^2)}(A)/\Om(\ep^2)}\)</span>.</li>
</ul>
<h2 id="method">Method</h2>
<h3 id="dense-graphs-sa">Dense graphs, SA</h3>
<ul>
<li>(Variational characterization of <span class="math inline">\(\ln Z\)</span>) <span class="math inline">\(\ln Z = \max_{\mu\in M} \sum_{i\sim j} J_{i,j} \E_\mu [x_ix_j] + H(\mu)\)</span> where <span class="math inline">\(M\)</span> is the polytope of distributions over <span class="math inline">\(\{-1,1\}^n\)</span>. (Maximum achieved at <span class="math inline">\(\mu=p\)</span>.)
<ul>
<li><em>Proof</em>. The KL divergence to <span class="math inline">\(p\)</span> is <span class="math inline">\(\ge 0\)</span>.</li>
</ul></li>
<li>Relax to <span class="math inline">\(M'\supeq M\)</span>.</li>
<li>Need to design surrogates for <span class="math inline">\(H(\mu)\)</span>.
<ul>
<li>Popular choice: Bethe entropy. (But it is not an upper bound in general.)</li>
<li>Instead, design <span class="math inline">\(\wt H(\mu) \ge H(\mu)\)</span> whenever <span class="math inline">\(\mu\in M\)</span>, then round to distributions.</li>
</ul></li>
<li>Define the <strong>augmented mean-field pseudo-entropy functional</strong> <span class="math display">\[
H_{aMF,k}(\mu) = \min_{|S|\le k}H(\mu_S) + \sum_{i\nin S} H(\mu_i |\mu_S).
\]</span>
<ul>
<li>By the chain rule and “conditioning reduces entropy”, <span class="math inline">\(H(\mu)\le H_{aMF,k}(\mu)\)</span>.</li>
<li><span class="math inline">\(H(\mu_i|\mu_S)\)</span> is concave, so <span class="math inline">\(H_{aMF,k}(\mu)\)</span> is concave. (Proof omitted.)</li>
</ul></li>
<li>The relaxation is <span class="math display">\[
\max_{\mu\in SA(O(1/(\De \ep^2)))} \bc{
\sum_{i,j} J_{i,j} \E_\mu[x_ix_j] + H_{aMF,k}(\mu)
}
\]</span></li>
<li>Correlation rounding: pick a seed set, condition on it, and round other variables independently. (YoshidaZhou2014) There is a set of size <span class="math inline">\(k=O\prc{\De \ep^2}\)</span> such that <span class="math display">\[
\ab{\sum_{i,j} J_{i,j} \E_\mu[x_ix_j|x_S] - \sum_{i,j} J_{i,j} \E_\mu [x_i|x_S] \E_\mu[x_j|x_S]}\le \fc{100}{\De k}J_T.
\]</span></li>
<li>Letting <span class="math inline">\(\wt\mu(x) = \mu(x_S) \prod_{i\nin S} \mu(x_i|x_S)\)</span>, <span class="math inline">\(\E_{\wt\mu} [x_ix_j|x_S] = \E_\mu[x_i|x_S]\mu[x_j|x_S]\)</span>.Combine YZ14 with <span class="math inline">\(H\le H_{aMF,k}\)</span>.</li>
</ul>
<h3 id="low-threshold-rank-lasserre">Low threshold rank, Lasserre</h3>
<ul>
<li>The correlation rounding bound changes to: <span class="math inline">\(\exists |S|\le \rank_{\Om(\ep^2)}(A)\)</span>, <span class="math inline">\(...\le \ep J_T\)</span>. <!-- $$\max_{\mu\in LAS(k)} \ba{\sum_{i,j} \E_\mu [x_ix_j] + H_{aMF,k}(\mu)}.$$--></li>
</ul>
<h2 id="interpretation">Interpretation</h2>
<ul>
<li>High temperature <span class="math inline">\(|H| = O\prc{d}\)</span>
<ul>
<li>Dobrushin’s uniqueness criterion: correlation decay</li>
<li>MC methods work and give <span class="math inline">\((1+\ep)\)</span> factor approximation, stronger.</li>
</ul></li>
<li>Transition threshold <span class="math inline">\(|J|=\Te\prc{d}\)</span>.
<ul>
<li>MC provide no nontrivial guarantee.</li>
<li>We get <span class="math inline">\(\ep n\)</span> additive factor approximation.</li>
</ul></li>
<li>Low temperature <span class="math inline">\(|J|=\om\prc{d}\)</span>. :(</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Legendre transform</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/legendre_transform.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/legendre_transform.html</id>
    <published>2016-12-28T00:00:00Z</published>
    <updated>2016-12-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Legendre transform</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-12-28 
          , Modified: 2016-12-28 
	</p>
      
       <p>Tags: <a href="/tags/convexity.html">convexity</a>, <a href="/tags/duality.html">duality</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#definition">Definition</a></li>
 <li><a href="#appendix-von-neumann-minimax">Appendix: von Neumann minimax</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="https://en.wikipedia.org/wiki/Legendre_transformation">Wikipedia</a></p>
<p><a href="http://www.maths.manchester.ac.uk/~goran/legendre.pdf">Reference</a></p>
<h2 id="definition">Definition</h2>
<p>The <strong>Legendre transform</strong> (or conjugate dual) of a function <span class="math inline">\(f:X \to \R\)</span> is defined as<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> <span class="math display">\[
f^*(y) = \sup_{x\in \Om} [\an{y,x} - f(x)]
\]</span> <span class="math inline">\(f^*\)</span> is a function <span class="math inline">\(Y\to \R\)</span> where <span class="math inline">\(Y=\set{y}{\sup_{x\in X} [\an{y,x} - f(x)]&lt;\iy}\)</span>.</p>
<p><strong>Proposition</strong>. <span class="math inline">\(f^*\)</span> is convex.</p>
<p><em>Proof</em>. <span class="math inline">\(f^*\)</span> is a sup of linear (hence convex) functions.</p>
<p>Note that if <span class="math inline">\(f\)</span> is differentiable, the argmax satisfies <span class="math inline">\(f'(x) = y\)</span>.</p>
<p><strong>Theorem</strong>. If <span class="math inline">\(f\)</span> is convex with domain <span class="math inline">\(X\)</span> that is compact (and convex), then <span class="math inline">\(f=f^{**}\)</span>.</p>
<p>(Does this work without <span class="math inline">\(X\)</span> being compact?)</p>
<em>Proof</em>. We have <span class="math display">\[
f^{**}(x') = \sup_{y\in Y} \inf_{x\in X} \an{x',y}-\an{y,x} + f(x).
\]</span> We want this to <span class="math display">\[
=\inf_{x\in X} \sup_{y\in Y}  \an{x',y}-\an{y,x} + f(x).
\]</span> <span class="math inline">\(\le\)</span> is clear (it’s better to go second). We want to show <span class="math inline">\(\ge\)</span> using minimax. A technical point is that we have to restrict to compact sets by adding a penalty term. Take compact sets <span class="math inline">\(\bigcup K_i = \R^n\)</span>
<span class="math display">\[\begin{align}
&amp;= \lim_{i\to \iy}
\sup_{y\in K_i} \inf_{x\in X} \an{x',y}-\an{y,x} + f(x)\\
&amp; = \lim_{i\to \iy}
\inf_{x\in X} \sup_{y\in K_i} \an{x'-x,y} f(x)\\
&amp; = \lim_{i\to \iy} \inf_{x\in X} f(x) + (\sup_{y\in K_i} \an{x'-x,y}).
\end{align}\]</span>
<p>Thus first player would choose <span class="math inline">\(x=x'\)</span>, giving the value <span class="math inline">\(f(x')\)</span>.</p>
<h2 id="appendix-von-neumann-minimax">Appendix: von Neumann minimax</h2>
<p>Let <span class="math inline">\(K\subeq \R^n\)</span>, <span class="math inline">\(L\subeq \R^m\)</span> be compact and convex. Let <span class="math inline">\(f:K\times L\to \R\)</span> where <span class="math inline">\(f\)</span> is concave in <span class="math inline">\(x\)</span> and convex in <span class="math inline">\(y\)</span>. Then there exists <span class="math inline">\((x_*,y_*)\in K\times L\)</span> such that for all <span class="math inline">\(x\in K\)</span>, <span class="math inline">\(y\in L\)</span>, <span class="math display">\[
f(x,y^*) \le f(x_*,y_*) \le f(x_*,y).
\]</span></p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>This definition is suited for convex functions. The definition suited for concave functions is <span class="math display">\[f^(y) = \inf_{x\in \Om} [\an{y,x} - f(x)].\]</span><a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Hierarchies of convex relaxations</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/hierarchies.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/hierarchies.html</id>
    <published>2016-12-28T00:00:00Z</published>
    <updated>2016-12-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Hierarchies of convex relaxations</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-12-28 
          , Modified: 2016-12-28 
	</p>
      
       <p>Tags: <a href="/tags/convex%20relaxation.html">convex relaxation</a>, <a href="/tags/Sherali-Adams.html">Sherali-Adams</a>, <a href="/tags/Lasserre.html">Lasserre</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#sherali-adams">Sherali-Adams</a></li>
 <li><a href="#lasserre">Lasserre</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>See also <a href="../../complexity/sos.html">SoS</a>.</p>
<h2 id="sherali-adams">Sherali-Adams</h2>
<p>The <span class="math inline">\(k\)</span>-level Sherali-Adams hierarchy <span class="math inline">\(SA(k)\)</span> is a LP with variables <span class="math inline">\(\mu_S(x_S)\)</span>, <span class="math inline">\(x_S\in \{-1,1\}^{|S|}\)</span> specifying marginals over all <span class="math inline">\(S\subeq [n]\)</span>, <span class="math inline">\(|S|\le k\)</span>. <span class="math inline">\(\mu_S\)</span> satisfies consistency: <span class="math display">\[
\forall |S\cup T|\le k, 
\mu_S[\{x_{S\cap T}=\al\}] 
= \mu_T[\{x_{S\cap T} = \al\}].
\]</span></p>
<p>The conditioning operation - sampling according to <span class="math inline">\(\mu_{\{v\}}\)</span> - defines a map from solutions of the <span class="math inline">\(k\)</span>th level to solutions of the <span class="math inline">\(k-1\)</span>th level. <span class="math display">\[
\mu_S(x_S) = \mu_{S\cup \{v\}}(x_{S\cup v}).
\]</span></p>
<h2 id="lasserre">Lasserre</h2>
<p>LAS(<span class="math inline">\(k\)</span>). This is a SDP: <span class="math display">\[
\forall |S\cup T|\le k, \mu_{S\cup T}(x_S=\al, x_T=\be) = \an{v_{S,\al}, v_{T,\be}}.
\]</span></p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
