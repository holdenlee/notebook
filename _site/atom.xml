<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2016-06-28T00:00:00Z</updated>
    <entry>
    <title>k-means clustering</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/k-means.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/k-means.html</id>
    <published>2016-06-28T00:00:00Z</published>
    <updated>2016-06-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>k-means clustering</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-06-28 
          , Modified: 2016-06-28 
	</p>
      
       <p>Tags: <a href="/tags/clustering.html">clustering</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#questions">Questions</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="http://cs229.stanford.edu/notes/cs229-notes7a.pdf">Reference</a></p>
<p>The <span class="math inline">\(k\)</span>-means clustering is the following. (Lloyd’s algorithm) Let data points be <span class="math inline">\(x^{(i)}\)</span>.</p>
<ol type="1">
<li>Initialize <span class="math inline">\(\mu_1,\ldots, \mu_k\)</span>.</li>
<li>Repeat until convergence.
<ol type="1">
<li>Let <span class="math inline">\(c(i) = \amin_j |x^{(i)}-\mu_j|\)</span>.</li>
<li>Let <span class="math inline">\(\mu_j = \EE_{c(i)=j} x^{(i)}\)</span>.</li>
</ol></li>
</ol>
<p>This is alternating minimization on the distortion function <span class="math display">\[J(c,\mu) = \sumo im \ve{x^{(i)}-\mu_{c(i)}}^2\]</span> with respect to <span class="math inline">\(c\)</span> and <span class="math inline">\(\mu\)</span>. This value decreases and so converges (possible to local optimum); in practice, the <span class="math inline">\(\mu\)</span>’s converge.</p>
<h2 id="questions">Questions</h2>
<p>Are there provable guarantees on <span class="math inline">\(k\)</span>-means clustering—if so, what? And/or is there a (worst-case) hardness result?</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>ICA (Independent components analysis)</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/ica.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/ica.html</id>
    <published>2016-06-28T00:00:00Z</published>
    <updated>2016-06-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>ICA (Independent components analysis)</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-06-28 
          , Modified: 2016-06-28 
	</p>
      
       <p>Tags: <a href="/tags/ICA.html">ICA</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#algorithm">Algorithm</a></li>
 <li><a href="#tensor-algorithm">Tensor algorithm</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="http://cs229.stanford.edu/notes/cs229-notes11.pdf">Reference</a></p>
<p>Given that the components of <span class="math inline">\(s\in \R^n\)</span> are independent, and we observe <span class="math inline">\(x=As\)</span>, we want to find <span class="math inline">\(W=A^{-1}\)</span> and recover <span class="math inline">\(s\)</span>. Let <span class="math inline">\(w_i^T\)</span> be the rows of <span class="math inline">\(W\)</span>.</p>
<p>When the <span class="math inline">\(s_i\)</span> are non-gaussian, the solution is unique up to permutation and scaling. (Otherwise, there is rotational invariance.)</p>
<h2 id="algorithm">Algorithm</h2>
<p>Suppose the cdf of the components is logistic. (This is a reasonable default. Mean-center first.) Let <span class="math inline">\(g(s) = \rc{1+e^{-s}}\)</span>.</p>
<p>Change of coordinates gives <span class="math inline">\(p_x(x) = p_s(Wx)\det(W)\)</span>. The log-likelihood is <span class="math display">\[\ell(W) = \sumo im \pa{\sumo jn \ln g'(w_j^T x^{(i)}) + \ln |W|}.\]</span> Use stochastic gradient ascent.</p>
<p>Note: for problems where successive training examples are correlated, when implementing stochastic gradient ascent, it also sometimes helps accelerate convergence if we visit training examples in a randomly permuted order.</p>
<h2 id="tensor-algorithm">Tensor algorithm</h2>
<p>See “new_thread.pdf” page 30.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Factor analysis</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/factor-analysis.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/factor-analysis.html</id>
    <published>2016-06-28T00:00:00Z</published>
    <updated>2016-06-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Factor analysis</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-06-28 
          , Modified: 2016-06-28 
	</p>
      
       <p>Tags: <a href="/tags/factor%20analysis.html">factor analysis</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#solution-1-assume-independence">Solution 1: Assume independence</a></li>
 <li><a href="#solution-2-factor-analysis">Solution 2: Factor analysis</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="http://cs229.stanford.edu/notes/cs229-notes9.pdf">Reference</a></p>
<p>If <span class="math inline">\(n\gg m\)</span>, and we have data points <span class="math inline">\(x^{(i)}\in \R^n\)</span>, <span class="math inline">\(1\le i\le m\)</span>, how can we find Gaussian structure? We don’t have enough data points to even fit a single Gaussian.</p>
<h2 id="solution-1-assume-independence">Solution 1: Assume independence</h2>
<p>If the covariance matrix <span class="math inline">\(\Si\)</span> is diagonal, minimize the negative log likelihood <span class="math display">\[\sum\pa{\pf{\pa{x_j^{(i)}-\mu_j}^2}{2\si_j^2} + \ln \si_j}\]</span> to get <span class="math inline">\(\Si_{jj} = \EE_{i=1}^m (x_j^{(i)}-\mu_j)^2\)</span>. If <span class="math inline">\(\Si=\si I\)</span>, then <span class="math inline">\(\si^2 = \EE_{i,j}(x_j^{(i)}- \mu_j)^2\)</span>.</p>
<h2 id="solution-2-factor-analysis">Solution 2: Factor analysis</h2>
Break the coordinates into 2 parts <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span> and assume
\begin{align}
z&amp;\sim N(0,I)\\
\ep &amp;\sim N(0,\Psi)\\
x &amp;= \mu+ \La z + \ep.
\end{align}
<p>Calculate <span class="math display">\[\coltwo zx \sim N\pa{\coltwo 0\mu, \matt{I}{\La^T}{\La}{\La\La^T+\Psi}}.\]</span> Now do EM on the log likelihood with respect to <span class="math inline">\(z\)</span> and <span class="math inline">\(\La\)</span>. (details…)</p>
<ul>
<li>“This is just matrix factorization.”</li>
<li>“Often allows more domain knowledge to be incorporated.”</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>CCA (Canonical correlation analysis)</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/cca.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/cca.html</id>
    <published>2016-06-28T00:00:00Z</published>
    <updated>2016-06-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>CCA (Canonical correlation analysis)</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-06-28 
          , Modified: 2016-06-28 
	</p>
      
       <p>Tags: <a href="/tags/CCA.html">CCA</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p><a href="https://en.wikipedia.org/wiki/Canonical_correlation">Wikipedia</a></p>
<p>Goal: Find the linear combination of <span class="math inline">\((X_i)\)</span> and <span class="math inline">\((Y_j)\)</span> with maximum correlation.</p>
<p>Let <span class="math inline">\(\Si_{XY} = \Cov(X,Y)\)</span>.</p>
<p>We want to maximize (let <span class="math inline">\(\ve{v}_M=v^TMv\)</span>) <span class="math display">\[\max_{a,b} \ve{a^T\Si_{XY}b}{\ve{a}_{\Si_{XX}}\ve{b}_{\Si_{YY}}}.\]</span> Let <span class="math inline">\(c=\Si_{XX}a^{\rc 2}\)</span> and <span class="math inline">\(d=\Si_{YY}^{\rc 2}b\)</span>. Then this is <span class="math display">\[\fc{c^T \Si_{XX}^{-\rc2} \Si_{XY} \Si_{YY}^{-\rc2}d}{\ve{c}_2\ve{d}_2}.\]</span> Thus, find the SVD of <span class="math display">\[\Si_{XX}^{-\rc2} \Si_{XY} \Si_{YY}^{-1} \Si_{YX} \Si_{XX}^{-\rc 2}.\]</span> Change coordinates back to find <span class="math inline">\(a,b\)</span>.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[JSA15] Beating the Perils of Non-Convexity - Guaranteed Training of Neural Networks using Tensor Methods</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/JSA15.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/JSA15.html</id>
    <published>2016-06-27T00:00:00Z</published>
    <updated>2016-06-27T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[JSA15] Beating the Perils of Non-Convexity - Guaranteed Training of Neural Networks using Tensor Methods</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-06-27 
          , Modified: 2016-06-27 
	</p>
      
       <p>Tags: <a href="/tags/neural%20net.html">neural net</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#notes">Notes</a></li>
 <li><a href="#algorithm">Algorithm</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>Gives the first provable risk bounds for 2-layer neural nets via an efficient algorithm. Assumes that the generative distribution for the inputs <span class="math inline">\(x\)</span>’s is known or estimable (the algorithm uses the 3rd order score function). Algorithm is based on tensor decomposition.</p>
<p>Remarks:</p>
<ul>
<li>The distribution <span class="math inline">\(p(y|x)\)</span> defined by the NN seems funny. <span class="math inline">\(y\in \{0,1\}\)</span> with <span class="math display">\[f(x) = \E[\wt y|x] = \an{\si_2,\si(A_1^T x + b_1)}+b_2.\]</span> But this means that we must have <span class="math inline">\(f(x)\in \{0,1\}\)</span>! Usually this is put through another sigmoid function…</li>
</ul>
<h2 id="notes">Notes</h2>
<p>See [JSA14].</p>
<p><strong>Lemma (Stein’s identity)</strong>: Under some regularity conditions, <span class="math display">\[\EE_{x\sim p} [G(x) \ot \nb_x\ln p(x)] = \EE_{x\sim p} \nb_x G(x).\]</span> (For example, for <span class="math inline">\(p(x)\)</span> Gaussian, <span class="math inline">\(\nb_x \ln p(x) = -x\)</span>.)</p>
<p><em>Proof</em>. By IbP, <span class="math display">\[\int G(x)_i \pdd{x_j} \ln p(x) p(x) \dx = \int G(x)_i \pdd{x_j}p(x) = -\int \pdd{x_j} G(x)_i p(x)\dx.\]</span></p>
<h2 id="algorithm">Algorithm</h2>
<ol type="1">
<li>Tensor decomposition</li>
<li>Fourier algorithm</li>
<li>Ridge (linear) regression</li>
</ol>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[BGKP16] Non-negative matrix factorization under heavy noise</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/BGKP16.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/BGKP16.html</id>
    <published>2016-06-27T00:00:00Z</published>
    <updated>2016-06-27T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[BGKP16] Non-negative matrix factorization under heavy noise</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-06-27 
          , Modified: 2016-06-27 
	</p>
      
       <p>Tags: <a href="/tags/NMF.html">NMF</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#summary">Summary</a></li>
 <li><a href="#assumptions">Assumptions</a></li>
 <li><a href="#algorithm">Algorithm</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="summary">Summary</h2>
<p>A provable algorithm for NMF <span class="math inline">\(A=BC\)</span> without assuming that the noise of every column of <span class="math inline">\(A\)</span> has small noise.</p>
<p>Under <strong>heavy noise</strong> <span class="math display">\[\forall T\subeq [n], |T|\ge \ep n\implies \rc{|T|}\ve{\sum_{j\in T}N_{\bullet,j}}_1\le \ep\]</span> and in the dominant NMF model (dominant features, dominant basis vectors, nearly pure records), the TSVDNMF algorithm finds <span class="math display">\[\ve{B_{\bullet,l}-\wt B_{\bullet, l}}_1\le \ep_0.\]</span></p>
<p>Under dominant NMF assumptions D1, D3, <span class="math inline">\(B\)</span> is identifiable.</p>
<p>Remarks:</p>
<ul>
<li>Dominant features is a relaxation of anchor words.</li>
<li>[AGKM12] (the original algorithm for NMF) requires separability, and does poorly under noise (because under noise the vertices of the simplex may no longer be the vertices of the simplex). Under error <span class="math inline">\(\ve{M-AW}_F^2\le \ep \ve{M}_F^2\)</span>, the algorithm takes time <span class="math inline">\(\exp\pf{r^2}{\ep^2}\)</span>.</li>
<li>Almost pure documents is an assumption not in AGKM12.</li>
<li>It only achieves constant error. (Can we do better than this?)</li>
<li>Heavy noise subsumes many noise models. Note that heavy noise is a bound on <span class="math inline">\(\ve{\sum_{j\in T}N_{\bullet, j}}_1\)</span>, not <span class="math inline">\(\sum_{j\in T}\ve{N_{\bullet, j}}_1\)</span>.</li>
</ul>
<h2 id="assumptions">Assumptions</h2>
<ul>
<li>Heavy noise was defined above. If the covariance of noise in each column is large enough, <span class="math inline">\(\ve{\Si_j}_2=O\pf{\sqrt d}{\ep^2}\)</span>, then whp the heavy noise assumption holds.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>
<ul>
<li>For example, if it is the sum of <span class="math inline">\(w\)</span> random vectors each with covariance matrix <span class="math inline">\(O(1)\)</span> in norm, then we need <span class="math inline">\(w=\Om\pf{d}{\ep^4}\)</span>. Ex. multinomial noise.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></li>
</ul></li>
</ul>
<ul>
<li>Dominant NMF: See picture. Left: Dominant features (D1). (Note: sets have to be disjoint, not necessarily a partition.) Right: Dominant basis vectors (D2) and nearly pure records (D3).</li>
</ul>
<p><img src="/images/bgkp16-dnmf.png" width="640"></p>
<h2 id="algorithm">Algorithm</h2>
<ol type="1">
<li>Apply thresholding to get <span class="math inline">\(D\)</span>.
<ul>
<li>Initialize <span class="math inline">\(R=[d]\)</span>.</li>
<li>For each row <span class="math inline">\(i\)</span>, calculate a cutoff <span class="math inline">\(\ze_i\)</span>. Set <span class="math inline">\(D_{ij}=(A_{ij}\ge \ze_i) \sqrt{\ze_i}\)</span>.</li>
<li>Sort rows in ascending order and prune rows as follows. (Why? We want to prune the non-catchwords. They may be associated with significantly more documents than the catchwords.) <img src="/images/bgkp16-alg1.png" width="640"></li>
</ul></li>
<li><p>Take rank-<span class="math inline">\(k\)</span> SVD <span class="math inline">\(D^{(k)}\)</span> of <span class="math inline">\(D\)</span>. (We hope that this is close to a block matrix with nonzeros in <span class="math inline">\(S_l\times T_l\)</span>.)</p>
<img src="/images/bgkp16-alg2.png" width="640"></li>
<li>Identify dominant basis vectors.
<ul>
<li><span class="math inline">\(k\)</span>-means clustering of columns of <span class="math inline">\(D^{(k)}\)</span>.</li>
<li>Apply Lloyd’s algorithm to <span class="math inline">\(D\)</span> with this initialization.</li>
<li>Let <span class="math inline">\((R_i)\)</span> be the <span class="math inline">\(k\)</span>-partition of <span class="math inline">\([n]\)</span>.</li>
</ul></li>
<li>Identify dominant features <span class="math inline">\(J_l\)</span> for each basis vector by: for each <span class="math inline">\(l\)</span>, take the features <span class="math inline">\(i\)</span> (words) with largest <span class="math inline">\(A_{il}\)</span>.</li>
<li><p>Find the basis vectors by averaging the “purest” documents in each <span class="math inline">\(J_l\)</span>. <img src="/images/bgkp16-alg3.png" width="640"></p></li>
</ol>
<!-- Suppose $\ve{M-AW}_F^2\le \ep \ve{M}_F^2$. Devise a better algorithm for separable approximate NMF. -->
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>The paper isn’t clear on the <span class="math inline">\(\ep\)</span> dependence… see supplement. In any case this is true in the case that noise is a sum of many <span class="math inline">\(O(1)\)</span> noises.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>The covariance matrix is <span class="math inline">\((-1_{i\ne j} p_ip_j)_{i,j}\)</span>. Max eigenvalue is at most <span class="math inline">\(\max p_i(1-p_i)\)</span> in absolute value.<a href="#fnref2">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Relevant coordinates: Low-rank</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/relevant_coordinates_svd.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/relevant_coordinates_svd.html</id>
    <published>2016-06-07T00:00:00Z</published>
    <updated>2016-06-07T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Relevant coordinates: Low-rank</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-06-07 
          , Modified: 2016-06-07 
	</p>
      
       <p>Tags: <a href="/tags/linear%20algebra%2B%2B.html">linear algebra++</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#low-rank-submatrix">Low-rank submatrix</a><ul>
 <li><a href="#assumptions">Assumptions</a></li>
 <li><a href="#algorithm">Algorithm</a></li>
 <li><a href="#proof">Proof</a></li>
 <li><a href="#extensions">Extensions</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <p>See also <a href="relevant_coordinates.html">Relevant coordinates</a>.</p>
<h2 id="low-rank-submatrix">Low-rank submatrix</h2>
<p>Suppose <span class="math inline">\(I\subeq [m]\)</span> is a subset of indices. Draw each column of <span class="math inline">\(A\)</span> as follows. <span class="math inline">\(v_I\)</span> is drawn from some distribution close to being supported on a <span class="math inline">\(k\)</span>-dimensional subspace. (For example, first draw <span class="math inline">\(w\in \R^I\)</span> from a distribution supported on a <span class="math inline">\(k\)</span>-dimensional subspace. Now let <span class="math inline">\(v_I=w+\ep\)</span> where <span class="math inline">\(\ep\)</span> is error, independent on different indices. We can relax this in various ways.) For each <span class="math inline">\(i\nin I\)</span>, suppose there is a distribution <span class="math inline">\(D_i\)</span>. Draw <span class="math inline">\(v_i\)</span> from <span class="math inline">\(D_i\)</span> (independently).</p>
<!-- Let $B_{I\times [n]}$ be a rank-$k$ matrix, and let $A_{I\times [n]}=B_{I\times[n]}+E$ where $E$ is noise. For $i\nin I$, let $A_{ij},1\le j\le m$ be independent draws from some distribution, and suppose that $A_{ij}$ for $i\nin I$ are independent. Recover $I$. -->
<h3 id="assumptions">Assumptions</h3>
<ul>
<li>Suppose for simplicity that for each <span class="math inline">\(i\)</span>, <span class="math inline">\(\E v_i=0\)</span>, <span class="math inline">\(\E v_i^2=\Var(v_i)=1\)</span>. We can always normalize so this is the case (paying a little bit error for estimation for <span class="math inline">\(\E v_i\)</span>, <span class="math inline">\(\Var(v_i)\)</span>).</li>
<li>Suppose the distribution on <span class="math inline">\(\R^I\)</span> satisfies the following. For every <span class="math inline">\(i\in I\)</span>, there exists <span class="math inline">\(j\in I, j\ne i\)</span> such that <span class="math inline">\(\E |v_iv_j|\ge \ep\)</span> (for example, <span class="math inline">\(\ep=\rc k\)</span>). (Is this a reasonable assumption? Can we replace it by something more standard?) (This is basically saying that you can’t delete a coordinate and make the distribution on <span class="math inline">\(\R^{I\bs \{i\}}\)</span> almost <span class="math inline">\((k-1)\)</span>-dimensional.)
<ul>
<li><p>Actually, we don’t need this, because:</p>
<p>Lemma: if <span class="math inline">\(B_{ii}=1\)</span>, <span class="math inline">\(B\)</span> is symmetric, has entries in <span class="math inline">\([-1,1]\)</span>, and <span class="math inline">\(B_i\)</span> is a linear combination of <span class="math inline">\(k\)</span> other rows, then there is some <span class="math inline">\(j\ne i\)</span> such that <span class="math inline">\(|B_{ij}|\ge \rc k\)</span>.</p>
<p>Proof. Suppose the <span class="math inline">\(k\)</span> rows are <span class="math inline">\([1,k]\)</span>, and the <span class="math inline">\((k+1)\)</span>th row is a linear combination of these with coefficients <span class="math inline">\(w\)</span>, i.e., letting <span class="math inline">\(C=B_{[1,n]\times [1,n]}\)</span>, <span class="math inline">\(w^TC = [B_{k+1,1},\ldots, B_{k+1,k}]\)</span>. Looking at this linear combination on the <span class="math inline">\(k+1\)</span>th column, <span class="math inline">\(w^T[B_{k+1,1},\ldots, B_{k+1,k}]=1\)</span>. Putting these together, we get <span class="math inline">\(w^TCw=1\)</span>. Because the entries of <span class="math inline">\(C\)</span> are at most 1 in absolute value, this means <span class="math display">\[(|w_1|+\cdots +|w_{k}|)^2\ge 1\implies \exists i, |w_i|\ge \rc k.\]</span></p>
Apply this to the covariance matrix.</li>
</ul></li>
<li>Suppose <span class="math inline">\(\ep\)</span>, the noise, is such that each entry is in <span class="math inline">\([-C,C]\)</span>, zero-centered, and independent.</li>
</ul>
<h3 id="algorithm">Algorithm</h3>
<p>Idea: on average, there is greater correlation between 2 random vectors in a <span class="math inline">\(k\)</span>-dimensional subspace than 2 random vectors in <span class="math inline">\(n\)</span> dimensions.</p>
<ul>
<li>Take <span class="math inline">\(n=\Om(\prc{\ep^2}(\ln m))\)</span> samples. Let them be the columns of <span class="math inline">\(A\)</span>.</li>
<li>Calculate <span class="math inline">\(\rc n AA^T\)</span> (as a guess for the covariance matrix for <span class="math inline">\(v\)</span>).</li>
<li>Declare <span class="math inline">\(i\in I\)</span> iff there is <span class="math inline">\(j\ne i\)</span> such that <span class="math inline">\((\rc{n}AA^T)_{ij}&gt;\fc{\ep}2\)</span>. <!-- * In a graph, connect up $i,j\in[n]$ by an edge iff $|A_{ij}|>\fc{\ep}{2}$.
* W.h.p., all non-isolated vertices are in a connected component. This is $I$. (I.e., declare $i\in I$ iff there is $j\ne i$ such that $(\rc{n}AA^T)_{ij}>\fc{\ep}2$.)--></li>
</ul>
<h3 id="proof">Proof</h3>
<ul>
<li>Let <span class="math inline">\(\wt \E\)</span> be the empirical average, i.e., <span class="math inline">\(\wt \E v_iv_j = \rc n(AA^T)_{ij}\)</span>. We have <span class="math inline">\(|v_iv_j|\le 1\)</span>, so by Chernoff, <span class="math display">\[\Pj(\ab{\wt \E v_iv_j - \E v_iv_j} \ge t) \le 2e^{-nt^2}\]</span> for <span class="math inline">\(i,j\nin I\)</span>. We get a similar bound if <span class="math inline">\(i\in I\)</span> or <span class="math inline">\(j\in I\)</span>, where the randomness is over the noise in <span class="math inline">\(\ep\)</span>. Union-bounding, <span class="math display">\[\Pj(\text{for some $i\ne j$, }\ab{\wt \E v_iv_j - \E v_iv_j} \ge \fc{\ep}{2})\le m^2 e^{-cn\ep^2}.\]</span> Take <span class="math inline">\(n=\Om(\prc{\ep^2}\ln m)\)</span> to get this to be <span class="math inline">\(o(1)\)</span>.</li>
<li>When either <span class="math inline">\(i\nin I\)</span> or <span class="math inline">\(j\nin I\)</span>, we have <span class="math inline">\(\E v_iv_j=0\)</span> so the above event makes <span class="math inline">\(|\wt\E v_iv_j|&lt;\fc{\ep}{2}\)</span>; when <span class="math inline">\(i\in I\)</span>, there is <span class="math inline">\(j\in I\)</span> such that <span class="math inline">\(\E |v_iv_j|&gt;\fc{\ep}{2}\)</span> by assumption so the above event makes <span class="math inline">\(|\wt \E v_iv_j|&gt;\fc{\ep}{2}\)</span>.</li>
</ul>
<h3 id="extensions">Extensions</h3>
<ul>
<li>It’s not really necessary for <span class="math inline">\(v_I\)</span> to be independent. It’s enough that <span class="math inline">\(A_{I\times [n]} = B_{I\times [n]}+E\)</span> where <span class="math inline">\(B_{I\times [n]}\)</span> is a rank <span class="math inline">\(k\)</span>-matrix such that for every <span class="math inline">\(i\in I\)</span>, there is <span class="math inline">\(j\in I\)</span> such that <span class="math inline">\(\an{B_i,B_j} &gt;\ep \ve{B_i}\ve{B_j}\)</span>, and <span class="math inline">\(E\)</span> is random-enough error.</li>
<li>Can we get a result using fewer generative assumptions?
<ul>
<li>Connect <span class="math inline">\(i,j\)</span> if <span class="math inline">\(|\wt \E v_iv_j|&gt;\fc{\ep}{2}\)</span>. We probably reduce to a graph problem, where we want to find a subset where the graph is more dense.</li>
<li>How much can we relax independence? Esp. independence of coordinates <span class="math inline">\(i\nin I\)</span>.</li>
<li>Is worst-case hard (cf. max clique, planted clique)? “Find the subset of size <span class="math inline">\(cn\)</span> that is <span class="math inline">\(\ep\)</span>-close to a rank <span class="math inline">\(k\)</span> matrix.” What about a gap-problem, i.e., there’s a guarantee that there is no subset of size <span class="math inline">\(c'n\)</span> that is <span class="math inline">\(\ep'\)</span> close to rank <span class="math inline">\(c''k\)</span>?</li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Relevant coordinates</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/relevant_coordinates.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/relevant_coordinates.html</id>
    <published>2016-06-02T00:00:00Z</published>
    <updated>2016-06-02T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Relevant coordinates</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-06-02 
          , Modified: 2016-06-02 
	</p>
      
       <p>Tags: <a href="/tags/linear%20algebra%2B%2B.html">linear algebra++</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#problem">Problem</a><ul>
 <li><a href="#clustering">Clustering</a></li>
 <li><a href="#svd">SVD</a></li>
 </ul></li>
 <li><a href="#approaches">Approaches</a><ul>
 <li><a href="#l1-vs.l2-norm">L1 vs. L2 norm</a></li>
 <li><a href="#community-detectionsdp">Community detection/SDP</a></li>
 </ul></li>
 <li><a href="#baby-problem-recovering-clusters">Baby problem: Recovering clusters</a><ul>
 <li><a href="#try-1-svd">Try 1: SVD</a></li>
 <li><a href="#try-2-sdp">Try 2: SDP</a></li>
 <li><a href="#more-than-2-groups">More than 2 groups</a></li>
 <li><a href="#clustering-1">Clustering</a></li>
 </ul></li>
 <li><a href="#svd-1">SVD</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="problem">Problem</h2>
<p>General problem: Often, given high dimensional data in <span class="math inline">\(\R^n\)</span>, only a subset <span class="math inline">\(S\sub [n]\)</span> of coordinates are “relevant”. Identify them.</p>
<p>Here are possible formalizations of the problem.</p>
<h3 id="clustering">Clustering</h3>
Let <span class="math inline">\(|S|=c_1m\)</span> be a random subset of <span class="math inline">\([m]\)</span>. Let <span class="math inline">\(k\)</span> be fixed. Let <span class="math inline">\(((x^{(i)},\si^{(i)2})\in \R^{S})_{i=1}^k\)</span> be <span class="math inline">\(k\)</span> centers and variances. (Assume some reasonable separation.) Each data point comes from some cluster <span class="math inline">\(i\in [k]\)</span> (ex. uniformly at random). If <span class="math inline">\(x\)</span> comes from cluster <span class="math inline">\(i\)</span>, then generate <span class="math inline">\(x\)</span> by
\begin{align}
x_S &amp;\sim N(x^{(i)}, \si^{(i)2})\\
x_j &amp;\sim N(0,1),&amp;j&amp;\nin S.
\end{align}
<p>The goal is to find <span class="math inline">\(S\)</span>. (Then we can recover the centers by standard clustering algorithms.)</p>
<p>Note: There may be a cheap way to do this by identifying which coordinates are <span class="math inline">\(\sim N(0,1)\)</span>. We want something more generalizable. Ex. assume <span class="math inline">\(x_j\sim N(y_j, \tau_j^2)\)</span>. Even better, assume that the <span class="math inline">\(x_j\)</span> are independent but their distribution is arbitrary.</p>
<h3 id="svd">SVD</h3>
<p>Let <span class="math inline">\(A\)</span> be a matrix such that taking the rows with indices in <span class="math inline">\(S\)</span> (<span class="math inline">\(|S|=c_1m\)</span> as above), and suppose that <span class="math inline">\(A\)</span> is <span class="math inline">\(\ep\)</span>-close<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> in <span class="math inline">\(L^2\)</span> to a rank <span class="math inline">\(k\)</span> matrix, and the rest of the entries are generated as <span class="math inline">\(N(0,1)\)</span>. Identify <span class="math inline">\(S\)</span>.</p>
<p>A probabilistic version: <span class="math inline">\(A_{S,i}\)</span> is <span class="math inline">\(N(0, \Si)\)</span> where <span class="math inline">\(\Si\)</span> is close to rank <span class="math inline">\(k\)</span>.</p>
<h2 id="approaches">Approaches</h2>
<h3 id="l1-vs.l2-norm">L1 vs. L2 norm</h3>
<p>Look at the <span class="math inline">\(L^1\)</span> norm and variance (<span class="math inline">\(L^2\)</span> norm) of each row.</p>
<ul>
<li>For the noise rows, we have (whp)
\begin{align}
A=\EE_{j=1}^n |A_{ij}| &amp; =2\iiy x e^{-\fc{x^2}2}\rc{\sqrt{2\pi}}\dx\\
B=\EE_{j=1}^n |A_{ij}|^2 - (\EE_{j=1}^n |A_{ij}|)^2 &amp;=1,
\end{align}
and the ratio <span class="math inline">\(\fc{\sqrt B}A\)</span> between these is concentrated at some constant (whp).</li>
<li><pre><code>For the non-noise rows,</code></pre>
\begin{align}
A=\EE_{j=1}^n |A_{ij}| &amp; &gt; \sfc{2}{\pi}\sum|\si_{k_j}|\\
B=\EE_{j=1}^n |A_{ij}|^2 - (\EE_{j=1}^n |A_{ij}|)^2 &amp;=\sum_i \si_i^2,
\end{align}
and <span class="math inline">\(\fc{\sqrt B}{A}&gt;\sfc{2}{\pi}\)</span>.</li>
</ul>
<p>But this is heavily dependent on the exact distribution of noise! We want something more generalizable.</p>
<h3 id="community-detectionsdp">Community detection/SDP</h3>
<p>Sometimes we can recover even if the noise seems to be larger than the signal, because the noise is uncorrelated. Ex. a random <span class="math inline">\(n\times n\)</span> matrix with <span class="math inline">\(N(0,1)\)</span> entries has eigenvalues on the order of <span class="math inline">\(O(\sqrt n)\)</span>.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> <!-- check this--></p>
<p>Reference: Synchronization problem, Amit Singer.</p>
<h2 id="baby-problem-recovering-clusters">Baby problem: Recovering clusters</h2>
<p>Problem: Let <span class="math inline">\(x\in \{\pm 1\}^n\)</span>. Given <span class="math inline">\(xx^T + E\)</span> where each entry of <span class="math inline">\(E\)</span> is <span class="math inline">\(N(0,\si^2)\)</span>, recover <span class="math inline">\(x\)</span> (whp). What level of noise <span class="math inline">\(\si\)</span> can we tolerate?</p>
<p>(We want to recover all of <span class="math inline">\(x\)</span>, rather than just something correlated with <span class="math inline">\(x\)</span>.)</p>
<h3 id="try-1-svd">Try 1: SVD</h3>
<p>The largest eigenvector of <span class="math inline">\(A=xx^T\)</span> is <span class="math inline">\(v_1(A) = x\)</span>. By Wedin’s Theorem, <span class="math display">\[ \sin\angle(v_1,v_1') \le \fc{\ve{E}_2}{|\la_1(A) - \la_2(A)|} = \fc{\ve{E}_2}{n} \stackrel?{\ll} \rc{\sqrt n}, \]</span> since the closest vector <span class="math inline">\(w\)</span> to <span class="math inline">\(v_1\)</span> is at an angle <span class="math inline">\(\te\)</span> away where <span class="math inline">\(\cos \te = \fc{n-2}n\)</span>, <span class="math inline">\(\sin \te \sim \rc{\sqrt n}\)</span>. So we need <span class="math inline">\(\ve{E}_2=\wt o(\sqrt n)\)</span><a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>.</p>
<p>To get <span class="math inline">\(\ve{E}_2=\wt o(\sqrt n)\)</span> we need <span class="math inline">\(\si=\wt o(1)\)</span>. Using this method we can only tolerate a level of noise <span class="math inline">\(\ll 1\)</span>.</p>
<h3 id="try-2-sdp">Try 2: SDP</h3>
<p>We want <span class="math inline">\(\min_{x\in \{\pm 1\}^n}\ve{xx^T-A}_F^2\)</span>. (Minimize the sum of squares because this is <span class="math inline">\(\propto\)</span> the log-likelihood.)</p>
<p>Relax this: <span class="math display">\[
\amin_{x\in \{\pm 1\}^n}\ve{xx^T-A}_F^2
=\amax_{x\in \{\pm 1\}^n}x_iA_{ij}x_j
=\amax_{x\in \{\pm 1\}^n} \Tr(xx^T)
=\amax_{B\succeq 0 , B_{ii}=1} \Tr(AB)
\]</span></p>
<p>WLOG the optimal solution is <span class="math inline">\(\one\)</span><a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a>. By SDP duality, <span class="math inline">\(\one\one^T\)</span> is the unique best solution if there exists <span class="math inline">\(Q\succeq 0\)</span>, <span class="math inline">\(\Tr(\one\one^T(Q-W))=0\)</span> where <span class="math inline">\(W=A+E\)</span> is observed.</p>
<p>Let’s consider when <span class="math inline">\(E\)</span> is symmetric.<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> We choose <span class="math inline">\(Q=\diag((\sum_{j}A_{ij})_i)\)</span>. Then <span class="math inline">\(M:=Q-W = \mattn{A_{12}+\cdots +A_{1n}}{-A_{12}}{\cdots}{-A_{21}}{A_{21}+A_{23}+\cdots}{\cdots}{\vdots}{\vdots}{\ddots}\)</span>.</p>
<p>We can think of this as a sum of random matrices <span class="math inline">\(\matt{1+\ep}{-(1+\ep)}{-(1+\ep)}{1+\ep}\)</span> in the submatrix at indices <span class="math inline">\(\{i,j\}\times \{i,j\}\)</span>. The average is <span class="math inline">\(\mattn{(n-1)}{-1}{\cdots}{-1}{n-1}{\cdots}{\ldots}{\ldots}{\ddots}\)</span> with eigenvalues <span class="math inline">\(n,\ldots, n,0\)</span>.</p>
<p>There is concentration up to <span class="math inline">\(\sqrt{n\ln n}\si\)</span>. We just need this to be <span class="math inline">\(&lt;n\)</span> in order to make the second smallest eigenvalue of <span class="math inline">\(M\)</span> be <span class="math inline">\(&gt;0\)</span>. (The smallest eigenvalue is 0.)</p>
<p>Thus we can tolerate noise up to <span class="math inline">\(\si = o\pa{\sfc{n}{\ln n}}\)</span>, much better!</p>
<p><strong>Next step: try to generalize this!</strong></p>
<h3 id="more-than-2-groups">More than 2 groups</h3>
<p>We want to recover the clustering from <span class="math inline">\(\mattn JOOOJOOOJ\)</span> plus noise.</p>
<p>Let <span class="math inline">\(x\in\R^{3n}\)</span> be the (proposed) clustering, where we think of <span class="math inline">\(x\in (\R^3)^n\)</span> and <span class="math inline">\(x_i=e_j\)</span> if <span class="math inline">\(i\)</span> is in the <span class="math inline">\(j\)</span>th cluster.</p>
<p>We want <span class="math inline">\(\min\sum_{i,j} (x_{i1}x_{j1}+x_{i2}x_{j2}+x_{i3}x_{j3} - a_{ij})^2\)</span> over valid <span class="math inline">\(x\)</span>’s. For valid <span class="math inline">\(x\)</span>’s, the quadratic term is 0 or 1, so we want <span class="math display">\[\amax_{x_{i1}^2+x_{i2}^2+x_{i3}^2=1, x_{i1}+x_{i2}+x_{i3}=1} \sum (a_{ij}-1)(x_{i1}x_{j1}+x_{i2}x_{j2}+x_{i3}x_{j3}).\]</span> We can write the expression in matrix form as <span class="math inline">\(\an{B,xx^T}\)</span> where <span class="math inline">\(B=(a_{ij}I)_{i,j}\)</span> (block matrix). For a SDP relaxation, replace <span class="math inline">\(xx^T\)</span> with <span class="math inline">\(M\)</span>, <span class="math inline">\(M\succeq 0\)</span>.</p>
<p>How does this relaxation do?</p>
<h3 id="clustering-1">Clustering</h3>
<p>In clustering we’re given <span class="math inline">\(v_1,\ldots, v_n\)</span> each of which is either <span class="math inline">\(v\)</span> or <span class="math inline">\(w\)</span> plus noise. We can find <span class="math inline">\(V^TV\)</span> and work off that matrix.</p>
<p>Look up literature on clustering and learning mixtures of Gaussians.</p>
<h2 id="svd-1">SVD</h2>
<p>See <a href="relevant_coordinates_svd.html">Relevant coordinates: low-rank matrix</a>.</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>What <span class="math inline">\(\ep\)</span>?<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p><span class="math inline">\(\Pj(\la_{\max} \ge t) \le 2ne^{-\fc{t^2}{2\si^2}}\)</span>. Here <span class="math inline">\(\si^2 = \ve{\sum E_{ij}^2}=\sqrt n\)</span>, get concentration to <span class="math inline">\(O(\sqrt n\ln n)\)</span>.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Write <span class="math inline">\(\wt o(f(n))\)</span> to mean <span class="math inline">\(o\pf{f(n)}{\poly\log(n)}\)</span>.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Why WLOG?<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>We can reduce to this case, I think.<a href="#fnref5">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Inference for topic models: Code</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/topic_code.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/topic_code.html</id>
    <published>2016-05-18T00:00:00Z</published>
    <updated>2016-05-18T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Inference for topic models: Code</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-05-18 
          , Modified: 2016-05-18 
	</p>
      
       <p>Tags: <a href="/tags/topic%20models.html">topic models</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <ul>
<li><code>gen_topic_vector</code>: Generate row vectors of size <span class="math inline">\(K\)</span> using Dirichlet (or Gaussian) <span class="math inline">\((\al,\ldots, \al)\)</span>.</li>
<li><code>gen_doc</code>: Generate topic vector, multiply by A</li>
<li><code>interleave</code></li>
<li><code>singlen1</code>: <span class="math inline">\(i\)</span>th standard basis vector</li>
<li><code>process_row_delta</code>: ?</li>
<li><code>process_row_C</code>: ?</li>
<li><code>new_compute_B</code>:</li>
<li><code>threshold</code>:</li>
<li><code>log_likelihood</code>:</li>
<li><code>fast_grad_log_likelihood</code>:</li>
<li><code>project</code></li>
<li><code>l1_distance</code></li>
<li><code>projected_ascent</code>: ?</li>
<li><code>top</code></li>
<li><code>top5</code></li>
<li><code>is_anchorword</code></li>
<li><code>near_anchorword</code></li>
<li><code>sparse_topic</code></li>
<li><code>superflat_topic</code></li>
<li><code>top_likelihood_support</code></li>
<li><code>test_inference</code></li>
<li><code>safe_mkdir</code></li>
<li><code>write_doc</code></li>
<li><code>write_flat</code></li>
<li><code>estimate_support</code></li>
<li><code>estimate_support_map</code></li>
<li><code>split_heldout</code></li>
<li><code>heldout_score</code></li>
<li><code>held_out_probability_ascent</code></li>
<li><code>nearest_neighbors</code></li>
<li><code>get_train_data</code></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[DKLPRS] Markov logic</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/probabilistic/markov_logic.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/probabilistic/markov_logic.html</id>
    <published>2016-05-17T00:00:00Z</published>
    <updated>2016-05-17T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[DKLPRS] Markov logic</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-05-17 
          , Modified: 2016-05-17 
	</p>
      
       <p>Tags: <a href="/tags/graphical%20model.html">graphical model</a>, <a href="/tags/probabilistic%20model.html">probabilistic model</a>, <a href="/tags/logic.html">logic</a>, <a href="/tags/FOL.html">FOL</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#markov-logic-questions">Markov logic: Questions</a><ul>
 <li><a href="#structure-learning-how-do-we-even-find-the-formulae">6.3 Structure learning: how do we even find the formulae?</a></li>
 </ul></li>
 <li><a href="#ksj-a-general-stochastic-approach-to-solving-problems-with-hard-and-soft-constraints">[KSJ] A general stochastic approach to solving problems with hard and soft constraints</a></li>
 <li><a href="#scraps">Scraps</a></li>
 <li><a href="#directions">Directions</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="markov-logic-questions">Markov logic: Questions</h2>
<ul>
<li>Consider a statement like Moral(x) <span class="math inline">\(\implies\)</span> Happy(x). Consider 2 worlds: one where most people are moral, and all but 5 moral people are happy, another where only 5 people are moral, and they are all unhappy. It seems in the Markov model they would have the same weight, when the statement is represented as <span class="math inline">\(\neg\text{Moral}(x) \wedge \text{Happy}(x)\)</span>. Do we want this? I’m guessing this is easier than only restricting to moral people, because that involves dividing.</li>
<li>I don’t understand the terminology used in LazySAT. My understanding of <code>active_atoms</code> is that we take every clause that is not satisfied and add ALL of its atoms. (Ex. if we have a formula <span class="math inline">\(Friend(A,B)\implies Happy(A) \wedge Happy(B)\)</span>, and <span class="math inline">\(\text{Friend}(x,y) \wedge \neg \text{Happy}(x)\wedge \text{Happy}(y)\)</span>, then both <span class="math inline">\(x,y\)</span> are considered active atoms.)
<ul>
<li>But then choosing unsatisfied <span class="math inline">\(x\)</span> and <span class="math inline">\(v_f\sim c\)</span>, it’salways true that <span class="math inline">\(v\in\)</span> active atoms, whereas in Algorithm 2, they consider when <span class="math inline">\(v\nin\)</span> active atoms!</li>
<li>Note DeltaCost can be easily calculated. Local flipping doesn’t require recalculation of the whole cost.</li>
<li>Shouldn’t DeltaCost be a function of a setting of variables, not a variable? Or is it like the derivative in the direction of variable <span class="math inline">\(v\)</span>, i.e., what you’d get by flipping <span class="math inline">\(v\)</span>? But you may need to flip two variables in a clause at the same time to get something better!</li>
</ul></li>
<li>I don’t understand the explanation on p. 8: The initial active atoms are all those appearing in clauses that are unsatisfied if only the atoms in the database are true, and all others are false.</li>
<li>More fundamentally, I don’t understand what the database is!</li>
<li>At each step, take a random unsatisfied clause. Maintain a set of active atoms.
<ul>
<li>With probability <span class="math inline">\(p\)</span> choose a random variable from the clause.</li>
<li>Otherwise, choose the non-active variable <span class="math inline">\(v\)</span> in <span class="math inline">\(c\)</span> is lowest DeltaCost. (Why do we ignore active atoms? Isn’t it true that flipping them can also decrease the cost?)</li>
<li>Add the chosen variable <span class="math inline">\(v_f\)</span> to <code>active_atoms</code> and add activated clauses. Flip <span class="math inline">\(v_f\)</span>.</li>
<li>(Do we not want variables to be flipped twice?)</li>
<li>NO: we choose the variable (active OR non-acive) in <span class="math inline">\(c\)</span> with lowest DeltaCost. It’s merely saying that we have to do a more involved computation if the variable is non-active. (Why? This has something to do with the DB vs. KB, no comprendo.)</li>
<li>At the beginning, only include active atoms/clauses. Think of these as the dangerous atoms—flipping them could make a clause not satisfied. These clauses are the only clauses we will need to worry about. Add clauses to the active set only if we need to worry about them.</li>
</ul></li>
<li>So what’s the difference between SAT and LazySAT? I think it’s just: are you summing over all possible instantiations (unnecessary!), or just looking at the differences when you flip a variable? You can ignore formulas that don’t invove the current variable.</li>
</ul>
<p>(OK to have some kind of “complexity measure” of formulae saying how amenable they are to random-walk methods?)</p>
<ul>
<li>Intuition about deterministic vs. non-deterministic dependencies. (Tradeoff between search and following stochasticity cf. gradient? The smaller the weights the easier the flow but the lower the probability is to find a satisfying instance?)</li>
<li>MC-SAT has hard satisfiability. Look at all clauses satisfied in the last step; the larger the weight, the more likely you are to force it to be satisfied next step. It calls on a uniform SAT solver (which doesn’t really exist—but there are things that do well in practice).</li>
<li>p. 12. Indistinguishable objects. What if you’re just strying to find a distribution that’s close, ex. for at most binary relations, look at the proportion of <span class="math inline">\(\{a,b\}\)</span> with values of <span class="math inline">\(u_1(a),\ldots, u_1(b),\ldots, b_1(a,b),\ldots\)</span> where <span class="math inline">\(u\)</span> are unary and <span class="math inline">\(b\)</span> are binary relations.</li>
</ul>
<p>Perhaps I need to learn about databases.</p>
<ul>
<li>What is the closed-world assumption?</li>
</ul>
<p>Wait, you aren’t flipping nodes! I thought of the nodes as the actors, and unary functions on them. But are the nodes supposed to be formulae? Like <span class="math inline">\(u_1(x)\)</span>, <span class="math inline">\(b_1(x,y)\)</span>, and everything not in it is false?</p>
<p>An atom is not a node but predicates applied to nodes!</p>
<p>(Note the logical contradictions is not on the level of node values, but on the formulas relating them? Don’t have <span class="math inline">\(A\wedge B\)</span> as nodes, only have <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>.) Atoms are formulas without connectives like <span class="math inline">\(\wedge, \implies,\forall...\)</span></p>
<blockquote>
<p>basic inference problem in first-order logic is to determine whether a knowledge base KB entails a formula F, i.e., if F is true in all worlds where KB is true</p>
</blockquote>
<p>Sparsity in the sense that only the relations in the database can be true; everything else has to be false?</p>
<p>Also, I don’t understand how to deal with intermediate functions like <span class="math inline">\(f(x,g(y))\)</span>. It seems there’s too many possible values of <span class="math inline">\(g\)</span>!</p>
<p>Approaches to get past intractability:</p>
<ul>
<li>The <strong>pseudo-likelihood</strong> is <span class="math inline">\(\ln \Pj_w^*(X=x) = \sumo ln \log \Pj_w(X_l=x_l|MB_x(X_l))\)</span> (markov blanket). (When is this a good approx?)</li>
<li><strong>Discriminative weight learning</strong>. Partition into evidence and query atoms and calculate conditional log-likelihood. (cf. RBM)</li>
</ul>
<h3 id="structure-learning-how-do-we-even-find-the-formulae">6.3 Structure learning: how do we even find the formulae?</h3>
<ul>
<li>Inductive logic programming #lookup. But use evaluation function based on pseudo-likelihood rather than accuracy and coverage.</li>
</ul>
<p>19 Dependencies are really probabilistic. Ex. two titles sharing a word are more likely to be the same?</p>
<p>Applications.</p>
<ul>
<li>Entity resolution (when do things refer to the same object? Include formulae like transitivity of equality.)</li>
<li>Citation resolution. (Ex. which article corresponds to which citation, when are they the same? You can imagine there’s some general rules which would do well.)</li>
</ul>
<p>Lifted inference, lifted resolution in FOL #lookup</p>
<h2 id="ksj-a-general-stochastic-approach-to-solving-problems-with-hard-and-soft-constraints">[KSJ] A general stochastic approach to solving problems with hard and soft constraints</h2>
<ul>
<li>My understanding of “partitioning”: solve a problem hierarchically: break into smaller pieces, find a list of good solutions of each piece, and then figure out how to combine those solutions.</li>
<li>Q: is there an algorithm agnostic to the encoding in some sense? (Doubtful—probably can embed crypto problems? But what if you only allow the simplest transformations?)</li>
<li>WalkSAT <em>does</em> only flip one variable at a time.</li>
<li>Hard constraints have greater weight than soft constraints.</li>
</ul>
<p>I think people do MCMC in problem-solving. Try to tweak things locally. Sometimes though there’s a larger restructuring—view it as a step in a more abstract space, a more abstract transformation?</p>
<h2 id="scraps">Scraps</h2>
<p>(Patterns you construct are patterns you could have found?)</p>
<p>Goal in learning should be to find the formula with weights which decrease the entropy the most?</p>
<p>SAT refutations corresponding to reasonable mathematical statements have short proofs but not the SAT framework? (Any way to compile down?)</p>
<h2 id="directions">Directions</h2>
<ul>
<li>First, understand the classical results on learning and inference in graphical models.</li>
<li>Understand MCMC and variational inference, and the situations where they have provable guarantees.</li>
<li>Try to formulate the simplest problem where you do have some element of first-order logic (ex. a statement with <span class="math inline">\(\forall\)</span>) and/or some combinatorial complexity.</li>
<li>What’s a test problem in this domain? They give citation resolution. Maybe this is practical but it seems like a rather boring problem!</li>
<li>Understand some ideas from the logic-learning community (inductive logic programming, etc.) <a href="https://en.wikipedia.org/wiki/Inductive_logic_programming">ILP</a>. Machine learning <span class="math inline">\(\cap\)</span> programming languages???</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
