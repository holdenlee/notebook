<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2017-01-10T00:00:00Z</updated>
    <entry>
    <title>Linear convex regulator 2</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/lcr2.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/lcr2.html</id>
    <published>2017-01-10T00:00:00Z</published>
    <updated>2017-01-10T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Linear convex regulator 2</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-01-10 
          , Modified: 2017-01-10 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#convex-optimization">Convex optimization</a><ul>
 <li><a href="#full-information">Full information</a></li>
 <li><a href="#see-only-costs-of-sampled-trajectories">See only costs of sampled trajectories</a></li>
 <li><a href="#notes">Notes</a></li>
 </ul></li>
 <li><a href="#noisy-dynamics">Noisy dynamics</a></li>
 <li><a href="#references">References</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="lcr.html">Part 1</a></p>
<p>Fix <span class="math inline">\(x_1\)</span>. Let the cost function (jointly convex in <span class="math inline">\(x,a\)</span>) be <span class="math inline">\(c(x,a)\)</span> (for example, <span class="math inline">\(g(x) + \ve{a}^2\)</span>) the actions be <span class="math inline">\(a_1,a_2,...\)</span> and the recurrence describing the dynamics be <span class="math inline">\(x_{n+1} =Ax_n+Ba_n\)</span>, we want</p>
<span class="math display">\[\begin{align}
&amp;\quad
\min_{a_1} [c(x_1,a_1) + \min_{a_2} [c(Ax_1+Ba_1,a_2) + ...]] \\
&amp;=
\min_{a_1,...,a_T} c(x_1,a_1) + c(Ax_1+Ba_1,a_2) + ...
\end{align}\]</span>
<p>This objective is jointly convex because we are assuming <span class="math inline">\(c\)</span> is convex, and a convex function composed with a linear function is convex. Letting <span class="math inline">\(f_a(x) = Ax + Ba\)</span> be the function describing the dynamics and <span class="math inline">\(f_{a_1\cdots a_n} (x) = f_{a_n}\circ \cdots \circ f_{a_1}(x)\)</span>, we can write this as <span class="math display">\[
\min_{a_1,...,a_T} \ub{c(x_1,a_1) + c(f_{a_1}(x_1),a_2) + c(f_{a_1a_2} (x_1),a_3)+\cdots}{F(a_1,\ldots, a_T)}
\]</span></p>
<h2 id="convex-optimization">Convex optimization</h2>
<p>What guarantees do we get from convex optimization? We can consider different settings:</p>
<ul>
<li>We know the function <span class="math inline">\(c\)</span> (full information) vs. we don’t know <span class="math inline">\(c\)</span>, only get access to <span class="math inline">\(c\)</span> through the trajectories that we sample, i.e., for each episode we choose a sequence of actions <span class="math inline">\((a_1,\ldots, a_T)\)</span> and observe <span class="math inline">\(c(x_1,a_1), c(f_{a_1}(x_1),a_2),\ldots\)</span>. (Then we can care about regret bounds, etc.)</li>
<li>If we don’t know <span class="math inline">\(c\)</span>: <span class="math inline">\(c\)</span> can be deterministic or stochastic (so we care about <span class="math inline">\(\min_{a_1,...,a_T} \E c(x_1,a_1) + c(f_{a_1}(x_1),a_2) + c(f_{a_1a_2} (x_1),a_3)+\cdots\)</span>). (Stochasticity not incorporated below, but easy to include.)</li>
</ul>
<h3 id="full-information">Full information</h3>
<p>This is just gradient descent. The dimension is <span class="math inline">\(Tn\)</span>, the gradient is a sum of gradients of <span class="math inline">\(c\)</span>’s, see below. If <span class="math inline">\(c\)</span> is stochastic, then this is SGD.</p>
<h3 id="see-only-costs-of-sampled-trajectories">See only costs of sampled trajectories</h3>
<p><strong>Theorem 6.6 in OCO</strong>. Online gradient descent without a gradient attains regret bound <span class="math display">\[
\E R_T \le 9nDGT^{\fc 34}
\]</span> where</p>
<ul>
<li><span class="math inline">\(n\)</span> is the dimension.</li>
<li><span class="math inline">\(D=\diam K\)</span>, where <span class="math inline">\(K\)</span> is the convex set we’re optimizing over.</li>
<li><span class="math inline">\(G=\sup_{x\in K} \ve{\nb f(x)}\)</span>.</li>
<li><span class="math inline">\(T\)</span> is the number of steps.</li>
</ul>
<p>Applying this here, we have</p>
<ul>
<li>The dimension is <span class="math inline">\(Tn\)</span>.</li>
<li>Suppose each action is in <span class="math inline">\(K\)</span>. Then <span class="math inline">\(D=\sqrt n \diam(B)\)</span>.</li>
<li>Suppose <span class="math inline">\(f_a(x)\)</span> is a function <span class="math inline">\(K\times C \to C\)</span> and <span class="math inline">\(x_1\in C\)</span>. Let
<span class="math display">\[\begin{align}
m &amp;= \max_{x\in C, a\in K^T}\nb_x c(x,a)\\
L &amp;= \max_{x\in C, a\in K^T}\nb_a c(x,a).
\end{align}\]</span>
Let <span class="math inline">\(\ga\)</span> be the max eigenvalue of <span class="math inline">\(B\)</span>. Then
<span class="math display">\[\begin{align}
\ve{\nb_{a_i}F} &amp;= \ve{
\nb_{a_i} c(f_{a_{1:i-1}}(x_1),a_i)
+\nb_{a_i} c(f_{a_{1:i}}(x_1), a_{i+1})
+\cdots \nb_{a_i} c(f_{a_{1:T-1}}(x_1),a_T)}\\
&amp;\le L + \be (1+\ga +\cdots + \ga^{T-1-i})m\\
&amp;\le L + \fc{\be}{1-\ga}m\\
\ve{\nb_{a_{1:T}}F} &amp; \le \sqrt{T} (L + \fc{\be}{1-\ga}m).
\end{align}\]</span></li>
</ul>
<h3 id="notes">Notes</h3>
<ul>
<li>This does not use any kind of function approximation, so the optimization only gives us information about the optimal action at <span class="math inline">\(x_1\)</span>. <strong>If we choose a different starting point, we have to run the optimization procedure with that new starting point.</strong></li>
<li><strong>We ignore the structure of each point <span class="math inline">\(x\)</span> in the space being a tuple of actions</strong> <span class="math inline">\((a_1,\ldots, a_T)\)</span>. Is there a way to use the structure of the cost function (as a sum of costs) to get better complexity?</li>
<li>Complexity scales as lookahead time <span class="math inline">\(T\)</span>. We can do better by realizing that later actions are less important - so e.g. to estimate the gradient we can do ellipsoid sampling instead of sphere sampling.</li>
<li>This assumes we know the dynamics. Otherwise we would have to learn them (cf. <a href="../optimization/HMR16.html">HMR16</a>). (This is only interesting if there is noise in the observation.)</li>
</ul>
<h2 id="noisy-dynamics">Noisy dynamics</h2>
<p>The problem changes when there is noise in the dynamics. <span class="math display">\[
x_{n+1} = Ax_n+Ba_n + \ep_n.
\]</span></p>
<ul>
<li>The convex optimization problem solves for the best actions if all actions <span class="math inline">\(a_{1:T}\)</span> have to be chosen ahead of time - you don’t get to choose your action <span class="math inline">\(a_n\)</span> online after observing <span class="math inline">\(x_n\)</span>. This may be very suboptimal.
<ul>
<li>For general MDP’s, there can be a significant gap between what you can achieve with and without an online strategy (not sure how significant the gap is for convex <span class="math inline">\(c\)</span>, but I expect it will still be there).</li>
</ul></li>
<li>(If the noise is nice, ex. Gaussian, then because we are basically convolving with the noise, convexity is preserved.)</li>
<li>If we want an online strategy (which makes much more sense), we can’t unroll like this and get a convex optimization problem. <strong>What to do?</strong></li>
</ul>
<h2 id="references">References</h2>
<p>Might be useful.</p>
<ul>
<li>Kernel methods solve bandit convex optimization: [BEL16] Kernel-based methods for bandit convex optimization (Can something like this work in the RL setting?)</li>
<li>Relationship between sampling and interior point methods: [AH15] Faster Convex Optimization - Simulated Annealing with an Efficient Universal Barrier</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Arrows</title>
    <link href="http://holdenlee.github.io/notebook/posts/cs/functional_programming/arrows.html" />
    <id>http://holdenlee.github.io/notebook/posts/cs/functional_programming/arrows.html</id>
    <published>2016-12-31T00:00:00Z</published>
    <updated>2016-12-31T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Arrows</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-12-31 
          , Modified: 2016-12-31 
	</p>
      
       <p>Tags: <a href="/tags/haskell.html">haskell</a>, <a href="/tags/functional%20programming.html">functional programming</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p>References</p>
<ul>
<li>Wikibooks
<ul>
<li><a href="https://en.m.wikibooks.org/wiki/Haskell/Understanding_arrows">Understanding</a></li>
<li><a href="https://en.m.wikibooks.org/wiki/Haskell/Arrow_tutorial">Tutorial</a></li>
</ul></li>
<li><a href="http://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#arrow-notation">GHC</a></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Transfer learning</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/transfer.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/transfer.html</id>
    <published>2016-12-29T00:00:00Z</published>
    <updated>2016-12-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Transfer learning</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-12-29 
          , Modified: 2016-12-29 
	</p>
      
       <p>Tags: <a href="/tags/transfer%20learning.html">transfer learning</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#changes-at-test-time">Changes at test time</a></li>
 <li><a href="#domain-adaptationmulti-task-learning">Domain adaptation/multi-task learning</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="changes-at-test-time">Changes at test time</h2>
<p>Ex. Google Flu Trends</p>
<p>Training <span class="math inline">\(x\sim p^*\)</span>, test <span class="math inline">\(x\sim q^*\)</span>.</p>
<p>Instance weighting: upweight examples underrepresented at test time <span class="math display">\[
\wh L(\te) = \rc n \sumo in \wh w(x) \ell((x,y);\te).
\]</span></p>
<p>Problems: have to estimate <span class="math inline">\(\wh w\)</span>, have to assume <span class="math inline">\(q^*\)</span> is absolutely continuous wrt <span class="math inline">\(p^*\)</span>.</p>
<h2 id="domain-adaptationmulti-task-learning">Domain adaptation/multi-task learning</h2>
<p>Even <span class="math inline">\(p^*(y|x)\)</span> can be different.</p>
<p>Solve joint ERM problem assuming weight vectors are close.</p>
<p>Regularize by e.g. <span class="math inline">\(\sum_{i\ne j}\ve{w_i-w_j}^2\)</span>, <span class="math inline">\(\ve{W}_*\)</span>, or <span class="math inline">\(\ve{W}_{2,1}\)</span> (sum of <span class="math inline">\(L^2\)</span> norms of rows - for sparse set of features).</p>
<p>NN: share same hidden layer.</p>
<p>Deep NN non-robust: perturbation <span class="math display">\[
\min_{r\in \R^d} (f(x+r) - y_{\text{wrong}})^2
+ \la\ve{r}^2.
\]</span></p>
<p>Robust optimization: <span class="math inline">\(K\)</span> features zeroed out at test time.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Kernels</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/kernels.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/kernels.html</id>
    <published>2016-12-29T00:00:00Z</published>
    <updated>2016-12-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Kernels</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-12-29 
          , Modified: 2016-12-29 
	</p>
      
       <p>Tags: <a href="/tags/kernels.html">kernels</a>, <a href="/tags/RKHS.html">RKHS</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#kernels">Kernels</a></li>
 <li><a href="#equivalences">Equivalences</a></li>
 <li><a href="#rkhs">RKHS</a></li>
 <li><a href="#learning">Learning</a></li>
 <li><a href="#fourier-properties">Fourier properties</a></li>
 <li><a href="#efficient-computation">Efficient computation</a><ul>
 <li><a href="#random-features">Random features</a></li>
 <li><a href="#nystrom">Nystrom</a></li>
 </ul></li>
 <li><a href="#universality">Universality</a></li>
 <li><a href="#rkhs-embedding-of-probability-distributions">RKHS embedding of probability distributions</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>Mostly from Percy Liang’s 229T notes.</p>
<h2 id="kernels">Kernels</h2>
<p>If <span class="math inline">\(w=\sumo in \al_i \phi(x^{(i)})\)</span> then <span class="math inline">\(\an{w,\phi(x)} = \sumo in \al_i \an{\phi(x^{(i)}), \phi(x)}\)</span>.</p>
<h2 id="equivalences">Equivalences</h2>
<p>The following are equivalent (although multiple feature maps can correspond to the same kernel/RKHS):</p>
<ul>
<li>Feature map <span class="math inline">\(\phi:X\to H\)</span>.</li>
<li>Symmetric, positive (semi)definite kernel <span class="math inline">\(k:X\times X\to \R\)</span>. (for all <span class="math inline">\(S\)</span>, <span class="math inline">\((k(x_i,x_j))_{i,j\in S}\)</span> is PSD.)</li>
<li>RKHS <span class="math inline">\(\{f:X\to \R\}, \ved_H\)</span>.</li>
</ul>
<p>Proof:</p>
<ul>
<li>Feature map to kernel: <span class="math inline">\(k(x,x') = \an{\phi(x), \phi(x')}\)</span>.</li>
<li>RKHS to kernel: Riesz representation (see below). <span class="math inline">\(L_x(f) = \an{R_x,f}\)</span>.</li>
<li>Kernel to RKHS: Moore-Aronszajn. <span class="math inline">\(f=\sumo in \al_i k(x_i,x)\)</span>, <span class="math inline">\(\an{f,g} - \sumo in \sumo j{n'} \al_i\be_j k(x_i,x_j')\)</span>. (See below.)</li>
</ul>
<h2 id="rkhs">RKHS</h2>
<p>A <a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space">reproducing kernel Hilbert space (RKHS)</a> is a Hilbert space of functions <span class="math inline">\(X\to \R\)</span> in which point evaluation <span class="math inline">\(L_x[f] = f(x)\)</span> is a continuous linear functional. (Equivalently, it is bounded: <span class="math inline">\(f(x)\le M_x\ve{f}_H\)</span>.)</p>
<ul>
<li><span class="math inline">\(L^2\)</span> is not a RKHS—it consists of equivalence classes of functions.</li>
<li><span class="math inline">\(H=\set{f\in L_2(\R)}{\Supp(\wh f)\subeq [-a,a]}\)</span> is RKHS. Here <span class="math inline">\(K_x(y) = \fc{\sin(a(y-x))}{\pi(y-x)}\)</span> (bandlimited Dirac delta).</li>
</ul>
<p><strong>Theorem</strong> A Hilbert space of functions is a RKHS iff for every <span class="math inline">\(x\in X\)</span>, there exists <span class="math inline">\(K_x\)</span> such that <span class="math inline">\(g(x) = \an{g, K_x}\)</span>.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<p><em>Proof</em>. <span class="math inline">\(\Leftarrow\)</span>: inner product is continuous. <span class="math inline">\(\Rightarrow\)</span>: Riesz representation.</p>
<p>Note <span class="math inline">\(K_x(y) = \an{K_x,K_y}=:K(x,y)\)</span>. <span class="math inline">\(K\)</span> is symmetric and positive definite. (Think of <span class="math inline">\(K\)</span> as <span class="math inline">\(X_0^\R\times X_0^\R\to \R\)</span> where <span class="math inline">\(X_0^\R\)</span> is the set of functions <span class="math inline">\(X\to \R\)</span> nonzero only at a finite number of <span class="math inline">\(x\)</span>’s. I.e. formal span of <span class="math inline">\(X\)</span>.)</p>
<p><strong>Theorem (Moore-Aronszajn)</strong>. Suppose <span class="math inline">\(K\)</span> is a symmetric, positive definite kernel on a set <span class="math inline">\(X\)</span>. Then there is a unique Hilbert space of functions on <span class="math inline">\(X\)</span> for which <span class="math inline">\(K\)</span> is a reproducing kernel.</p>
<p><em>Proof</em>. Extend <span class="math inline">\(K\)</span> by bilinearity to <span class="math inline">\(\spn(X)\)</span>. Take the completion. (Extend to functions <span class="math inline">\(f=\sumo i{\iy} a_i K_{x_i}(x)\)</span> where <span class="math inline">\(\sumo i{\iy} a_i^2 K(x_i,x_i)\)</span>.)</p>
<p>(Question: how to realize this inner product as an integral?)</p>
<p>See also: Mercer’s theorem</p>
<h2 id="learning">Learning</h2>
<p><a href="https://en.wikipedia.org/wiki/Representer_theorem">Representer theorem</a> states that every function in an RKHS that minimises an empirical risk function can be written as a linear combination of the kernel function evaluated at the training points.</p>
<p>If <span class="math display">\[
f^*\in \amin_{f\in H} L(\{(x_i,y_i,f(x_i))\}) + Q(\ve{f}_H^2)
\]</span> where <span class="math inline">\(Q:[0,\iy)\to \R\)</span> strictly increasing is a regularizer, then <span class="math display">\[
f^*\in \spn(\set{k(x_i, \cdot)}{i\in [n]}).
\]</span></p>
<p>Example: kernel ridge regression <span class="math inline">\(\al = (K+\la I)^{-1}Y\)</span>.</p>
<h2 id="fourier-properties">Fourier properties</h2>
<p><span class="math inline">\(k:X\times X\to \R\)</span>, <span class="math inline">\(X\subeq \R^b\)</span> is shift-invariant if <span class="math inline">\(k(x,x') = h(x-x')\)</span>. Let <span class="math inline">\(t=x-x'\)</span>.</p>
<p><strong>Bochner’s theorem</strong>. Let <span class="math inline">\(k(x,x') = h(x-x')\)</span> be a continuous shift-invariant kernel (<span class="math inline">\(x\in \R^b\)</span>). There exists a unique finite <em>nonnegative</em> measure <span class="math inline">\(\mu\)</span> (spectral measure) such that <span class="math display">\[
h(t) = \int e^{-i\an{t,\om}}\mu(d\om).
\]</span> (<span class="math inline">\(h\)</span> is the characteristic function of <span class="math inline">\(\mu\)</span>. For <span class="math inline">\(\mu(d\om)=s(\om)d\om\)</span>, call <span class="math inline">\(s\)</span> the spectral density.) (See probability notes.)</p>
<h2 id="efficient-computation">Efficient computation</h2>
<ul>
<li>Random features: We will write the kernel function as an integral, and using Monte Carlo approximations of this integral. These approximations are of the kernel function and are data-independent.</li>
<li>Nystrom method: We will sample a subset of the n points and use these points to approximate the kernel matrix. These approximations are of the kernel matrix and are data-dependent.</li>
</ul>
<p>Ex. when points are clustered into <span class="math inline">\(m\)</span> clusters, kernel matrix looks like block diagonal with <span class="math inline">\(m\)</span> blocks <span class="math inline">\(J\)</span>, so rank <span class="math inline">\(m\)</span> approximation is effective.</p>
<h3 id="random-features">Random features</h3>
<p>Draw <span class="math inline">\(\om_i\sim \mu\)</span>, and estimate <span class="math display">\[
\wh k(x,x') = \rc m \sumo im \phi_{\om_i}(x)\ol{\phi_{\om_i}(x)}.
\]</span></p>
<strong>Theorem</strong>. Let
<span class="math display">\[\begin{align}
\mathcal F &amp;= \set{x\mapsto \int \al(\om)\phi_\om(x)\mu(d\om)}{\forall \om, |\al(\om)|\le C}\\
\wh{\mathcal F} &amp;= \set{x\mapsto \rc m\sumo im \al(\om_i)\phi_{\om_i}(x)\mu(d\om)}{\forall \om, |\al(\om)|\le C}
\end{align}\]</span>
<p>where <span class="math inline">\(\om_i\sim \mu\)</span>. Let <span class="math inline">\(\an{f,g}=\EE_{x\sim p^*}[fg]\)</span>. Then <span class="math display">\[
\Pj\pa{
\exists \wh f\in \wh{\mathcal F},
\ve{\wh f- f^*}\le \fc{C}{\sqrt m} 
\pa{1+\sqrt{2\ln(1/\de)}}
}.
\]</span></p>
<p><em>Proof</em>. McDiarmid and Jensen. (p. 133-5, omitted.)</p>
<p>(Q: what about generalization guarantee? See p. 134)</p>
<p>For other kernels, note <span class="math display">\[\an{x,x'}=\E[\an{\om, x}\an{\om,x'}].\]</span> For <span class="math inline">\(\an{x,x'}^p\)</span>, take <span class="math inline">\(p\)</span> independent draws and multiply together. For general <span class="math inline">\(f\)</span>, expand in Taylor series.</p>
<p>Comparison to neural nets: “the random features view of kernels defines a function class where <span class="math inline">\(\om_{1:m}\)</span> is chosen randomly rather than optimized, while <span class="math inline">\(\alpha_{1:m}\)</span> are optimized. The fact that random features approximates a kernel suggests that even the random initialization is quite sensible starting point.”</p>
For <span class="math inline">\(\phi_\om(x) = \one_{\om^Tx\ge 0}(\om^Tx)^q\)</span>, as <span class="math inline">\(m\to \iy\)</span>, get the kernel
<span class="math display">\[\begin{align}
k(x,x') &amp;= 2\int\phi_\om(x)\phi_\om(x')p(\om)d\om\\
&amp;= \rc\pi \ve{x}^q\ve{x'}^q J_q\ub{\pf{\cos^{-1}\pf{x^Tx'}}{\ve{x}\ve{x'}}}{\te}\\
J_0(\te) &amp;= \pi - \te\\
J_1(\te) &amp;= \sin\te + (\pi - \te)\cos\te.
\end{align}\]</span>
<p>(Bessel function?) Decouples magnitude from angle. (Cho/Saul 2009)</p>
<h3 id="nystrom">Nystrom</h3>
<p>$$</p>
<p><span class="math display">\[K \approx \matt{K_{II}}{K_{IJ}}{K_{JI}}{K_{JI}K_{II}^{+}K_{IJ}}.\]</span> <span class="math inline">\(K_{JJ} - K_{JI}K_{II}^{+}K_{IJ}\)</span> is the Schur complement.</p>
<p>Drineas/Mahoney: Choose <span class="math inline">\(I\)</span> by sampling <span class="math inline">\(i\)</span> with probability <span class="math inline">\(\fc{K_{ii}}{\sumo jn K_{jj}}\)</span>. (p. 138)</p>
<h2 id="universality">Universality</h2>
<p>Let <span class="math inline">\(X\)</span> be locally compact Hausdorff. <span class="math inline">\(k\)</span> is a <span class="math inline">\(c_0\)</span>-universal kernel if <span class="math inline">\(H\)</span> with reproducing kernel <span class="math inline">\(k\)</span> is dense in <span class="math inline">\(C_0(X)\)</span> (continuous bounded functions wrt uniform (<span class="math inline">\(L^{\iy}\)</span>) norm).</p>
<p>Carmeli2010: Let <span class="math inline">\(k\)</span> be shift-invariant with spectral measure <span class="math inline">\(\mu\)</span>. If <span class="math inline">\(\Supp(\mu) = \R^b\)</span>, then <span class="math inline">\(k\)</span> is universal.</p>
<h2 id="rkhs-embedding-of-probability-distributions">RKHS embedding of probability distributions</h2>
<p>Kernels can be used to represent and answer questions about probability distributions without having to explicitly estimate them.</p>
<p><strong>Maximum mean discrepancy</strong> <span class="math display">\[
D(P,Q,F) := \sup_{f\in\mathcal F}
\pa{
\EE_{x\sim P}[f(x)] - \EE_{x\sim Q}[f(x)]
}.
\]</span></p>
<p>If <span class="math inline">\(\mathcal F=C_0(X)\)</span>, then <span class="math inline">\(D(P,Q,\mathcal F)=0\implies P=Q\)</span>. Better: Can let <span class="math inline">\(\mathcal F = \set{f\in H}{\ve{f}_H\le 1}\)</span> where <span class="math inline">\(k\)</span> is universal. (Check proof p. 140)</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Letting <span class="math inline">\(f_x=\delta_x\)</span> is cheating—we want actual functions, not distributions.<a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>(R16) How to calculate partition functions using convex programming hierarchies - provable bounds for variational methods</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/probabilistic/R16.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/probabilistic/R16.html</id>
    <published>2016-12-28T00:00:00Z</published>
    <updated>2016-12-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>(R16) How to calculate partition functions using convex programming hierarchies - provable bounds for variational methods</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-12-28 
          , Modified: 2016-12-28 
	</p>
      
       <p>Tags: <a href="/tags/dynamical%20systems.html">dynamical systems</a>, <a href="/tags/quasi-convexity.html">quasi-convexity</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#introduction">Introduction</a></li>
 <li><a href="#definitions-and-setting">Definitions and setting</a></li>
 <li><a href="#result">Result</a></li>
 <li><a href="#method">Method</a><ul>
 <li><a href="#dense-graphs-sa">Dense graphs, SA</a></li>
 <li><a href="#low-threshold-rank-lasserre">Low threshold rank, Lasserre</a></li>
 </ul></li>
 <li><a href="#interpretation">Interpretation</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="introduction">Introduction</h2>
<p>Two approaches for calculating partition functions</p>
<ol type="1">
<li>Markov chains to sample
<ul>
<li>(Jerrum04, JerrumSinclair1993) Certain Markov chains mix rapidly; used to approximate permanent with nonnegative entries and partition function for ferromagnetic Ising.</li>
</ul></li>
<li>Variational methods: partition function is the solution of a (intractable) optimization problem over the polytope of valid distributions.
<ul>
<li>Popular, faster, easier to parallelize</li>
<li>Little theoretical understanding</li>
<li>Another algorithm: Belief propagation (solving non-convex relaxation)
<ul>
<li>Regime of correlation decay and locally tree-like graphs.</li>
</ul></li>
</ul></li>
</ol>
<p>Use Sherali-Adams and Lasserre convex programming hierarchies to get the first provable, convex variational methods. They work <em>because</em> local correlations propagate to global correlations, the opposite of correlation decay.</p>
<p>(See <a href="../optimization/hierarchies.html">hierarchies</a>.)</p>
<h2 id="definitions-and-setting">Definitions and setting</h2>
<p>Ising model <span class="math inline">\(p(x)\propto \exp\pa{\sum_{i,j} J_{i,j}x_ix_j}\)</span>, <span class="math inline">\(x\in \{-1,1\}^n\)</span> is <span class="math inline">\(\De\)</span>-dense if letting <span class="math inline">\(J_T=\sum_{i,j}|J_{i,j}|\)</span>, <span class="math display">\[
\forall i,j\in [n], \De |J_{i,j}|\le \fc{J_T}{n^2},
\]</span> (Ex. if the <span class="math inline">\(J_{i,j}\)</span> is 1 for an edge and 0 for a non-edge, and the graph has density <span class="math inline">\(cn^2\)</span>, then <span class="math inline">\(\De = \rc{c}\)</span>.)</p>
<p><span class="math inline">\(p\)</span> is <strong>regular</strong> if <span class="math inline">\(\sum_j |J_{i,j}|=J'\)</span>. The adjacency matrix is <span class="math inline">\(A_{i,j} = \fc{|J_{i,j}|}{J'}\)</span>. Let <span class="math inline">\(\rank_\tau(A)\)</span> be the number of eigenvalues with <span class="math inline">\(\ad\ge\tau\)</span>.</p>
<p>Why dense Ising models?</p>
<ul>
<li>There are PTAS for CSPs for dense constraint graphs.</li>
<li>Mean-field ferromagnetic Ising model: each spin interacts with every other spin.</li>
</ul>
<h2 id="result">Result</h2>
<ul>
<li>Algorithm based on SA hierarchy achieving additive approximation of <span class="math inline">\(\ep J_T\)</span> to <span class="math inline">\(\ln Z\)</span> in time <span class="math inline">\(n^{O\prc{\De \ep^2}}\)</span>.</li>
<li>Algorithm based on Lasserre hierarchy achieving additive approximation of <span class="math inline">\(\ep n J'\)</span> to <span class="math inline">\(\ln Z\)</span> in time <span class="math inline">\(n^{\rank_{\Om(\ep^2)}(A)/\Om(\ep^2)}\)</span>.</li>
</ul>
<h2 id="method">Method</h2>
<h3 id="dense-graphs-sa">Dense graphs, SA</h3>
<ul>
<li>(Variational characterization of <span class="math inline">\(\ln Z\)</span>) <span class="math inline">\(\ln Z = \max_{\mu\in M} \sum_{i\sim j} J_{i,j} \E_\mu [x_ix_j] + H(\mu)\)</span> where <span class="math inline">\(M\)</span> is the polytope of distributions over <span class="math inline">\(\{-1,1\}^n\)</span>. (Maximum achieved at <span class="math inline">\(\mu=p\)</span>.)
<ul>
<li><em>Proof</em>. The KL divergence to <span class="math inline">\(p\)</span> is <span class="math inline">\(\ge 0\)</span>.</li>
</ul></li>
<li>Relax to <span class="math inline">\(M'\supeq M\)</span>.</li>
<li>Need to design surrogates for <span class="math inline">\(H(\mu)\)</span>.
<ul>
<li>Popular choice: Bethe entropy. (But it is not an upper bound in general.)</li>
<li>Instead, design <span class="math inline">\(\wt H(\mu) \ge H(\mu)\)</span> whenever <span class="math inline">\(\mu\in M\)</span>, then round to distributions.</li>
</ul></li>
<li>Define the <strong>augmented mean-field pseudo-entropy functional</strong> <span class="math display">\[
H_{aMF,k}(\mu) = \min_{|S|\le k}H(\mu_S) + \sum_{i\nin S} H(\mu_i |\mu_S).
\]</span>
<ul>
<li>By the chain rule and “conditioning reduces entropy”, <span class="math inline">\(H(\mu)\le H_{aMF,k}(\mu)\)</span>.</li>
<li><span class="math inline">\(H(\mu_i|\mu_S)\)</span> is concave, so <span class="math inline">\(H_{aMF,k}(\mu)\)</span> is concave. (Proof omitted.)</li>
</ul></li>
<li>The relaxation is <span class="math display">\[
\max_{\mu\in SA(O(1/(\De \ep^2)))} \bc{
\sum_{i,j} J_{i,j} \E_\mu[x_ix_j] + H_{aMF,k}(\mu)
}
\]</span></li>
<li>Correlation rounding: pick a seed set, condition on it, and round other variables independently. (YoshidaZhou2014) There is a set of size <span class="math inline">\(k=O\prc{\De \ep^2}\)</span> such that <span class="math display">\[
\ab{\sum_{i,j} J_{i,j} \E_\mu[x_ix_j|x_S] - \sum_{i,j} J_{i,j} \E_\mu [x_i|x_S] \E_\mu[x_j|x_S]}\le \fc{100}{\De k}J_T.
\]</span></li>
<li>Letting <span class="math inline">\(\wt\mu(x) = \mu(x_S) \prod_{i\nin S} \mu(x_i|x_S)\)</span>, <span class="math inline">\(\E_{\wt\mu} [x_ix_j|x_S] = \E_\mu[x_i|x_S]\mu[x_j|x_S]\)</span>.Combine YZ14 with <span class="math inline">\(H\le H_{aMF,k}\)</span>.</li>
</ul>
<h3 id="low-threshold-rank-lasserre">Low threshold rank, Lasserre</h3>
<ul>
<li>The correlation rounding bound changes to: <span class="math inline">\(\exists |S|\le \rank_{\Om(\ep^2)}(A)\)</span>, <span class="math inline">\(...\le \ep J_T\)</span>. <!-- $$\max_{\mu\in LAS(k)} \ba{\sum_{i,j} \E_\mu [x_ix_j] + H_{aMF,k}(\mu)}.$$--></li>
</ul>
<h2 id="interpretation">Interpretation</h2>
<ul>
<li>High temperature <span class="math inline">\(|H| = O\prc{d}\)</span>
<ul>
<li>Dobrushin’s uniqueness criterion: correlation decay</li>
<li>MC methods work and give <span class="math inline">\((1+\ep)\)</span> factor approximation, stronger.</li>
</ul></li>
<li>Transition threshold <span class="math inline">\(|J|=\Te\prc{d}\)</span>.
<ul>
<li>MC provide no nontrivial guarantee.</li>
<li>We get <span class="math inline">\(\ep n\)</span> additive factor approximation.</li>
</ul></li>
<li>Low temperature <span class="math inline">\(|J|=\om\prc{d}\)</span>. :(</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Legendre transform</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/legendre_transform.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/legendre_transform.html</id>
    <published>2016-12-28T00:00:00Z</published>
    <updated>2016-12-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Legendre transform</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-12-28 
          , Modified: 2016-12-28 
	</p>
      
       <p>Tags: <a href="/tags/convexity.html">convexity</a>, <a href="/tags/duality.html">duality</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#definition">Definition</a></li>
 <li><a href="#appendix-von-neumann-minimax">Appendix: von Neumann minimax</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="https://en.wikipedia.org/wiki/Legendre_transformation">Wikipedia</a></p>
<p><a href="http://www.maths.manchester.ac.uk/~goran/legendre.pdf">Reference</a></p>
<h2 id="definition">Definition</h2>
<p>The <strong>Legendre transform</strong> (or conjugate dual) of a function <span class="math inline">\(f:X \to \R\)</span> is defined as<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> <span class="math display">\[
f^*(y) = \sup_{x\in \Om} [\an{y,x} - f(x)]
\]</span> <span class="math inline">\(f^*\)</span> is a function <span class="math inline">\(Y\to \R\)</span> where <span class="math inline">\(Y=\set{y}{\sup_{x\in X} [\an{y,x} - f(x)]&lt;\iy}\)</span>.</p>
<p><strong>Proposition</strong>. <span class="math inline">\(f^*\)</span> is convex.</p>
<p><em>Proof</em>. <span class="math inline">\(f^*\)</span> is a sup of linear (hence convex) functions.</p>
<p>Note that if <span class="math inline">\(f\)</span> is differentiable, the argmax satisfies <span class="math inline">\(f'(x) = y\)</span>.</p>
<p><strong>Theorem</strong>. If <span class="math inline">\(f\)</span> is convex with domain <span class="math inline">\(X\)</span> that is compact (and convex), then <span class="math inline">\(f=f^{**}\)</span>.</p>
<p>(Does this work without <span class="math inline">\(X\)</span> being compact?)</p>
<em>Proof</em>. We have <span class="math display">\[
f^{**}(x') = \sup_{y\in Y} \inf_{x\in X} \an{x',y}-\an{y,x} + f(x).
\]</span> We want this to <span class="math display">\[
=\inf_{x\in X} \sup_{y\in Y}  \an{x',y}-\an{y,x} + f(x).
\]</span> <span class="math inline">\(\le\)</span> is clear (it’s better to go second). We want to show <span class="math inline">\(\ge\)</span> using minimax. A technical point is that we have to restrict to compact sets by adding a penalty term. Take compact sets <span class="math inline">\(\bigcup K_i = \R^n\)</span>
<span class="math display">\[\begin{align}
&amp;= \lim_{i\to \iy}
\sup_{y\in K_i} \inf_{x\in X} \an{x',y}-\an{y,x} + f(x)\\
&amp; = \lim_{i\to \iy}
\inf_{x\in X} \sup_{y\in K_i} \an{x'-x,y} f(x)\\
&amp; = \lim_{i\to \iy} \inf_{x\in X} f(x) + (\sup_{y\in K_i} \an{x'-x,y}).
\end{align}\]</span>
<p>Thus first player would choose <span class="math inline">\(x=x'\)</span>, giving the value <span class="math inline">\(f(x')\)</span>.</p>
<h2 id="appendix-von-neumann-minimax">Appendix: von Neumann minimax</h2>
<p>Let <span class="math inline">\(K\subeq \R^n\)</span>, <span class="math inline">\(L\subeq \R^m\)</span> be compact and convex. Let <span class="math inline">\(f:K\times L\to \R\)</span> where <span class="math inline">\(f\)</span> is concave in <span class="math inline">\(x\)</span> and convex in <span class="math inline">\(y\)</span>. Then there exists <span class="math inline">\((x_*,y_*)\in K\times L\)</span> such that for all <span class="math inline">\(x\in K\)</span>, <span class="math inline">\(y\in L\)</span>, <span class="math display">\[
f(x,y^*) \le f(x_*,y_*) \le f(x_*,y).
\]</span></p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>This definition is suited for convex functions. The definition suited for concave functions is <span class="math display">\[f^(y) = \inf_{x\in \Om} [\an{y,x} - f(x)].\]</span><a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Hierarchies of convex relaxations</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/hierarchies.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/hierarchies.html</id>
    <published>2016-12-28T00:00:00Z</published>
    <updated>2016-12-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Hierarchies of convex relaxations</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-12-28 
          , Modified: 2016-12-28 
	</p>
      
       <p>Tags: <a href="/tags/convex%20relaxation.html">convex relaxation</a>, <a href="/tags/Sherali-Adams.html">Sherali-Adams</a>, <a href="/tags/Lasserre.html">Lasserre</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#sherali-adams">Sherali-Adams</a></li>
 <li><a href="#lasserre">Lasserre</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>See also <a href="../../complexity/sos.html">SoS</a>.</p>
<h2 id="sherali-adams">Sherali-Adams</h2>
<p>The <span class="math inline">\(k\)</span>-level Sherali-Adams hierarchy <span class="math inline">\(SA(k)\)</span> is a LP with variables <span class="math inline">\(\mu_S(x_S)\)</span>, <span class="math inline">\(x_S\in \{-1,1\}^{|S|}\)</span> specifying marginals over all <span class="math inline">\(S\subeq [n]\)</span>, <span class="math inline">\(|S|\le k\)</span>. <span class="math inline">\(\mu_S\)</span> satisfies consistency: <span class="math display">\[
\forall |S\cup T|\le k, 
\mu_S[\{x_{S\cap T}=\al\}] 
= \mu_T[\{x_{S\cap T} = \al\}].
\]</span></p>
<p>The conditioning operation - sampling according to <span class="math inline">\(\mu_{\{v\}}\)</span> - defines a map from solutions of the <span class="math inline">\(k\)</span>th level to solutions of the <span class="math inline">\(k-1\)</span>th level. <span class="math display">\[
\mu_S(x_S) = \mu_{S\cup \{v\}}(x_{S\cup v}).
\]</span></p>
<h2 id="lasserre">Lasserre</h2>
<p>LAS(<span class="math inline">\(k\)</span>). This is a SDP: <span class="math display">\[
\forall |S\cup T|\le k, \mu_{S\cup T}(x_S=\al, x_T=\be) = \an{v_{S,\al}, v_{T,\be}}.
\]</span></p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>GANs</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/GANs.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/GANs.html</id>
    <published>2016-12-28T00:00:00Z</published>
    <updated>2016-12-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>GANs</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-12-28 
          , Modified: 2016-12-28 
	</p>
      
       <p>Tags: <a href="/tags/neural%20nets.html">neural nets</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#gans">GANs</a><ul>
 <li><a href="#objective">Objective</a></li>
 <li><a href="#interpretation-of-objective">Interpretation of objective</a></li>
 <li><a href="#another-interpretation">Another interpretation</a></li>
 <li><a href="#training">Training</a></li>
 <li><a href="#notes">Notes</a></li>
 </ul></li>
 <li><a href="#improved-techniques.">Improved techniques.</a><ul>
 <li><a href="#semi-supervised-learning">Semi-supervised learning</a></li>
 </ul></li>
 <li><a href="#infogan">InfoGAN</a></li>
 <li><a href="#two-views-of-gans">Two views of GANs</a></li>
 <li><a href="#adversarial-training-with-maximum-mean-discrepancy">Adversarial training with maximum mean discrepancy</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>References</p>
<ul>
<li>Papers
<ul>
<li><a href="https://arxiv.org/pdf/1606.03657v1.pdf">InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</a></li>
<li><a href="https://arxiv.org/abs/1606.03498">Improved techniques for training GANs</a></li>
<li><a href="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf">Generative adversarial nets</a></li>
</ul></li>
<li>Blog posts
<ul>
<li><a href="http://www.inference.vc/my-summary-of-adversarial-training-nips-workshop/">New perspectives, NIPS2016</a> <a href="http://scrible.com/s/g5QQ6">h</a></li>
<li><a href="http://www.inference.vc/how-to-train-your-generative-models-why-generative-adversarial-networks-work-so-well-2/">How to train your generative models</a> <a href="http://scrible.com/s/i5AQ6">h</a></li>
<li><a href="http://www.inference.vc/infogan-variational-bound-on-mutual-information-twice/">InfoGAN</a> <a href="http://scrible.com/s/6BAQ6">h</a></li>
<li><a href="https://openai.com/blog/generative-models/">OpenAI</a></li>
<li><a href="http://www.yingzhenli.net/home/blog/?p=421">GANs, MI, and algorithm selection</a></li>
</ul></li>
</ul>
<h2 id="gans">GANs</h2>
<p>Ferenc: When the goal is to train a model that can generate natural-looking samples, maximum likelihood is not a desirable training objective. Under model misspecification and finite data (that is, in pretty much every practically interesting scenario), it has a tendency to produce models that overgeneralise.</p>
<h3 id="objective">Objective</h3>
<p>Train 2 neural networks <span class="math inline">\(D\)</span> and <span class="math inline">\(G\)</span>. <span class="math inline">\(P\)</span> generates real data, and <span class="math inline">\(G=\wh P\)</span> generates fake data. Suppose either <span class="math inline">\(P\)</span> or <span class="math inline">\(\wh P\)</span> is chosen with probability <span class="math inline">\(\rc2\)</span> and then one same <span class="math inline">\(x\)</span> is drawn. (I.e., consider <span class="math inline">\(\rc2 (P+\wh P)\)</span>.)</p>
<ul>
<li><span class="math inline">\(G\)</span> is generator: it tries to generate <span class="math inline">\(x\)</span> with distribution close to <span class="math inline">\(P\)</span>.</li>
<li><span class="math inline">\(D\)</span> is discriminator: given <span class="math inline">\(x\)</span>, it tries to output the probability that <span class="math inline">\(x\)</span> is real data.</li>
</ul>
<p><span class="math inline">\(G\)</span> tries to minimize the objective and <span class="math inline">\(D\)</span> tries to maximize it: <span class="math display">\[
V(D, G) = \E_P \ln D + \E_G \ln (1-D).
\]</span> (The second expectation is over <span class="math inline">\(G(z)\)</span>, <span class="math inline">\(z\sim p_z\)</span> a fixed distribution.) If the data was real, there is a loss <span class="math inline">\(\ln \wh\Pj\pat{real}\)</span>, and if the data was fake, there is a loss <span class="math inline">\(\ln \wh \Pj\pat{fake}\)</span>. (Penalize for mistakes.)</p>
Note that given <span class="math inline">\(G\)</span>, the optimal <span class="math inline">\(D\)</span> is <span class="math inline">\(\ln \fc{p(x)}{p(x) + \wh p(x)}\)</span>,
<span class="math display">\[\begin{align}
\max_D V(D,G) &amp;= \EE_p \ln \fc{p}{p+\wh p} + \EE_{\wh p} \ln \fc{\wh p}{p+\wh p}\\
&amp;= -2\ln 2 + KL\ba{p||\fc{p+\wh p}2} + KL\ba{\wh p ||\fc{p+\wh p}2}.
\end{align}\]</span>
<h3 id="interpretation-of-objective">Interpretation of objective</h3>
<span class="math display">\[
KL[Q||P] := \EE_Q\ln \fc{Q}{P} = H(Q) - \EE_Q\ln P = H(Q) + CE(Q,P)
\]</span> where CE is cross-entropy.
<span class="math display">\[\begin{align}
JSD[P||Q] &amp;= \rc 2 KL\ba{P||\fc{P+Q}2}
+ \rc 2 KL\ba{Q||\fc{P+Q}2}\\
JSD[Q||P] &amp;= \pi KL\ba{P||\pi P+(1-\pi)Q}
+ (1-\pi) KL\ba{Q||\pi P+(1-\pi)Q}
\end{align}\]</span>
<p>Let <span class="math inline">\(P\)</span> be the natural distribution and <span class="math inline">\(Q\)</span> be the estimated one.</p>
<ul>
<li><span class="math inline">\(KL[Q||P] = H(Q) + CE(Q,P)\)</span>, <span class="math inline">\(CE(Q,P)\)</span> is the perplexity of seeing samples from <span class="math inline">\(P\)</span> when you think the distribution is <span class="math inline">\(Q\)</span>. It is maximized by a model <span class="math inline">\(Q\)</span> that deterministically picks the most likely stimulus. <span class="math inline">\(H(Q)\)</span> enforces diversity.
<ul>
<li>Favors approximations <span class="math inline">\(Q\)</span> to overgeneralize <span class="math inline">\(P\)</span>. It’s allowed to introduce probability mass where <span class="math inline">\(P\)</span> has no or little mass.</li>
<li>Hard to optimize based on a finite sample. (Why?)</li>
</ul></li>
<li><span class="math inline">\(KL[P||Q] = H(P) + CE(P,Q)\)</span>, <span class="math inline">\(H(P)\)</span> is constant and <span class="math inline">\(CE(P,Q)\)</span> is the average negative log likelihood. Optimizing this corresponds to maximizing the log likelihood.
<ul>
<li>Favors undergeneralization, ex. describing the largest mode only. It’s infinitely penalized in introducing probability mass when <span class="math inline">\(P\)</span> has none.</li>
</ul></li>
<li>JSD is a compromise.</li>
</ul>
<h3 id="another-interpretation">Another interpretation</h3>
<p>Let <span class="math inline">\(s=0,1\)</span> wp <span class="math inline">\(\rc2\)</span>, determining whether we see real or fake data. Let <span class="math inline">\(\wt p\)</span> be combined distribution. <span class="math display">\[I[s;x] = KL[\wt p(s,x)||\wt p(s) \wt p(x)].\]</span> This is intractable to estimate. Subtract out KL divergence: <span class="math display">\[
L[p;q] := I[s;x] - \E_{\wt p(x)} [KL[\wt p(s|x)||w(s|x)]] = H[s] + \E_{\wt p(s,x)} [\ln q(s|x)]
\]</span> (CHECK THIS) The second term is the GAN objective function. “GAN can be viewed as an augmented generative model which is trained by minimizing mutual information.” <a href="http://www.yingzhenli.net/home/blog/?p=421">YZL</a></p>
<h3 id="training">Training</h3>
<ul>
<li>Alternate between <span class="math inline">\(k\)</span> steps of optimizing <span class="math inline">\(D\)</span> and one step of optimizing <span class="math inline">\(G\)</span>.</li>
<li>At the beginning <span class="math inline">\(G\)</span> is poor so <span class="math inline">\(D\)</span> predicts close to 1, and <span class="math inline">\(\ln (1-D)\)</span> can blow up. So at the beginning train <span class="math inline">\(G\)</span> to maximize <span class="math inline">\(\E_G\ln D\)</span> instead.</li>
<li>Use stochastic backprop.</li>
<li>Use momentum.</li>
<li>Generator nets used ReLU and sigmoid, discriminator used maxout, dropout.</li>
<li>Estimate probability of test set data under <span class="math inline">\(p_g\)</span> by fitting Gaussian Parzen window. (What is this? [7])</li>
</ul>
<h3 id="notes">Notes</h3>
<ul>
<li>No explicit representation of <span class="math inline">\(p_g\)</span>.</li>
<li><span class="math inline">\(D\)</span> must be synchronized well with <span class="math inline">\(G\)</span> during training. (? Avoid “Helvetica”)</li>
</ul>
<p>Table p. 8</p>
<h2 id="improved-techniques.">Improved techniques.</h2>
<p>“GANs are typically trained using gradient descent techniques that are designed to find a low value of a cost function, rather than to find the Nash equilibrium of a game.”</p>
<ul>
<li>Feature matching: new objective for generator that prevents it from overtraining on current discriminator. Train the generator to match the expected value of features <span class="math inline">\(f(x)\)</span> on intermediate layer of discriminator. <span class="math display">\[ \ve{\E_{x\sim p_{\text{data}}}f(x) - \E_{z\sim p_z(z)} f(G(z))}_2^2.\]</span></li>
<li>Minibatch discrimination: A failure mode for GAN is for the generator to collapse to a point. So allow discriminator to look at multiple data examples in combination.</li>
<li>Historical averaging: include term <span class="math inline">\(\ve{\te - \rc t \sumo it \te[i]}^2\)</span>. cf. fictitious play [16] algorithm that works for other games.</li>
<li>One-sided label smoothing. Replace positive and negative classification targets 1, 0 with <span class="math inline">\(\al,\be\)</span>. This makes it less vulnerable to adversarial examples. Actually, don’t change <span class="math inline">\(\be=0\)</span>.</li>
<li>Virtual batch normalization</li>
</ul>
<h3 id="semi-supervised-learning">Semi-supervised learning</h3>
<p>Label generated samples with “generated” class <span class="math inline">\(K+1\)</span>. For unlabeled data, maximize <span class="math inline">\(\ln p_{\text{model}}(y\in [K]|x)\)</span>.</p>
<h2 id="infogan">InfoGAN</h2>
<ul>
<li>Extend the GAN objective with new term encouraging high mutual information between generated samples and subset of latent variables <span class="math inline">\(c\)</span>.</li>
<li>Hopefully <span class="math inline">\(c\)</span> will represent the most meaningful sources of variation.</li>
<li>Use a variational lower bound to maximize mutual information. (cf. variational autoencoders)</li>
</ul>
<p><span class="math display">\[L_{infoGAN}(\te) = I[x,y] - \la I[x_{\text{fake}},c].\]</span></p>
<p>Want <span class="math inline">\(c\)</span> to effectively explain most variation in fake data.</p>
<p>Variational bound on MI: <span class="math display">\[I[X,Y] = \max_q \bc{H[Y] + \EE_{x,y} \ln q(y|x)}.\]</span></p>
<p>GANs use this bound in the wrong direction. InfoGANs use it twice.</p>
<h2 id="two-views-of-gans">Two views of GANs</h2>
<ol type="1">
<li>Divergence minimization: minimize divergence between real data and implicit generative model <span class="math inline">\(q_\te\)</span>. Problems
<ul>
<li>GAN algorithms minimize lower bounds. The discriminator must be powerful.</li>
<li>Degenerate distributions</li>
</ul></li>
<li>Constrast function view: learn a function that takes low values on data manifold and high values everywhere else. The generator is a smart way of generating contrastive points.</li>
</ol>
<h2 id="adversarial-training-with-maximum-mean-discrepancy">Adversarial training with maximum mean discrepancy</h2>
<ul>
<li><a href="http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf">A kernel two sample test</a></li>
<li><a href="https://arxiv.org/abs/1505.03906">Training generative neural networks via Maximum Mean Discrepancy optimization</a></li>
<li><a href="https://arxiv.org/pdf/1502.02761v1.pdf">Generative Moment Matching Networks</a></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>(HMR16) Gradient descent learns linear dynamical systems</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/HMR16.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/HMR16.html</id>
    <published>2016-12-27T00:00:00Z</published>
    <updated>2016-12-27T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>(HMR16) Gradient descent learns linear dynamical systems</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-12-27 
          , Modified: 2016-12-27 
	</p>
      
       <p>Tags: <a href="/tags/dynamical%20systems.html">dynamical systems</a>, <a href="/tags/quasi-convexity.html">quasi-convexity</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#optimization-background">Optimization background</a></li>
 <li><a href="#control-theory-background">Control theory background</a></li>
 <li><a href="#conditions">Conditions</a></li>
 <li><a href="#proof">Proof</a><ul>
 <li><a href="#showing-quasi-convexity">Showing quasi-convexity</a></li>
 <li><a href="#unbiased-estimator">Unbiased estimator</a></li>
 <li><a href="#bounded-variance">Bounded variance</a></li>
 <li><a href="#extensions">Extensions</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <p>Main theorem:</p>
<ol type="1">
<li>(5.1) Suppose <span class="math inline">\(\Te\)</span> is <span class="math inline">\(\al\)</span>-acquiescent and <span class="math inline">\(\ve{C}\le 1\)</span>. With <span class="math inline">\(N\)</span> samples of length <span class="math inline">\(T\ge \Om\pa{n+\rc{1-\al}}\)</span>, SGD with projection set <span class="math inline">\(B_\al\)</span> returns <span class="math inline">\(\wh \Te = (\wh A, \wh B, \wh C, \wh D)\)</span> with population risk <span class="math display">\[
f(\wh \Te) - f(\Te) \le O\pa{\fc{n^2}N + \sfc{n^5 + \si^2 n^3}{TN}}\poly\pa{\rc{1-\al}, \rc{\tau_0}, \rc{\tau_1}, \tau_2, R=\ve{a}}.
\]</span></li>
<li>(5.2) For long sequences, dividing into samples of length <span class="math inline">\(\be n\)</span> for large enough constant <span class="math inline">\(\be\)</span> gives (suppressing the polynomial dependence on the other parameters) <span class="math display">\[
f(\wh \Te) - f(\Te) \le f(\Te) + O\pa{\sfc{n^5 + \si^2 n^3}{TN}}.
\]</span></li>
<li>(6.2) Suppose <span class="math inline">\(\Te\)</span> has transfer function <span class="math inline">\(G(z) = \fc{s(z)}{p(z)}\)</span> with characteristic function <span class="math inline">\(p(z)\)</span> that is <span class="math inline">\(\al\)</span>-acquiescent by extension of degree <span class="math inline">\(d\)</span>, <span class="math inline">\(\ve{G}_{H_2}\le1\)</span>. Projected SGD with <span class="math inline">\(m=n+d\)</span> states returns <span class="math inline">\(\wh \Te\)</span> with risk <span class="math display">\[
f(\wh \Te) - f(\Te) \le f(\Te) + O\pa{\sfc{m^5 + \si^2 m^3}{TK}}.
\]</span></li>
</ol>
<p>Note:</p>
<ol type="1">
<li>Any poly of degree <span class="math inline">\(n\)</span> with distinct roots inside circle of radius <span class="math inline">\(\al\)</span> is <span class="math inline">\(\al\)</span>-acquiescent by extension of degree <span class="math inline">\(d = O(\max\{(1-\al)^{-1} \ln (\sqrt n \Ga\ve{p}_{H_2}), 0\})\)</span>.</li>
<li>If <span class="math inline">\(p\)</span> has random conjugate pairs of roots inside a circle of radius <span class="math inline">\(\al\)</span>, whp <span class="math inline">\(\Ga(p)\le \exp(\wt O(\sqrt n))\)</span> and <span class="math inline">\(\ve{p}_{H_2}\le \exp(\wt O(\sqrt n))\)</span>, so <span class="math inline">\(p\)</span> is <span class="math inline">\(\al\)</span>-acquiescent by extension of degree <span class="math inline">\(\wt O((1-\al)^{-1}n)\)</span>.</li>
</ol>
<h2 id="optimization-background">Optimization background</h2>
<p>A function <span class="math inline">\(f\)</span> is <span class="math inline">\(\tau\)</span>-weakly-quasi-convex (<span class="math inline">\(\tau\)</span>-WQC) over <span class="math inline">\(B\)</span> with respect to a global minimum <span class="math inline">\(\te^*\)</span> if there is a positive <span class="math inline">\(\tau&gt;0\)</span> such that for all <span class="math inline">\(\te\in B\)</span>, <span class="math display">\[
\nb f(\te)^T(\te-\te^*) \ge \tau (f(\te) - f(\te^*)).
\]</span> <span class="math inline">\(f\)</span> is <span class="math inline">\(\Ga\)</span>-weakly smooth if <span class="math display">\[
\ve{\nb f(\te)}^2 \le \Ga(f(\te)-f(\te^*)).
\]</span></p>
<p>Note a convex function satisfies <span class="math display">\[
f(\te^*) - f(\te) \ge \nb f(\te)^T (\te^*-\te)
\]</span> so is 1-WQC. If <span class="math inline">\(f''\ge a\)</span> everywhere, then <span class="math inline">\(f(\te)-f(\te^*) \ge \ve{\nb f(\te)}^2\)</span> so <span class="math inline">\(f\)</span> is <span class="math inline">\(4a\)</span>-weakly smooth.</p>
<p>For stochastic gradient descent, weak QC and S are enough for convergence guarantees.</p>
<p><strong>Theorem</strong>. Suppose <span class="math inline">\(f\)</span> is <span class="math inline">\(\tau\)</span>-WQC and <span class="math inline">\(\Ga\)</span>-WS and <span class="math inline">\(r\)</span> is an unbiased estimator for <span class="math inline">\(\nb f(\te)\)</span> with variance <span class="math inline">\(V\)</span>, <span class="math inline">\(\te^*\in B\)</span>, <span class="math inline">\(\ve{\te_0-\te^*}\le R\)</span>. Then projected SGD with a suitable learning rate returns <span class="math inline">\(\te_K\)</span> with error <span class="math display">\[
\E f(\te_K)-f(\te^*) \le O\pa{
\max\bc{\fc{\Ga R^2}{\tau^2 K}, \fc{R\sqrt V}{\tau \sqrt K}}
}.
\]</span> (Dependence on <span class="math inline">\(K\)</span> is most important here. Check this.) (Variance makes the convergence <span class="math inline">\(\rc{\sqrt K}\)</span> rather than <span class="math inline">\(\rc K\)</span>.)</p>
<h2 id="control-theory-background">Control theory background</h2>
Consider dynamical systems in the form
<span class="math display">\[\begin{align}
h_{t+1} &amp;= A h_t + B x_t\\
y_t &amp;= C h_t + D x_t + \xi_t\\
\xi_t &amp; \sim N(0,\si^2).
\end{align}\]</span>
<p>(This is a SISO - single-input single-output system.) The population risk is <span class="math display">\[
f(\wh \Te) = \EE_{\{x_t\}, \{\xi_t\}} \ba{
\rc T \sumo tT \ve{\wh y_t - y_t}^2
}
\]</span> where <span class="math inline">\(\wh y_t\)</span> is generated from the estimated parameters without noise. Evert SISO of order <span class="math inline">\(n\)</span> can be put into <strong>controllable canonical form</strong> <span class="math display">\[
A = \matt{\vec 0}{I_{n-1}}{-a_n}{-a_{n-1:1}}, \quad B = \colthree{0}{\vdots}{1} = e_n.
\]</span> Write <span class="math inline">\(A=CC(a)\)</span>, where <span class="math inline">\(a=-[a_n,\ldots, a_1]\)</span>.</p>
<p>A SISO is <strong>controllable</strong> iff <span class="math inline">\([B|AB|\cdots|A^{n-1}B]\)</span> has rank <span class="math inline">\(n\)</span>. (See <a href="../reinforcement_learning/control_theory.html#controllability">Control theory/2 Controllability</a>.)</p>
<p>Let <span class="math display">\[
p_a(z) = z^n + a_1z^{n-1}+\cdots + a_n = \det (zI-A)
\]</span> be the characteristic polynomial of <span class="math inline">\(A\)</span>.</p>
The <strong>transfer function</strong> of the linear system is
<span class="math display">\[\begin{align}
G(z) &amp;= C(zI - A)^{-1}B=:\fc{s(z)}{p(z)}\\
&amp; = \sumo t{\iy} z^{-t} \ub{CA^{t-1}B}{r_{t-1}}\\
&amp; = \fc{c_1+\cdots + c_n z^{n-1}}{z^n + a_1z^{n-1}+\cdots + a_n}
\end{align}\]</span>
<p>where <span class="math inline">\(p\)</span> is monic. (We have <span class="math inline">\(M^{-1} = \fc{\text{cofactor}(M)}{\det(M)}\)</span>.)</p>
<p>(Why is the transfer function useful?)</p>
<p>Define the <strong>idealized risk</strong> as <span class="math display">\[
g(\wh A, \wh C) = \sumz k{\iy} (\wh C\wh A^k B - CA^k B)^2.
\]</span> In the Fourier domain, <span class="math display">\[
g(\wh A, \wh C) = \int_0^{2\pi} |\wh G (e^{i\te}) - G(e^{i\te})|^2\,d\te
\]</span> (The hat on the parameters means “estimate”, the hat on <span class="math inline">\(G\)</span> means “Fourier transform”.)</p>
<em>Explanation</em>. By unfolding the recurrences, then taking the average,
<span class="math display">\[\begin{align}
\E [\ve{\wh y_t - y_t}^2] &amp; = \ve{\wh D - D}^2 + \sumo k{t-1} \ve{\wh C \wh A^{t-k-1} \wh B - CA^{t-k-1} B}^2 + \E[\ve{\xi_t}^2]\\
f(\wh \Te) &amp; = \ve{\wh D - D}^2 + \sumo k{t-1} \pa{1-\fc kT}\ve{\wh C \wh A^{k-1} B - CA^{k-1} B}^2 + \si^2.
\end{align}\]</span>
The idealized risk takes <span class="math inline">\(T\to \iy\)</span>, and ignores the first term. Now by Parseval’s Theorem,
<span class="math display">\[\begin{align}
G(z) &amp;= \sumo t{\iy} CA^{t-1} Bz^{-t}\\
\wh G(z) &amp;= \sumo t{\iy} \wh C \wh A^{t-1} B z^{-t}\\
\int_0^{2\pi} |\wh G(e^{i\te}) - G(e^{i\te})|^2 &amp; = \sumo t{\iy} \ve{CA^{t-1}B - \wh C \wh A^{t-1} B}^2.
\end{align}\]</span>
<p>(This can give some motivation for definition of <span class="math inline">\(G\)</span>. Simply put <span class="math inline">\(CA^kB\)</span> as coefficients of power series; we get <span class="math inline">\(G\)</span>. This also motivates looking at the Fourier series.)</p>
<p>(Why is it useful to pass to the Fourier domain?)</p>
<h2 id="conditions">Conditions</h2>
<ul>
<li>Fix <span class="math inline">\(\tau_0,\tau_1,\tau_2\)</span>. Consider the trapezoidal region on the real axis, <span class="math display">\[ C = \set{z}{\Re z \ge (1+\tau_0)|\Im z|} \cap \set{z}{\tau_1&lt;\Re z &lt;\tau_2}.\]</span>.</li>
<li><span class="math inline">\(p\)</span> is <span class="math inline">\(\al\)</span>-acquiescent if <span class="math inline">\(\set{\fc{p(z)}{z^n}}{|z|=\al}\subeq C\)</span>. A linear system with transfer function <span class="math inline">\(\fc sp\)</span> is <span class="math inline">\(\al\)</span>-acquiescent if <span class="math inline">\(p\)</span> is.</li>
<li><span class="math inline">\(p\)</span> is <span class="math inline">\(\al\)</span>-acquiescent by extension of degree <span class="math inline">\(d\)</span> if there is monic <span class="math inline">\(u\)</span> of degree <span class="math inline">\(d\)</span> such that <span class="math inline">\(pu\)</span> is <span class="math inline">\(\al\)</span>-acquiescent.</li>
<li>Let <span class="math inline">\(B_\al=\set{a\in \R^n}{\set{\fc{p_a(z)}{z^n}}{|z|=\al}\subeq C}\)</span>.
<ul>
<li>If <span class="math inline">\(a\in B_\al\)</span>, then <span class="math inline">\(A\)</span> has spectral radius <span class="math inline">\(\rh(A)&lt;\al\)</span>.</li>
<li><em>Proof</em>. <span class="math inline">\(C\)</span> does not intersect the negative real axis, so by Rouche’s Theorem <span class="math inline">\(z^n, p_a\)</span> have the same number of roots. Thus <span class="math inline">\(g\)</span> has all roots inside <span class="math inline">\(|z|=\al\)</span>. (NOTE: this only requires <span class="math inline">\(\fc{p_n}{z^n}\nin\R\)</span>. Where do we use <span class="math inline">\(\subeq C\)</span>? In the no blow-up property.)</li>
<li><span class="math inline">\(0&lt;\al&lt;\be\implies B_\al\sub B_\be\)</span>. <em>Proof</em>. Maximum modulo principle. If <span class="math inline">\(q(z)\subeq C\)</span> for <span class="math inline">\(|z|=\rc \al\)</span>, then <span class="math inline">\(q(z)\subeq C\)</span> for <span class="math inline">\(|z|=\rc \be\)</span>.</li>
</ul></li>
<li>No blow-up:
<span class="math display">\[\begin{align}
 \sumz k{\iy} \ve{\al^{-k} A^k B}^2 &amp;\le 2\pi n\al^{-2n}/\tau_1^2\\
 \ve{A^k B}^2&amp;\le \min\{2\pi n/\tau_1^2, 2\pi n\al^{2k-2n}/\tau_1^2\}.
 \end{align}\]</span>
<ul>
<li>Load these as coefficients into a Fourier series and use Parseval. Get <span class="math inline">\(\int_0^{2\pi} |(I-\al^{-1}e^{i\la}A)^{-1}B|^2\,d\la\)</span> which depends on the value of <span class="math inline">\(p_a\)</span> at <span class="math inline">\(|z|=\al\)</span>. (Use forms of <span class="math inline">\(A,B\)</span>.) (NOTE: this only requires <span class="math inline">\(\fc{p_n}{z^n}\nin \{|z|\le \tau_1\}\)</span>.</li>
</ul></li>
<li>Define <span class="math inline">\(H_2\)</span> norm by <span class="math inline">\(\ve{G}_{H_2}^2 = \rc{2\pi}\int_0^{2\pi} |G(e^{i\te})|^2\,d\te\)</span>.</li>
<li>Let <span class="math inline">\(\Ga(h) = \sum_{j\in [n]}\af{\la_j^n}{\prod_{i\ne j}(\la_i-\la_j)}\)</span>. (TODO, Q: how related to Lagrange, Chebyshev interpolation?)</li>
</ul>
<p><strong>Rouche’s Theorem</strong>. <a href="https://en.wikipedia.org/wiki/Rouch%C3%A9's_theorem">wikipedia</a></p>
<ol type="1">
<li>If <span class="math inline">\(f,g\)</span> are holomorphic in closed <span class="math inline">\(K\)</span>, <span class="math inline">\(|g|&lt;|f|\)</span> on <span class="math inline">\(\pl K\)</span>, then <span class="math inline">\(f\)</span>, <span class="math inline">\(f+g\)</span> have the same number of zeros inside <span class="math inline">\(K\)</span>.</li>
<li>(Symmetric form) … <span class="math inline">\(|f-g|&lt;|f|+|g|\)</span> on <span class="math inline">\(\pl K\)</span> then <span class="math inline">\(f,g\)</span> have the same number of zeros.</li>
</ol>
<h2 id="proof">Proof</h2>
<h3 id="showing-quasi-convexity">Showing quasi-convexity</h3>
<ul>
<li><span class="math inline">\(g(\wh A, \wh C)\)</span> is <span class="math inline">\(\tau\)</span>-WQC over <span class="math display">\[N_\tau(a) = \set{\wh a\in \R^n}{\forall |z|=1, \Re\pa{p_a}{p_{\wh a}}\ge \fc\tau2}.\]</span>
<ul>
<li><em>Proof</em>. Calculate <span class="math inline">\(h_{\wh s},h_{\wh p}\)</span>. Then compute <span class="math display">\[ \an{h_{\wh a}, \wh a - a} + \an{h_{\wh C}, \wh C-C} = 2\Re \pa{p}{\wh p} |\wh G - G|^2.\]</span> Use the Fourier expression for <span class="math inline">\(g\)</span>.</li>
</ul></li>
<li><span class="math inline">\(g\)</span> is <span class="math inline">\(\Om\pf{\tau_0\tau_1}{\tau_2}\)</span>-WQC in <span class="math inline">\(B_\al \ot \R^n\)</span> and <span class="math inline">\(O\pf{n^2}{\tau_1^4}\)</span>-WS.
<ul>
<li><em>Proof</em>. For <span class="math inline">\(\wh a, a\in B_\al\)</span>, show that <span class="math inline">\(\wh a\in N_\tau(a)\)</span>. Restricting <span class="math inline">\(C\)</span> to be a sector means the angle between any 2 points in <span class="math inline">\(C\)</span> is at most <span class="math inline">\(\pi - \Om(\tau_0)\)</span> and the magnitude is at least <span class="math inline">\(\Om\pf{\tau_1}{\tau_2}\)</span>.</li>
<li>Smoothness (3.4). Use chain rule with <span class="math inline">\(p\)</span> and CS.</li>
</ul></li>
</ul>
<h3 id="unbiased-estimator">Unbiased estimator</h3>
<p>Let <span class="math inline">\(T_1=\fc T4\)</span>. Using loss function <span class="math display">\[
\ell((x,y),\wh\Te) = \rc{T-T_1} \sum_{t&gt;T_1} \ve{\wt y_t - y_t}^2,
\]</span> gradients wrt <span class="math inline">\(\wh a, \wh C, \wh D\)</span> are almost (exponentially) unbiased.</p>
<h3 id="bounded-variance">Bounded variance</h3>
<p>Omitted.</p>
<h3 id="extensions">Extensions</h3>
<p>Ex. when we can’t hope to properly recover: <span class="math inline">\(G(z) = G_1(z) \fc{z^n-r_1^n}{z^n-r_2^n}\)</span> where <span class="math inline">\(r_1\approx r_2\approx 1\)</span>. They are exponentially close but the characteristic polys are very different.</p>
<p>Approximation: Prove that the inverse of a poly can be easily approximated (cf. proof for conjugate gradients, Chebyshev approx).</p>
<p>Using the approximation result, get <span class="math inline">\(\ab{\fc{pu}{z^{n+d}} - 1} &lt;0.5\)</span>, so <span class="math inline">\(\fc{pu}{z^{n+d}}\in C\)</span> for appropriate <span class="math inline">\(\tau_i\)</span>.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>(HM16) A non-generative framework and convex relaxations for unsupervised learning</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/HM16.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/HM16.html</id>
    <published>2016-12-27T00:00:00Z</published>
    <updated>2016-12-27T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>(HM16) A non-generative framework and convex relaxations for unsupervised learning</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-12-27 
          , Modified: 2016-12-27 
	</p>
      
       <p>Tags: <a href="/tags/unsupervised.html">unsupervised</a>, <a href="/tags/convex%20relaxation.html">convex relaxation</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#framework">Framework</a></li>
 <li><a href="#generalization-theory">Generalization theory</a></li>
 <li><a href="#pca">PCA</a></li>
 <li><a href="#spectral-autoencoder">Spectral autoencoder</a></li>
 <li><a href="#dictionary-learning">Dictionary learning</a><ul>
 <li><a href="#group-encoding-and-decoding">Group encoding and decoding</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="framework">Framework</h2>
<p>Given</p>
<ol type="1">
<li>Instance domain <span class="math inline">\(X\)</span> and target space <span class="math inline">\(Y\)</span> (ex. <span class="math inline">\(\R^d\)</span>, <span class="math inline">\(\R^k\)</span>, <span class="math inline">\(d\gg k\)</span>).</li>
<li>Unknown distribution <span class="math inline">\(D\)</span> on domain <span class="math inline">\(X\)</span>. (NOT assumed to be generated by a function in <span class="math inline">\(H\)</span>.)</li>
<li>Hypothesis class of decoding and encoding pairs <span class="math display">\[ H \subeq \{X\to Y\}\times \{Y\to X\}.\]</span></li>
<li>Loss function <span class="math inline">\(l:H\times X\to \R_{\ge0}\)</span>. (Ex. <span class="math inline">\(\ell_2\)</span> loss <span class="math inline">\(\ve{g(h(x))-x}_2^2\)</span>.
<ul>
<li>Define <span class="math display">\[
 \text{loss}_D(f) = \EE_{x\sim D} l(f,x).
 \]</span></li>
<li>Define sample loss <span class="math display">\[
 \text{loss}_S(f) = \rc m \sum_{x\in S}l(f,x).
 \]</span></li>
</ul></li>
</ol>
<p><span class="math inline">\(D,X\)</span> is <span class="math inline">\((k,\ga)\)</span> <span class="math inline">\(C\)</span>-learnable wrt <span class="math inline">\(H\)</span> if there exists an algorithm that given <span class="math inline">\(\de,\ep&gt;0\)</span>, after seeing <span class="math inline">\(m(\ep, \de) = \poly\pa{\rc \ep, \ln \prc{\de}, d}\)</span> examples, returns <span class="math inline">\((h,g)\)</span> (NOT necessarily in <span class="math inline">\(H\)</span>) such that</p>
<ol type="1">
<li>wp <span class="math inline">\(\ge 1-\de\)</span>, <span class="math display">\[\text{loss}_D((h,g)) \le \min_{(h,g)\in H} \text{loss}_D((h,g)) + \ep + \ga.\]</span></li>
<li><span class="math inline">\(h\)</span> has an explicit representation with <span class="math inline">\(\le k\)</span> bits.</li>
</ol>
<p>(<span class="math inline">\(\ga\)</span> is the bias.)</p>
<p>(Q: ? how to understand “<span class="math inline">\(\le k\)</span>” bits?)</p>
<h2 id="generalization-theory">Generalization theory</h2>
<span class="math display">\[\begin{align}
\wh f_{ERM} &amp;= \amin_{f\in H} \text{loss}_S(\wh f)\\
R_{S,l}(H) &amp;= \EE_{\si\sim \{\pm 1\}^m} \ba{
\sup_{f\in H} \rc m \sum_{x\in S}\si_i l(f,x)
}\\
\Pj\pa{
\text{loss}_D(\wh f_{ERM})
 \le \min_{f\in H} \text{loss}_D(f) + 6 R_m(H) + \sfc{4\ln \prc{\de}}{2m}}\ge 1-\de.
\end{align}\]</span>
<h2 id="pca">PCA</h2>
For <span class="math inline">\(H_k^{pca}\)</span>,
<span class="math display">\[\begin{align}
X&amp;=\R^n\\
Y&amp;=\R^k\\
h(x) &amp;= Ax\\
g(y) &amp;= A^+y\\
l((g,h),x) &amp; = \ve{(A^+A - I)x}^2.
\end{align}\]</span>
<p>For <span class="math inline">\(H_{k,s}^{pca}\)</span>, replace <span class="math inline">\(d\)</span> by <span class="math inline">\(d^s\)</span>, <span class="math inline">\(x\)</span> by <span class="math inline">\(x^{\ot s}\)</span>.</p>
<p><span class="math inline">\(H_{k,s}^{pca}\)</span> is efficiently <span class="math inline">\((k,0)\)</span> C-learnable.</p>
<p>(Proof: Use SVD. Rademacher complexity is <span class="math inline">\(O\pf{d^s}{m}\)</span>.)</p>
<h2 id="spectral-autoencoder">Spectral autoencoder</h2>
For <span class="math inline">\(H_{k,s}^{s,a}\)</span>,
<span class="math display">\[\begin{align}
h(x) &amp;= Ax^{\ot s},&amp; A&amp;\in \R^{k\times d^s}\\
g(y) &amp;= v_{\max}(By),&amp; B&amp;\in \R^{d^s\times k}
\end{align}\]</span>
<p>where <span class="math inline">\(v_{\max}\)</span> is max eigenvector of tensor.</p>
<p>For <span class="math inline">\(s=2\)</span>:</p>
<p><span class="math inline">\(D\)</span> is <span class="math inline">\((k,\ep)\)</span>-regularly spectral decodable <span class="math inline">\(A\in \R^{k\times d^2}\)</span>, <span class="math inline">\(B\in \R^{d^2\times k}\)</span> with <span class="math inline">\(\ve{BA}\le \tau\)</span> such that for <span class="math inline">\(x\sim D\)</span>, <span class="math display">\[ M(BAx^{\ot 2}) = xx^T + E, \quad \ve{E}\le \ep.\]</span> Here <span class="math inline">\(M\)</span> means matrix representation.</p>
<p>(order of qualifiers for <span class="math inline">\(\tau\)</span>?)</p>
<p><span class="math inline">\(H_{k,2}^{sa}\)</span> is <span class="math inline">\(\pa{O\pa{\tau^4k^4}{\de^4}, \de}\)</span> <span class="math inline">\(C\)</span>-learnable in poly-time.</p>
<p><em>Proof</em>.</p>
<ol type="1">
<li>Rademacher complexity bound for <span class="math inline">\(\Phi = \set{\ve{Rx^{\ot 2}-x^{\ot 2}}}{\ve{R}_{S_1}\le \tau k}\)</span>: <span class="math display">\[R_m(\Phi) \le 2\tau k \sfc1m.\]</span> (CHECK. Omitted.)
<ul>
<li><span class="math inline">\(L^2\)</span> bound on <span class="math inline">\(x\)</span> gives <span class="math inline">\(L^1\)</span> bound on <span class="math inline">\(x^{\ot 2}\)</span>, which is why we need <span class="math inline">\(S_1\)</span> bound.</li>
</ul></li>
<li><span class="math inline">\(f(R) := \E[\ve{Rx^{\ot 2}-x^{\ot 2}}]\)</span> is convex and 1-Lipschitz.
<ul>
<li><em>Proof</em>. <span class="math inline">\((u\ot v)(x^{\ot 2}^T)\)</span> is a subgradient, where <span class="math inline">\(u,v\in \R^d\)</span> are top left/right singular vectors of <span class="math inline">\(M(Rx^{\ot 2} - x^{\ot 2})\)</span>.</li>
</ul></li>
<li>Optimization: Use (non-smooth) Frank-Wolfe algorithm (CHECK).
<ul>
<li>Each update is rank 1.</li>
<li><span class="math inline">\(R^*\)</span> is a feasible solution with <span class="math inline">\(\ve{R^*}_{S_1}\le \rank(R^*)\ve{R}\le \tau k\)</span>, so the diameter of <span class="math inline">\(\Phi\)</span> is <span class="math inline">\(\le \tau k\)</span>.</li>
<li><span class="math inline">\(f\)</span> is 1-Lipschitz.</li>
<li>In <span class="math inline">\(O\pf{\tau^4k^4}{\de^4}\)</span> steps get <span class="math inline">\(\wh R\)</span> with <span class="math inline">\(f(\wh R)\le \de + \ep\)</span>.</li>
</ul></li>
</ol>
<h2 id="dictionary-learning">Dictionary learning</h2>
<span class="math display">\[\begin{align}
h_A(x) &amp;= \amin{\ve{y}_\be\le k} \rc d |x-Ay|\\
g_A(y) &amp;= Ay\\
A&amp;= \amin_{\ve{A}_\al \le c_\al}
\EE_{x\sim D} \ba{
\min_{y\in \R^r: \ve{y}_\be \le k} \rc d |x-Ay|_1
}.
\end{align}\]</span>
<p>Ex. <span class="math inline">\(\ved_\al\)</span> is max (column <span class="math inline">\(\ell_2\)</span> or <span class="math inline">\(\ell_\iy\)</span> norm) and <span class="math inline">\(\ved_b\)</span> is <span class="math inline">\(\ell_0\)</span>.</p>
<p>Here, use max column <span class="math inline">\(\ell_\iy\)</span> norm (i.e., <span class="math inline">\(\ell_1\to \ell_\iy\)</span>) for <span class="math inline">\(A\)</span> and <span class="math inline">\(\ell_1\)</span> norm for <span class="math inline">\(B\)</span>. (Q: How does this compare to usual setting? <span class="math inline">\(\ell_1\)</span> norm is looser than <span class="math inline">\(\ell_0\)</span>, OK. <span class="math inline">\(\ell_\iy\)</span> is also looser than <span class="math inline">\(\ell_2\)</span>.)</p>
<p>For any <span class="math inline">\(\de&gt;0, p\ge 1\)</span>, <span class="math inline">\(H_k^{dict}\)</span> is C-learnable with encoding length <span class="math inline">\(\wt O\pf{k^2 r^{\rc p}}{\de^2}\)</span>, bias <span class="math inline">\(\de+ O(\ep^*)\)</span>, and sample complexity <span class="math inline">\(d^{O(p)}\)</span> in time <span class="math inline">\(n^{O(p^2)}\)</span>.</p>
<p>Interpretation:</p>
<ul>
<li>For <span class="math inline">\(p\to \iy\)</span>, get <span class="math inline">\(\wt \pf{k^2}{\de^2}\)</span>. Can get nontrivial approximation by taking <span class="math inline">\(\de = o(\sqrt k)\)</span>, so can get close to <span class="math inline">\(k\)</span>. This makes sense thinking of the input as <span class="math inline">\(k\)</span>-sparse.</li>
</ul>
<p>Note this isn’t doing “dictionary learning” in the usual sense of finding the dictionary. It only shows that you can reduce dimension and still preserve information. It does not “undo” the dictionary multiplication and give a <em>useful</em> representation, only a <em>succinct</em> one.</p>
<h3 id="group-encoding-and-decoding">Group encoding and decoding</h3>
<p>This is a simpler algorithm which achieves a weaker goal of encoding multiple examples at a time.</p>
<p>Given <span class="math inline">\(N\)</span> points <span class="math inline">\(X\in \R^{d\times N}\sim D^N\)</span>, convex set <span class="math inline">\(Q\)</span>,</p>
<ol type="1">
<li>Group encoding <span class="math inline">\(h(X)\)</span>:
<ul>
<li>Compute <span class="math inline">\(Z=\amin{C\in Q} |X-C|_1\)</span>.</li>
<li>Random sample <span class="math inline">\(B\)</span> taking each entry with probability <span class="math inline">\(\rh\)</span>, <span class="math inline">\(Y=P_{\Om}(Z)\)</span>.</li>
</ul></li>
<li>Group decoding <span class="math inline">\(g(Y)\)</span>:
<ul>
<li><span class="math inline">\(\amin_{C\in Q} |P_\Om(C)-Y|_1\)</span>.</li>
</ul></li>
</ol>
<p>We need a conex relaxation <span class="math inline">\(Q\)</span> <span class="math display">\[
\set{g_{A^*}(h_{A^*}(X))}{X\in \R^{d\times N}} \sub
\set{AY}{\ve{A}_{\ell^1\to \ell^\iy}\le 1, \ve{Y}_{\ell_1\to \ell_1}}\sub Q.
\]</span> with low sampling Rademacher width and that is efficiently optimizable.</p>
<ol type="1">
<li>(Lemma 5.2) If a convex set has low sampling Rademacher width, then we can compress it by random sampling.</li>
<li>Define the factorable “norm” <span class="math inline">\(\Ga_{\al,\be}(Z) = \inf_{Z=AB}\ve{A}_\al\ve{B}_\be\)</span>. <span class="math inline">\(\Ga_{1,q,1,t}(\cdot) = \Ga_{\ell_1\to \ell_q, \ell_1\to \ell_t}\)</span> is a norm (5.3-4). <span class="math inline">\(Q_{1,\iy,1,1} = \set{C\in \R^{N\times d}}{\Ga_{1,\iy,1,1}(C)\le k}\)</span> has low SRW (5.5).</li>
<li>This is not efficiently optimizable. Instead consider the SoS relaxation
<span class="math display">\[\begin{align}
Q_p^{sos} &amp;= \set{C\in \R^{d\times N}}{\exists \text{degree-}O(p^2)\text{pseudo-expectation $\wt\E$ that satisfies }A(C)}\\
A(C) &amp; = \set{C=AB}\cup 
\bc{ \forall i,j, B_{ij} = b_{ij}^{p-1}, \sumo lr b_{lj}^p \le k^{\fc p{p-1}},\forall i,j, A_{ij}^2\le 1}.
\end{align}\]</span>
Now redo the proof for low SRW using a SoS proof. Motivation: for <span class="math inline">\(Q\)</span> we use an inequality involving <span class="math inline">\(L^1,L^{\iy}\)</span>. In oreder to convert to SoS proof, we need to turn this into <span class="math inline">\(L^{\fc{p}{p-1}},L^p\)</span>.</li>
</ol>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
