<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2017-07-20T00:00:00Z</updated>
    <entry>
    <title>Annealed importance sampling</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/probabilistic/ais.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/probabilistic/ais.html</id>
    <published>2017-07-20T00:00:00Z</published>
    <updated>2017-07-20T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Annealed importance sampling</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-07-20 
          , Modified: 2017-07-20 
	</p>
      
       <p>Tags: <a href="/tags/sampling.html">sampling</a>, <a href="/tags/annealing.html">annealing</a>, <a href="/tags/temperature.html">temperature</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#papers">Papers</a></li>
 <li><a href="#log-p">log p</a></li>
 <li><a href="#elbo">ELBO</a></li>
 <li><a href="#ais">AIS</a></li>
 <li><a href="#raise">RAISE</a></li>
 <li><a href="#notes">Notes</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="papers">Papers</h2>
<ul>
<li>[N98] Annealed Importance Sampling</li>
<li>[BGS14] Accurate and Conservative Estimates of MRF Log-likelihood using Reverse Annealing</li>
<li>[WBSG17] ON THE QUANTITATIVE ANALYSIS OF DECODER-BASED GENERATIVE MODELS</li>
</ul>
<h2 id="log-p">log p</h2>
<p>We can get an unbiased estimator for <span class="math inline">\(p(x)\)</span>, say <span class="math inline">\(\wh p\)</span>. But we often want <span class="math inline">\(\ln p(x)\)</span>. We use Jensen’s inequality and Markov’s inequality. So <span class="math inline">\(\ln \wh p\)</span> is a probabilistic lower bound. <span class="math display">\[
\E[\ln \wh p] \le \ln p\implies \quad \Pj(\ln \wh p&gt; \ln p + b) &lt;e^{-b}.
\]</span> (This is true no matter what the variance is. However, this can be a very loose bound. There is no good way of estimating <span class="math inline">\(\E \ln X\)</span> from draws of <span class="math inline">\(X\)</span> (why not?). Oddly, there is a good way of estimating <span class="math inline">\(\E e^X\)</span> from <span class="math inline">\(X\)</span> by power series expansion. (Power series for <span class="math inline">\(\ln\)</span> is terrible over long distances.))</p>
<p>Note this is prone to overestimation with little indication anything is wrong.</p>
<h2 id="elbo">ELBO</h2>
<p>Goal: posterior distribution <span class="math display">\[
p(z|x,\al) = \fc{p(z,x|\al)}{\int_z p(z,x|\al)}.
\]</span> Pick a family of distributions with variational parameters <span class="math inline">\(q(z_{1:m}|\nu)\)</span>. Use <span class="math inline">\(q\)</span> with fitted parameters as proxy.</p>
<p>So want to minimize <span class="math inline">\(KL(q||p)\)</span>.</p>
<p>Why <span class="math inline">\(q||p\)</span>, not <span class="math inline">\(p||q\)</span>?</p>
<ul>
<li>q high, p low is bad. Don’t want to make impossible events happen!</li>
<li>q low, p high is not so bad.</li>
</ul>
<span class="math display">\[\begin{align}
KL(q||p) &amp;=\EE_q\ba{\ln \fc{q(z)}{p(z|x)}}\\
\ln p &amp;=\ln \int \EE_{z\sim q}\ba{\fc{p(x,z)}{q(z)}}\\
&amp; \ge \EE_q \ln p(x,z) - \EE_q [\ln q] :=ELBO\\
KL(q||p) &amp;=-ELBO - \ln p(x)
\end{align}\]</span>
<p><span class="math inline">\(\ln p\)</span> doesn’t depend on <span class="math inline">\(q\)</span>.</p>
<h2 id="ais">AIS</h2>
<p>Given annealed distributions <span class="math inline">\(p_i\propto f_i\)</span>, <span class="math inline">\(p_K=p\)</span> with Markov chains <span class="math inline">\(M_i\)</span> (with transition kernels <span class="math inline">\(T_i\)</span>), to estimate <span class="math inline">\(Z=Z_K\)</span>,</p>
<ul>
<li>Sample from <span class="math inline">\(p_0\)</span>.</li>
<li>Let <span class="math inline">\(w=Z_0\)</span>.</li>
<li>For <span class="math inline">\(k=1:K\)</span>
<ul>
<li><span class="math inline">\(w\leftarrow w \fc{f_k(x_{k-1})}{f_{k-1}(x_{k-1})}\)</span></li>
<li><span class="math inline">\(x_k \sim T_k(\cdot |x_{k-1})\)</span>.</li>
</ul></li>
<li>Estimate is <span class="math inline">\(w\)</span>.</li>
</ul>
<p>For probabilistic neural nets, use Gibbs sampler (alternately sample <span class="math inline">\(h\)</span> and <span class="math inline">\(x\)</span>) as transition.</p>
<p>Think of this as proposing the distribution given by applying the <span class="math inline">\(T_i\)</span> in sequence.</p>
<p>Giving a stochastic lower bound for <span class="math inline">\(Z\)</span> means we overestimate log-likelihood.</p>
<h2 id="raise">RAISE</h2>
<p>Go the other way using samples from the target distribution This gives a probabilistic lower bound on <span class="math inline">\(\fc{Z_0}{Z}\)</span>, so a probabilistic upper bound on <span class="math inline">\(Z\)</span>, so we underestimate log-likelihood.</p>
<p><span class="math inline">\(p(h|v)\)</span> needs to be tractable. ((z|x) in above notation)</p>
<p>For intractable <span class="math inline">\(p(h|v)\)</span> combine the AIS (fixing <span class="math inline">\(v\)</span>) and RAISE steps to get a single estimate. See Algorithm 3 for details.</p>
<h2 id="notes">Notes</h2>
<ul>
<li>Is RAISE a provable (stochastic) upper bound? Even in the intractable case?</li>
<li>I’m surprised AIS/RAISE match so closely. Does this mean partition function calculation for deep belief nets is in practice tractable???</li>
<li>I think AIS can have large variance. It seems better to do the “evolutionary multiplicative update” thing. Does that have provable guarantees under similar conditions as Langevin annealing? Can AIS fail where this works? (Ex. continuously miss the high-prob stuff, stepping into the low-ratios between layers.)
<ul>
<li>Does this give a better bound in practice? I.e. larger estimate for <span class="math inline">\(Z\)</span>? (Warning: not quite unbiased anymore…)</li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>(HKY17) Hyperparameter Optimization - A Spectral Approach</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/HKY17.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/HKY17.html</id>
    <published>2017-07-19T00:00:00Z</published>
    <updated>2017-07-19T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>(HKY17) Hyperparameter Optimization - A Spectral Approach</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-07-19 
          , Modified: 2017-07-19 
	</p>
      
       <p>Tags: <a href="/tags/hyperparameters.html">hyperparameters</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#compressed-sensing-for-orthonormal-polynomials">Compressed sensing for orthonormal polynomials</a></li>
 <li><a href="#harmonica">Harmonica</a></li>
 <li><a href="#questions">Questions</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>The main theorem (Alg. 1, Thm. 6) is a theorem on learning Fourier-concentrated functions with much better sample complexity than [LMN93], using compressed sensing applied to orthonormal polynomials. Apply this theorem to the loss as a function of hyperparameters (A.g. 2, Harmonica). (Note this is heuristic.)</p>
<h2 id="compressed-sensing-for-orthonormal-polynomials">Compressed sensing for orthonormal polynomials</h2>
<p>An orthonormal family with respect to distribution <span class="math inline">\(D\)</span> has <span class="math inline">\(\EE_D[\psi_i (X) \psi_j(X)]=\de_{ij}\)</span>.</p>
<ul>
<li><span class="math inline">\(s\)</span>-sparse: <span class="math inline">\(L_0(f)\le s\)</span></li>
<li><span class="math inline">\((\ep,d)\)</span> concentrated: <span class="math inline">\(\ve{f - \pi_{\{\psi_{i}\}_{i\in S}}(f)}_2\le \ep\)</span>.</li>
<li><span class="math inline">\((\ep,d,s)\)</span>-bounded: additionally, <span class="math inline">\(\ve{f}_1\le s\)</span>.</li>
<li>Note we can approximate <span class="math inline">\(L_1(f)\le s\)</span> to <span class="math inline">\(\ep\)</span> with <span class="math inline">\(L_0(g)\le \fc{s^2}{\ep}\)</span>. (Sampling. Cf. Barron proof)</li>
</ul>
<p>LASSO: With appropriate <span class="math inline">\(\la\)</span>, <span class="math display">\[
\min_{x\in \R^n} [\ve{x}_1 + \la \ve{Ax-y}_2^2]
\]</span></p>
<p>For <span class="math inline">\(z^1,\ldots, z^m\sim D\)</span>, <span class="math inline">\(A_{ij}=\psi_j(z^i)\)</span>, <span class="math inline">\(y=Ax+e\)</span>, <span class="math inline">\(\ve{e}_2\le \eta\sqrt m\)</span>, <span class="math inline">\(x^*\)</span> solving LASSO, <span class="math display">\[
\Pj(\ve{x-x^*}_2\le C \fc{\si_s(x)_1}{\sqrt s}+d\eta) \ge 1-\de
\]</span> where <span class="math inline">\(\si_s(x)_1=\min\set{\ve{x-z}_1}{z\text{ is s-sparse}}\)</span>, <span class="math inline">\(c,d\)</span> constants, with <span class="math inline">\(m\ge CK^2 s \poly\log(K,s,N,\rc{\de})\)</span> samples.</p>
<p>Apply for low-degree recovery: if <span class="math inline">\(f\)</span> is (<span class="math inline">\(\ep,d,s\)</span>)-bounded, then using this finds <span class="math inline">\(g\equiv_\ep f\)</span> in time <span class="math inline">\(O(n^d)\)</span>, with <span class="math inline">\(T=\wt O(K^2s^2 \ln N/\ep)\)</span> samples. (? <span class="math inline">\(\ep\)</span> outside)</p>
<p>(LMN93 needs <span class="math inline">\(\Om\pf{NL_\iy(f)^2}{\ep}\)</span> samples.) (? What is <span class="math inline">\(N\)</span> here? Number of orthonormal polys. Shouldn’t it be <span class="math inline">\(n^d\)</span>?)</p>
<h2 id="harmonica">Harmonica</h2>
<p>Apply in stages, with some degree <span class="math inline">\(d\)</span> and sparsity <span class="math inline">\(s\)</span>. Note this can involve at most <span class="math inline">\(ds\)</span> variables. Suppose the approximation <span class="math inline">\(g\)</span> to <span class="math inline">\(f\)</span> only involves variables in <span class="math inline">\(J\)</span>.</p>
<p>Take the best <span class="math inline">\(t\)</span> solutions <span class="math inline">\(x_i*\)</span> to <span class="math inline">\(g\)</span> on <span class="math inline">\(J\)</span>, and now apply to <span class="math inline">\(\rc t \sumo it f_{J \leftarrow x_i^*}(x)\)</span>.</p>
<h2 id="questions">Questions</h2>
<ul>
<li>Why does multiple stages help?</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>(AR17) Provable benefits of representation learning</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/representation/AR17.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/representation/AR17.html</id>
    <published>2017-07-18T00:00:00Z</published>
    <updated>2017-07-18T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>(AR17) Provable benefits of representation learning</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-07-18 
          , Modified: 2017-07-18 
	</p>
      
       <p>Tags: <a href="/tags/representation%20learning.html">representation learning</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#intro">Intro</a></li>
 <li><a href="#section">2</a><ul>
 <li><a href="#section-1">2.2</a></li>
 </ul></li>
 <li><a href="#section-2">3</a><ul>
 <li><a href="#section-3">3.1</a></li>
 <li><a href="#section-4">3.2</a></li>
 </ul></li>
 <li><a href="#section-5">4</a><ul>
 <li><a href="#section-6">4.1</a></li>
 <li><a href="#section-7">4.2</a></li>
 </ul></li>
 <li><a href="#section-8">5</a><ul>
 <li><a href="#lower-bounds-for-nearest-neighbors">5.1 Lower bounds for nearest neighbors</a></li>
 <li><a href="#section-9">5.3</a></li>
 </ul></li>
 <li><a href="#notes">Notes</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="intro">Intro</h2>
<ul>
<li>Contributions
<ul>
<li>Formalizes representation learning, unifying disparate settings.</li>
<li>Quantifies “utility” from representation learning.</li>
<li>Prove separation results between representation learning and simpler algorithms.</li>
</ul></li>
<li>“Bayes+”</li>
<li>Why representation learning?
<ul>
<li>Allows semi-supervised learning.</li>
<li>Simpler methods need too many samples.</li>
<li>Provably better than manifold learning in some cases.</li>
</ul></li>
<li>The framework
<ul>
<li>Many-to-one map <span class="math inline">\(x\mapsto h\)</span>. <span class="math inline">\(h\)</span> is “high-level” representation.</li>
<li>Generative model <span class="math inline">\(h\to x\)</span>.</li>
<li>Similarity in the latent space (of <span class="math inline">\(h\)</span>) is more informative.</li>
<li>Defintion: A <span class="math inline">\((\ga,\be)\)</span>-valid decoder has <span class="math inline">\(\Pj(\ve{f(x)-h}\le (1-\ga) \ve{h})\ge \be\)</span>. (Think of <span class="math inline">\(\ga,\be\approx 1\)</span>.)</li>
<li>Utility: If <span class="math inline">\(C\)</span> is <span class="math inline">\(\al\)</span>-Lipschitz, <span class="math inline">\(\ve{C(f(x)) - C(h)}_\iy\le (1-\ga)\al \ve{h}\)</span>.</li>
</ul></li>
<li>Examples
<ul>
<li>Clustering</li>
<li>Manifold</li>
<li>Kernel learning</li>
</ul></li>
<li>Non-examples
<ul>
<li>Nearest neighbor (provably weaker in some settings)</li>
<li>LSH - this preserves distance, which is not our goal.</li>
</ul></li>
<li>Contrast [HM16], which is assumption-free and basically lossless compression. (ex. Lempel-Ziv) This notion is different, ex. allows throwing away noise.</li>
<li>Compare to the usual: Maximize log probability (MLE), then <span class="math inline">\(\amax_h p_\te(h|x)\)</span>.
<ul>
<li>Unlike Bayesian which gives a distribution over <span class="math inline">\(h\)</span>, we output single <span class="math inline">\(h\)</span>.</li>
</ul></li>
</ul>
<h2 id="section">2</h2>
<h3 id="section-1">2.2</h3>
<ul>
<li>Encoder exists <span class="math inline">\(\implies\)</span> given <span class="math inline">\(x\)</span>, <span class="math inline">\(h\)</span> is concentrated around <span class="math inline">\(f(x)\)</span>, almost uniquely defined.</li>
<li>Having concentration is stronger than just being able to do inference.</li>
</ul>
<h2 id="section-2">3</h2>
<h3 id="section-3">3.1</h3>
<p>Topic modeling</p>
<ul>
<li><span class="math inline">\(k\)</span> topices</li>
<li>Each distribution on <span class="math inline">\(M\)</span> words. <span class="math inline">\(A_i\in \R^M\)</span>.</li>
<li>Mixture coefficients <span class="math inline">\(h_i\)</span>.</li>
<li>Draw bag of words <span class="math inline">\(x\sim \sum h_i A_i\)</span>, <span class="math inline">\(x\in \Z^N\)</span>.</li>
</ul>
<h3 id="section-4">3.2</h3>
<p>Loglinear model: (continued below)</p>
<p><span class="math display">\[p(x,h) = p(h)p(x|h).\]</span></p>
<h2 id="section-5">4</h2>
<h3 id="section-6">4.1</h3>
<p>Topic modeling: want <span class="math inline">\(\ell_\iy\to \ell_1\)</span> condition number to be small.</p>
<h3 id="section-7">4.2</h3>
<ul>
<li><span class="math inline">\(h\in \R^d\)</span> randomly on unit sphere.</li>
<li><span class="math inline">\(\Pj(x|h)\propto e^{\an{W_x,h}}\)</span>.
<ul>
<li><span class="math inline">\(W_x = Bv\)</span>, <span class="math inline">\(B=O(1)\)</span>, <span class="math inline">\(v\sim N(0,I)\)</span>.</li>
</ul></li>
<li>Take <span class="math inline">\(f(x) = \nv{\sum_i W_{x_i}}\)</span>.</li>
</ul>
<h2 id="section-8">5</h2>
<h3 id="lower-bounds-for-nearest-neighbors">5.1 Lower bounds for nearest neighbors</h3>
<ul>
<li><span class="math inline">\(M\)</span> movies, <span class="math inline">\(k\)</span> genres
<ul>
<li><span class="math inline">\(\ve{h}_0 = s\)</span>, <span class="math inline">\(h\in \{0,1\}^k\)</span></li>
<li>Draw movies <span class="math inline">\(\ve{x}_0=T\)</span>.</li>
</ul></li>
<li>For <span class="math inline">\(T\ll \sqrt m\)</span>, can’t learn using NN because
<ul>
<li>Users will share few movies in common.</li>
<li>Users who share movies won’t share genres. (Construct example where some movies belong to all genres.)</li>
</ul></li>
</ul>
<h3 id="section-9">5.3</h3>
<ul>
<li><span class="math inline">\(k\)</span> genres</li>
<li><span class="math inline">\(T=\Om(\ln M)\)</span> ratings per user</li>
<li><span class="math inline">\(s\)</span> genres per user</li>
<li><span class="math inline">\(\ell(h) = \sgn(\an{w,2h-1})\)</span>.</li>
</ul>
<p>Can do semi-supervised learning by doing representation learn using [AKM16]. (S4.1)</p>
<h2 id="notes">Notes</h2>
<ul>
<li>Check Thm. 4.1 using [AKM16] - review “condition number”.</li>
<li>Check Thm. 5.1. Look up background on NN.</li>
<li>Check Thm. 5.4.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>(AKV17) A sparse recovery view of sentence embeddings, bag of n-grams, and LSTMs</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/BONGs.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/BONGs.html</id>
    <published>2017-07-18T00:00:00Z</published>
    <updated>2017-07-18T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>(AKV17) A sparse recovery view of sentence embeddings, bag of n-grams, and LSTMs</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-07-18 
          , Modified: 2017-07-18 
	</p>
      
       <p>Tags: <a href="/tags/sparse%20recovery.html">sparse recovery</a>, <a href="/tags/n-grams.html">n-grams</a>, <a href="/tags/lstm.html">lstm</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <ul>
<li>LSTMs can do as well as linear classification over BonGs.</li>
<li>Calderbank: SVM can do as well given compressed <span class="math inline">\(Ax\)</span> instead of <span class="math inline">\(x\)</span>, if <span class="math inline">\(A\)</span> is RIP.
<ul>
<li>But word vectors are not RIP, not even statistically.</li>
</ul></li>
<li>Basis pursuit/LASSO does better on word embeddings than random embeddings, even though word embeddings aren’t incoherent/RIP.
<ul>
<li>What’s a good explanation? Word vectors in sentence are often linearly separable from other vectors - why?</li>
<li>MP/OMP does worse on word embeddings.</li>
</ul></li>
</ul>
<p>(RIP stronger than incoherency? Ex. incoherency can have linear dependencies… Incoherency gives something like statistical RIP?)</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2017-07-22</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2017-07-22.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2017-07-22.html</id>
    <published>2017-07-18T00:00:00Z</published>
    <updated>2017-07-18T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2017-07-22</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-07-18 
          , Modified: 2017-07-18 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#current-projects">Current projects</a></li>
 <li><a href="#reading">Reading</a></li>
 <li><a href="#explorations">Explorations</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="current-projects">Current projects</h2>
<ul>
<li>Langevin (Mon)</li>
<li>EGNN</li>
<li>NLP: (Mon)
<ul>
<li>BoNGs: make recovery work for <span class="math inline">\(n\)</span>-grams, <span class="math inline">\(n\ge 2\)</span>.</li>
<li>Treegrams
<ul>
<li>cf. Sida Wang</li>
<li>hyperdim vectors</li>
</ul></li>
<li>document embedding (axioms?)</li>
<li>GANs for BoNGs</li>
</ul></li>
<li>Long-term memory (COLT open problem) (Tue)
<ul>
<li>For convex optimization</li>
<li>Tue: this seems difficult because of “bottleneck” of probability <span class="math inline">\(\ll \rc{\poly(n)}\)</span>. Next step: familiarize with lower-bound techniques and try to prove lower bound.</li>
</ul></li>
<li>Reinforcement learning (experiments - what to do?)</li>
</ul>
<h2 id="reading">Reading</h2>
<ul>
<li><a href="https://workflowy.com/#/cc7e392e4fff">Do GANs learn?</a></li>
<li><a href="../tcs/machine_learning/representation/AR17.md">Representation learning</a></li>
<li>BONGs</li>
<li>Hyperparameter tuning</li>
</ul>
<h2 id="explorations">Explorations</h2>
<ul>
<li>Further on temperature varying and Langevin
<ul>
<li>AIS/RAISE estimator - similar criterion?
<ul>
<li>See [BGS14] Accurate and Conservative Estimates of MRF Log-likelihood using Reverse Annealing</li>
<li>“Multiplicative” weights</li>
</ul></li>
<li>Other settings where annealing helps. Analogue on Boolean cube.</li>
<li>Tensor decomposition.
<ul>
<li>Beyond the homotopy method.</li>
<li>[MR16]</li>
</ul></li>
</ul></li>
<li>Co-training - predict one part from other. Relating to/extending CCA.</li>
<li>RL - kernel?</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>(TPGB17) The space of transferable adversarial examples</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/TPGB17.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/TPGB17.html</id>
    <published>2017-05-24T00:00:00Z</published>
    <updated>2017-05-24T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>(TPGB17) The space of transferable adversarial examples</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-05-24 
          , Modified: 2017-05-24 
	</p>
      
       <p>Tags: <a href="/tags/neural%20nets.html">neural nets</a>, <a href="/tags/aaml.html">aaml</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#adversarial-subspaces">Adversarial subspaces</a></li>
 <li><a href="#decision-boundaries">Decision boundaries</a><ul>
 <li><a href="#experiments">Experiments</a></li>
 </ul></li>
 <li><a href="#limits-of-transferability">Limits of transferability</a></li>
 </ul> </div>

  <div class="blog-main">
    <!--
See also 

* [intro to problem](adversarial.html).
* [my experiments](adversarial_experiments.html). 
* [confidence](confidence.html).
-->
<h2 id="adversarial-subspaces">Adversarial subspaces</h2>
<ul>
<li>Introduces methods for discovering a subspace of adversarial perturbations.
<ul>
<li>MNIST: 25 dimensions</li>
</ul></li>
<li>Distance traveled before reaching decision boundary is on average larger than distance separating decision boundaries of 2 models in that direction. (This doesn’t seem surprising.)</li>
</ul>
<p>Recall that FGSM is <span class="math display">\[
x^* = x + \ep \nv{\nb_x J(x,y)}.
\]</span></p>
<p>Techniques:</p>
<ul>
<li>Solve optimization problem multiple times, constraining the next direction to be orthogonal to the previous. <strong>GAAS works well, others don’t.</strong>
<ul>
<li>Second-order approximation of loss function <span class="math inline">\(\max_{\ve{r}\le \ep} g^T r + \rc 2 r^T H r\)</span>.</li>
<li>Convex optimization: Write an LP for the region where <span class="math inline">\(f\)</span> is piecewise linear, and throw in orthogonality condition.</li>
<li>GAAS (gradient aligned adversarial subspace): Find orthogonal <span class="math inline">\(r_1,\ldots, r_k\)</span> with <span class="math display">\[\ve{r_i}_2\le \ep, \quad r_i^T \nb_x J(x,y)\ge \ga.\]</span> (Think of a right-angled simplex with vertex at <span class="math inline">\(x\)</span>. We can compute how many there are given desired <span class="math inline">\(\fc{g^Tr_i}{\ve{g}_2}\)</span>: <span class="math inline">\(\min\{\ff{1}{\al^2}, d\}\)</span>.)</li>
<li>JSMA: partition most salient features into <span class="math inline">\(k\)</span> bins; use these <span class="math inline">\(k\)</span> sets to get <span class="math inline">\(k\)</span> orthogonal perturbations.</li>
</ul></li>
</ul>
<p>For DNN, get 44 directions, 25 of which transfer. For CNN, get 15 directions, 2 of which transfer.</p>
<h2 id="decision-boundaries">Decision boundaries</h2>
<p>Adversarial training does not significantly displace decision boundary.</p>
<p>Define unit norm directions <span class="math display">\[
d(f,x) := \fc{x'-x}{\ve{x'-x}}
\]</span> where <span class="math inline">\(x'\)</span> is defined differently in 3 cases:</p>
<ol type="1">
<li>Legitimate direction <span class="math inline">\(d_{leg}\)</span>: <span class="math inline">\(x'\)</span> is closest data point with different class label.</li>
<li>Adversarial example <span class="math inline">\(d_{adv}\)</span>: adversarial example generated from <span class="math inline">\(x\)</span>.</li>
<li><span class="math inline">\(d_{rand}\)</span> random <span class="math inline">\(x'\)</span> in input domain that is classified differently.</li>
</ol>
<p>Define minimum distance <span class="math display">\[
MD_d(f,x) = \amin_{\ep&gt;0} f(x+\ep \cdot d) \ne f(x)
\]</span> and interboundary distance as <span class="math display">\[
ID_d(f_1,f_2,x) = 
|MD_d(f_1,x) - MD_d (f_2,x)|
\]</span></p>
<h3 id="experiments">Experiments</h3>
<p>Transfer from</p>
<ul>
<li>logistic regression (LR)</li>
<li>support vector machine (SVM)</li>
<li>DNN</li>
</ul>
<p>Defenses only prevent white-box attacks by reducing reliability of 1st order approximations (gradient masking).</p>
<h2 id="limits-of-transferability">Limits of transferability</h2>
<p>This hypothesis is false: If 2 models achieve low error while exhibiting low robustness, then adversarial examples transfer between models.</p>
<p>Ex. Adversarial examples on MNIST don’t transfer between linear and quadratic models.</p>
Model-agnostic perturbation: For a fixed feature mapping <span class="math inline">\(\phi\)</span>, define <span class="math inline">\(\de_\phi\)</span> as difference in intra-class means, and the adversarial direction <span class="math inline">\(r_\phi\)</span> for <span class="math inline">\((x,y)\)</span>,
<span class="math display">\[\begin{align}
\de_\phi:&amp;=\rc 2 (\E_{\mu_{+1}} [\phi(x)]
- \E_{\mu_{-1}}[\phi(x)])\\
r_\phi:&amp;= - \ep y \wh \de_\phi.
\end{align}\]</span>
<p>If <span class="math inline">\(f(x) = w^T\phi(x)+b\)</span>, and <span class="math inline">\(\De:=\wh w^T \wh\de_\phi\)</span> is large, and <span class="math inline">\(\phi\)</span> is “pseudo-linear” (<span class="math inline">\(\phi(x+r)-\phi(x)\approx r_\phi\)</span>) then <span class="math inline">\(x+r\)</span> transfers to <span class="math inline">\(f\)</span>.</p>
<p>TLDR: shift points in direction of difference of class means; this transfers well.</p>
<p>Can models with access to same set of input features learn representations that don’t transfer?</p>
<p>There’s a simple (but not very informative…) example where this works: MNIST with XOR artifacts trained on linear and quadratic.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2017-04-29</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2017-04-29.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2017-04-29.html</id>
    <published>2017-04-25T00:00:00Z</published>
    <updated>2017-04-25T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2017-04-29</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-04-25 
          , Modified: 2017-04-25 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#day-by-day">Day by day</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="day-by-day">Day by day</h2>
<ul>
<li><p>Mon. 4-24: If I can bound <span class="math display">\[
\an{\mathcal L g, \an{\nb f, \nb g}} \ll \an{\mathcal L g, \mathcal L g}
\]</span> independent of <span class="math inline">\(f\)</span>, then I can argue that for small <span class="math inline">\(\de\)</span>, eigenvectors for Langevin on <span class="math inline">\((1-\de)f\)</span> are close to eigenvectors for Langevin on <span class="math inline">\(f\)</span>. (One has to be careful with <a href="/posts/math/algebra/linear/matrix_analysis/perturbation.html">which eigenspaces to work with</a>.)</p>
I don’t know how to do this. The best I can do is
<span class="math display">\[\begin{align}
\an{f, \mathcal L_\mu f}_\mu &amp;\le -k \ve{f}_\mu^2\\
\implies \an{f, \mathcal L_{\mu'} f}_{\mu'} &amp;\le -\fc{k}{1+O(\de)}\ve{f}_{\mu'}^2.
\end{align}\]</span>
where <span class="math inline">\(\mu'\)</span> is for <span class="math inline">\((1-\de)f\)</span>, which gives an angle between eigenspaces of <span class="math inline">\(1+O\pf{\la_k}{\la_l}\)</span> where <span class="math inline">\(\la_k,\la_l\)</span> are the threshold values for the eigenspaces. This does NOT go to 1. I need something that goes to 1.</li>
<li>Tue. 4-25:
<ul>
<li>Perceptron writeup.</li>
<li>Experiment with semi-random features.
<ul>
<li>Distribution is normal, square loss. (But maybe square loss is just a proxy?)</li>
<li>Doesn’t seem to work: loss on batch of 100 is on order of 20, even with 50 nodes.</li>
</ul></li>
<li>Can estimate gaussian robustly given samples from something with <span class="math inline">\(L^\iy\)</span> distance (in log space) <span class="math inline">\(h\)</span> with <span class="math inline">\(\wt O (h^2)\)</span> samples. Can we extend this to convex things?</li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Adversarial thoughts</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/adversarial_thoughts.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/adversarial_thoughts.html</id>
    <published>2017-04-16T00:00:00Z</published>
    <updated>2017-04-16T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Adversarial thoughts</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-04-16 
          , Modified: 2017-04-16 
	</p>
      
       <p>Tags: <a href="/tags/neural%20nets.html">neural nets</a>, <a href="/tags/uncertainty.html">uncertainty</a>, <a href="/tags/aaml.html">aaml</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#not-surprising">Not surprising</a></li>
 <li><a href="#a-formalization">A formalization</a></li>
 <li><a href="#a-key-question">A key question</a></li>
 <li><a href="#several-subproblems">Several subproblems</a></li>
 <li><a href="#hypotheses-and-approaches">Hypotheses and approaches</a><ul>
 <li><a href="#glue-approach">Glue approach</a></li>
 <li><a href="#mixture">Mixture</a></li>
 <li><a href="#sampling-from-nns">Sampling from NN’s</a></li>
 <li><a href="#regularization">Regularization</a></li>
 <li><a href="#conditioning">Conditioning</a></li>
 <li><a href="#thresholding-and-quantization">Thresholding and quantization</a></li>
 <li><a href="#is-the-first-layer-the-problem">Is the first layer the problem?</a></li>
 <li><a href="#conservative-concepts-and-detection">Conservative concepts and detection</a></li>
 <li><a href="#more-human-approaches">More human approaches</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <p>See also</p>
<ul>
<li><a href="adversarial.html">intro to problem</a>.</li>
<li><a href="adversarial_experiments.html">my experiments</a>.</li>
<li><a href="confidence.html">confidence</a>.</li>
</ul>
<p>Some explanations for adversarial examples.</p>
<h2 id="not-surprising">Not surprising</h2>
<p>Firstly, I think adversarial examples aren’t very surprising: we shouldn’t expect neural nets to do well against an adversary if we didn’t train against it. Neural nets will do the “laziest” thing, which does not involve the “broader” conceptual class we want them to learn (ex. everything close to a ‘5’ is also a ‘5’); vanilla training doesn’t communicate to them this broader class.</p>
<p>That said, why can’t neural nets do well after adversarial training?</p>
<h2 id="a-formalization">A formalization</h2>
<p>Let <span class="math inline">\(A_a\)</span> be a set of “adversarial” modifications to inputs <span class="math inline">\(x\)</span>. We say that an algorithm adversarially-PAC learns <span class="math inline">\((x,y)\sim D\)</span> if after poly samples and time, it produces <span class="math inline">\(f\in F\)</span> such that <span class="math display">\[
\Pj_{x,y} (\max_a L(f(A_a(x)), y)\ne y) \le
\ep
\]</span> in the realizable case, or <span class="math display">\[
\Pj_{x,y} (\max_a L(f(A_a(x)), y)\ne y) \le
\min_{f\in F} \Pj_{x,y} (\max_a L(f(A_a(x)), y)\ne y) + \ep
\]</span> in the agnostic case, where <span class="math inline">\(L\)</span> is the loss function (ex. 0-1).</p>
<p>We probably also want to assume access to an oracle, which given <span class="math inline">\(f\)</span> and <span class="math inline">\(x\)</span>, produces some <span class="math inline">\(a\)</span> that maximizes <span class="math inline">\(L(f(A_a(x)),y)\)</span>.</p>
<p>Some work has been done here: see Yishay Mansour, Robust learning and inference. Cf. also boosting vs. game theory.</p>
<p>Example: <span class="math inline">\(A_a(x) = x+a\)</span>, restricted to <span class="math inline">\(\ve{a}\le \ep\)</span>.</p>
<p>Now there are settings where the adversarial setting isn’t harder - for example, for SVM, in the above example, if there is still a margin <span class="math inline">\(\ga\)</span> between positive and negative examples even after adversarial perturbation, then we’ll make at most <span class="math inline">\(\rc{\ga^2}\)</span> mistakes by perceptron analysis, which doesn’t care about the distribution.</p>
<p>But this is not the case for neural nets. (The fact that we can get training error to 0 suggests that there may be some “margin” at play (cf. Telgarsky). However, we <em>can’t</em> get to 0 training error if we include adversarial examples!)</p>
<p>Can we study a toy example here like dictionary learning? Generate <span class="math inline">\(Ah = x\)</span>, there is a SVM <span class="math inline">\(\sign(w^Th)=y\)</span>. Make it robust to <span class="math inline">\(+a\)</span>.</p>
<p>(This doesn’t seem to capture what’s going on though in <span class="math inline">\(L^2\)</span> - here the adversary’s best bet is to change in the direction <span class="math inline">\(a_i\)</span> where <span class="math inline">\(i=\amax_i |w_i|\)</span>, which would in fact change the sign of <span class="math inline">\(w^Th\)</span>.)</p>
<h2 id="a-key-question">A key question</h2>
<p>It’s not surprising that adversarial examples exist. A better question is why we can’t “train them away” by normal training methods, and what can we do to fix this.</p>
<p>I think there are 2 possibilities.</p>
<ol type="1">
<li>There exists a neural network, with reasonable parameter sizes (under whatever regularization we are using), that can do well on most adversarial inputs. The issue is that using our optimization methods aren’t finding these parameters.</li>
<li>There is something fundamentally limited about the (typical) neural net architecture that doesn’t capture human concept boundaries.</li>
</ol>
<p>The “margin” analysis above is in favor of (2). (1) could still be true if there are e.g. exponentially more choices of parameters that would do well on normal examples than choices that also do well on the adversarial examples. But this isn’t enough to explain it, because adversarial training stalls eventually.</p>
<h2 id="several-subproblems">Several subproblems</h2>
<p>We can talk about <span class="math inline">\(L^\iy\)</span> or <span class="math inline">\(L^2\)</span> adversarial examples. Also whatever threshold we choose, there shouldn’t be human-adversarial examples below that threshold. (I don’t think this is so much of an issue now.)</p>
<h2 id="hypotheses-and-approaches">Hypotheses and approaches</h2>
<h3 id="glue-approach">Glue approach</h3>
<p>Just take a hodgepodge of things that reduce adversarial examples, including things that detect and reject adversarial examples. For example,</p>
<ul>
<li>check if it has abnormally large coefficients for singular vectors for small singular values.</li>
<li>train a detector for adversarial examples on the internal representation.</li>
</ul>
<p>However, this would be an arms race: likely, for many of these, you can find adversarial examples that pass the test (ex. restrict search to top singular vectors). If this were not the case, that would be <em>very interesting</em>.</p>
<p>Are there things like clipping, normalizing, that you do on input before feeding into the network that could help? (Ex. can damping the smaller singular vectors help?)</p>
<p>These are interesting questions but not as satisfying an approach.</p>
<h3 id="mixture">Mixture</h3>
<p>This doesn’t seem to help.</p>
<p>One question: after doing adversarial training for a while, does the NN still do well against the <em>original</em> adversarial examples? If not, then it’s catastrophically forgetting. If it is forgetting, this should be easy to fix.</p>
<h3 id="sampling-from-nns">Sampling from NN’s</h3>
<p>Train a bunch of NN’s on the real examples and then use majority vote.</p>
<p>Problem: this doesn’t work.</p>
<p>Are we somehow not exploring the space of NN’s which do well on the real examples? Can we use Fisher information, Langevin to sample better?</p>
<p>Probably we can’t do much better. Also the existence of a universal perturbation and the fact that adversarial examples from a “linear” model transfer suggest that most NN’s which do well on real examples suffer from the same adversarial examples. (cf. there being exponentially more NN’s which are weak against these adversarial examples, then good.)</p>
<h3 id="regularization">Regularization</h3>
<p>The objective/regularization is wrong. Ex. we should be encouraging sparsity, using exponentiated gradient/multiplicative weights, etc.</p>
<h3 id="conditioning">Conditioning</h3>
<p>Perhaps the Lipschitz constant is just really large. Can we train a NN to have small Lipschitz constant in the correct norm? (Note this can be challenging because for example the <span class="math inline">\(\iy\to 2\)</span> norm is difficult to compute; even approximating it takes a linear program, which is too computationally intensive to do at every step.)</p>
<p>Also if we can’t find a NN with small Lipschitz constant why not? Does it not exist or is there something wrong with the geometry of the optimization problem?</p>
<p>Path norm, batch normalization seem attractive here.</p>
<h3 id="thresholding-and-quantization">Thresholding and quantization</h3>
<p>ReLUs seem like a very bad idea - the adversary can just keep increasing. But is the problem more for</p>
<ul>
<li>ReLU(x) where <span class="math inline">\(x\)</span> is large, or</li>
<li>ReLU(x) where <span class="math inline">\(x\)</span> is small?</li>
</ul>
<p>The first suggests capping - having <span class="math inline">\(y=ReLU(x)\)</span> grow unlimitedly is a bad idea. (We still have the same problem for sigmoids.) The second suggests that we are getting past thresholds by adding a lot of correlated noise, so we should zero out small ReLU(x)’s.</p>
<p>In the brain we don’t have thresholds exceeded by contributions from hundreds of neurons - there’s some kind of attenuation or normalization (of activations because of limited energy?) that prevents this. What if we just take the top <span class="math inline">\(k\)</span> activations? (During training or testing?)</p>
<p>What about quantization/binarization? For MNIST, binarizing all pixels to be 0 or 1 is fine, and helps against adversarial examples (because small norm changes don’t have much effect - although you should consider other attacks here!). But this is in some sense cheating because MNIST is basically black-white. Can we binarize/quantize intermediate features?</p>
<h3 id="is-the-first-layer-the-problem">Is the first layer the problem?</h3>
<p>I.e. are we just screwed after the first layer because it somehow destroyed the input?</p>
<p>Ex. consider the toy problem of dictionary learning. If the layers of a NN were <em>really</em> doing sparse coding, would this be a problem? I.e. does sparse coding suffer from the same problem? Or are NN really <em>not</em> doing sparse coding? While we sort-of say it is, I don’t think it is at all! For the convolutional NN, the convolution <em>isn’t</em> computing <span class="math inline">\(h\)</span> such that <span class="math inline">\(Ah=x\)</span>. In fact, the overlaps between different patches might make things badly conditioned.</p>
<p>If we actually did DL on the first layer, then solved the sparse recovery problem for every input, would we still have the same problem?</p>
<h3 id="conservative-concepts-and-detection">Conservative concepts and detection</h3>
<p>Have the network be able to “abstain” (like a confidence score). This should be similar in principle to training detection between real and adversarial examples.</p>
<p>What if we use RBF’s on the first layer? Even something like pretraining to find the right RBF’s, and keeping them fixed. The idea is that RBF’s are very conservative. Maybe we’ll need the <span class="math inline">\(L^\iy\)</span> analogue if we’re protecting against that…</p>
<h3 id="more-human-approaches">More human approaches</h3>
<p>A vital thing that’s missing in current NN’s is back-and-forth between higher and lower layers. Humans can reinterpret lower-level data when they see a higher-level pattern.</p>
<p>Also humans have some kind of idea of a “prototype” of a digit - another instance must have its curves match up with that prototype in some fashion. There are these ideas of strokes, etc. Something hierarchical, probabilistic model? cf. Tenenbaum.</p>
<p>It may be that there needs to be something fundamentally new in the architecture - something more principled. Anything from neuroscience?</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Inverse RL</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/inverse_rl.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/inverse_rl.html</id>
    <published>2017-04-03T00:00:00Z</published>
    <updated>2017-04-03T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Inverse RL</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-04-03 
          , Modified: 2017-04-03 
	</p>
      
       <p>Tags: <a href="/tags/reinforcement%20learning.html">reinforcement learning</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#nr00-inverse-reinforcement-learning">[NR00] Inverse reinforcement learning</a></li>
 <li><a href="#hdar16-cooperative-inverse-reinforcement-learning">[HDAR16] Cooperative inverse reinforcement learning</a><ul>
 <li><a href="#formalism">Formalism</a></li>
 <li><a href="#simple-approximation-scheme">Simple approximation scheme</a></li>
 <li><a href="#future-work">Future work</a></li>
 </ul></li>
 <li><a href="#other-papers">Other papers</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="nr00-inverse-reinforcement-learning">[NR00] Inverse reinforcement learning</h2>
<p>Compute constraints that characterize set of reward functions so observed behavior maximizes reward. Use max-margin heuristic.</p>
<h2 id="hdar16-cooperative-inverse-reinforcement-learning">[HDAR16] Cooperative inverse reinforcement learning</h2>
<p>Human knows reward function. Robot does not. Robot payoff is human reward.</p>
<p>IRL: <span class="math inline">\(\Pj(u|\te, x_0) \propto e^{U_\te(x_0,u)}\)</span>.</p>
<ul>
<li>Reduction to POMDP and sufficient statistics</li>
<li>Apprenticeship learning and suboptimality of IRL-like solutions (because H can use a suboptimal action to convey more information to R).</li>
</ul>
<p>Desiderata:</p>
<ol type="1">
<li>Leverage action to improve learning.</li>
<li>Human is not uninterested expert, but cooperative teacher.</li>
</ol>
<h3 id="formalism">Formalism</h3>
<ul>
<li><span class="math inline">\(\Te\)</span>: static reward parameters observed by <span class="math inline">\(H\)</span>.</li>
<li><span class="math inline">\(R: S\times A^H\times A^R\times \Te \to \R\)</span>, reward.</li>
<li><span class="math inline">\(\ga\)</span>: discount</li>
</ul>
<p>At time <span class="math inline">\(t\)</span>, observe <span class="math inline">\(s_t\)</span> and select action <span class="math inline">\(a_t^H, a_t^R\)</span>. Both achieve reward <span class="math inline">\(r_t=R(s_t,a_t^H, a_t^R;\te)\)</span>.</p>
<p>Note: decentralized POMDP - compute optimal joint policy is NEXP-complete.</p>
<p>Here, private info is restricted to <span class="math inline">\(\te\)</span>, so reduction to coordination-POMDP does not blow up state space. (<span class="math inline">\(|S_C|=|S||\Te|\)</span>). (State is tuple or world state, reward parameters, and R’s belief.)</p>
<p>Belief about <span class="math inline">\(\te\)</span> is sufficient statistic for optimal behavior. <span class="math inline">\((\pi^{H*},\pi^{R*})\)</span> depends only on current state and R’s belief.</p>
<p>Apprenticeship learning: imitate demonstrations.</p>
<p>ACIRL: 2 phases, human and robot takes turns; then robot acts independently (deployment).</p>
<p>Ex. With linear dependence on <span class="math inline">\(\te\)</span>, in deployment, optimal policy is to maximize reward induced by mean.</p>
<p>DBE (demonstration by expert): greedily maximizes immediate reward. Best response is to compute posterior over <span class="math inline">\(\te\)</span>.</p>
<p>There exist ACIRL games where <span class="math inline">\(br(br(\pi^E))\ne \pi^E\)</span>.</p>
<!--Human objective $U_\te(x_0,u_R,u_H)$.-->
<ul>
<li>? Seems to require human knowing how robot learns. Unrealistic teaching assumption.</li>
<li>? Is reward observed by robot? No.</li>
</ul>
<h3 id="simple-approximation-scheme">Simple approximation scheme</h3>
<p>Suppose reward is <span class="math inline">\(\phi(s)^T\te\)</span>.</p>
<p><span class="math display">\[\tau^H = \amax_\tau \phi(\tau)^T \te - \eta\ve{\phi_\te-\phi(\tau)}^2.\]</span></p>
<p>Optimal <span class="math inline">\(\pi^R\)</span> under DBE tries to match observed feature counts. (<strong>I don’t get this.</strong>.)</p>
<h3 id="future-work">Future work</h3>
<p>Coordination problem.</p>
<h2 id="other-papers">Other papers</h2>
<ul>
<li>[RA07], [Z…08]: <span class="math inline">\(\pi^H\)</span> is noisy expert. Bayesian approach: prior on rewards, vs. prior on reward functions.</li>
<li>[Nat…10] observe cooperating multiple actors.</li>
<li>[Wau…11], [KS15]: infer payoffs from observed behavior in general.</li>
<li>[Fer…14], hidden-goal MDP, goal unobserved. Human as part of environment</li>
<li>[CL12] Teach learner reward for MDP.</li>
<li>[DS13] Motion best communicating agent’s intention.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Confidence in neural nets</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/confidence.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/confidence.html</id>
    <published>2017-04-03T00:00:00Z</published>
    <updated>2017-04-03T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Confidence in neural nets</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-04-03 
          , Modified: 2017-04-03 
	</p>
      
       <p>Tags: <a href="/tags/neural%20net.html">neural net</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#hg17-baseline-for-detecting-misclassified-and-out-of-distribution-examples">[HG17] Baseline for detecting misclassified and out-of-distribution examples</a><ul>
 <li><a href="#abnormality-detection-with-auxiliary-decoders">Abnormality detection with auxiliary decoders</a></li>
 <li><a href="#discussion">Discussion</a></li>
 </ul></li>
 <li><a href="#hg17-early-methods-for-detecting-adversarial-images">[HG17] EARLY METHODS FOR DETECTING ADVERSARIAL IMAGES</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>Related: <a href="adversarial.html">adversarial examples</a>.</p>
<p>From AAML: RBF/conservative classifier for in vs. out-of-distribution examples. <a href="/posts/cs/ai/control/aaml_workshop.html">AAML workshop notes</a></p>
<p>Q: how go get a neural net to keep a confidence bound?</p>
<h2 id="hg17-baseline-for-detecting-misclassified-and-out-of-distribution-examples">[HG17] Baseline for detecting misclassified and out-of-distribution examples</h2>
<p>High-confidence predictions frequently produced by softmaxes. Ex. random Gaussian noise gives 90+% confidence. (Q: what if you do before softmaxes?)</p>
<p>Prediction probability of incorrect/ood examples are lower.</p>
<p>Give tasks to evaluate.</p>
<p>2 problems</p>
<ol type="1">
<li>error and success prediction: Can we predict whether a classifier will make an error on a held-out test example? (Use this to output <span class="math inline">\(\perp\)</span>.) Tradeoff between false negatives and positives.</li>
<li>In/out-of-distribution detection: Predict whether test example is from different distribution.</li>
</ol>
<p>ROC (receiver operating characteristic) shows <span class="math display">\[
\pa{tpr = \fc{tp}{tp+fn}, fpr = \fc{fp}{fp+tn}}.
\]</span> PR (precision-recall) shows <span class="math display">\[
\pa{\text{precision} = \fc{tp}{tp+fp}, \text{recall} = \fc{tp}{tp+fn}}.
\]</span></p>
<ul>
<li>AUROC is prob that a positive example has greater score than negative example. Not great when different base rates.</li>
<li>AUPR (precision-recall)</li>
</ul>
<h3 id="abnormality-detection-with-auxiliary-decoders">Abnormality detection with auxiliary decoders</h3>
<ul>
<li>Train normal classifier and append auxiliary decoder which reconstructs input. (Blue layers)</li>
<li>Train jointly on in-distribution examples. Freeze blue layers. Train red layers (on top) with clean and noised training examples. (Noised are abnormal.)</li>
</ul>
<p>Improves detection.</p>
<h3 id="discussion">Discussion</h3>
<ul>
<li>Baseline beaten by exploiting representations.</li>
<li>Intra-class variance: if distance from example to another is abnormally high, may be out of distribution.</li>
<li>known-unknown vs. unknown-unknown.</li>
</ul>
<h2 id="hg17-early-methods-for-detecting-adversarial-images">[HG17] EARLY METHODS FOR DETECTING ADVERSARIAL IMAGES</h2>
<p>Adversarial images place abnormal emphasis on lower-ranked principal components from PCA.</p>
<p>(Q: can you do this even independent of PCA - just by looking at e.g. wavelet/Fourier coefficients? Also, what if you adversarially keep PCA components low, incorporate weighted norm into adversarial optimization?)</p>
<p>Use variance of PCA coefficients of whitened images to detect. (What is whitening again?)</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
