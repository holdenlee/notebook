---
title: Weekly summary 2016-09-24
published: 2016-09-23
modified: 2016-09-23
tags: none
type: summary
showTOC: True
---

[Last week](2016-09-17.html).

# Threads

* PMI: WSVD + DL/NMF. [PMI for images](/posts/tcs/machine_learning/neural_nets/pmi_images.html)
	* Used NMF code from [here](http://www.ee.columbia.edu/~grindlay/code.html).
* [DL generalization](/posts/tcs/machine_learning/matrices/DL_generalization.html)
	* Can we tackle NMF using similar algorithm as DL---trying to isolate a column of A by looking at samples which are close to a pair of samples? Before, closeness was inner product; now use another kernel? Ex. TV distance, or KL, or some kind of regularized KL. Ex. 
		* In what sense is this important? I would still rely on a distributional assumption on the $x$'s. (DL algorithm required this: sparsity and some kind of independence.) But topic modeling already solves this.
		* Why can't we solve NMF using the topic modeling, saying that the distribution is simply the samples that we get?
* SoS

TODO: run again with 1800-dimensional.

TODO: Review polysemy paper, and understand CKN paper well to rederive equations.
