---
title: Weekly summary 2017-04-29
published: 2017-04-25
modified: 2017-04-25
tags: none
type: summary
showTOC: True
---

# Day by day

*   Mon. 4-24: If I can bound
	$$
	\an{\mathcal L g, \an{\nb f, \nb g}} \ll \an{\mathcal L g, \mathcal L g}
	$$
	independent of $f$, then I can argue that for small $\de$, eigenvectors for Langevin on $(1-\de)f$ are close to eigenvectors for Langevin on $f$. (One has to be careful with [which eigenspaces to work with](/posts/math/algebra/linear/matrix_analysis/perturbation.html).)
	
	I don't know how to do this. The best I can do is
	\begin{align}
	\an{f, \mathcal L_\mu f}_\mu &\le -k \ve{f}_\mu^2\\
	\implies \an{f, \mathcal L_{\mu'} f}_{\mu'} &\le -\fc{k}{1+O(\de)}\ve{f}_{\mu'}^2.
	\end{align}
	where $\mu'$ is for $(1-\de)f$, 
	which gives an angle between eigenspaces of $1+O\pf{\la_k}{\la_l}$ where $\la_k,\la_l$ are the threshold values for the eigenspaces. This does NOT go to 1. I need something that goes to 1.
*   Tue. 4-25:
	* Perceptron writeup.
	* Experiment with semi-random features.
	    * Distribution is normal, square loss. (But maybe square loss is just a proxy?)
		* Doesn't seem to work: loss on batch of 100 is on order of 20, even with 50 nodes.
	* Can estimate gaussian robustly given samples from something with $L^\iy$ distance (in log space) $h$ with $\wt O (h^2)$ samples. Can we extend this to convex things?

