---
title: Weekly summary 2016-12-10
published: 2016-12-10
modified: 2016-12-10
tags: reinforcement learning, control theory
type: summary
showTOC: True
---

See also [last week](2016-12-03.html) for unfinished threads.

# RL project

* Learned [control theory](../tcs/machine_learning/reinforcement_learning/control_theory.html). (Ch. 1-5)
* Wrote up 
	* [Discrete LQR](https://onedrive.live.com/view.aspx?cid=5f18ce7c7533d083&id=documents&resid=5F18CE7C7533D083%21465&app=OneNote&authkey=!ADwaKCUz_Q1nRSM&&wd=target%28%2F%2FResearch.one%7C3dc2fa9e-847a-4ff2-ba10-094a253311df%2FLinear%20quadratic%20regulator%7Cd61f5b02-9dac-4a89-b019-11c61dc2baa2%2F%29)
	* [Continuous LQR](../tcs/machine_learning/reinforcement_learning/lqr.html)
	
# Alexa/NLP experiments

* [Minutes](https://workflowy.com/#/ca56745b6f0e)
* [EMNLP](../tcs/machine_learning/nlp/emnlp.html)

# Directions

* Learning the LQR: Suppose we get a noisy observation of reward, and possibly there is noise in the dynamical system. We don't know the parameters (matrices). Can we "PAC learn" the optimal policy?
	* Have to define "PAC learn" in this context.
	* I think so. cf. Contextual bandits, UCRL, LinUCB. There will be a lot of details to work out and a lot of possible ways to state a result. The question is whether this is actually valuable to work on.
	* This is more the optimization standpoint, not the "existence of good solution" standpoint.
* Replace the quadratic function by a convex function $f$.
	* Suppose we know the dynamics and $f$. Approximate the optimal policy. (Ex. Find a simple class of functions such that optimizing within this class will give an approximate solution.)
	* So here we just care about "existence of good solution" (in a convenient class).
	* Start with linear controls first. How do they do?
	* Optimization standpoint: given a (nice, ex. linear) class of functions (for value or for policy) find the optimal within that class knowing dynamics including $f$.
* Combine the previous two: don't know dynamics or $f$, and learn it!
