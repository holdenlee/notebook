---
title: Weekly summary 2017-09-09
published: 2017-09-05
modified: 2017-09-05
tags: none
type: summary
showTOC: True
---

# Current projects

* Reinforcement learning
	* LQR
	* Experiments
	* Kernel
* Next steps with sampling problems, temperature-varying
	* Understand AIS/RAISE
	* AIS/RAISE estimator - similar criterion?
		* See [BGS14] Accurate and Conservative Estimates of MRF Log-likelihood using Reverse Annealing
		* "Multiplicative" weights
	* Other settings where annealing helps. Analogue on Boolean cube.
	* Tensor decomposition. 
		* Beyond the homotopy method.
		* [MR16]
* EGNN
* NLP: 
	* BoNGs: make recovery work for $n$-grams, $n\ge 2$.
	* Treegrams
		* cf. Sida Wang
		* hyperdim vectors
	* document embedding (axioms?) 

# Sidelined 

* Long-term memory (COLT open problem) (Tue)
	* For convex optimization
	* Tue: this seems difficult because of "bottleneck" of probability $\ll \rc{\poly(n)}$. Next step: familiarize with lower-bound techniques and try to prove lower bound.
	
# Logic learning (with Kiran)

* Coming up with the objective
	* Ex. restrict to separable $\sum w_i f_i$.
* Learn representations with desired properties by human feedback
* IRL, CIRL. Learn human references, loss function.
* Picking out part of image that matters
* Graphs?
* Recovery as good property to have?
* Learn logic/PCFG over curriculum without supervised data. Language generation.
* Readings (Hrishikesh)
	* Probabilistic sentential decision diagrams
		* [KVCD14] PSSD
		* [D11] SDD
		* [N86] Probabilistic logic
	* Markov random fields
	* Fuzzy logic
	* Continuous representations of boolean functions
	* Neural tensor machines (matrix product)
* Learning language games through interaction
	* [Naturalizing PL](https://worksheets.codalab.org/worksheets/0xbf8f4f5b42e54eba9921f7654b3c5c5d/)
	* [Blocks grammar](https://github.com/holdenlee/Blocks)
* Learning grammar: see messenger.
	* [TH] Unsupervised learning of probabilistic context-free grammar using iterative biclustering
	* [CTC] Automatic Learning of Context-Free Grammar
	
Code

* [learn-grammar](https://github.com/holdenlee/learn-grammar)
* [Blocks](https://github.com/holdenlee/Blocks)
* [MathGrammar](https://github.com/holdenlee/MathGrammar)
	
## Misc

Instead of a log-linear model with features from (logical forms, canonical utterances), have something more principled.

Semantic Parsing via Paraphrasing

Agenda-based parsing: reduce from n^3

Unanimous Prediction for 100% Precision with Application to
Learning Semantic Mappings: certainty

Learning Executable Semantic Parsers for
Natural Language Understanding: score derivations with log-linear (survey)

Bringing machine learning and compositional
semantics together (survey)

Simpler Context-Dependent Logical Forms via Model Projections: add context dependence.

Inferring Logical Forms From Denotations: getting consistent logical forms

Building a Semantic Parser Overnight: entire pipeline, with crowdsourcing

"To generate candidate logical forms, we use a
simple beam search" - it seems better to parse, keep top from beam search, and then convert?

moral: throw in lots of features

Paraphrasing and transformations

The cat was chased by a dog.
The cat was bitten.

If CFG doesn't have too much "latent", everything is close to surface, then have hope? Prevents combinatorial blowup of possible hidden states/transitions.

[Beyond CFGs](https://www.uio.no/studier/emner/matnat/ifi/INF2820/v12/undervisningsmateriale/unification.pdf)

Have a superset of the right rules. Now use gradient update to find which best explain it. Also would like to match things that transform, like "You want x." and "What do you want?"

# Neuroscience reading

+ https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5021692/ - the brain optimizing cost functions, ie the generalization of gradient descent, as a hypothesis for how local neuron wiring is learned.
+ http://www.cell.com/neuron/abstract/S0896-6273(17)30509-3 - this is the one that takes these others and generalizes them to the whole brain, in particular this explains how complicated architectures in deep learning can be used to understand the large scale functional architecture of the brain
+ http://physrev.physiology.org/content/physrev/95/3/853.full.pdf - overview of the human reward system
+ http://www.nature.com/neuro/journal/v19/n3/abs/nn.4244.html - comparing supervised deep learning to the (unsupervised probably) vision system
+ http://rstb.royalsocietypublishing.org/content/371/1705/20160278 - actually physically modeling readings from the brain with the functional statistics from deep learning models trained on the same tasks
	
# AISFP preparation

* Write up AI safety thoughts (from books, etc.)
* Review:
	* Lob's theorem
	* Decision theories

