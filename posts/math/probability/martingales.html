<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

  <meta name="description" content="Holden Lee's Research Notebook">
  <meta name="author" content="Holden Lee">
    
  <title>Martingales</title>

  <!-- Bootstrap core CSS -->
  <link href="../../../bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

  <!-- Custom styles for this template -->
  <link href="../../../css/blog.css" rel="stylesheet">
  <link href="../../../css/default.css" rel="stylesheet">

  <!-- Extension : Footnotes -->
  <link href="../../../footnotes/css/footnotes.css" rel="stylesheet">

  <!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->
  

</head>

<body>

<!-- Navigation bar. navbar-inverse is black, navbar-default is white.-->
<!-- To make a button active (pressed), use <li class="active"> -->
<div id="header">
  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../">Notebook</a>
      </div>
      <div id="navbar" class="collapse navbar-collapse">
        <ul class="nav navbar-nav">
          <li><a href="../../../">Home</a></li>
          <li><a href="../../../sitemap.html">Sitemap</a></li>
<!-- TODO: Distinguish between PAPERS, RESEARCH QUESTIONS, BOOKS -->
<!-- TODO: make this part a for loop over main pages -->
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </nav>
</div>
<!-- Content -->
<!--div id="content">
  <h1>Mental Wilderness</h1>-->



<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Martingales</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-08 
          , Modified: 2016-04-08 
	</p>
      
       <p>Tags: <a href="../../../tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#conditional-expectation">Conditional expectation</a><ul>
 <li><a href="#examples">Examples</a></li>
 <li><a href="#properties">Properties</a></li>
 </ul></li>
 <li><a href="#martingales-almost-sure-convergence">Martingales, Almost Sure Convergence</a><ul>
 <li><a href="#exercises">Exercises</a></li>
 </ul></li>
 <li><a href="#examples-1">Examples</a></li>
 <li><a href="#doobs-inequality-lp-convergence">Doob’s inequality, <span class="math inline"><em>L</em><sup><em>p</em></sup></span> convergence</a><ul>
 <li><a href="#exercises-1">Exercises</a></li>
 </ul></li>
 <li><a href="#uniform-integrability-convergence-in-l1">5 Uniform integrability; convergence in <span class="math inline"><em>L</em><sup>1</sup></span></a><ul>
 <li><a href="#exercises-2">Exercises</a></li>
 </ul></li>
 <li><a href="#backwards-martingales">6 Backwards Martingales</a><ul>
 <li><a href="#examples-2">Examples</a></li>
 </ul></li>
 <li><a href="#optional-stopping-theorems">7 Optional stopping theorems</a><ul>
 <li><a href="#example">Example</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <p>Durrett Ch. 5</p>
<h2 id="conditional-expectation">Conditional expectation</h2>
<p>Define conditional expectation and show it is well defined.</p>
<p>Let <span class="math inline">\(X\)</span> be a random variable on <span class="math inline">\((\Om, \mathcal F_0, P)\)</span> with <span class="math inline">\(\E|X|&lt;\iy\)</span>. The <strong>conditional expectation</strong> of <span class="math inline">\(X\)</span> given <span class="math inline">\(\mathcal F\)</span> is any (the unique) random variable <span class="math inline">\(Y\)</span> such that</p>
<ol type="1">
<li><span class="math inline">\(Y\in \mathcal F\)</span>.</li>
<li>for all <span class="math inline">\(A\in \mathcal F\)</span>, <span class="math inline">\(\int_A X\,dP = \int_B Y\,dP\)</span>.</li>
</ol>
<p>We show uniqueness and existence. First note that any <span class="math inline">\(Y\)</span> satisfying the above is integrable: show <span class="math inline">\(\E |Y|\le \E |X|\)</span> by partitioning based on <span class="math inline">\(\sgn(Y)\)</span>.</p>
<ol type="1">
<li>Uniqueness (up to a.s.): We have <span class="math inline">\(0=\int_A Y-Y'\, dP\)</span>. Now take <span class="math inline">\(A=\{Y-Y'\ge \ep&gt;0\}\)</span>.</li>
<li><p>Existence is based on the <strong>Radon-Nikodym Theorem</strong>. Let <span class="math inline">\(\mu,\nu\)</span> be <span class="math inline">\(\si\)</span>-finite measures on <span class="math inline">\((\Om, \mathcal F)\)</span>. If <span class="math inline">\(\nu\ll \mu\)</span> (<span class="math inline">\(\nu\)</span> is absolutely continuous with respect to <span class="math inline">\(\mu\)</span>, i.e., <span class="math inline">\(\mu(A)=0\implies \nu(A)=0\)</span>), then there is a function <span class="math inline">\(f\in \mathcal F\)</span> such that for all <span class="math inline">\(A\in \mathcal F\)</span>, <span class="math display">\[\int_A f\,d\mu = \nu(A).\]</span> Write <span class="math inline">\(f=\dd{v}{\mu}\)</span>, and call it the <strong>Radon-Nikodym derivative</strong>.</p>
<p>For <span class="math inline">\(X\ge 0\)</span>, we have <span class="math display">\[ \E(X|\mathcal F) = \dd{\nu}{\mu}\]</span> where <span class="math inline">\(\mu=P\)</span> and <span class="math inline">\(\nu(A) = \int_A X\,dP\)</span>. For general <span class="math inline">\(X\)</span>, write <span class="math inline">\(X=X^+-X^-\)</span>.</p></li>
</ol>
<h3 id="examples">Examples</h3>
<p>The general strategy for finding the conditional expectation is to “guess and verify”.</p>
<ol type="1">
<li>If <span class="math inline">\(X\in \mathcal F\)</span> the <span class="math inline">\(\E[X|\mathcal F]=X\)</span>.</li>
<li>If <span class="math inline">\(X\)</span> is independent of <span class="math inline">\(\mathcal F\)</span> then <span class="math inline">\(\E[X|\mathcal F]=\E X\)</span>.</li>
<li><p>(Relate to conditional probability) If <span class="math inline">\(\mathcal F = \si(\Om_1,\ldots)\)</span> and each <span class="math inline">\(\Om_i\)</span> has positive probability, then <span class="math inline">\(\E[X|\mathcal F] = \fc{\E[X;\Om_i]}{\Pj(\Om_i)}\)</span> on <span class="math inline">\(\Om_i\)</span>.</p>
Generalize conditional probability by letting
\begin{align}
\Pj(A|\mathcal G) &amp;= \E[1_A|\mathcal G]\\
\Pj(A|B) &amp;= \fc{P(A\cap B)}{P(B)}\\
\E[X|Y] &amp;= \E[X|\si(Y)].
\end{align}</li>
<li><p>If the probability density of <span class="math inline">\((X,Y)\)</span> is given by <span class="math inline">\(f(x,y)\)</span>, then <span class="math display">\[\E[g(X)|Y] = h(Y)\]</span> where <span class="math display">\[h(y) = \fc{\int g(x) f(x,y)\dx}{\int f(x,y)\dx}.\]</span> (5 and 6 omitted.)</p></li>
</ol>
<h3 id="properties">Properties</h3>
<ol type="1">
<li>Linearity: <span class="math inline">\(\E[aX+Y|\mathcal F] = a\E[X|\mathcal F] + \E[Y|\mathcal F]\)</span>.</li>
<li>Monotonicity: <span class="math inline">\(X\le Y\implies \E[X|\mathcal F] \le E[Y|\mathcal F]\)</span>.</li>
<li>Monotone convergence: <span class="math inline">\(X_n\ge 0, X_n\uparrow X\)</span>, <span class="math inline">\(\E X&lt;\iy \implies \E[X_n|\mathcal F] \uparrow \E[X|\mathcal F]\)</span>.</li>
<li>Jensen: If <span class="math inline">\(\ph\)</span> is convex and <span class="math inline">\(\E|X|, \E|\ph(X)|&lt;\iy\)</span>, then <span class="math inline">\(\ph(\E[X|\mathcal F])\le \E[\ph(X)|\mathcal F]\)</span>.</li>
<li>Contraction in <span class="math inline">\(L^p\)</span></li>
<li><span class="math inline">\(\E[\E[Y|\mathcal F]]=\E Y\)</span>.</li>
<li>(Tower property) If <span class="math inline">\(\mathcal F_1\subeq \mathcal F_2\)</span> then <span class="math display">\[\E[\E[X|\mathcal F_1]|\mathcal F_2] = \E[X|\mathcal F_1] = \E[\E[X|\mathcal F_2]|\mathcal F_1].\]</span> (The coarser field wins.)</li>
<li>(Taking <span class="math inline">\(X\in \mathcal F\)</span> outside) If <span class="math inline">\(X\in \mathcal F\)</span> and <span class="math inline">\(\E |Y|, \E|XY|&lt;\iy\)</span> then <span class="math inline">\(\E[XY|\mathcal F] = X\E[Y|\mathcal F]\)</span>.</li>
<li>(Least squares, projection) If <span class="math inline">\(\E X^2&lt;\iy\)</span>, then <span class="math inline">\(\E[X|\mathcal F]\)</span> is the variable <span class="math inline">\(Y\in \mathcal F\)</span> that minimizes <span class="math inline">\(\E(X-Y)^2\)</span>.</li>
</ol>
<p>(Proofs straightforward from definition, omitted.)</p>
<h2 id="martingales-almost-sure-convergence">Martingales, Almost Sure Convergence</h2>
<p>A <strong>filtration</strong> <span class="math inline">\(\mathcal F\)</span> is an increasing sequence of <span class="math inline">\(\si\)</span>-fields.</p>
<p><span class="math inline">\(X_n\)</span> is <strong>adapted</strong> to <span class="math inline">\(\mathcal F_n\)</span> if for all <span class="math inline">\(n\)</span>, <span class="math inline">\(X_n\in \mathcal F_n\)</span>.</p>
<p><span class="math inline">\(X_n\)</span> is a <strong>martingale</strong> if 1. <span class="math inline">\(\E|X_n|&lt;\iy\)</span> 2. <span class="math inline">\(X_n\)</span> is adapted to <span class="math inline">\(\mathcal F_n\)</span> 3. <span class="math inline">\(\E[X_{n+1}|\mathcal F_n]=X_n\)</span> for all <span class="math inline">\(n\)</span>.</p>
<p>If 3 holds with <span class="math inline">\(\le,\ge\)</span>, <span class="math inline">\(X_n\)</span> is a <strong>supermartingale, submartingale</strong>. (Note a supermartingale goes <em>down</em> and a submartingle goes <em>up</em>.)</p>
<ol type="1">
<li><p>If <span class="math inline">\(X_n\)</span> is a supermartingale and <span class="math inline">\(n&gt;m\)</span>, then <span class="math inline">\(\E[X_n|\mathcal F_m]\le X_m\)</span>.</p>
<em>Proof</em>. Induct and use monotonicity,
\begin{align}
\E[X_{m+k}|\mathcal F_m] &amp;= \E[\E[X_{m+k}|\mathcal F_{n+k-1}]|\mathcal F_m]\\
&amp;\le \E[X_{m+k-1}|\mathcal F_m]
\end{align}</li>
<li>We get a reversed inequality for a submartingale, and equality for a martingale</li>
<li><p>If <span class="math inline">\(X_n\)</span> is a martingale, <span class="math inline">\(\ph\)</span> is convex, and <span class="math inline">\(\E[\ph(X_n)]&lt;\iy\)</span>, then <span class="math inline">\(\ph(X_n)\)</span> is a submartingale.</p>
<em>Proof</em>. Use Jensen to bring the <span class="math inline">\(\ph\)</span> outside in <span class="math inline">\(\E[\ph(X_{n+1})|\mathcal F_n]\)</span>. We need <span class="math inline">\(X_n\)</span> to be a martingale so the inside becomes <span class="math inline">\(X_n\)</span>.</li>
<li>Ex. let <span class="math inline">\(\ph(x) = |x|^p\)</span>.</li>
<li>The same holds if we change the conditions to: <span class="math inline">\(X_n\)</span> is a submartingale wrt <span class="math inline">\(\mathcal F_n\)</span>, <span class="math inline">\(\ph\)</span> is <em>increasing</em> convex.</li>
<li><p>Cor. If <span class="math inline">\(X_n\)</span> is sub, then <span class="math inline">\((X_n-a)^+\)</span> is sub.</p>
If <span class="math inline">\(X_n\)</span> is super, <span class="math inline">\(X_n\wedge a\)</span> is super.</li>
<li><p>Define <span class="math inline">\((H\cdot X)_n = \sum_{m=1}^n H_m (X_m-X_{m-1})\)</span>. <span class="math inline">\(H_n\)</span> is <strong>predictable</strong> if <span class="math inline">\(H_n\in \mathcal F_{n-1}\)</span> (predictable from information at time <span class="math inline">\(n-1\)</span>).</p>
<p>(You can’t beat an unfavorable game.) If <span class="math inline">\(X_n\)</span> is super, <span class="math inline">\(H_n\ge 0\)</span> is predictable, and each <span class="math inline">\(H_n\)</span> is bounded, then <span class="math inline">\((H\cdot X)_n\)</span> is super.</p>
<em>Proof</em>. Expand <span class="math inline">\(\E[(H\cdot X)_{n+1}|\mathcal F_{n}]\)</span> and take the <span class="math inline">\(H_{n+1}\)</span> out.</li>
<li><p><strong>Corollary</strong>: If <span class="math inline">\(N\)</span> is a stopping time and <span class="math inline">\(X_n\)</span> is super, then <span class="math inline">\(X_{N\wedge n}\)</span> is super.</p>
<em>Proof</em>. Let <span class="math inline">\(H_n = 1_{N\ge n}\)</span>.</li>
<li><strong>Theorem (Upcrossing inequality)</strong>: Let
\begin{align}
N_0&amp;=-1\\
N_{2k-1}&amp;=\inf\set{m&gt;N_{2k-2}}{X_m\le a}\\
N_{2k}&amp;=\inf\set{m&gt;N_{2k-1}}{X_m\ge b}\\
U_n&amp;=\sup\set{k}{N_{2k}\le n},
\end{align}
<p>number of upcrossings by time <span class="math inline">\(n\)</span>.</p>
<p>If <span class="math inline">\(X_m\)</span> is sub, then <span class="math display">\[(b-a) \E U_n\le E(X_n-a)^+ - \E (X_0-a)^+.\]</span></p>
<p><em>Motivation</em>. To prove the martingale convergence theorem (10). A martingale doesn’t converge if it has infinitely many upcrossings of some <span class="math inline">\([a,b]\)</span>. Show this happens with probability 0 and union-bound.</p>
<em>Proof</em>. Consider a betting system trying to take advantage of upcrossings, <span class="math display">\[H_m=\begin{cases} 1, &amp;\text{if }N_{2k-1}&lt;m\le N_{2k}\text{for some }k\\
0,&amp;\text{otherwise}\end{cases}.\]</span> Let <span class="math inline">\(Y_n = a+(X_n-a)^+\)</span>. We have <span class="math display">\[ \E(Y_n - Y_0) \ge \E(H\cdot Y)_n \ge (b-a) \E U_n.\]</span></li>
<li><p><strong>Martingale convergence theorem</strong>: If <span class="math inline">\(X_n\)</span> is sub, <span class="math inline">\(\sup \E X_n^+&lt;\iy\)</span>, then as <span class="math inline">\(n\to \iy\)</span>, <span class="math inline">\(X_n\)</span> converges a.s. to a limit <span class="math inline">\(X\)</span> with <span class="math inline">\(\E|X|&lt;\iy\)</span>.</p>
<em>Proof</em>. The expected number of upcrossings is finite. So <span class="math inline">\(\limsup X_n=\liminf X_n\)</span> a.s.</li>
<li>If <span class="math inline">\(X_n\ge 0\)</span> is super then as <span class="math inline">\(n\to \iy\)</span>, <span class="math inline">\(X_n\to X\)</span> a.s., <span class="math inline">\(\E X\le \E X_0\)</span>.</li>
<li><p><strong>Doob’s decomposition</strong>: Any submartingale <span class="math inline">\(X_n\)</span> can be written uniquely as <span class="math inline">\(X_n=M_n+A_n\)</span>, <span class="math inline">\(M_n\)</span> a martingale and <span class="math inline">\(A_n\)</span> predictable with <span class="math inline">\(A_0=0\)</span>.</p>
<p><em>Proof</em>. Solve for what <span class="math inline">\(A_n\)</span> must be: <span class="math inline">\(A_n = A_{n-1} + \E[X_n|\mathcal F_{n-1}] - X_{n-1}\)</span>.</p></li>
</ol>
<h3 id="exercises">Exercises</h3>
<ol type="1">
<li><p>For <span class="math inline">\(i\le j\)</span>, <span class="math inline">\(\si(X_i)\subeq \mathcal F_i \subeq \mathcal F_j\)</span> so <span class="math inline">\(\si(X_i)\subeq \mathcal F_j\)</span>. Hence <span class="math inline">\(\mathcal G_j\subeq \mathcal F_j\)</span>.</p>
Using definition and then taking <span class="math inline">\(\E[\bullet|\mathcal F_n]\)</span>,
\begin{align}
\E[X_{n+1}|\mathcal G_n]&amp;=X_n\\
\E[X_{n+1}|\mathcal F_n] = \E[\E[X_{n+1}|\mathcal G_n]|\mathcal F_n]&amp;=X_n.
\end{align}</li>
<li>Let <span class="math inline">\(\mathcal F_n = \si(S_n,\ldots, S_1)\)</span>.
\begin{align}
\E[X_{n+1}|\mathcal F_n] &amp;= \E[X_{n+1}|\si(S_n,\ldots, S_1)]\\
&amp;=\E[f(S_n+\xi_n)|S_n] &amp;\text{why?}\\
&amp;=\rc{B(0,r)} \int_{B(S_n,r)} f(y)\le f(S_n)=X_n.
\end{align}</li>
<li>Let <span class="math inline">\(X_n = -\rc n\)</span>.</li>
<li><p>Let <span class="math inline">\(X_n=\sumo in \xi_i\)</span>, <span class="math inline">\(\xi_i\)</span></p></li>
</ol>
<h2 id="examples-1">Examples</h2>
<h2 id="doobs-inequality-lp-convergence">Doob’s inequality, <span class="math inline">\(L^p\)</span> convergence</h2>
<ol type="1">
<li>If <span class="math inline">\(X_n\)</span> is a submartingale and <span class="math inline">\(N\)</span> is a stopping time with <span class="math inline">\(\Pj(N\le k)=1\)</span>, then <span class="math display">\[ \E X_0\le \E X_N \le \E X_k.\]</span> (Note the first inequality is not true in general if <span class="math inline">\(N\)</span> is unbounded.) <em>Proof.</em>
<ol type="1">
<li>Show that <span class="math inline">\(X_n - X_{N\wedge n}\)</span> is a submartingale. To see this, wrie it as a dot product with a predictable sequence, <span class="math inline">\(X_n-X_{N\wedge n} = 1_{\{n-1 \ge N\}}\cdot X\)</span>.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></li>
<li>(Alternative) Condition on <span class="math inline">\(N\)</span> and use the submartingale property. See exercise 1.</li>
</ol></li>
<li><p><strong>Doob’s inequality</strong>: Let <span class="math inline">\(X_m\)</span> be a submartingale, <span class="math inline">\(\ol X_n = \max_{0\le m\le n} X_m^+\)</span>, <span class="math inline">\(\la&gt;0\)</span>. Then <span class="math display">\[ \la \Pj(A) \le \E X_n 1_A \le \E X_n^+.\]</span></p>
<p><em>Proof.</em> Interpret the event <span class="math inline">\(A\)</span> as early stopping by defining the stopping time <span class="math inline">\(N=\inf \set{m}{X_m\ge \la \text{ or }m=n}\)</span>. Now apply 1.</p>
<em>Corollary</em>: Kolmogorov’s inequality</li>
<li><strong><span class="math inline">\(L^p\)</span> maximum inequality</strong>: If <span class="math inline">\(X_n\)</span> is a submartingale then for <span class="math inline">\(p&gt;1\)</span>, <span class="math display">\[ \E (\ol X_n^p) \le \pf{p}{p-1}^p \E(X_n^+)^p.\]</span> <em>Proof.</em> Our “only tool” is to use Doob’s inequality to bound <span class="math inline">\(\Pj(\ol X_n\ge \la)\)</span> (actually, <span class="math inline">\(\ol X_n\wedge M\)</span>). To make this term appear, bound <span class="math inline">\(\E[(\ol X_n\wedge M)^p]\)</span> by <span class="math inline">\(\int_0^{\iy} p\la^{p-1}\Pj(\ol X_n\wedge M \ge \la)\)</span>. We get one less power of <span class="math inline">\(\ol X_n \wedge M\)</span> on the RHS. Use Holder and bootstrap.</li>
<li>(<span class="math inline">\(L^1\)</span> version)</li>
<li><strong><span class="math inline">\(L^p\)</span> convergence theorem</strong>: If <span class="math inline">\(X_n\)</span> is a matringale with <span class="math inline">\(\sup \E|X_n|^p&lt;\iy\)</span> when <span class="math inline">\(p&gt;1\)</span>, then <span class="math inline">\(X_n\to X\)</span> a.s. and in <span class="math inline">\(L^p\)</span>. <em>Proof.</em> The hypothesis implies <span class="math inline">\((\E X_n^+)\)</span> is bounded so a.s. follows from martingale convergence. To show <span class="math inline">\(X_n\to X\)</span> in <span class="math inline">\(L^p\)</span>, it suffices to show that <span class="math inline">\(|X_n-X|^p\le Y\)</span> with <span class="math inline">\(\E|Y|&lt;\iy\)</span> and <span class="math inline">\(X_n\to X\)</span> a.s. (Dominated convergence theorem.) Bound <span class="math inline">\(|X_n-X|^p\le (2\sup |X_n|^p)\)</span> and use <span class="math inline">\(L^p\)</span> maximal inequality to bound the RHS (use MCT on <span class="math inline">\(\sup_{0\le m\le n}X_n\)</span>).</li>
<li><strong>Orthogonality of martingale increments</strong>: Let <span class="math inline">\(X_n\)</span> be a martingale with <span class="math inline">\(\E X_n^2&lt;\iy\)</span> for all <span class="math inline">\(n\)</span>. If <span class="math inline">\(m\le n, Y\in \cal F_m, \E Y^2&lt;\iy\)</span>, then <span class="math display">\[ \E[(X_n - X_m)Y]=0. \]</span> <em>Proof.</em> Cauchy Schwarz gives this is <span class="math inline">\(L^1\)</span>. Now just use the martingale property.</li>
<li><strong>Conditional variance formula</strong>: If <span class="math inline">\(X_n\)</span> is a <span class="math inline">\(L^2\)</span> martingale, <span class="math inline">\(\E[(X_n-X_m)^2|\cal F_m]=\E [X_n^2|\cal F_m]-X_m^2\)</span>.</li>
<li><p><strong>Branching processes</strong>: Letting <span class="math inline">\(X_n=Z_n/\mu^2\)</span>, use 7 to get an expression for <span class="math inline">\(\E[X_n^2|\cal F_{n-1}]\)</span> and <span class="math inline">\(\E X_n^2\)</span>. This shows <span class="math inline">\(\sup \E X_n^2&lt;\iy\)</span>, so <span class="math inline">\(X_n\to X\)</span> in <span class="math inline">\(L^2\)</span>.</p></li>
</ol>
<h3 id="exercises-1">Exercises</h3>
<ol type="1">
<li>Use submartingale property with the fact that <span class="math inline">\(N=j\)</span> is in <span class="math inline">\(\cal F_j\)</span>. Now sum over <span class="math inline">\(N\)</span>.</li>
<li></li>
<li></li>
<li>?</li>
<li></li>
</ol>
<p>A good counterexample is the 1-D simple random walk starting at 1 with absorbing barrier at 0.</p>
<ol type="1">
<li>We have <span class="math inline">\(\E X_0=1&gt;\E S_N\)</span> where <span class="math inline">\(N=\inf\set{n}{S_n=0}\)</span>.</li>
<li>We have <span class="math inline">\(\E(\max_m X_m) = \iy\)</span>, so no <span class="math inline">\(L^1\)</span> maximal inequality holds.</li>
</ol>
<h2 id="uniform-integrability-convergence-in-l1">5 Uniform integrability; convergence in <span class="math inline">\(L^1\)</span></h2>
<p><span class="math inline">\(X_i\)</span> are <strong>uniformly integrable</strong> if <span class="math display">\[\lim_{M\to \iy}\pa{\sup_{i\in I} \E(|X_i|;|X_i|&gt;M)}=0.\]</span> Note this implies <span class="math inline">\(\sup_{i\in I}\E|X_i|&lt;\iy\)</span>.</p>
<ol type="1">
<li><p>Given <span class="math inline">\((\Om,\mathcal F_0, P)\)</span> and <span class="math inline">\(X\in L^1\)</span>, <span class="math inline">\(\set{\E(X|\mathcal F)}{\mathcal F\text{ is a $\si$-field $\subeq \mathcal F_0$}}\)</span> is uniformly integrable.</p>
<em>Proof</em>. <span class="math inline">\(\le \E(|X|;\E(|X||\mathcal F)&gt;M)\)</span> and use Chebyshev and the fact that <span class="math inline">\(P(A_n)\to 0\implies \E(|X|;A_n)\to 0\)</span>.</li>
<li>(Relationship between uniform integrability and convergence in <span class="math inline">\(L^1\)</span>: If <span class="math inline">\(X_n\to X\)</span> in probability, TFAE:
<ol type="1">
<li><span class="math inline">\(\set{X_n}{n\ge 0}\)</span> is uniformly integrable.</li>
<li><span class="math inline">\(X_n\to X\)</span> in <span class="math inline">\(L^1\)</span>.</li>
<li><span class="math inline">\(\E|X_n|\to \E|X|&lt;\iy\)</span>.</li>
</ol>
<p><em>Proof</em>. (<span class="math inline">\(n\implies n+1\)</span>)</p>
<ol type="1">
<li>Let <span class="math inline">\(\ph_M=\text{clamp}(-M,M)\)</span>. By the triangle inequality, <span class="math display">\[\E|X_n-X| \le \E|\ph_M(X_n) - \ph_M(X)| + \E(|X_n|; |X_n|&gt;M) + \E(|X|;|X|&gt;M).\]</span> The first term goes to 0 (<span class="math inline">\(\lim_{n\to \iy}\)</span>) by bounded convergence, the second term goes to 0 (<span class="math inline">\(\lim_M\to \iy}\lim_{n\to \iy}\)</span>) by uniform integrability; the last term goes to 0 by Fatou.</li>
<li>Jensen.</li>
<li>Write <span class="math inline">\(\E(|X_n|; |X_n|&gt;M) \le\E|X_n| - \E \psi_M(|X_n|)\)</span> where <span class="math inline">\(\psi_M(x)=x\)</span> on <span class="math inline">\([0,M-1]\)</span> and 0 on <span class="math inline">\([M,\iy)\)</span>. Use DCT.</li>
</ol></li>
<li><strong>Theorem (Submartingale L1 convergence)</strong>: For a submartingale, TFAE:
<ol type="1">
<li>Uniformly integrable</li>
<li>Converges a.s. and in <span class="math inline">\(L^1\)</span></li>
<li>Converges in <span class="math inline">\(L^1\)</span> <em>Proof</em>.</li>
<li>Uniform integrability <span class="math inline">\(\implies \sup \E|X_n|&lt;\iy \implies X_n\to X\)</span> a.s. by martingale convergence theorem. + Use 2.</li>
<li>Clear.</li>
<li><span class="math inline">\(X_n\to X\)</span> in <span class="math inline">\(L^1 \implies\)</span> <span class="math inline">\(X_n\to X\)</span> i.p. + Use 2.</li>
</ol></li>
<li>If <span class="math inline">\(X_n\to X\)</span> in <span class="math inline">\(L^1\)</span> then <span class="math inline">\(\E(X_n;A)\to \E(X;A)\)</span>.</li>
<li><p>If martingale <span class="math inline">\(X_n\to X\)</span> in <span class="math inline">\(L^1\)</span> then <span class="math inline">\(X_n = \E(X|\mathcal F_n)\)</span>.</p>
<em>Proof</em>. Use 4 with <span class="math inline">\(A\in \mathcal F_n\)</span>.</li>
<li><strong>Theorem (Martingale L1 convergence)</strong>: For a martingle, TFAE:
<ol type="1">
<li>Uniformly integrable</li>
<li>Converges a.s. and in <span class="math inline">\(L^1\)</span></li>
<li>Converges in <span class="math inline">\(L^1\)</span></li>
<li>There is an integrable random variable <span class="math inline">\(X\)</span> so that <span class="math inline">\(X_n=\E (X|\mathcal F_n)\)</span></li>
</ol></li>
<li><p>If <span class="math inline">\(\mathcal F_n \uparrow \mathcal F_\iy\)</span> (i.e., <span class="math inline">\(\mathcal F_\iy = \si\pa{\bigcup_n \mathcal F_n}\)</span>), then as <span class="math inline">\(n\to \iy\)</span>, <span class="math inline">\(\E(X|\mathcal F_n) \to \E(X|\mathcal F_\iy)\)</span> a.s. and in <span class="math inline">\(L^1\)</span>.</p>
<em>Proof</em>. <span class="math inline">\(Y_n=\E(X|\mathcal F_n)\)</span> is a martingale. Use 6 and 5. Use the <span class="math inline">\(\pi\)</span>-<span class="math inline">\(\la\)</span> theorem to get that <span class="math inline">\(\int_A X\,dP = \int_A Y_\iy\,dP\)</span> for all <span class="math inline">\(A\in \mathcal F_\iy\)</span>.</li>
<li><p><strong>Theorem (Levy’s 0-1 law)</strong>: If <span class="math inline">\(\mathcal F_n\uparrow \mathcal F_{\iy}\)</span> and <span class="math inline">\(A\in \mathcal F_{\iy}\)</span> then <span class="math inline">\(\E(1_A|\mathcal F_n)\to 1_A\)</span> a.s.</p>
Note this proves Kolmogorov’s 0-1 law: take <span class="math inline">\(A\)</span> to be in the tail <span class="math inline">\(\si\)</span>-field.</li>
<li><p><strong>Theorem (DCT for conditional expectations)</strong>: Suppose <span class="math inline">\(Y_n\to Y\)</span> a.s., <span class="math inline">\(|Y_n|\le Z\)</span> for all <span class="math inline">\(n\)</span> where <span class="math inline">\(\E Z&lt;\iy\)</span>, and <span class="math inline">\(\mathcal F_n\uparrow \mathcal F_\iy\)</span> then <span class="math display">\[\E(Y_n|\mathcl F_n)\to \E(Y|\mathcal F_\iy)\text{ a.s.}\]</span>. (If instead <span class="math inline">\(Y_n\to Y\)</span> is <span class="math inline">\(L^1\)</span>, then convergence is <span class="math inline">\(L^1\)</span>.)</p>
<p><em>Proof</em>. Show <span class="math inline">\(|\E(Y_n|\mathcal F_n) - \E(Y|\mathcal F_n)| \to 0\)</span> a.s. as <span class="math inline">\(n\to \iy\)</span>, and then use 5.7. To show the inequality, bound by a variable that doesn’t depend on <span class="math inline">\(N\)</span>, <span class="math display">\[ \limsup_{n\to \iy} \E(|Y_n-Y||\mathcal F_n)\le \lim_{n\to \iy} \E(W_N|\mathcal F_n)\]</span> where <span class="math inline">\(W_N\)</span> is defined as the maximum Cauchy difference.</p></li>
</ol>
<h3 id="exercises-2">Exercises</h3>
<ol type="1">
<li>We have <span class="math inline">\(\fc{\E(|X_i|;|X_i|\ge M)}{\E(\ph(|X_i|);|X_i|\ge M)}\to 0\)</span>, and the denominator is <span class="math inline">\(\le C\)</span>.</li>
<li>By 5.7, <span class="math inline">\(\E(\te|Y_{[1,n]})\to \E(\te|Y_{[1,\iy)})\)</span>. By the Law of Large Numbers, the average of <span class="math inline">\(Y_{[1,n]}\)</span> converges, say to <span class="math inline">\(\mu\)</span>. We have by calculation that <span class="math inline">\(\fc{\E(\mu|Y_{[1,n]})}{\E(\mu'|Y_{[1,n]})}\to \iy\)</span>. It converges i.p. to <span class="math inline">\(\te\)</span>. 5.6/7 gives that it converges a.s.</li>
<li>?</li>
<li>?</li>
<li>By <span class="math inline">\(L^1\)</span> convergence,
\begin{align}
\int_{X_m\le x\text{ i.o.},\neg D} \E(1_D|X_1,\ldots, X_n) &amp;\to \int_{X_m\le x\text{ i.o.},\neg D} 1_D = 0\\
\pat{LHS} &amp;\ge \int_{X_m\le x\text{ i.o.},\neg D, X_n\le x} \E(1_D|X_1,\ldots, X_n)\\
&amp;\ge \Pj(X_n\le x) \Pj(D^c \wedge X_m\le x\text{ i.o.}|X_n\le x).
\end{align}
We may assume that if <span class="math inline">\(X_n=0\)</span>, then <span class="math inline">\(X_{n+1}=0\)</span> (redefine the random variable so this is true). <span class="math inline">\(\Pj(X_n\le x)\)</span> is bounded away from 0, so the second term goes to 0. Now <span class="math display">\[\Pj(D^c\wedge X_m\le x\text{ i.o.}) = \Pj(D^c\wedge X_m\le x\text{ i.o.}|X_n\le x) + \cdots.\]</span> Choosing <span class="math inline">\(n\)</span> large enough, we make the RHS as small as we want.</li>
<li>The condition in 5 holds.</li>
</ol>
<h2 id="backwards-martingales">6 Backwards Martingales</h2>
<p>A <strong>backwards martingale</strong> is a martingale indexed by <span class="math inline">\(n\le 0\)</span> adapted to an increasing sequence <span class="math inline">\(\mathcal F_n\)</span>: <span class="math display">\[ \E(X_{n+1}|\mathcal F_n) = X_n,\quad n\le -1. \]</span></p>
<ol type="1">
<li><p><span class="math inline">\(X_{-\iy}=\lim_{n\to -\iy} X_n\)</span> exists a.s. and in <span class="math inline">\(L^1\)</span>. (If <span class="math inline">\(X_0\in L^p\)</span>, then convergence is in <span class="math inline">\(L^p\)</span>.)</p>
<em>Proof</em>. Use the upcrossing inequality to show the limit exists (the proof is same as before except that <span class="math inline">\(n\to -\iy\)</span> instead of <span class="math inline">\(\iy\)</span>). <span class="math display">\[(b-a)\E U_n \le \E(X_0-a)^+.\]</span> By 5.1, <span class="math inline">\(X_n=\E(X_0|\mathcal F_n)\)</span> is uniformly integrable; 5.2 shows convergence in <span class="math inline">\(L^1\)</span>.</li>
<li><p>If <span class="math inline">\(X_{-\iy}= \lim_{n\to -\iy} X_n\)</span> and <span class="math inline">\(\mathcal F_{-\iy} = \bigcap_n \mathcal F_n\)</span>, then <span class="math inline">\(X_{-\iy}= \E(X_0|\mathcal F_{-\iy})\)</span>.</p>
<em>Proof</em>. Check that <span class="math inline">\(\int_A X_{-\iy} \,dP = \int_A X_0\,dP\)</span> for all <span class="math inline">\(A\in \mathcal F_{-\iy}\)</span>.</li>
<li><p>If <span class="math inline">\(\mathcal F_n\downarrow \mathcal F_{-\iy}\)</span> as <span class="math inline">\(n\to -\iy\)</span> (<span class="math inline">\(\mathcal F_{-\iy} = \bigcap_n\mathcal F_n\)</span>), then <span class="math display">\[\E(Y|\mathcal F_n)\to \E(Y|\mathcal F_{-\iy})\text{ a.s., in }L^1.\]</span></p></li>
</ol>
<h3 id="examples-2">Examples</h3>
<ol type="1">
<li>Strong law of large numbers: Let <span class="math inline">\(\xi_i\)</span> be iid with <span class="math inline">\(\E|\xi_i|&lt;\iy\)</span>. Let <span class="math inline">\(S_n= \sumo in \xi_i\)</span> and <span class="math inline">\(X_{-n} = \fc{S_n}{n}\)</span>. Calculate by symmetry that <span class="math inline">\(\E(\xi_{n+1}|\mathcal F_{-(n+1)}\)</span> and show <span class="math inline">\(X_{-n}\)</span> is a backwards martingale. So <span class="math inline">\(\limn \fc{S_n}n = \E(X_{-1}|\mathcal F_{-\iy})\)</span>. Now use the Hewitt-Savage 0-1 law, <span class="math inline">\(\mathcal E\supeq \mathcal F_{-\iy}\)</span> is trivial.</li>
<li><p>Ballot Theorem: Let <span class="math inline">\(\xi_j\in \N\)</span> (ex. <span class="math inline">\(\{0,2\}\)</span>) rv’s, <span class="math inline">\(S_k=\sumo ik \xi_i\)</span>, <span class="math inline">\(G=\set{S_j&lt;j\text{ for }1\le j\le n}\)</span>. Then <span class="math inline">\(\Pj(G|S_n)\ge \pa{1-\fc{S_n}{n}}^+\)</span> with equality if <span class="math inline">\(\Pj(\xi_j\le 2)=1\)</span>.</p>
<em>Proof</em>: Let <span class="math inline">\(T=\inf\set{k\ge -n}{X_k}\)</span>. (Take averages over fewer elements; stop when it’s <span class="math inline">\(\ge 1\)</span>.) (Let <span class="math inline">\(T=-1\)</span> if it’s <span class="math inline">\(\phi\)</span>.) <span class="math inline">\(X_T\ge 1\)</span> on <span class="math inline">\(G^c\)</span> and <span class="math inline">\(0\)</span> on <span class="math inline">\(G\)</span>. <span class="math display">\[\Pj(G^c|S_n) \le \E(X_T|\mathcal F_{-n}) = \fc{S_n}n.\]</span></li>
<li><p>Hewitt-Savage 0-1 law (alternate proof): <strong>Lemma</strong>: Let <span class="math inline">\(A_n(\ph) = \rc{n\fp k} \sum_i \ph(X_{i_1},\ldots, X_{i_k})\)</span>. If <span class="math inline">\(X_i\)</span> are iid and <span class="math inline">\(\ph\)</span> is bounded, <span class="math inline">\(A_n(\ph) \to \E\ph(X_1,\ldots, X_k)\)</span> a.s.</p>
<em>Proof</em>.
\begin{align}
A_n(\ph) &amp;= \E(\ph(X_1,\ldots, X_k)|\mathcal E_n) &amp; \text{symmetry}\\
&amp;\to \E (\ph(X_1,\ldots, X_k)|\mathcal E)\\
&amp;=\E (\ph(X_1,\ldots, X_k))
\end{align}
<p>The last is because most permutations move <span class="math inline">\(X_1,\ldots, X_k\)</span> away from positions in <span class="math inline">\([1,k]\)</span>. Formally, we first have <span class="math inline">\(\E[\ph(X_1,\ldots, X_k)|\mathcal E] \in \si(X_{k+1},\ldots)\)</span>, and then show lemma: if <span class="math inline">\(\E X^2&lt;\iy\)</span>, <span class="math inline">\(\E(X|\mathcal G)\in \mathcal F\)</span>, and <span class="math inline">\(X\)</span> is independent of <span class="math inline">\(\mathcal F\)</span>, then <span class="math inline">\(\E(X|\mathcal G) = \E X\)</span>. (Proof: Show <span class="math inline">\(\Var(Y)=0\)</span>.)</p>
Then <span class="math inline">\(\mathcal E\)</span> is independent of <span class="math inline">\(\mathcal G_k = \si(X_1,\ldots, X_k)\)</span>, so <span class="math inline">\(\mathcal E\)</span> is independent of <span class="math inline">\(\si\pa{\bigcup_k \mathcal G_k}\)</span>.</li>
<li><p><strong>Theorem (de Finetti)</strong>: If <span class="math inline">\(X_1,\ldots\)</span> are exchangeable, then conditional on <span class="math inline">\(\mathcal E\)</span>, <span class="math inline">\(X_1,\ldots\)</span> are iid.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<p>(<span class="math inline">\(X_1,\ldots\)</span> is <strong>exchangeable</strong> if for each <span class="math inline">\(n\)</span>, <span class="math inline">\(\pi\in S_n\)</span>, <span class="math inline">\((X_1,\ldots, X_n)\)</span> and <span class="math inline">\((X_{\pi(1)},\ldots, X_{\pi(n)})\)</span> have the same distribution.)</p>
<p><em>Proof</em>. Again by symmetry, <span class="math inline">\(A_n(\ph)\to \E[\ph(X_1,\ldots, X_k)|\mathcal E]\)</span>. For <span class="math inline">\(f,g\)</span> on <span class="math inline">\(\R^k, \R\)</span>, calculate <span class="math inline">\(A_n(f)A_n(g)\)</span> in terms of <span class="math inline">\(A_n(\ph), A_n(\ph_j)\)</span> where <span class="math inline">\(\ph_j = (f\circ \pi_{-j})(g\circ \pi_j)\)</span> to get <span class="math display">\[ A_n(\ph) = \fc{n}{n-k+1} A_n(f) A_n(g) - \rc{n-k+1} \sumo j{k-1} A_n(\ph_j).\]</span> Take limits, note that <span class="math inline">\(\E(\ph|\mathcal E) = \E(\ph_j | \mathcal E)\)</span>, and induct.</p></li>
</ol>
<h2 id="optional-stopping-theorems">7 Optional stopping theorems</h2>
<ol type="1">
<li><p>If <span class="math inline">\(X_n\)</span> is a uniformly integrable submartingale then for any stopping time <span class="math inline">\(N\)</span>, <span class="math inline">\(X_{N\wedge n}\)</span> is uniformly integrable.</p>
<em>Proof</em>. From 4.1, <span class="math inline">\(\E X_{N\wedge n}^+ \le \E X_n^+\)</span>. Use the martingale convergence theorem to ge <span class="math inline">\(X_{N\wedge n} \to X_N\)</span> a.s. and <span class="math inline">\(\E |X_N|&lt;\iy\)</span>. Now split up <span class="math inline">\(|X_{N\wedge n}|\)</span> based on <span class="math inline">\(N\wedge n \stackrel?n\)</span> to show it’s uniformly integrable.</li>
<li>If <span class="math inline">\(\E|X_N|&lt;\iy\)</span> and <span class="math inline">\(X_n1_{N&gt;n}\)</span> is uniofrmly integrable, then <span class="math inline">\(X_{N\wedge n}\)</span> is uniformly integrable.</li>
<li><p>If <span class="math inline">\(X_n\)</span> is a uniformly integrable submartingale, then for any stopping time <span class="math inline">\(N\le \iy\)</span>, <span class="math inline">\(\E X_0\le \E X_n\le \E X_\iy\)</span> where <span class="math inline">\(X_\iy = \lim X_n\)</span>.</p>
<em>Proof</em>. Use 4.1 and 5.3, which shows that a uniformly integrable submartingale converges in <span class="math inline">\(L^1\)</span>.</li>
<li><strong>Theorem (Optional stopping)</strong>: If <span class="math inline">\(L\le M\)</span> are stopping times and <span class="math inline">\(Y_{M\wedge n}\)</span> is a uniformly integrable submartingale then
\begin{align}
\E Y_L &amp;\le \E Y_M\\
\E X_N &amp;\le \mathcal F_L.
\end{align}
<p><em>Proof</em>. Use 7.3 with <span class="math inline">\(X_n=Y_{M\wedge n}, N=L\)</span>.</p>
Let <span class="math inline">\(N=\begin{cases} L,&amp;\text{on }A\\ M,&amp;\text{on }A^c\)</span>. Use the first part on <span class="math inline">\(N\le M\)</span> to get <span class="math inline">\(\E Y_N\le \E Y_M\)</span>. Note that <span class="math inline">\(N=M\)</span> on <span class="math inline">\(A^c\)</span> to get <span class="math display">\[A\in \mathcal F_L\implies \E(Y_L;A) \le \E(Y_M;A) = \E[\E[Y_M|\mathcal F_L];A].\]</span> (Define <span class="math inline">\(N\)</span> so we can just localize the inequality to <span class="math inline">\(A\)</span>.)</li>
<li><p><strong>Theorem (Generalization of Wald’s equation)</strong>: If <span class="math inline">\(X_n\)</span> is a submartingale, <span class="math inline">\(\E[|X_{n+1}-X_n||\mathcal F_n]\le B\)</span> a.s., and <span class="math inline">\(\E N&lt;\iy\)</span>, then <span class="math inline">\(X_{N\wedge n}\)</span> is uniformly integrable and <span class="math inline">\(\E X_N\ge \E X_0\)</span>.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> (For a martingale, we have <span class="math inline">\(=\)</span>.)</p>
<p><em>Proof</em>. <span class="math display">\[|X_{N \wedge n}| \le |X_0| + \sum_{m=0}^{\iy} |X_{m+1} - X_m| 1_{(N&gt;m)} \le B\sum_{m=0}^{\iy} \Pj(N&gt;m) = B \E N.\]</span></p></li>
</ol>
<h3 id="example">Example</h3>
<p>Let <span class="math inline">\(\xi_i=1,-1\)</span> with probability <span class="math inline">\(p,1-p\)</span>. Suppose <span class="math inline">\(p&gt;\rc 2\)</span>, <span class="math inline">\(a&lt;0&lt;b\)</span>.</p>
<ol type="1">
<li>Let <span class="math inline">\(\ph(x) = \pf{1-p}{p}^x\)</span>. <span class="math inline">\(\ph(S_n)\)</span> is a martingale.</li>
<li><p>(Absorption probabilities) Let <span class="math inline">\(T_x=\inf\set{n}{S_n=x}\)</span>. Then <span class="math display">\[\Pj(T_a&lt;T_b) = \fc{\ph(b)-\ph(0)}{\ph(b)-\ph(a)}.\]</span></p>
Check that <span class="math inline">\(T_a\wedge T_b&lt;\iy\)</span> a.s. and apply 7.3. (Check directly, or use 5.6 on <span class="math inline">\(\ph(S_{N\wedge n})\)</span>.)</li>
<li>\begin{align}
\Pj(T_a&lt;\iy) &amp;= \pf{1-p}{p}^{-a}\\
\Pj(T_b&lt;\iy) &amp;= 1.
\end{align}
<em>Proof</em>. Let <span class="math inline">\(b\to \iy\)</span>; <span class="math inline">\(a\to \iy\)</span>, respectively in 2.</li>
<li><p><span class="math inline">\(\E T_b = \fc{b}{2p-1}\)</span>.</p>
<p><em>Proof</em>. <span class="math inline">\(S_n-(p-q)n\)</span> is a martingale. Either show <span class="math inline">\(\E T_b&lt;\iy\)</span>, or apply 4.1 to <span class="math inline">\(T_b\wedge n\)</span> and let <span class="math inline">\(n\to iy\)</span>.</p></li>
</ol>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Recall this means dot product with the differences.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>ex. Chinese restaurant process.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>To get Wald’s equation, let <span class="math inline">\(S_n\)</span> be a random walk and <span class="math inline">\(X_n = S_n-n\E(S_1-S_0)\)</span>. 6. (Stopping theorem not requiring uniform integrability) If <span class="math inline">\(X_n\ge 0\)</span> is a supermartingale and <span class="math inline">\(N\le \iy\)</span> is a stopping time, then <span class="math inline">\(\E X_0\ge \E X_N\)</span>.</p>
<p><em>Proof</em>. <span class="math inline">\(\E X_0\ge \E X_{N\wedge n}\)</span>. Now use MCT and Fatou on <span class="math inline">\(\E(X_N; N&lt;\iy) + \E(X_N; N=\iy)\)</span>.<a href="#fnref3">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class="st_facebook_large" displayText="Facebook"></span>
    <span class="st_twitter_large" displayText="Tweet"></span>
    <span class="st_googleplus_large" displayText="Google +"></span>
    <span class="st_reddit_large" displayText="Reddit"></span>
    <span class="st__large" displayText></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>


<!-- Footer -->
<div id="footer">
  <div class="container">
    Built with
    <a href="http://jaspervdj.be/hakyll">Hakyll</a> 
    using 
    <a href="http://www.getbootstrap.com">Bootstrap</a>, 
    <a href="http://www.disqus.com">Disqus</a>,
    <a href="http://ignorethecode.net/blog/2010/04/20/footnotes/">Footnotes.js</a>,
    <a href="http://highlightjs.org/">Highlight.js</a>, 
    <a href="http://www.mathjax.org">MathJax</a>, 
    and <a href="http://www.sharethis.com">ShareThis</a>.
  </div>
</div>
</body>

</html>

<!-- SCRIPTS -->
<!-- jQuery-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="../../../bootstrap/js/bootstrap.min.js"></script>

<!-- Extension : Highlight.js @ https://highlightjs.org/ -->
<!-- Syntax highlighting tomorrow-night-bright, agate-->
<link rel="stylesheet" href="../../../highlight/css/tomorrow-night-bright.css">
<script src="../../../highlight/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- Extension : MathJax @ https://docs.mathjax.org/en/v2.5-latest/tex.html -->
<!-- MathJax/config/local/local.js contains macros. Need to provide entire URL-->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,http://holdenlee.github.io/notebook/MathJax/config/local/local"></script>

<!-- Extension : Footnotes @ http://ignorethecode.net/blog/2010/04/20/footnotes/ -->
<script src="../../../footnotes/js/footnotes.js"></script>

<!-- Extension : Disqus @ http://disqus.com -->
<!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->

<script src="../../../disqus/js/disqus.js"></script>



<!-- Extension : Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73261814-1', 'auto');
  ga('send', 'pageview');

</script>

