<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

  <meta name="description" content="Holden Lee's Research Notebook">
  <meta name="author" content="Holden Lee">
    
  <title>Sum of squares</title>

  <!-- Bootstrap core CSS -->
  <link href="../../../bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

  <!-- Custom styles for this template -->
  <link href="../../../css/blog.css" rel="stylesheet">
  <link href="../../../css/default.css" rel="stylesheet">

  <!-- Extension : Footnotes -->
  <link href="../../../footnotes/css/footnotes.css" rel="stylesheet">

  <!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->
  

  <!-- Extension : Collapsible lists @ http://code.stephenmorley.org/javascript/collapsible-lists/-->
  <link href="../../../collapsible_lists/css/collapsible.css" rel="stylesheet">
  <script type="text/javascript" src="../../../collapsible_lists/js/CollapsibleLists.js"></script>

</head>

<body>

<!-- Navigation bar. navbar-inverse is black, navbar-default is white.-->
<!-- To make a button active (pressed), use <li class="active"> -->
<div id="header">
  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../">Notebook</a>
      </div>
      <div id="navbar" class="collapse navbar-collapse">
        <ul class="nav navbar-nav">
          <li><a href="../../../">Home</a></li>
          <li><a href="../../../sitemap.html">Sitemap</a></li>
<!-- TODO: Distinguish between PAPERS, RESEARCH QUESTIONS, BOOKS -->
<!-- TODO: make this part a for loop over main pages -->
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </nav>
</div>
<!-- Content -->
<!--div id="content">
  <h1>Mental Wilderness</h1>-->



<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Sum of squares</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-24 
          , Modified: 2016-10-31 
	</p>
      
       <p>Tags: <a href="../../../tags/SoS.html">SoS</a>, <a href="../../../tags/SDP.html">SDP</a>, <a href="../../../tags/maxcut.html">maxcut</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#introduction">Introduction</a></li>
 <li><a href="#definitions">Definitions</a><ul>
 <li><a href="#equivalence">Equivalence</a></li>
 </ul></li>
 <li><a href="#sum-of-squares-as-semidefinite-programs">Sum-of-squares as semidefinite programs</a></li>
 <li><a href="#chapter-2">Chapter 2</a><ul>
 <li><a href="#cut-problems">Cut problems</a></li>
 <li><a href="#quadratic-sampling-lemma">Quadratic sampling lemma</a></li>
 <li><a href="#goemans-williamson">Goemans-Williamson</a></li>
 <li><a href="#degree-4-sos-breaks-this-hard-instance">Degree 4 SoS breaks this hard instance</a></li>
 </ul></li>
 <li><a href="#exercises">Exercises</a><ul>
 <li><a href="#chapter-1">Chapter 1</a></li>
 <li><a href="#chapter-2-1">Chapter 2</a></li>
 </ul></li>
 <li><a href="#misc.">Misc.</a></li>
 <li><a href="#proofs-beliefs-and-algorithms-through-the-lens-of-sum-of-squares">Proofs, Beliefs, and Algorithms through the lens of Sum-of-Squares</a><ul>
 <li><a href="#lower-bounds">3 Lower bounds</a><ul>
 <li><a href="#maxcut">3.1 Maxcut</a></li>
 <li><a href="#csp">3.2 CSP</a></li>
 <li><a href="#from-integrality-gaps-to-hardness">3.3 From integrality gaps to hardness</a></li>
 </ul></li>
 <li><a href="#arora-rao-vazirani-approximation-for-expansion">4 Arora-Rao-Vazirani approximation for expansion</a></li>
 <li><a href="#bayesian-view">5 Bayesian view</a><ul>
 <li><a href="#bayesian-view-1">5.1 Bayesian view</a></li>
 <li><a href="#general-domains">5.2 General domains</a></li>
 <li><a href="#planted-clique-hardness">5.3 Planted clique hardness</a></li>
 </ul></li>
 <li><a href="#sphere">6 Sphere</a><ul>
 <li><a href="#tensor-decomposition-1114">Tensor decomposition (11/14)</a></li>
 <li><a href="#tensor-decomposition-1117-talk">Tensor decomposition (11/17) talk</a></li>
 </ul></li>
 <li><a href="#reading">Reading</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <p>Reference: Barak’s notes. Barak and Steurer’s survey.</p>
<h2 id="introduction">Introduction</h2>
<p>“Sum-of-squares” is an algorithm that attempts to find feasible solutions to systems of polynomial inequalities, or a proof that there is no solution. It is widely applicable because many computational problems can naturally be put into the form of polynomial inequalities (ex. <span class="math inline">\(k\)</span>-SAT can be encoded with degree <span class="math inline">\(k\)</span>). The degree <span class="math inline">\(d\)</span> of sum-of-squares is a tunable parameter; the algorithm runs in <span class="math inline">\(n^d\)</span> time. We can look at</p>
<ul>
<li>lower bounds: sum-of-squares of degree <span class="math inline">\(d\)</span> cannot solve a certain problem. This is necessary, e.g., to show a problem has no polynomial-time algorithm - but very little is known about lower bounds even though sum-of-squares is a <em>single</em> algorithm, not a class of algorithms!</li>
<li>upper bounds: use sum-of-squares of higher constant degree (e.g., <span class="math inline">\(d=4\)</span>) to get better approximation algorithms, etc.</li>
</ul>
<p>Why is SoS a natural notion?</p>
<ul>
<li><span class="math inline">\(d=1\)</span> is linear programming, <span class="math inline">\(d=2\)</span> is semidefinite programming, and <span class="math inline">\(d=n\)</span> is brute-force-search. Everything in the middle is “dark matter” which we don’t understand. In many cases it seems like you don’t get better algorithms by looking t <span class="math inline">\(d&gt;2\)</span>, but there are also exceptions. Compare to how most natural problems seem to be either polynomial time or NP-hard/conjectured exponential time. By Ladner’s Theorem (time hierarchy) there are problems of essentially any time complexity, but these aren’t natural.</li>
<li>The quest for optimal algorithms: In complexity theory the hope is to find a property <span class="math inline">\(P\)</span> such that easy problems have property <span class="math inline">\(P\)</span> while hard problems do not. Even more ambitiously, one hopes for a <em>single</em> “optimal algorithm” for all problems in a large class, in the sense that when a problem has an efficient algorithm, then this single optimal algorithm will solve it. SoS comes the closest to being such an algorithm. (Thus it’s interesting to see what kinds of techniques are encompassed by the SoS framework, i.e., what facts have “SoS proofs”, because then they will be solvable by the SoS algorithm.)</li>
</ul>
<h2 id="definitions">Definitions</h2>
<p>There are many equivalent notions of sum-of-squares; we give 4. The first definition introduces an algorithm that searches for a pseudo-expectation operator; on the boolean cube this is essentially equivalent to a pseudo-distribution. We also give equivalences to a sum-of-squares proof and sum-of-squares representation.</p>
<p>In the next section we will motivate these definitions and show equivalences.</p>
<p>Let <span class="math inline">\(x=(x_1,\ldots, x_n)\)</span> and <span class="math inline">\(\R[x]_{\le d}\)</span> denote polynomials in <span class="math inline">\(x_1,\ldots, x_n\)</span> of degree <span class="math inline">\(\le d\)</span>.</p>
<ol type="1">
<li>(Convex optimization) A degree-<span class="math inline">\(l\)</span> pseudo-expectation operator satisfying <span class="math inline">\(p_1=\cdots =p_m=0\)</span>, where <span class="math inline">\(\deg p_i\le d, 2d\mid l\)</span>, is a bilinear form <span class="math inline">\(M:\R[x]_{\le l/2}\times \R[x]_{\le l/2}\to \R\)</span> such that
<ul>
<li>(Normalization) <span class="math inline">\(M(1,1)=1\)</span>.</li>
<li>(Consistency) If <span class="math inline">\(p,q,r,s\in \R[x]_{\le l/2}\)</span> and <span class="math inline">\(pq=rs\)</span>, then <span class="math inline">\(M(p,q)=M(r,s)\)</span>.</li>
<li>(Non-negativity) <span class="math inline">\(M(p,p)\ge 0\)</span>.</li>
<li>(Feasibility) For all <span class="math inline">\(p_i\)</span> and <span class="math inline">\(q\in \R[x]_{\le l/2-d}\)</span>, <span class="math inline">\(M(p_iq,p)=0\)</span>. In other words, <span class="math inline">\(M\)</span> is a positive semidefinite quadratic form that, as a bilinear form, factors through <span class="math inline">\((\R[x]/\an{p_i})_{\le l}\)</span>, <span class="math display">\[\R[x]_{\le l/2}\times \R[x]_{\le l/2} \to (\R[x]/\an{p_i})_{\le l} \to \R.\]</span> The degree-<span class="math inline">\(l\)</span> SoS algorithm is the algorithm that solves the feasibility problem for this semidefinite program.</li>
</ul></li>
<li>(Pseudo-expectation) For a function <span class="math inline">\(\mu:\{\pm 1\}^n\to \R\)</span> <span class="math display">\[\wt{\EE_\mu} p(x) = \sum_x \mu(x)p(x).\]</span> We say <span class="math inline">\(\mu\)</span> is a degree-<span class="math inline">\(l\)</span> <strong>pseudo-expectation</strong> if
<ul>
<li>(Normalization) <span class="math inline">\(\sum_x\mu(x)=1\)</span>.</li>
<li>(Restricted non-negativity) For all <span class="math inline">\(p\in \R[x]_{\le l/2}\)</span>, <span class="math inline">\(\wt{\EE_\mu} p(x) \ge 0\)</span>.</li>
</ul></li>
<li>(Sum-of-square refutation) A <strong>sum-of-squares refutation</strong> for the system of polynomial equations <span class="math inline">\(p_i\ge 0\)</span><a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> is a proof of <span class="math inline">\(-1\ge 0\)</span> from deduction system starting with <span class="math inline">\(p_i\ge 0\)</span> with the following rules
<ul>
<li><span class="math inline">\(p\ge 0, q\ge 0\vDash p+q\ge 0\)</span>.</li>
<li><span class="math inline">\(p\ge 0, q\ge 0\vDash pq\ge 0\)</span>.</li>
<li><span class="math inline">\(\vDash p^2\ge 0\)</span>. It is degree <span class="math inline">\(l\)</span> if the <em>syntactic</em> degree of each expression is at most <span class="math inline">\(l\)</span>. (Explain.)</li>
</ul></li>
<li>(Sum-of-square representation) A degree-<span class="math inline">\(l\)</span> <strong>sum-of-squares representation</strong> for the infeasibility of <span class="math inline">\(p_1=0,\ldots, p_m=0\)</span> is <span class="math display">\[\sum_i p_i q_i = 1+\sum_i r_i^2, \quad \deg(p_i),\deg(r_i)\le l.\]</span></li>
</ol>
<h3 id="equivalence">Equivalence</h3>
<p>1$$2: See exercise 1.7.</p>
<p>2$$3: This is the SOS Theorem. It encompasses the Positivstellensatz, which says that every unsatisfiable system of equalities has a finite degre proof of unsatisfiability. (See exercise 1.14, 15 for part of the theorem.)</p>
<p>3$$4: See exercise 1.11.</p>
<h2 id="sum-of-squares-as-semidefinite-programs">Sum-of-squares as semidefinite programs</h2>
<p>The most common use of sum-of-squares is in SDP relaxations of combinatorial problems, as follows. Let <span class="math inline">\(f\)</span> be a convex function. The goal is to find <span class="math display">\[
\min_{x\in \{-1,1\}^n}f(x).
\]</span> One way to relax this is to write <span class="math inline">\(f(x)=g(x^Tx/n)\)</span>, and find <span class="math display">\[
\min_{M\succeq 0, M_{ii}=1} g(M).
\]</span> This is a relaxation because if <span class="math inline">\(x\)</span> is the optimal solution to the first problem, <span class="math inline">\(x^Tx/n\)</span> is a feasible point for the second problem achieving the same value.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<p>Think of <span class="math inline">\(M_{ii}=1\)</span> as degree-2 constraints that shrink the space we’re minizing over to be closer to just the set <span class="math inline">\(\set{x^Tx/n}{x\in \{-1,1\}^n}\)</span>.</p>
<p>Note this is the degree-2 SoS relaxation of this problem!<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> The degree-<span class="math inline">\(l\)</span> SoS relaxation is the optimization problem <span class="math display">\[\min_{M\text{ pseudo-expectation satisfying }x_i^2=1} h(M)\]</span> (note we showed that the set of pseudo-expectations is convex; it is defined by <span class="math inline">\(M\succeq 0\)</span> and some linear equations). (<span class="math inline">\(x_i^2=1\)</span> corresponds to <span class="math inline">\(M_{ii}=1\)</span>.) In the equation above we thought of <span class="math inline">\(M\)</span> as a bilinear form on <span class="math inline">\(\R^n\times \R^n = \R[x]_{=1}\times \R[x]_{=1}\)</span>; here we’re just adding 1 to the vector space to get <span class="math inline">\(\R[x]_{\le 1}\times \R[x]_{\le 1}\)</span> and stipulating <span class="math inline">\(M(1,1)=1\)</span>, which doesn’t change anything.</p>
<p>(If <span class="math inline">\(f\)</span> is a linear function <span class="math inline">\(f(x_1,\ldots, x_n,x_1^2,x_1x_2,\ldots, x_1^3,\ldots)\)</span>, then <span class="math inline">\(g(M)\)</span> is the function <span class="math inline">\(f(M(x_1,1),\ldots, M(x_n,1), M(x_1,x_1),\ldots)\)</span>.)</p>
<p>For example, the degree-4 SoS relaxation would correspond to optimizing over <span class="math inline">\((\R^2,\R^1,\R)^{\ot 2}\)</span>, where the solutions corresponding to solutions of the original problem are in the form <span class="math inline">\((x^{\ot 2},x,1)^{\ot 2},x\in \{-1,1\}^n\)</span>. For $M<span class="math inline">\((\R^2,\R^1,\R)^{\ot 2}\)</span>, the solutions satisfy equations like <span class="math inline">\(M_{ii,jk}=1, M_{i,ij}=1\)</span>, etc.—corresponding to multiples of polynomials <span class="math inline">\(x_i^2-1\)</span>.</p>
<h2 id="chapter-2">Chapter 2</h2>
<h3 id="cut-problems">Cut problems</h3>
<p>Define</p>
<ul>
<li><strong>expansion/conductance</strong>
\begin{align}
\phi(S) &amp;= \fc{E(S,\ol S)}{d\min\{|S|,n-|S|\}}\\
\phi(S) &amp;= \min_S \phi(S).
\end{align}
(which is at most a factor of 2 from <span class="math inline">\(\fc{n E(S,\ol S)}{d|S||\ol S|}\)</span>). This is the <strong>sparsest cut</strong> problem.</li>
<li>(fractional) <strong>cut size</strong>
\begin{align}
\text{cut}(S) &amp;= \fc{E(S,\ol S)}{|E|}\\
\text{maxcut}(G) &amp;= \max_S \text{cut}(S).
\end{align}
This is the <strong>max-cut</strong> problem.</li>
</ul>
<p>We can express this in terms of the characteristic function <span class="math inline">\(x\)</span> of <span class="math inline">\(S\)</span> defined as <span class="math inline">\(x_i=(i\in S)-(i\nin S)\)</span> by <span class="math display">\[ \an{x,Lx} = \rc{2d}\sum_{i\sim j} (x_i-x_j)^2 =\fc{4E(S,\ol S)}{d} = 2n\text{cut}(S). \]</span></p>
<h3 id="quadratic-sampling-lemma">Quadratic sampling lemma</h3>
<p><strong>Lemma</strong>: Let <span class="math inline">\(\wt{\E}\)</span> be a degree-2 pseudo-expectation operator. Then there is a Gaussian distribution <span class="math inline">\(N\)</span> such that <span class="math display">\[\wt{\E_{x\sim\mu}} p(x) = \E_{y\sim N} p(y).\]</span> The pseudo-expectation operator is a bilinear form on <span class="math inline">\(\R[x]_{\le 1}\times \R[x]_{\le 1}\)</span> is associated with a matrix <span class="math inline">\(A=B^2\)</span>. Then <span class="math inline">\(y=Bx\)</span> where <span class="math inline">\(x\sim N(0,1)\)</span>, i.e., <span class="math inline">\(y\)</span> is the Gaussian with covariance matrix <span class="math inline">\(B\)</span>.</p>
<p>A lemma about Gaussians.</p>
<p><strong>Lemma</strong>: If <span class="math inline">\((x,y)\)</span> are <span class="math inline">\(1-\ep\)</span>-correlated Gaussians, then <span class="math inline">\((x,y)^T =Av\)</span> where <span class="math inline">\(A=\rc2 \smatt{\sqrt\ep+\sqrt{2-\ep}}{\sqrt{\ep}-\sqrt{2-\ep}}{\sqrt{\ep}-\sqrt{2-\ep}}{\sqrt{\ep}+\sqrt{2-\ep}}\)</span>, <span class="math inline">\(v\sim N(0,I)\)</span>. In other words, if <span class="math inline">\((x,y)\)</span> are <span class="math inline">\(\cos(\te)\)</span>-correlated Gaussians, then <span class="math inline">\(A=\smatt{\sin(\fc\pi4-\fc\te2)}{\sin(\fc\pi4+\fc\te2)}{\cos(\fc\pi4-\fc\te2)}{\cos(\fc\pi4+\fc\te2)}\)</span> (I might have gotten cos/sin switched).</p>
<p>(Sanity check: when <span class="math inline">\(\te=0\)</span> they point in <span class="math inline">\(\pi/4\)</span>, when <span class="math inline">\(\te=\fc \pi2\)</span> they point in <span class="math inline">\(0,\fc\pi2\)</span>.)</p>
<p><strong>Lemma</strong>: Let <span class="math inline">\(y,y'\)</span> be Gaussians with variance 1 such that <span class="math display">\[\E(y-y')^2 \ge 4(1-\de) = 2+2\cos(\te).\]</span> Then <span class="math display">\[\Pj(\sgn(y) = \sgn(y')) = \fc{\te}{\pi} = O(\sqrt{\de}).\]</span></p>
<h3 id="goemans-williamson">Goemans-Williamson</h3>
<p><strong>Theorem</strong>: There is a polynomial-time algorithm which given a <span class="math inline">\(n\)</span>-vertex <span class="math inline">\(d\)</span>-regular graph <span class="math inline">\(G=(V,E)\)</span> and a degree 2 pseudo-distribution with <span class="math inline">\(\wt E_{x\sim \mu} x_i^2=1\)</span>, <span class="math inline">\(\wt E\an{x,Lx} \ge 2n(1-\ep)\)</span>, outputs <span class="math inline">\(z\in \{\pm1\}^n\)</span> such that <span class="math inline">\(\an{z,Lz} \ge (1-f_{GW}(\ep))=O(\sqrt \ep)\)</span>. (Add the precise form of <span class="math inline">\(f\)</span>.)</p>
<p><em>Proof</em>: Let <span class="math inline">\(z_i=\sgn(y_i)\)</span> and use Lemma on Gaussian variables on each term.</p>
<p>This is optimal. To see optimality in order of magnitude, take the odd cycle on <span class="math inline">\(n=\rc{\sqrt{\ep}}\)</span> vertices, or a union of these. To see optimality in an additive sense, use the Feige-Schectman graph (random points on sphere connected in <span class="math inline">\(\an{v_i,v_j}\le -1+\ep\)</span>).</p>
<h3 id="degree-4-sos-breaks-this-hard-instance">Degree 4 SoS breaks this hard instance</h3>
<p>Show that the FS graph can be solved by a degree 4 SoS. This is the same as saying there is no degree-4 distribution over <span class="math inline">\(\R^n\)</span> consistent with <span class="math inline">\(\{x_i^2=1\}\)</span> such that <span class="math display">\[\wt E\sum (x_i-x_{i+1})^2&gt;4(n-1).\]</span> (We had a degree-2 distribution with <span class="math inline">\(\wt E&gt;4n\pa{1-O\prc{n^2}}\)</span>, which gave the gap.)</p>
<p>Proof: The squared triangle inequality holds for degree-4 pseudodistributions. Sum up inequalities <span class="math inline">\(\wt E(x_i-x_{i+1})^2 \le \sum_{j\ne i} \wt E(x_j+x_{j+1})^2\)</span>.</p>
<h2 id="exercises">Exercises</h2>
<h3 id="chapter-1">Chapter 1</h3>
<ol type="1">
<li></li>
<li>Use linearity.</li>
<li><span class="math inline">\(M(p,p)\ge 0\)</span> is semidefiniteness; all the other constraints are linear.</li>
<li></li>
<li>Let <span class="math inline">\(p=\sum_{y\ne y^0} \prod_i (x_i-y_i)^2\)</span>; note <span class="math inline">\(p(y^0)\ne 0\)</span>. Then <span class="math inline">\(\wt{\EE_{x\sim \mu}} [p(x)^2] = \mu(y^0)p(y^0)\)</span>. Thus <span class="math inline">\(\mu(y^0)\ge 0\)</span>.</li>
<li>Note the problem should say “degree <span class="math inline">\(l\)</span> polynomials”. Write <span class="math inline">\(\mu(x)\)</span> as a multilinear polynomial. First, mod out by <span class="math inline">\(x_i^2-x_i\)</span>. Now note that for every degree <span class="math inline">\(\ge l+1\)</span> monomial <span class="math inline">\(x^L\)</span>, in <span class="math inline">\(\wt{\EE_{x\sim x^L}} q(x)\)</span>, if <span class="math inline">\(\deg q\le l\)</span> is a monomial, then one term will be uncancelled and average out to 0. Now use linearity.</li>
<li>The map is <span class="math inline">\(\mu \mapsto ((p,q) \mapsto \wt{\EE_\mu pq})\)</span>. We have <span class="math inline">\(\sum \mu(x) (x_i^2-1)pq=0\)</span>. (Where do we use 6?)</li>
<li></li>
<li>?</li>
<li>?</li>
<li>Induct to say that at each stage, we have an expression in the form <span class="math inline">\(\sum r_i^2 + \sum q_ip_i\)</span>. At the end we get <span class="math inline">\(\sum r_i^2 + \sum q_ip_i = -1\ge 0\)</span>, which re-arranges to a SoS representation.</li>
<li></li>
<li></li>
<li></li>
<li></li>
</ol>
<h3 id="chapter-2-1">Chapter 2</h3>
<ol type="1">
<li>Imitate the proof of CS. We have <span class="math inline">\(\wt{\EE_{\mu}} t^2P^2-2tPQ+Q^2\)</span>. Now set the discriminant to be <span class="math inline">\(\le 0\)</span>.</li>
</ol>
<h2 id="misc.">Misc.</h2>
<ul>
<li>SoS in universal learning (Paul Christiano)</li>
</ul>
<h2 id="proofs-beliefs-and-algorithms-through-the-lens-of-sum-of-squares">Proofs, Beliefs, and Algorithms through the lens of Sum-of-Squares</h2>
<ul>
<li><a href="http://sos16.dsteurer.org">Course</a></li>
<li><a href="http://sumofsquares.org">Lecture notes</a></li>
</ul>
<p>Applications</p>
<ul>
<li>Math</li>
<li>Algorithms</li>
<li>Complexity</li>
<li>Information</li>
<li>Physics</li>
<li>Optimization</li>
<li>Learning</li>
</ul>
<p>Sum-of-squares is a powerful meta-algorithm.</p>
<p>Other meta-algorithms:</p>
<ul>
<li>Gradient descent
<ul>
<li>Multiplicative weight updates, FTRL</li>
</ul></li>
</ul>
<p>Gradient descent, integer linear programming: for a given problem there are many ways of formulating it. It depends crucially on how you set it up. SoS doesn’t suffer this. There is a mechanical way to apply it.</p>
<ul>
<li>Applies to “any” problem</li>
<li>Often matches best known theoretical guarantees</li>
<li>Sometimes significantly better</li>
<li>Plausible: SOS is “optimal” for many problems.</li>
<li>Strongest evidence for difficulty of problems.</li>
<li>SOS is driven by duality between proofs and beliefs. Compare to belief propagation.</li>
</ul>
<p>The vanilla version is impractical, but there are efforts to extract practical algorithms. They resemble heuristic algorithms used in practice.</p>
<p>Given <span class="math inline">\(p\in \R[x_1,\ldots, x_n]\)</span>, <span class="math inline">\(p\ge 0\)</span> over <span class="math inline">\(\R^n\)</span>,</p>
<ul>
<li>(Hilbert) It is not always possible to write it as a sum of squares of polynomials.</li>
<li>(Artin) It is always possible to write it as a sos of rational functions.</li>
</ul>
<p>Krivine characterized systems of polynomial inequalities without solutions over <span class="math inline">\(\R^n\)</span>. (Positivstellensatz, cf. Farkas lemma)</p>
<p>Given <span class="math inline">\(f:\{0,1\}^n\to \R\)</span> (given as coefficients in monomial basis up to degree <span class="math inline">\(\deg f\)</span>), is <span class="math inline">\(f\ge 0\)</span> or <span class="math inline">\(\exists x\in \{0,1}^n, f(x)&lt;0\)</span>? The number of coefficients is <span class="math inline">\(\le n^{\deg f}\)</span>. (Every monomial is actually a subset.) This is NP-hard for degree 2.</p>
<p>Max-cut: <span class="math display">\[
\max cut(G) \le c \iff c - \sum_{\{i,j\}\in E(G)} (x_i-x_j)^2 \ge 0.
\]</span></p>
<p>Given <span class="math inline">\(f\)</span>, the algorithm outputs either a short proof for <span class="math inline">\(f&gt;0\)</span> or an object ptends to be a collection of <span class="math inline">\(x\in \{0,1\}^n\)</span>.</p>
<p>Degree <span class="math inline">\(d\)</span> SoS certificate for <span class="math inline">\(f\)</span>: <span class="math inline">\(\deg g_i\le \fc d2\)</span>, <span class="math display">\[\forall x\in \{0,1\}^n, f(x) = \sumo iv g_i(x)^2.\]</span> It can be the case that <span class="math inline">\(\deg g_i&gt;\deg f\)</span>! That would be true over <span class="math inline">\(\R\)</span>, but we are working over <span class="math inline">\(\{0,1\}^n\)</span>.</p>
<p>For degree 2, we can do this efficiently over <span class="math inline">\(\R\)</span>; this question is hard because we restrict to the hypercube.</p>
<p>If <span class="math inline">\(f\)</span> has a degree <span class="math inline">\(d\)</span> sos certificate, we can find degree-<span class="math inline">\(d\)</span> certificate for <span class="math inline">\(f+2^{-n^d}\)</span> in time <span class="math inline">\(n^{O(d)}\)</span>. (Can’t solve general convex problems exactly.)</p>
<!-- sdp feasibility -->
<p><strong>Theorem</strong>. For all <span class="math inline">\(G\)</span>, there exists a degree-2 sos certificate, <span class="math inline">\(\max(f_G) - 0.878 f_G\)</span>. This estimates MAX-CUT up to <span class="math inline">\(0.878\)</span>.</p>
<p>Open question: if we replace 2 by 4, can we replace <span class="math inline">\(0.878\)</span> by a larger constant? This would disprove unique games.</p>
<!-- constructions of graphs? can make constant 1.-->
<!-- known algorithms that solve decision problem solves search problem-->
<!-- certificate starts existing in 0.878(max-cut) and (max-cut)-->
<p><strong>Theorem 5</strong>. <span class="math inline">\(f\)</span> has degree-<span class="math inline">\(d\)</span> sos certificate iff there exists positive semidefinite <span class="math inline">\(A\)</span> such that <span class="math display">\[\forall x\in \{0,1\}^n, f(x) = \an{(1,x)^{\ot d2}, A (1,x)^{\ot \fc d2}}.\]</span> (Proof. <span class="math inline">\(A\)</span> has a square root.)</p>
<!-- affine linear subspace -->
<p><strong>Theorem</strong>. If <span class="math inline">\(f\ge 0\)</span>, then it has a degree <span class="math inline">\(2n\)</span> sos certificate.</p>
<p><em>Proof</em>. <span class="math inline">\(f=g^2, g=\sqrt f\)</span>, <span class="math inline">\(\deg g\le n\)</span>, or <span class="math inline">\(f(x) = \sum_{y\in \{0,1\}^n} (\sqrt{f(y)}\one_y(x))^2\)</span>.</p>
<p><em>Proof (finding sos certificates)</em>. <span class="math inline">\(f(x) = \an{(1,x)^{\ot d/2}, F(1,x)^{\ot d/2}}\)</span>. Find <span class="math inline">\(A\)</span>, <span class="math display">\[
A-T\in W = \set{Q}{\an{(1,x)^{\ot d/2}, Q(1,x)^{\ot d/2}}\equiv 0\text{ over }\{0,1\}^n}.
\]</span> Look at <span class="math inline">\(F+W\cap\)</span>cone.</p>
<p>We just need an efficient separation oracle.</p>
<h3 id="lower-bounds">3 Lower bounds</h3>
<h4 id="maxcut">3.1 Maxcut</h4>
<p>Cheeger: Let <span class="math inline">\(G\)</span> be a <span class="math inline">\(d\)</span>-regular graph and <span class="math inline">\(L_G=I-\rc dA\)</span> be its Laplacian, and <span class="math inline">\(\la\)</span> be the second smallest eigenvalue. Then <span class="math display">\[\la \ge \Om(\ph(G)^2).\]</span></p>
<p>The second smallest eigenvalue of <span class="math inline">\(L_{C_n}\)</span> is <span class="math inline">\(O\prc{n^2}\)</span>. (Proof: compute.) This shows Cheeger is tight, as we have <span class="math display">\[ \la = \Te\prc{n^2}, \quad \ph(G) = \Te\prc{n}. \]</span></p>
<p>Maxcut: We showed that <span class="math display">\[maxcut (G)\le 1-\ep \implies (\forall \deg \mu=2, \wt{\EE_{\mu}} f_G(x) \le 1-\Om(\ep^2)).\]</span></p>
To show tightness up to constant, show that for <span class="math inline">\(G=C_n\)</span> there is <span class="math inline">\(\mu\)</span>:
\begin{align}
maxcut(C_n) &amp;\le 1-\rc n =:1- \ep\\
\wt{\EE_\mu} f_G(x) &amp;= 1-\Te\prc{n^2}. 
\end{align}
<em>Proof</em>. Let <span class="math inline">\(n=2k+1\)</span>. Define <span class="math inline">\(u, v, w, X, \mu\)</span> as follows.
\begin{align}
u=(\om^{jk})_{j=0}^{n-1} &amp;= u+iw\\
X&amp;=vv^T+ww^T\\
\wt{\EE_\mu} x &amp;= \rc2 \one\\
\wt{\EE_{\mu}} xx^T &amp;= \rc 4 \one \one^T + \rc 4 X
\end{align}
<p>(We can find a degree-2 pseudodistibution whenever <span class="math inline">\(\wt{\EE_\mu} xx^T - (\wt{\EE_\mu} x)(\wt{\EE_{\mu}}x)^T\)</span> is psd and the diagonal of <span class="math inline">\(\wt{\EE_\mu} xx^T\)</span> equals <span class="math inline">\(\wt{\EE_\mu}x\)</span>. (Do we want it to be in <span class="math inline">\([0,1]\)</span>?)) Calculation: <span class="math display">\[
\wt{\EE_{\mu}} f_G(x) = \rc 4 \sum |u_i-u_j|^2 = n\pa{1-\Te\prc{n^2}}.
\]</span> (Working backwards, setting <span class="math inline">\(X=vv^T+ww^T\)</span> and then finding what the values of <span class="math inline">\(v,w\)</span> should be, we want <span class="math inline">\(\sum_{(i,j)\in E}|u_i-u_j|^2\)</span> to be large. This motivates hops of <span class="math inline">\(\om^k\)</span>. Calculations: <span class="math inline">\(\E(x_i-x_j)^2 = \E[1-\fc{2v_iv_j+2w_iw_j}4]\)</span>.) <!-- $\EE_{x\sim N(\rc 2\one, \rc 4 X)} x^T(vv^T+ww^T)x = \Tr((\rc 4\one \one^T+\rc4 X)X)$--></p>
\begin{align}
\al_{GW} &amp;= \min_{0\le x\le 1} \fc{\cos^{-1}(1-2x)}{\pi x}\\
&amp;=\min_{-1\le \rh\le 1} \fc{2\cos^{-1}\rh}{(1-\rh)\pi}\\
\rh_{GW} &amp;= \amin_{-1\le \rh\le 1} \fc{2\cos^{-1}\rh}{(1-\rh)\pi}
\end{align}
<strong>Theorem</strong> (Tightness of GW): For every <span class="math inline">\(\ep&gt;0\)</span> there is <span class="math inline">\(G\)</span> and <span class="math inline">\(\mu\)</span> such that
\begin{align}
maxcut(G) &amp;\le \al_{GW} x_{GW} + \ep\\
\wt{\EE_{\mu}} f_G(x) &amp;\ge x_{GW}.
\end{align}
<p><em>Proof</em>. Feige-Schechtman graph: Connect up points on the sphere if <span class="math inline">\(\an{v,w}\le \rh_{GW}+\ep\)</span>. (Later reduce to a discrete graph by sampling.) We have variables <span class="math inline">\(\{X_v\}_{v\in \R^d}\)</span>, <span class="math inline">\(X_v = \rc 2 + \fc{\an{v,g}}2\)</span>, <span class="math inline">\(\Cov(X_u,X_v) = \E \an{u,g}\an{v,g} = \an{u,v} \le -\rh_{GW} + \ep\)</span> if <span class="math inline">\(\an{u,v}\le \rh\)</span>.</p>
<h4 id="csp">3.2 CSP</h4>
<p><strong>Theorem</strong>. It is NP-hard to solve <span class="math inline">\(\text{Max-3XOR}_{\rc 2+\de, 1-\ep}\)</span>.</p>
<p>There are reductions <span class="math inline">\(3\SAT\stackrel{\wt O(n)}{\le}\text{LabelCover} \stackrel{O(n)}\le \text{3XOR}\)</span>.</p>
<p>The fraction of constraints satisfied by 3XOR problem <span class="math inline">\(x_i+x_j+x_k = a_{ijk}\)</span> is <span class="math display">\[
f_\psi = \rc 2 + \rc{2|\psi|}\sum_{\{i,j,k\}\in \psi} (1-a_{ijk})(1-2x_i)(1-2x_j)(1-2x_k).
\]</span></p>
<p><strong>Theorem</strong> (Grigoriev). For every <span class="math inline">\(\ep&gt;0\)</span>, for large enough <span class="math inline">\(n\)</span>, there is an instance <span class="math inline">\(\psi\)</span> of Max-3XOR such that</p>
<ul>
<li>every assignment satisfies <span class="math inline">\(\le \rc2+\ep\)</span> fraction of equations in <span class="math inline">\(\psi\)</span>.</li>
<li>there exists a pseudodistribution of degree <span class="math inline">\(\Om(n)\)</span> consistent with <span class="math inline">\(x_i^2-x_i=0\)</span> and <span class="math inline">\((1-2x_i)(1-2x_j)(1-2x_k) = 1-2a_{ijk}\)</span>.</li>
</ul>
<p>Intuition: Unlike Gaussian elimintion, the sos algorithm cannot distinguish between a perfectly and <span class="math inline">\(1-o(1)\)</span> satisfiable system.</p>
<p><em>Proof</em>. Take a random <span class="math inline">\((m,n)\)</span> bipartite graph with left-degree 3. Left is constraints, right is variables.</p>
<ol type="1">
<li>Soundness: Chernoff gives for <span class="math inline">\(m&gt;\fc{9n}{\ep^2}\)</span>, <span class="math display">\[\Pj(\forall x\in \{0,1\}^n, \val_\psi(x) \le \rc 2+\ep)\ge 1-2^{-n}.\]</span> Union bound.</li>
<li><p>Completeness: (cf. expander codes.) Let <span class="math inline">\(\chi_S = \prod_{i\in S}(1-2x_i)\)</span>. Idea: make pseudistribution “uniform” subject to constraints.</p>
<p>We define <span class="math inline">\(\wt{\EE_{\mu}}\chi_S\)</span> for <span class="math inline">\(|S|\le \ep n\)</span>. If <span class="math inline">\(\wt \E \chi_S\)</span>, <span class="math inline">\(\wt \E \chi_T\)</span> have been defined and <span class="math inline">\(|S\triangle T|\le d=\ep n\)</span>, set <span class="math inline">\(\wt \E \chi_{S\triangle T} = (\wt \E \chi_S)(\wt \E \chi_T)\)</span>. Set remaining <span class="math inline">\(\wt \E \chi_S=0\)</span>.</p>
<p>A degree <span class="math inline">\(d\)</span> derivation is a set that is reached after some number of steps, each set on the way being <span class="math inline">\(\le d\)</span>.</p>
<p>WHP the graph is an expander. If we have degree <span class="math inline">\(d\)</span> derivations for <span class="math inline">\(\sum_{i\in S} x_i\equiv 0\)</span> and <span class="math inline">\(\sum_{i\in S} x_i\equiv 1\)</span>, then we get a degree <span class="math inline">\(2d\)</span> derivtion of <span class="math inline">\(\sum_{i\in \phi} x_i=1\)</span>. Impossible—similar to proof of expander codes: In order for <span class="math inline">\(T_t\)</span> to be <span class="math inline">\(\bigopl_{l\in T_t}\Ga(l) = \phi\)</span>, <span class="math inline">\(T_t\)</span> is large; take the first large <span class="math inline">\(T_i\)</span> in the derivation, it will have large neighbor set.</p>
<p>Show <span class="math inline">\(\deg p\le \fc d2\implies \wt \E p^2\ge 0\)</span> by Fourier expansion. Break <span class="math inline">\(|S|\le \fc d2\)</span> into equivalence classes based on <span class="math inline">\(\E[\chi_{S\triangle T}]\ne 0\)</span>. For <span class="math inline">\(p_i=\sum_{S\in C_i} p_S\chi_S\)</span>, <span class="math display">\[ \wt \E p_i^2 = \sum_i (\sum_S p_S\wt{\E} x_{S\triangle S_i})^2\ge 0.\]</span></p></li>
</ol>
<p><strong>Exercise 11</strong>. DO this!</p>
<p>To reduce from Max 3SAT <span class="math inline">\((\fc 78+\ep, 1)\)</span>, convert <span class="math inline">\(a_{\ell_i}x_i \vee a_{\ell_j}x_j \vee a_{\ell_k}x_k=1\)</span> to <span class="math inline">\(x_i+x_j+x_k \equiv a_{\ell_i}+a_{\ell_j} + a_{\ell_k}\)</span>.</p>
<p>A nice <span class="math inline">\(V\in \F_2^L\)</span> is one such that every <span class="math inline">\(u\in V^{\perp}\bs \{0\}\)</span> has <span class="math inline">\(\ve{u}_0\ge 3\)</span>. A <strong>nice subspace predicate</strong> is one such that there exists nice <span class="math inline">\(V\)</span>, <span class="math inline">\(P(x)=1\iff x\in V\)</span>.</p>
<p><strong>Theorem</strong> (SoS hardness for nice-subspace CSP, SoS PCP). Let <span class="math inline">\(k,\ep&gt;0\)</span> be given. There exist <span class="math inline">\(\be = 2^k\prc{\ep^2}\)</span>, <span class="math inline">\(c=\poly\prc{\be}\)</span> such that there is an instance <span class="math inline">\(\psi\)</span> of Max-P for [<span class="math inline">\(k\)</span>-variate predicate <span class="math inline">\(P\)</span> with <span class="math inline">\(\le 2k\)</span> satisfying assignments] on <span class="math inline">\(n\gg \rc c\)</span> variables and <span class="math inline">\(m=\be n\)</span> constraints such that</p>
<ul>
<li>Every assignment satisfies <span class="math inline">\(\le \fc{2k}{2^k} + \ep\)</span> fraction of constraints.</li>
<li>There exists degree <span class="math inline">\(cn\)</span> pseudodistribution that satisfies all constraints.</li>
</ul>
<p><strong>Read the rest (15-21)</strong></p>
<h4 id="from-integrality-gaps-to-hardness">3.3 From integrality gaps to hardness</h4>
<h3 id="arora-rao-vazirani-approximation-for-expansion">4 Arora-Rao-Vazirani approximation for expansion</h3>
<p><strong>Theorem</strong>. Let <span class="math inline">\(G\)</span> be a <span class="math inline">\(d\)</span>-regular graph with vertex set <span class="math inline">\([n]\)</span> and <span class="math inline">\(\mu:B^n\to \R\)</span> be a degree-4 pseudo-distribution. Then <span class="math display">\[
\ph(G) = \min_{x\in B^n} \fc{f_G}{\fc dn |x|(n-|x|)} \le O(\sqrt{\ln n}) \ph_\mu(G).
\]</span> There is a polynomial time algorithm that given <span class="math inline">\(G\)</span> and <span class="math inline">\(\mu\)</span>, finds <span class="math inline">\(x\)</span> such that <span class="math inline">\(\ph(G)\le O(\sqrt{\ln n})\ph_\mu(G,x)\)</span>.</p>
<p><em>Proof</em>.</p>
<p>Let <span class="math inline">\(d(i,j)=\wt{\EE_{\mu}} (x_i-x_j)^2\)</span>. For degree 4, we have <span class="math inline">\(d(i,j) + d(j,k) \ge d(i,k)\)</span>.</p>
<ol type="1">
<li>Structure Theorem.
<ul>
<li>Suppose that <span class="math inline">\(\sum_{i,j} d(i,j)\ge 0.1 n^2\)</span>. We want to show that there exist <span class="math inline">\(|A|,|B|\ge \Om(n)\)</span> such that <span class="math inline">\(d(i,j)\ge \De\)</span> for all <span class="math inline">\(i\in A, j\in B\)</span>.</li>
<li>Let <span class="math inline">\(\mu\)</span> be a distribution (obtained by quadratic sampling from the pd) satisfying <span class="math inline">\(\E x_i^2\le 1\)</span>, <span class="math inline">\(\sum_{i,j} d(i,j)\ge 0.1 n^2\)</span> and <span class="math inline">\(d(i,k) \ge d(i,j) + d(j,k)\)</span>. Then we can find <span class="math inline">\(|A|,|B|\ge \Om(n)\)</span>, <span class="math inline">\(d(A,B)\ge \Om\prc{\sqrt{\ln n}}\)</span>.</li>
<li>Algorithm: Pick <span class="math inline">\(X\)</span> randomly. Take <span class="math inline">\(A^0=\set{i}{X_i\le -1}\)</span> and <span class="math inline">\(B^0=\set{i}{X_i \ge 1}\)</span>. Remove the largest matching between them where the graph <span class="math inline">\(H\)</span> has <span class="math inline">\((i,j)\)</span> with <span class="math inline">\(d(i,j)\le \De\)</span>. (Go through vertices in fixed order.)</li>
<li><span class="math inline">\(\E |A||B| \ge 0.1 cn^2 - n\E |M|\)</span>.</li>
<li>The problematic case is <span class="math inline">\(\E|M|\ge 0.05cn\)</span>.</li>
<li>Estimate <span class="math inline">\(\E |M|\)</span> by relating it to the max of a Gaussian process. <span class="math display">\[ \fc{\Om(1)}{\De} \pf{\E |M|}{n}^3 \le \E \max_{i,j\in [n]} X_j-X_i \le \sqrt{2\ln n}.\]</span>
<ul>
<li>Let <span class="math inline">\(H^k(i) =B_k(i)\)</span> be vertices reached from <span class="math inline">\(i\)</span> in at most <span class="math inline">\(k\)</span> steps in <span class="math inline">\(H\)</span>. Let
\begin{align}
Y_i^{(k)} &amp;= \max_{j\in H^k(i)} X_j-X_i\\
\Phi(k) &amp;=\sumo in \E Y_i^{(k)}
\end{align}
<p>This gives a lower bound on <span class="math inline">\(\E \max_{i,j\in [n]} X_j-X_i\ge \fc{\Phi(k)}n\)</span>.</p>
Lemma (chaining). <span class="math display">\[\Phi(k+1) \ge \Phi(k) + \E |M| - O(n) \max_{i\in [n], j\in H^{k+1}(i)} \pa{\E(X_i-X_j)^2}^{\rc 2}.\]</span>
<ul>
<li>Variance of maxima of Gaussian processes: Let <span class="math inline">\(Z\)</span> be centered Gaussian. Then <span class="math display">\[\Var(\max(Z_1,\ldots, z-t))\le O(1) \max(\Var(Z_1),\ldots, \Var(Z_t)).\]</span></li>
<li>For <span class="math inline">\((i,j)\in E(H)\)</span>, because <span class="math inline">\(H^k(j)\subeq H^{k+1}(i)\)</span>, for <span class="math inline">\(N\)</span> an arbitrary matching of vertices not in <span class="math inline">\(M\)</span>,
\begin{align}
\forall (i,j)\in M, Y_i^{(k+1)}&amp;\ge Y_j^{(k)}+2\\
\forall (i,j)\in N, \rc2 Y_i^{k+1}+\rc2 Y_j^{(k+1)} &amp;\ge \rc 2 Y_i^{(k)} + \rc2 Y_j^{(k)}
\end{align}
Sum up inequalities and take expectation. Use <span class="math inline">\(|\E XY - \E X \E Y|\le \sqrt{\Var(X)\Var(Y)}\)</span>.
\begin{align}
\sum \E Y_i^{(k+1)} - \sum \E Y_j^{(k)}
&amp; = 2 \pa{\sum \E Y_i^{(k+1)} \E L_i - \sum \E Y_j^{(k)} \E R_i}\\
&amp; \le 2\ub{\pa{\sum \E Y_i^{(k+1)}L_i+\cdots}}{4|M|} - n O(1) (\max_{j\in H^{k+1}(i)} + \max_{i\in H^k(j)})[(\E (X_i-X_j)^2)^{\rc 2}].
\end{align}</li>
</ul></li>
</ul></li>
<li>Reduce to case <span class="math inline">\(\sum d(i,j) \ge 0.1n^2\)</span>. Let <span class="math inline">\(\de = \rc{n^2} \sum_{i,j} d(i,j)\)</span>.
<ul>
<li>Heavy cluster: (If there is a cluster that has a constant fraction of vertices, we get a good <span class="math inline">\(A\)</span>.) If <span class="math inline">\(A=B_{\de/4}(i), |A|\ge \Om(n)\)</span>, then <span class="math inline">\(|A|\sum d(j,A) \ge \Om(1) \sum d(i,j)\)</span>.
<ul>
<li><span class="math inline">\(\de \le \rc{n^2} \sum (d(i,A)+d(A,j) + \diam(A))\le \fc\de2 + \rc n \sum 2d(i,A)\)</span>.</li>
</ul></li>
<li>Heavy cluster or well-spread: Either
<ol type="1">
<li>There exists <span class="math inline">\(i\in [n]\)</span>, heavy cluster around <span class="math inline">\(i\)</span>, or</li>
<li>There exists <span class="math inline">\(U\subeq [n]\)</span>, <span class="math inline">\(|U|=\Om(n)\)</span>, Gaussian <span class="math inline">\(Y\)</span>, <span class="math inline">\(\E Y_i^2\le 1\)</span>, <span class="math inline">\(\sum \E (Y_i-Y_j)^2\ge 0.05\)</span>, <span class="math inline">\(\al=\Om(\de)\)</span>, <span class="math inline">\(\forall i,j\in U, d(i,j) =\al \E(Y_i-Y_j)^2\)</span>.</li>
</ol>
<ul>
<li>If not (1), then by averaging principle there is <span class="math inline">\(k\in [n]\)</span> such that <span class="math inline">\(\sumo in d(i,k)\le \de n\)</span>. (Q: <strong>WHY? This seems opposite</strong>.) Lower bound <span class="math inline">\(\sum_{i,j\in U=B_{2\de}(k)} d(i,j)\)</span> by <span class="math inline">\((i, j\in B_{2\de}(k) \be B_{\de/4}(i))\)</span>. Define <span class="math inline">\(X\)</span> as having the same first 2 moments as <span class="math inline">\(\mu\)</span>, and let <span class="math inline">\(Y_i = \fc{X_i-X_k}{\sqrt{2\de}}\)</span>.</li>
</ul></li>
</ul></li>
</ul></li>
<li>Region growing. Suppose there exists <span class="math inline">\(A\subeq [n]\)</span> such that <span class="math inline">\(|A|\sumo in d(i,A) \ge \De \sumo{i,j}n d(i,j)\)</span>. Then <span class="math display">\[\ph(G) \le \rc{\De} \ph_{\mu}(G).\]</span> In fact, <span class="math inline">\(\De \ph(G)\le \ph_{\mu}(G, B_t(A))\)</span> for some <span class="math inline">\(t\)</span>.
<ul>
<li>Define <span class="math inline">\(\mu'\)</span> by: Choose <span class="math inline">\(t\sim U_{[0,1]}\)</span> and let <span class="math inline">\(x_i'=1\iff d(i,A)\le t\)</span>.</li>
<li>Numerator: Then <span class="math inline">\(d(i,A)\le d(j,A) \implies \E_{\mu'} f_G \le \wt{\EE_{\mu}} f_G\)</span>.</li>
<li>Denominator: <span class="math inline">\(\EE_{\mu'} |x'|(x-|x'|) \ge |A| \sum (1-x_i') \ge \De \sum d(i,j) = \De \wt{\EE_{\mu}} |x|(n-|x|)\)</span>.</li>
<li>Combine.</li>
</ul></li>
</ol>
<h3 id="bayesian-view">5 Bayesian view</h3>
<h4 id="bayesian-view-1">5.1 Bayesian view</h4>
<p>We want <span class="math inline">\(\min_{x\in B^n}f\)</span>; degree <span class="math inline">\(d\)</span> SoS finds <span class="math inline">\(\min_{\deg \mu = d} \wt{\EE_{\mu}} f\)</span>.</p>
<p>Often the global minimum is at a single point, but pseudodistribution pretends to be high entropy over nonexistent elements, “supported on unicorns”.</p>
<h4 id="general-domains">5.2 General domains</h4>
<p>Replace <span class="math inline">\(B^n\)</span> with <span class="math inline">\(\Om\subeq \R^n\)</span> defined by polynomial inequalities <span class="math inline">\(A=\{f_1\ge 0,\ldots, f_m\ge 0\}\)</span>. Questions:</p>
<ul>
<li>Are these inequalities feasible, <span class="math inline">\(A\ne \phi\)</span>?</li>
<li>Is <span class="math inline">\(g\ge 0\)</span> on <span class="math inline">\(A\)</span>?</li>
</ul>
<p>A degree <span class="math inline">\(l\)</span> SoS proof is <span class="math inline">\(g=\sum_{S\subeq [n]} p_S \prod_{i\in S} f_i\)</span>, where <span class="math inline">\(p_S\)</span> is SoS, each <span class="math inline">\(\deg(p_S \prod_{i\in S} f_i)\le l\)</span>. Write <span class="math inline">\(A\vdash_l \{g\ge 0\}\)</span>. (There is a SoS proof of degree <span class="math inline">\(d\)</span> of <span class="math inline">\(g\ge 0\)</span> from <span class="math inline">\(A\)</span>.)</p>
<p><strong>Theorem</strong> (Positivstellensatz). Every <span class="math inline">\(A\)</span> is either feasible, or <span class="math inline">\(A\vdash_l\{-1\ge 0\}\)</span> for some <span class="math inline">\(l\in \N\)</span>.</p>
<p>SoS has inference rules for addition, multiplication (add degrees), transitivity (multiply degrees), and substitution (multiply with degree of substituted <span class="math inline">\(H\)</span>).</p>
<p><strong>Lemma</strong>. Let <span class="math inline">\(\mu\)</span> have finite support, <span class="math inline">\(\wt{\EE_{\mu}} 1=1\)</span>. <span class="math inline">\(\mu\)</span> is degree <span class="math inline">\(d\)</span> pd iff <span class="math inline">\(\wt{\EE_{\mu}} ((1,x)^{\ot d/2})((1,x)^{\ot d/2})^T\)</span> is psd.</p>
<p><em>Proof</em>. Writing <span class="math inline">\(p = \an{v,(1,x)^{\ot d/2}}\)</span>, <span class="math inline">\(\wt{\EE_{\mu}} p^2 = \wt{\EE_{\mu}} \an{v,(1,x)^{\ot d/2}}^2\)</span>.</p>
<p>Write <span class="math inline">\(\mu\vDash_l A\)</span> (satisfy <span class="math inline">\(A=\{f_i\}\)</span> at degree <span class="math inline">\(l\)</span>) if for all <span class="math inline">\(S\subeq [m]\)</span>, every SoS with <span class="math inline">\(\deg h + \sum_{i\in S} \max(\deg f_i,l)\le d\)</span> satisfies <span class="math inline">\(\wt \E_\mu h \prod_{i\in S}f_i\ge 0\)</span> for all SoS’s <span class="math inline">\(h\)</span>. (Ex. for <span class="math inline">\(A=\{f\ge 0\}\)</span>, take <span class="math inline">\(S=\{f\}\)</span>.)</p>
<p><strong>Question: how does this mesh with <span class="math inline">\(B^n\)</span> definition?</strong></p>
<p><strong>Theorem</strong> (Duality). Suppose <span class="math inline">\(\ve{x}^2\le M\)</span> is in <span class="math inline">\(A\)</span>. For all <span class="math inline">\(d\in \N\)</span>, <span class="math inline">\(f\in \R[x]_{\le d}\)</span>, either</p>
<ul>
<li>for all <span class="math inline">\(\ep&gt;0\)</span>, there exists degree <span class="math inline">\(d\)</span>, <span class="math inline">\(A\vdash_d \{f\ge -\ep\}\)</span>.</li>
<li><span class="math inline">\(\exists \deg \mu=d\)</span>, <span class="math inline">\(\mu \vDash A\)</span>, <span class="math inline">\(\wt{\EE_{\mu}} f\le 0\)</span>. (I.e., <span class="math inline">\(\exists \mu, \mu\vDash A\)</span>, <span class="math inline">\(\wt{\EE_{\mu}} f\le 0\)</span>.)
<ul>
<li>If <span class="math inline">\(A\)</span> is a variety, then we can replace with <span class="math inline">\(f \ge 0\)</span> and <span class="math inline">\(f&lt;0\)</span>.</li>
</ul></li>
</ul>
<p><em>Proof</em>. Consider 2 cases.</p>
<ul>
<li><span class="math inline">\(f\in \ol C\)</span>, <span class="math inline">\(C\)</span> the cone of the <span class="math inline">\(g\)</span> such that <span class="math inline">\(A\vdash_d \{g\ge 0\}\)</span>. Then <span class="math inline">\((1-\ep) f+ \ep g\in C\)</span>. <span class="math inline">\(f\ge -\ep(g-f) \ge \ep M'\)</span>. <!--Moving $f$ in the positive direction will eventually --></li>
<li>Else there is separating linear functional. <span class="math inline">\(\phi(1)&gt;0\)</span> because moving <span class="math inline">\(f\)</span> in direction of 1 eventually makes it in <span class="math inline">\(C\)</span>. Rescale so <span class="math inline">\(\phi(1)=1\)</span>.</li>
</ul>
<p>Think of <span class="math inline">\(\vdash\)</span> as a proof, and <span class="math inline">\(\mu \vDash\)</span> as saying <span class="math inline">\(\mu\)</span> is a model. <!--as saying, under a certain $\mu$, the conditions are in fact satisfied.--></p>
<p><strong>Lemma</strong></p>
<ol type="1">
<li>(Soundness) <span class="math inline">\(\mu \vDash_l A, A\vdash_{l'} B\implies \mu \vDash_{ll'}B\)</span>.</li>
<li>(Completness) If <span class="math inline">\(\forall \deg \mu=d, (\mu\vDash_l A \implies \mu\vDash_{l'} B)\)</span>, then <span class="math inline">\(\forall \ep&gt;0, A\vdash_d B_{\ep'}\)</span> (weaken each constraint by <span class="math inline">\(\ep\)</span>).</li>
</ol>
<p><strong>Theorem</strong> (SoS algorithm).</p>
<ul>
<li>Given satisfiable <span class="math inline">\(A\)</span>, output in time <span class="math inline">\(n^{O(d)}\)</span> a degree <span class="math inline">\(d\)</span> pseudo-distribution satisfies <span class="math inline">\(\mu\vDash A\)</span> up to error <span class="math inline">\(2^{-n}\)</span>.</li>
<li>(For varieties) Given a basis for <span class="math inline">\(\R[x]_{\le d} \cap I(\Om)\)</span>, can find a degree-<span class="math inline">\(d\)</span> Sos proof, or <span class="math inline">\(\mu\)</span> with <span class="math inline">\(\wt{\EE_{\mu}}\le 2^{-n}\)</span>.</li>
</ul>
<h5 id="exercises-1">Exercises</h5>
<ol type="1">
<li>NA</li>
<li>NA</li>
<li>Let <span class="math inline">\(r\)</span> be argmin. Write <span class="math inline">\(f(r) + (x-r)^2q(x)\)</span>. Repeat.</li>
<li><span class="math inline">\(\{\ve{x}^2\le 1\} \vdash_d \{-1\le x_i\le 1\}\)</span>. Use these.</li>
</ol>
<h4 id="planted-clique-hardness">5.3 Planted clique hardness</h4>
<p>Planted clique: Take <span class="math inline">\(G(n,\rc 2)\)</span>, add a random <span class="math inline">\(\om\)</span>-clique.</p>
<p>Whp the max clique of <span class="math inline">\(G(n,\rc 2)\)</span> is of size <span class="math inline">\(c\ln(n)\)</span>, so for <span class="math inline">\(\Om(\ln n)\)</span> size cliques there is quasipoly algorithm.</p>
<p><strong>Theorem</strong> (SoS hardness). Let <span class="math inline">\(d(n)\)</span> be a function. There is <span class="math inline">\(c\)</span> such that for <span class="math inline">\(\om = n^{\rc 2 - c\pf{d}{\ln n}^{\rc 2}}\)</span> and large enough <span class="math inline">\(n\)</span>, wp <span class="math inline">\(1-\rc n\)</span> over <span class="math inline">\(G\sim G(n,\rc 2)\)</span>, there is a degree <span class="math inline">\(d\)</span> pseudodistribution <span class="math inline">\(\mu\)</span> over <span class="math inline">\(B^n\)</span> that is consistent with <span class="math inline">\(x_ix_j=0\)</span> for every <span class="math inline">\(i\nsim j\)</span> in <span class="math inline">\(G\)</span> and such that <span class="math inline">\(\wt{\EE_{\mu}} \sumo in x_i\ge \om\)</span>.</p>
<p>In contrast with Max-3XOR, in planted clique, each variable has a weak but GLOBAL effect on all other variables.</p>
<p><strong>Definition</strong>. A degree <span class="math inline">\(d\)</span> pseudoexpectation map is <span class="math inline">\(G\mapsto \mu_G, \deg (\mu_G)=d\)</span>. It is <em>pseudocalibrated with respect to <span class="math inline">\(f\)</span></em> if <span class="math display">\[\sum_{G\sim G(n,\rc 2)} \wt{\EE_{G}} f_G = \EE_{(G,x)\sim G(n,\rc 2,\om)}f_G(x).\]</span> (<span class="math inline">\(x\)</span> is the characteristic vector of the planted clique.)</p>
<p>(Q: <strong>Why is the LHS <span class="math inline">\(G(n,\rc2)\)</span> rather than <span class="math inline">\(G(n,\rc 2,\om)\)</span>?</strong>)</p>
<p>Example: If <span class="math inline">\(f_G(x)=x_{17}\)</span>, then a first estimate could be <span class="math inline">\(\wt E x_{17}=\fc{\om}n\)</span>. But if we want to get higher degree functions, ex. covariance with degree, right, then we should not each <span class="math inline">\(\wt E x_i\)</span> to <span class="math inline">\(\fc{\om}n\)</span>.</p>
<p><em>Proof</em> (Sketch). Let <span class="math inline">\(\mu(G,x) = \mu_{planted}(G,x)_{\deg_x\le d, \deg_G\le \tau}\)</span>. Show it still satisfies normalization, etc. Hrd part is psd-ness.</p>
<p><strong>Finish</strong></p>
<h5 id="exercises-2">Exercises</h5>
<ol type="1">
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li><span class="math inline">\(\EE_{G\sim G(n,\rc2)}\wt{\EE_G} p_G^2 = \EE_{(G,x)\sim G(n, \rc 2,\om)} p_G^2(x)\)</span>. So all <span class="math inline">\(=0\)</span>. Now <span class="math inline">\(a\wt{\EE_G} p_G \le a^2 \wt{\EE_G} p_G^2 + 1\)</span>, take <span class="math inline">\(a\to \iy\)</span>.</li>
<li></li>
</ol>
<h3 id="sphere">6 Sphere</h3>
<ul>
<li>Tensor PCA: given <span class="math inline">\(\E X^{\ot d}\)</span>, find <span class="math inline">\(\max \E\an{X,x}^d\)</span>.</li>
<li>Sparsest vector in subspace.
<ul>
<li>Sparseness is approximated by <span class="math inline">\(\fc{\ve{v}_q}{\ve{v}_p}\)</span>, <span class="math inline">\(q&gt;p\)</span>.
<ul>
<li><span class="math inline">\(q=\iy, p=1\)</span> onlygives <span class="math inline">\(\wt O(\sqrt n)\)</span> approximation</li>
<li><span class="math inline">\(q=2,p=1\)</span> is good by inefficient to compute.</li>
<li><span class="math inline">\(q=4,p=2\)</span> is amenable to SoS</li>
</ul></li>
</ul></li>
<li>Quantum
<ul>
<li>Separable <span class="math inline">\(v=ww^T\)</span>, <span class="math inline">\(v\in \R^{N=M^2}\)</span>.</li>
<li>Quantum measurement: <span class="math inline">\(N\times N\)</span> matrix <span class="math inline">\(M\)</span>, <span class="math inline">\(M\preceq I\)</span>, <span class="math inline">\(\Pj(M\text{ accepts }\rh) = \Tr(M\rh)\)</span>.</li>
<li><span class="math inline">\(\ep\)</span>-separable: <span class="math inline">\(|\Tr(M\rh)-\Tr(M\rh')|\le \ep\)</span> for all <span class="math inline">\(M\)</span>.</li>
<li>Quantum separability problem: separable vs. not <span class="math inline">\(\ep\)</span>-separable.
<ul>
<li>Under ETH, needs <span class="math inline">\(N^{\Om_\ep(\ln N)}\)</span> timme.</li>
</ul></li>
<li>Best separable state: given <span class="math inline">\(1&gt;c&gt;s&gt;0\)</span>, decide: there exists <span class="math inline">\(\rh: \Tr(M\rh)\ge c\)</span>, <span class="math inline">\(\rh\)</span> separable, or for all separable, <span class="math inline">\(\Tr(M\rh)\le s)\)</span>.
<ul>
<li>For <span class="math inline">\((1,1-\ep)\)</span> this is QAM(2). <span class="math inline">\(M\)</span> is the verifier receiving quantum states from provers guaranteed to be non-entangled.</li>
<li>Known: <span class="math inline">\(QMA(2)\in EE\)</span>. Quasipoly algorithms places in EXP.</li>
<li>There always exists a pure state maximizing acceptance probability, so want <span class="math inline">\(\max_{\ve{x}=1} \Tr(M(xx^T)^{\ot 2})\)</span>.</li>
</ul></li>
</ul></li>
</ul>
<h4 id="tensor-decomposition-1114">Tensor decomposition (11/14)</h4>
<p>Tensor decomposition with and without SoS</p>
<p>Plan</p>
<ul>
<li>Problem definition</li>
<li>Application (mixture of Gaussians)</li>
<li>Simple algorithm</li>
<li>SoS algorithm</li>
</ul>
<p>Problem</p>
<ul>
<li>Unknown: <span class="math inline">\(a_1,\ldots, a_n\in \R^d\)</span>.</li>
<li>Given: <span class="math inline">\(\sumo in (1,a_i)^{\ot k}\)</span>. I.e., we get <span class="math inline">\(\pa{\sumo in a_i^{\ot l},l\le k}\)</span>. <!--how important is 1, all components same--></li>
<li>Goal: Find vectors close to <span class="math inline">\(a_1,\ldots, a_n\)</span>. <!--can't hope polytime in full generality: conditions on $a_1,\ldots, a_n,k$. Depends on assumptions. Most cases, not a big difference. Simpler if lower moments. --></li>
<li>Question: Under what conditions on the geometry of <span class="math inline">\(a_1,\ldots, a_n\)</span> and <span class="math inline">\(k\)</span> can we solve this problem?</li>
<li>Simplest case: <span class="math inline">\(a_1,\ldots, a_n\)</span> are orthogonal. <span class="math inline">\(k=3\)</span> is enough.
<ul>
<li>Also: <span class="math inline">\(a_1,\ldots, a_n\)</span> linearly independent. <span class="math inline">\(k=3\)</span> is enough, <span class="math inline">\(k=2\)</span> is not.</li>
</ul></li>
</ul>
<p>Application</p>
<ul>
<li>Unknown: probability distribution <span class="math inline">\(D\)</span> over <span class="math inline">\(\R^d\)</span>, uniform mixture of <span class="math inline">\(N(a_1,I),\ldots, N(a_n,I)\)</span>.</li>
<li>Given (polynomial number of) samples of <span class="math inline">\(D\)</span>, find <span class="math inline">\(a_1,\ldots, a_n\)</span>.</li>
</ul>
<p>Algorithm (high-level)</p>
<ul>
<li>Use samples to estimate <span class="math inline">\(\sumo in a_i^{\ot 3}\)</span>. (Possible for many families of distribution, e.g. Latent Dirichlet, topic modeling)</li>
<li>Use tensor decomposition to find <span class="math inline">\(a_1,\ldots, a_n\)</span>.</li>
</ul>
<p>Estimate <span class="math inline">\(\sum a_i^{\ot 3}\)</span> from samples</p>
<ul>
<li>Compute empirical moments from samples <span class="math inline">\(y_1,\ldots, y_N\)</span> from <span class="math inline">\(D\)</span>. <span class="math display">\[ \wt M_l = \rc N \sumo iN y_i^{\ot l}. \]</span></li>
<li><span class="math inline">\(\E \wt M_l = M_l = \EE_{y\sim D} y^{\ot l}\)</span>. For <span class="math inline">\(N\gg d^l\)</span>, <span class="math inline">\(\wt M_l\approx M_l\)</span> with high probability.</li>
<li>Estimate <span class="math inline">\(\sum a_i^{\ot3}\)</span> from <span class="math inline">\(\wt M_i\approx M_i\)</span> for <span class="math inline">\(i=1,3\)</span>.</li>
<li>\begin{align}
M_1 &amp;=\EE_{y\sim D} y = \rc n \sum a_i\\
M_2 &amp;=\EE y^{\ot 2} = \rc n \sum \E_{g\sim N(0,I)} (a_i+g)^{\ot 2}\\
&amp;= \rc n \sum a_i^{\ot 2} + \E g^{\ot 2}\\
&amp;= I + \rc n \sum a_i^{\ot 2}\\
M_3 &amp;= \rc n \sum \EE_{g\sim N(0,I)}(a_i+g)^{\ot 3}\\
&amp;= \rc n \sumo in [a_i^{\ot 3} + \ub{\E a_i^{\ot 2}\ot g}{0} + \ub{\E a_i \ot g^{\ot 2} + a_i\ot g\ot a_i + a_i^{\ot 2}\ot g}{a_i\ot I + I \ot a_i + \sumo id e_i\ot a_i \ot e_i}]\\
&amp;= \rc \sum a_i^{\ot 3} + \ub{\pa{\rc n \sum a_i}\ot I}{\text{already estimated}} + \pat{symmetric terms}
\sum a_i^{\ot 3} &amp;= \pat{simple function of $M_1,M_3$}.
\end{align}</li>
</ul>
<p>Random contraction algorithm (Jeinrich)</p>
<ul>
<li>Choose <span class="math inline">\(g\sim N(0,I)\)</span></li>
<li>Compute contraction (<span class="math inline">\((I\ot I\ot g^*) M_3=A\)</span>)</li>
<li>Compute top eigenvector of <span class="math inline">\(A\)</span>. <!--$A_{\{1\},\{2\}}$--></li>
</ul>
<p>Consider the orthogonal case first. Note that the most difficult case is if all the norms are the same (if they are all different, we can use SVD).</p>
<p>… (see notebook)</p>
<p>Decompose random gaussian along 2 directions. If correlation large ,more likely to recover <span class="math inline">\(e_1\)</span>.</p>
<p><strong>Claim</strong>. <span class="math inline">\(\E_g \ve{(I^{\ot 2}\ot g^T)X} \le \sqrt{\ln d} \si\)</span> where <span class="math inline">\(\si = \max\{\ve{X}_{13,2},\ve{X}_{1,23}\}\)</span>.</p>
<!-- how apply syntactically. both are linear functions of $g$. -->
<p>This is a linear function of <span class="math inline">\(g\)</span>, we can write it as in the concentration inequality.</p>
\begin{align}
(I^{\ot 2} \ot g^T)X &amp;=\sum g_i \ub{(I^{\ot 2} \ot e_i^T)X}{A_i}\\
\ve{X} &amp;=\ve{X^TX}^{\rc 2} = \ve{XX^T}^{\rc 2}\\
\sum A_iA_i^T &amp;= X_{3,12}^T X_{3,12}\\
\sum A_i^TA_i &amp;= X_{1,23}^T X_{1,23}
\end{align}
<p>(check the indices).</p>
<p>Davis-Kahan: <span class="math inline">\(\ve{A-aa^T}\le o(1) \ve{a}^2\)</span> implies that the top eigenvector of <span class="math inline">\(A\)</span> is <span class="math inline">\(o(1)\)</span> close to <span class="math inline">\(a\)</span>. So top eigenvector of <span class="math inline">\((I\ot I\ot a^T)X\)</span> is close to <span class="math inline">\(a\)</span>.</p>
<p><strong>Theorem</strong>. Let <span class="math inline">\(a_1,\ldots, a_n\in R^d\)</span>, <span class="math inline">\(\ve{a_i}=1\)</span>. Suppose <span class="math inline">\(\sum a_i a_i^T \preceq \si I\)</span> (<span class="math inline">\(\si\)</span> is overcompleteness), <span class="math inline">\(\max_{i\ne j} |\an{a_i,a_j}|\le \rh\)</span>. Then, given <span class="math inline">\(\sumo ih a_i^{\ot3}\)</span>, can recover vectors close to <span class="math inline">\(a_1,\ldots, a_n\)</span> in polytime whenever <span class="math inline">\(\rh \si^2=o(1)\)</span>.</p>
<p>E.g., <span class="math inline">\(\si=d^{0.1}\)</span>, <span class="math inline">\(\rh = d^{0.3}\)</span>. This is satisfied by <span class="math inline">\(d^{1.1}\)</span> random unit vectors, or <span class="math inline">\(n=d^{1.24}, (d^{.24},d^{-.5})\)</span>. <!-- ex. $\si$ random ortho bases, $\rh$ close to $\sqrt d$. When $\si\ll d^{\rc 4}$. Up to $d^{0.1}$. Spectral noise $d^{0.1}$. Condition on $g_1$ large enough to swallow, $d^{-O(c^2)}$. Exponential in $d^{.1}$--> <!-- can this go up to 3/2? If $\rh=d^{-.5}$, $\si$ up to $d^{.5}$, would imply for random unit. --></p>
<p>This algorithm can be poly in <span class="math inline">\(d\)</span> and <span class="math inline">\(n\)</span>.</p>
<p>Open question: Is this true for <span class="math inline">\(\rh\si &lt;o(1)\)</span>? This would imply for <span class="math inline">\(d^{1.5}\)</span> random vectors. (Though there is another argument that can give up to <span class="math inline">\(d^{1.5}\)</span>.) (Maybe a <span class="math inline">\(-\ep\)</span> in exponent.)</p>
<!-- isotropic position -->
<p>Algorithm: given <span class="math inline">\(M_3\)</span>,</p>
<ol type="1">
<li>Compute p.d. <span class="math inline">\(\mu\)</span> over unit sphere of <span class="math inline">\(\R^d\)</span> so as to maximize <span class="math inline">\(\an{\wt\E_{\mu} x^{\ot 3}, M_3}\)</span>, which satisfies <span class="math display">\[\ve{\wt \EE_{\mu} x^{\ot 6}}_{1234, 56}\le 2.\]</span></li>
<li>Run random contraction on <span class="math inline">\(\wt\EE_{\mu} x^{\ot 6} = \wt\EE_{\mu} (x^{\ot 2})^{\ot 3}\)</span>. (Letting <span class="math inline">\(X=\wt \E(x^{\ot 2})^{\ot 3}\in (\R^{d^2})^{\ot 3}\)</span>, this is <span class="math inline">\(\ve{X}_{12,3}\)</span>.)</li>
</ol>
<p>Use SoS to find tensor which we can run random contraction on, living in higher-dimensional space.</p>
<p>Under conditions of theorem, let <span class="math inline">\(b_i=a_i^{\ot 2}\)</span>, can show that random contraction works for <span class="math inline">\(X=\sum b_i^{\ot 3}\)</span>. (Show <span class="math inline">\(\ve{X}_{12,3}\le O(1)\)</span> if <span class="math inline">\(\rh\si^2\ll 1\)</span>.) Recover the <span class="math inline">\(b_i\)</span>’s, then recover <span class="math inline">\(a_i\)</span>’s. But we don’t get <span class="math inline">\(b_i^{\ot 3}\)</span>, we just get <span class="math inline">\(a_i^{\ot 3}\)</span>. SoS: given <span class="math inline">\(a_i^{\ot 3}\)</span> it’s computing <span class="math inline">\(b_i^{\ot 3}\)</span>. Find pseudodistribution matching 3rd moment and outputs 6th moments.</p>
<p>The intended <span class="math inline">\(\mu\)</span> is uniform over <span class="math inline">\(a_1,\ldots, a_n\)</span>. <span class="math inline">\(\wt \EE_{\mu} x^{\ot 3} = \rc n \sum a_i^{\ot 3}\)</span> and <span class="math inline">\(\E x^{\ot 6} = \rc n \sum a_i^{\ot 6} = \rc X\)</span>.</p>
<p>Philosophy: first prove statement for distributions, then verify that the steps fall in SoS proof system.8</p>
<h4 id="tensor-decomposition-1117-talk">Tensor decomposition (11/17) talk</h4>
<ul>
<li>3rd moment enough for <span class="math inline">\(n\le d\)</span> linearly independent vectors</li>
<li>4th moment for <span class="math inline">\(n\le \fc{d^2}{10}\)</span> generic vectors (e.g., randomly perturbed)</li>
</ul>
<p>When are 3rd moments enough for overcomplete tensors, <span class="math inline">\(n\gg d\)</span>? Robustness against noise in input tensor?</p>
<p>We use lower-degree moments, have better error robustness, and develop new improved analyses of classical tensor decomposition algorithms.</p>
<p>SoS a priori not practical but we can extract practical algorithms</p>
<p>Dream up higher-degree moments and apply classical algorithms.</p>
<p>Algorithm: Magic box lifts 3rd moments to 6th moment, <span class="math inline">\(M_3=\sum_i a_i^{\ot 3}\)</span> to <span class="math inline">\(M_6=\sum_i a_i^{\ot 6}\)</span>. Then apply Jennrich to get <span class="math inline">\(a_i^{\ot 2}\)</span>.</p>
<p>It’s not even clear this is information theoretically possible!</p>
<p>Ideal implementation: find <span class="math inline">\(D\)</span> over unit sphere subject to <span class="math inline">\(\EE_D x^{\ot 3} = \rc n M_3\)</span>, then use <span class="math inline">\(\EE_D x^{\ot 6}\)</span>. Claim is that this is <span class="math inline">\(\approx \rc n M_6\)</span>.</p>
Key inequality: by niceness of vectors, <span class="math inline">\(a_1,\ldots, a_n\)</span> are approximate global maximizers. For all <span class="math inline">\(x\in \bS^{d-1}\)</span>,
\begin{align}
P(x) &amp;\le \max_{i\in [n]} \an{a_i,x} + O(\rh si^2).\\
1 &amp;\approx \EE_D \sumo in \an{a_i,x}^3\\
\EE_{D} P(x) &amp;\ge 1-o(1).
\end{align}
<p>Also close to uniform over <span class="math inline">\(a_i\)</span>.</p>
<p>Remaining questions:</p>
<ol type="1">
<li>Find <span class="math inline">\(D\)</span>? Relax to pseudo-distributions.</li>
<li>Can Jennrich tolerate this error (Frobenius)? No. Fix by adding maximum entropy constraint <span class="math inline">\(\ve{\E_D x^{\ot 4}}_{\text{spectral}}\le \fc{1+o(1)}n\)</span>.</li>
</ol>
<p>SoS is slow. Instead find direct algorithm to fool Jennrich, <span class="math display">\[
\sum_{ijk} \an{a_i,a_k}\an{a_j,a_k}
(a_i\ot a_j) \ot (a_i\ot a_j) \ot a_k\approx \sumo in a_i^{\ot 5} = M_5.
\]</span></p>
<p>Tensor structure to implement Jennrich in time <span class="math inline">\(O(d^{1+\om})\)</span>.</p>
<p>Open: 3 tensors with <span class="math inline">\(d^{1+\ep}\)</span> generic components, <span class="math inline">\(d^{1.5+\ep}\)</span> random components?</p>
<p>Tensor power: fewer guarantees. Does better in amount of time needed to reduce error.</p>
<h3 id="reading">Reading</h3>
<ul>
<li><a href="https://windowsontheory.org/2016/08/27/proofs-beliefs-and-algorithms-through-the-lens-of-sum-of-squares/">Windows on theory</a>, <a href="http://scrible.com/s/2KMCS">h</a></li>
</ul>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>There is another way to relax this kind of problem, by changing the values <span class="math inline">\(x_i\)</span> could take from <span class="math inline">\(\pm1\)</span> to vectors, like in Goemans-Williamson. We don’t consider this kind of relaxation here. (Though we can do GW here too…?)<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>There is another way to relax this kind of problem, by changing the values <span class="math inline">\(x_i\)</span> could take from <span class="math inline">\(\pm1\)</span> to vectors, like in Goemans-Williamson. We don’t consider this kind of relaxation here. (Though we can do GW here too…?)<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Not really. I’m confused here: why don’t we factor through <span class="math inline">\((\R[x]/I)_{\le l}\)</span> instead of <span class="math inline">\((\R[x]/I)_{\le l/2}\times (\R[x]/I)_{\le l/2}\)</span>?<a href="#fnref3">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class="st_facebook_large" displayText="Facebook"></span>
    <span class="st_twitter_large" displayText="Tweet"></span>
    <span class="st_googleplus_large" displayText="Google +"></span>
    <span class="st_reddit_large" displayText="Reddit"></span>
    <span class="st__large" displayText></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>


<!-- Footer -->
<div id="footer">
  <div class="container">
    Built with
    <a href="http://jaspervdj.be/hakyll">Hakyll</a> 
    using 
    <a href="http://www.getbootstrap.com">Bootstrap</a>, 
    <a href="http://www.disqus.com">Disqus</a>,
    <a href="http://ignorethecode.net/blog/2010/04/20/footnotes/">Footnotes.js</a>,
    <a href="http://highlightjs.org/">Highlight.js</a>, 
    <a href="http://www.mathjax.org">MathJax</a>, 
    and <a href="http://www.sharethis.com">ShareThis</a>.
  </div>
</div>
</body>

</html>

<!-- SCRIPTS -->
<!-- jQuery-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="../../../bootstrap/js/bootstrap.min.js"></script>

<!-- Extension : Highlight.js @ https://highlightjs.org/ -->
<!-- Syntax highlighting tomorrow-night-bright, agate-->
<link rel="stylesheet" href="../../../highlight/css/tomorrow-night-bright.css">
<script src="../../../highlight/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- Extension : MathJax @ https://docs.mathjax.org/en/v2.5-latest/tex.html -->
<!-- MathJax/config/local/local.js contains macros. Need to provide entire URL-->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,http://holdenlee.github.io/notebook/MathJax/config/local/local"></script>

<!-- Extension : Footnotes @ http://ignorethecode.net/blog/2010/04/20/footnotes/ -->
<script src="../../../footnotes/js/footnotes.js"></script>

<!-- Extension : Disqus @ http://disqus.com -->
<!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->

<script src="../../../disqus/js/disqus.js"></script>



<!-- Extension : Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73261814-1', 'auto');
  ga('send', 'pageview');

</script>

