<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

  <meta name="description" content="Holden Lee's Research Notebook">
  <meta name="author" content="Holden Lee">
    
  <title>Real channels</title>

  <!-- Bootstrap core CSS -->
  <link href="../../../bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

  <!-- Custom styles for this template -->
  <link href="../../../css/blog.css" rel="stylesheet">
  <link href="../../../css/default.css" rel="stylesheet">

  <!-- Extension : Footnotes -->
  <link href="../../../footnotes/css/footnotes.css" rel="stylesheet">

  <!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->
  

  <!-- Extension : Collapsible lists @ http://code.stephenmorley.org/javascript/collapsible-lists/-->
  <link href="../../../collapsible_lists/css/collapsible.css" rel="stylesheet">
  <script type="text/javascript" src="../../../collapsible_lists/js/CollapsibleLists.js"></script>

</head>

<body>

<!-- Navigation bar. navbar-inverse is black, navbar-default is white.-->
<!-- To make a button active (pressed), use <li class="active"> -->
<div id="header">
  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../">Notebook</a>
      </div>
      <div id="navbar" class="collapse navbar-collapse">
        <ul class="nav navbar-nav">
          <li><a href="../../../">Home</a></li>
          <li><a href="../../../sitemap.html">Sitemap</a></li>
<!-- TODO: Distinguish between PAPERS, RESEARCH QUESTIONS, BOOKS -->
<!-- TODO: make this part a for loop over main pages -->
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </nav>
</div>
<!-- Content -->
<!--div id="content">
  <h1>Mental Wilderness</h1>-->



<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Real channels</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-02-27 
          , Modified: 2016-02-27 
	</p>
      
       <p>Tags: <a href="../../../tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#reading">Reading</a><ul>
 <li><a href="#mckay">McKay</a><ul>
 <li><a href="#exercises">Exercises</a></li>
 </ul></li>
 <li><a href="#cover-thomas">Cover, Thomas</a><ul>
 <li><a href="#ch.-8">Ch. 8</a></li>
 <li><a href="#ch.-9">Ch. 9</a></li>
 </ul></li>
 </ul></li>
 <li><a href="#scraps">Scraps</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>References:</p>
<ul>
<li>McKay Ch. 11</li>
<li>Cover, Thomas Ch. 9</li>
</ul>
<h2 id="reading">Reading</h2>
<h3 id="mckay">McKay</h3>
<p>What’s the model?</p>
<ul>
<li>Transmit a signal <span class="math inline">\(x(t)\)</span> in a channel with error at every moment in time Gaussian with variance <span class="math inline">\(\si^2\)</span>.</li>
<li>You are limited to using <span class="math inline">\(v\)</span> power, i.e., <span class="math inline">\(\int_0^T \fc{x(t)^2}{T}\,dt\le v\)</span>.</li>
<li>You are bandwidth-limited: <span class="math inline">\(x\)</span> must be a sum <span class="math inline">\(x=\sum_{n=1}^{N} x_n\phi_n\)</span> where <span class="math inline">\(\phi_n\)</span> are orthonormal basis. (i.e., <span class="math inline">\(\sum x_n^2=v\)</span>.) Take the functions to be <span class="math inline">\(\cos\pf{2\pi kx}{T}\)</span> and <span class="math inline">\(\sin\)</span> for <span class="math inline">\(1\le k\le TW\)</span>. We have <span class="math inline">\(N=2TW\)</span> where <span class="math inline">\(W\)</span> is the max allowable frequency.</li>
</ul>
<p>At what (limiting) rate can you transmit information?</p>
<p>Note: A sum is like an integral. Take exps instead of sin/cos for simplicity. Then for <span class="math inline">\(x\)</span> a sum of exponentials <span class="math inline">\(\phi_n\)</span>, <span class="math display">\[
\int_0^1 x(t)\phi_n(t) = a_n = \EE_{t=0}^{N-1}x\pf{t}{N} \phi_n\pf{t}{N}.
\]</span></p>
<p>Useful: <span class="math inline">\(N(y;x,\si^2)N(x;0,v) = N\pa{x;\fc{v}{v+\si^2}y, \pa{\rc{v}+\rc{\si^2}}^{-1}}\)</span>. Mean is means harmonically weighted by variances (i.e. weighted by precisions). “Whenever two independent sources contribute information, via Gaussian distributions, about an unknown variable, the precisions add.”</p>
<p>“Real continuous channel with <span class="math inline">\(W\)</span> and noise <span class="math inline">\(W_0\)</span> is <span class="math inline">\(\fc{N}{T}=2W\)</span> uses per second of Gaussian with <span class="math inline">\(\si^2=N_0/2, \ol{x_n^2}\le \fc{P}{2W}\)</span>.</p>
<p>(How do you think of discrete bits as encoding a point in real space? Or are you transmitting analog information?)</p>
<p>Bandwidth is more powerful than low noise.</p>
<p>(Q: Why does M say that entropy can’t be defined for continuous variables? I guess this should be taken to mean that the definition of <span class="math inline">\(h\)</span> wouldn’t be invariant under change of coordinates.)</p>
<h4 id="exercises">Exercises</h4>
<ol type="1">
<li><p>(Solution p. 189/201) Use Lagrange multipliers with <em>functions</em> (calculus of variations). Use <span class="math inline">\(\fc{\de F}{\de P^*}\int P(x) f(x,P) \dx = f(x^*,P) + \int P(x)\fc{\de f(x,P)}{\de P(x)}\)</span> (cf. normal product rule). Note <span class="math inline">\(P(y|x)\)</span> is fixed (normal), but <span class="math inline">\(P(y)\)</span> depends on <span class="math inline">\(P(x)\)</span>, it is really a function of <span class="math inline">\(P\)</span>, <span class="math inline">\(f(P)\)</span>. Simplify, and then substitute <span class="math inline">\(P(y|x)\)</span>. Match coefficients in Taylor expansion.</p>
<p>Question: Why does the constraint <span class="math inline">\(\int P\dx=1\)</span> become <span class="math inline">\(\mu\pa{\int P\dx}\)</span> rather than <span class="math inline">\(\mu(\bullet-1)\)</span>? They disappear after differentiation!</p>
<p>TAKEAWAY: Calculus of variations. Gaussian distributions is best. (Why not 2 points as far as possible?)</p></li>
<li><p>Just calculate the integral. Mutual info is <span class="math inline">\(\rc2\ln \pa{1+\fc{v}{\si^2}}\)</span>. This is the capacity (explained on p. 182) (?). The more power, the more capacity, scaling by log.</p></li>
</ol>
<h3 id="cover-thomas">Cover, Thomas</h3>
<h4 id="ch.-8">Ch. 8</h4>
<ul>
<li><strong>Differential entropy</strong> <span class="math inline">\(h=-\int f\lg f\dx\)</span>. “differential entropy is the logarithm of the equivalent side length of the smallest set that contains most of the probability.”
<ul>
<li>How differential entropy and entropy are related: Let <span class="math inline">\(X^{\De}\)</span> be <span class="math inline">\(X\)</span> quantized to <span class="math inline">\(\De\)</span>. Then <span class="math inline">\(\lim_{\De\to 0}H(X^{\De})+\lg \De = h(X)\)</span>.</li>
<li>Note mutual info is limit of quantized mutual info (the <span class="math inline">\(\lg \De\)</span>’s cancel).</li>
<li><strong>KL divergence</strong>: <span class="math inline">\(D(f||g)=\int f \lg \fc{f}{g}\ge 0\)</span>.</li>
</ul></li>
<li>Basic calculations
<ul>
<li><span class="math inline">\(h(N(0,\si^2)) = \rc2 \lg (2\pi e\si^2)\)</span></li>
<li><span class="math inline">\(\rh\)</span>-correlated Gaussians: <span class="math inline">\(I(X:Y) = -\rc2 \lg(1-\rh^2)\)</span>.</li>
</ul></li>
<li>Basic inequalities
<ul>
<li>By LoLN, AEP holds for continuous random variables (<span class="math inline">\(\log f\)</span> needs to be <span class="math inline">\(L^1\)</span>?).</li>
<li>(Hadamard) <span class="math inline">\(\det(K)\le \prod K_{ii}\)</span> by subadditivity of entropy.</li>
<li><span class="math inline">\(h(AX)=h(X)+\lg |\det(A)|\)</span>.</li>
</ul></li>
<li>Normal maximizes entropy over distributions with same variance. Proof: <span class="math inline">\(0\le D(g||\phi_K) = -h(g)+h(\phi_K)\)</span> where the second term follows from the fact that <span class="math inline">\(g,\phi_K\)</span> have the same second moments and <span class="math inline">\(\log \phi_K\)</span> is a quadratic form (write it out!).</li>
<li>Estimation version of Fano: <span class="math inline">\(\E[(X-\hat X)^2]\ge \rc{2\pi e}e^{2h(X)}\)</span>, equality only if <span class="math inline">\(X\)</span> Gaussian and <span class="math inline">\(\hat X = \E X\)</span>. Proof: use that Gaussian distribution has max entropy for given variance.
<ul>
<li>Cor: <span class="math inline">\(\E[(X-\hat X(Y))^2] \ge \rc{2\pi e} e^{2h(X|Y)}\)</span>.</li>
</ul></li>
</ul>
<p>Nice summary on p. 282.</p>
<h4 id="ch.-9">Ch. 9</h4>
<ul>
<li>A Gaussian channel with power <span class="math inline">\(P\)</span> can be converted to a binary symmetric channel with error <span class="math inline">\(1-\Phi\pa{\sfc{P}{\si^2}}\)</span>.</li>
<li>9.1: A more conceptual way to upper-bound the mutual information: <span class="math inline">\(Y=X+Z\)</span> where <span class="math inline">\(Z\)</span> is noise.
<ul>
<li>Theorem 8.6.5: normal maximizes the entropy for a given variance. (cf. entropy-maximizing distributions. Use CoV here?) Then <span class="math inline">\(I(X:Y)=h(Y)-h(Z)\le \rc2 \log \pa{1+\fc PN}\)</span>, equality only when <span class="math inline">\(X\)</span> is Gaussian.</li>
</ul></li>
<li>Model of Gaussian channel: Encoding, Gaussian noise, decoding, <span class="math inline">\([M]\xra{x}\mathcal{X}^n \xra{N}\mathcal{Y}^n \xra{g} [M]\)</span>.</li>
<li><strong>Theorem 9.1.1</strong>: Capacity of Gaussian channel is <span class="math inline">\(\rc 2 \lg\pa{1+\fc{P}N}\)</span>.
<ul>
<li>Heuristic proof: Pack spheres of radius <span class="math inline">\(\sqrt{nN}\)</span> in <span class="math inline">\(\sqrt{n(P+N)}\)</span>.</li>
<li>Think of mutual info as a limit achievable in infinite dimensions. It doesn’t make sense to encode a discrete set with a probability distribution, but in large dimensions you can approximate it!</li>
<li>Proof of achievability: (1) generate codewords iid (2) search for jointly typical. (3) It’s rare that either it’s far from the codeword, or it’s close to another one.</li>
<li>Proof of necessity: Take uniform distribution over inputs. Use Fano’s inequality to relate probability of error and mutual info.
<ul>
<li><strong>Fano’s inequality</strong> says that for <span class="math inline">\(X\to Y\to \wh X\)</span>, <span class="math inline">\(P_e=\Pj(X\ne \wh X)\)</span>, <span class="math inline">\(H(P_e)+P_e\lg |\mathcal X| \ge H(X|\wh X) \ge H(X|Y)\)</span>.</li>
<li>Applied here, <span class="math inline">\(H(W|\hat W)\le H(P_e) + P_e\lg |\mathcal X|\le 1 + (nR) P_e^{(n)}=o(n)\)</span>. Since <span class="math inline">\(nR=H(W)\)</span>, we get that <span class="math inline">\(I(W:\hat W) \ge \asymp nR\)</span> and hence <span class="math inline">\(\sum I(X_i:Y_i) \ge \asymp nR\)</span>. But this sum can be bounded by the power (use Jensen on the <span class="math inline">\(P_i\)</span>).</li>
</ul></li>
</ul></li>
<li>6.3 Bandlimited channels: “Rate <span class="math inline">\(\rc{2W}\)</span> is sufficient to reconstruct the signal because it cannot change substantially in <span class="math inline">\(&lt;\rc2\)</span> a cycle.” Proof: Note that the Fourier transform is 0 outside <span class="math inline">\([-W,W]\)</span>. <span class="math inline">\(f\pf{n}{2W}\)</span> are the Fourier coefficients of <span class="math inline">\(F(\om)\)</span>. (“a bandlimited function has only 2W degrees of freedom per second”)<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>
<ul>
<li>Note band-limitation can be expressed as convolution by <span class="math inline">\(\mathcal F^{-1} 1_{\le W}\)</span>.</li>
<li>The characteristic-function basis for sampling at <span class="math inline">\(\fc{n}{2W}\)</span> are the translates of <span class="math inline">\(\text{sinc}(Wt) = \sin(Wt)/(Wt)\)</span>.</li>
</ul></li>
<li>Relating the 2 models.
<ul>
<li>In the Gaussian noise model, <span class="math inline">\(C=\rc 2 \log\pa{1+\fc{P_G}{N_G}}\)</span> where <span class="math inline">\(P\)</span> is power (average magnitude) and <span class="math inline">\(N\)</span> is variance.</li>
<li>In the band-limited model, suppose <span class="math inline">\(W\)</span> is bandwidth, <span class="math inline">\(P\)</span> is power, and <span class="math inline">\(\fc{N_0}2\)</span> is variance (power spectral density - I don’t understand this).</li>
<li>Match up by <span class="math inline">\(P_G=\fc{P}{2W}\)</span> (divide by sampling rate) and <span class="math inline">\(N_G=N_0/2\)</span> to get <span class="math inline">\(\rc2\lg\pa{1+\fc{P}{N_0W}}\)</span> per sample. Multiply by <span class="math inline">\(2W\)</span>. (<span class="math inline">\(\fc{P}{N_0W}\)</span> is SNR.)
\begin{equation} C= W\lg\pa{1+\fc{P}{N_0W}}.\end{equation}
With infinite bandwidth, <span class="math inline">\(C=\fc{P}{N_0}\lg e\)</span>.</li>
</ul></li>
<li>9.4 Parallel Gaussian channels: set of Gaussian channels in parallel with common (total) power constraint <span class="math inline">\(P\)</span>. How to distribute.
<ul>
<li>Maximize <span class="math inline">\(\sum \rc2\lg\pa{1+\fc{P_i}{N_i}}\)</span> subject to <span class="math inline">\(\sum P_i=P\)</span>. By Lagrange multipliers, the best solution is water-filling, p. 277.</li>
</ul></li>
<li>9.5 Channels with colored (correlated) Gaussian noise (SKIP)</li>
<li>9.6 Gaussian channels with feedback: for channels with memory, where the noise is correlated from time instant to time instant, feedback does increase capacity. (SKIP)</li>
</ul>
<h5 id="exercises-1">Exercises</h5>
<p>9.10 looks interesting.</p>
<h2 id="scraps">Scraps</h2>
<ul>
<li>In the presence of noise, doesn’t it help to sample <span class="math inline">\(&gt;2W\)</span> times per second? Can’t you approach the true average with more samples?</li>
<li></li>
</ul>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I thought that this meant the <span class="math inline">\(f\)</span> live in a finite-dimensional space. No? This confuses me. Are we taking infinitely many samples spaced <span class="math inline">\(\rc{2W}\)</span> apart? Because <span class="math inline">\(F\)</span> is not in a finite dimensional space.<a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class="st_facebook_large" displayText="Facebook"></span>
    <span class="st_twitter_large" displayText="Tweet"></span>
    <span class="st_googleplus_large" displayText="Google +"></span>
    <span class="st_reddit_large" displayText="Reddit"></span>
    <span class="st__large" displayText></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>


<!-- Footer -->
<div id="footer">
  <div class="container">
    Built with
    <a href="http://jaspervdj.be/hakyll">Hakyll</a> 
    using 
    <a href="http://www.getbootstrap.com">Bootstrap</a>, 
    <a href="http://www.disqus.com">Disqus</a>,
    <a href="http://ignorethecode.net/blog/2010/04/20/footnotes/">Footnotes.js</a>,
    <a href="http://highlightjs.org/">Highlight.js</a>, 
    <a href="http://www.mathjax.org">MathJax</a>, 
    and <a href="http://www.sharethis.com">ShareThis</a>.
  </div>
</div>
</body>

</html>

<!-- SCRIPTS -->
<!-- jQuery-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="../../../bootstrap/js/bootstrap.min.js"></script>

<!-- Extension : Highlight.js @ https://highlightjs.org/ -->
<!-- Syntax highlighting tomorrow-night-bright, agate-->
<link rel="stylesheet" href="../../../highlight/css/tomorrow-night-bright.css">
<script src="../../../highlight/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- Extension : MathJax @ https://docs.mathjax.org/en/v2.5-latest/tex.html -->
<!-- MathJax/config/local/local.js contains macros. Need to provide entire URL-->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,http://holdenlee.github.io/notebook/MathJax/config/local/local"></script>

<!-- Extension : Footnotes @ http://ignorethecode.net/blog/2010/04/20/footnotes/ -->
<script src="../../../footnotes/js/footnotes.js"></script>

<!-- Extension : Disqus @ http://disqus.com -->
<!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->

<script src="../../../disqus/js/disqus.js"></script>



<!-- Extension : Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73261814-1', 'auto');
  ga('send', 'pageview');

</script>

