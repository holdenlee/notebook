<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

  <meta name="description" content="Holden Lee's Research Notebook">
  <meta name="author" content="Holden Lee">
    
  <title>Towards a better understanding of streaming PCA (Yuanzhi Li)</title>

  <!-- Bootstrap core CSS -->
  <link href="../../../../bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

  <!-- Custom styles for this template -->
  <link href="../../../../css/blog.css" rel="stylesheet">
  <link href="../../../../css/default.css" rel="stylesheet">

  <!-- Extension : Footnotes -->
  <link href="../../../../footnotes/css/footnotes.css" rel="stylesheet">

  <!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->
  

  <!-- Extension : Collapsible lists @ http://code.stephenmorley.org/javascript/collapsible-lists/-->
  <link href="../../../../collapsible_lists/css/collapsible.css" rel="stylesheet">
  <script type="text/javascript" src="../../../../collapsible_lists/js/CollapsibleLists.js"></script>

</head>

<body>

<!-- Navigation bar. navbar-inverse is black, navbar-default is white.-->
<!-- To make a button active (pressed), use <li class="active"> -->
<div id="header">
  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../../">Notebook</a>
      </div>
      <div id="navbar" class="collapse navbar-collapse">
        <ul class="nav navbar-nav">
          <li><a href="../../../../">Home</a></li>
          <li><a href="../../../../sitemap.html">Sitemap</a></li>
<!-- TODO: Distinguish between PAPERS, RESEARCH QUESTIONS, BOOKS -->
<!-- TODO: make this part a for loop over main pages -->
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </nav>
</div>
<!-- Content -->
<!--div id="content">
  <h1>Mental Wilderness</h1>-->



<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Towards a better understanding of streaming PCA (Yuanzhi Li)</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-05-10 
          , Modified: 2016-05-10 
	</p>
      
       <p>Tags: <a href="../../../../tags/pca.html">pca</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p>Given data <span class="math inline">\(x_i\in \R^d\)</span> sampled from an unknown distribution with <span class="math inline">\(\E[x_ix_i^T]=X\)</span>, where <span class="math inline">\(x\)</span>’s are independent, compute top <span class="math inline">\(k\)</span> eigenvectors of <span class="math inline">\(X\)</span>.</p>
<p>(For the asymmetric <span class="math inline">\(xy^T\)</span> case, finding the top <span class="math inline">\(k\)</span> is harder.)</p>
<p>Normal SVD is not memory efficient; you need to store a <span class="math inline">\(d\times d\)</span> matrix. Goal: compute <span class="math inline">\(k\)</span>-SVD in space <span class="math inline">\(O(kd)\)</span>.</p>
<p>A popular heuristic is to start with a random initial matrix <span class="math inline">\(U_0\in \R^{d\times k}\)</span>. On receiving <span class="math inline">\(x_i\)</span>, update <span class="math inline">\(U_i=(I+\eta_i x_ix_i^T)U_{i-1}\)</span> and let <span class="math inline">\(U_i=QR(U)\)</span> (orthogonalization). Let <span class="math inline">\(\eta_i\sim \rc{i}\)</span>. (Orthogonalization is not a projection. It’s a retraction onto the manifold.)</p>
<p>This works very well in practice but it’s complicated to analyze from a theoretical point of view.</p>
<p>Let <span class="math inline">\(g=\si_k-\si_{k+1}\)</span>. The bound is <span class="math inline">\(\fc{k}{g^3T}\)</span>. We use the measure <span class="math inline">\(\E\ba{\ve{W^TU}_F^2}\)</span> (projection to last <span class="math inline">\(k\)</span> singular values. <span class="math inline">\(U\)</span> is the output, <span class="math inline">\(W\)</span> has the last <span class="math inline">\(d-k\)</span> singular values).</p>
<p>(Rank 1 is just gradient descent; there is no projection.)</p>
<p>Consider the offline case first: we get <span class="math inline">\(X\)</span> directly instead of <span class="math inline">\(x_ix_i^T\)</span>. The algorithm is analogous to the <strong>subspace power method</strong>.</p>
<ul>
<li>Start with random <span class="math inline">\(U_0\in R^{d\times k}\)</span>.</li>
<li><span class="math inline">\(U=(I+\eta_iX)U_{i-1}\)</span>, <span class="math inline">\(U_i = QR(U)\)</span> decomposition. (This is not linear.)</li>
</ul>
<p>We can do the QR decomposition in the last step. It seems nonlinear, how can it commute? In practice you need to do QR decomposition for numerical accuracy. “To TCS people” you can move QR decomposition to the end.</p>
<p>Analysis: Let <span class="math inline">\(U_i = UR, U_{i+1} = U'R'\)</span>. Then <span class="math inline">\(U_{i+1} = (I+\eta_{i+1}X)(I+\eta_iX) U_{i-1}RR'\)</span>.</p>
<p>(For a regret bound, you can’t do this.)</p>
<p>This is how subspace QR is proved: you push everything to the end. <!-- mult update project at end. --></p>
<p>Let <span class="math inline">\(V\)</span> contain the top <span class="math inline">\(k\)</span> eigenvectors and <span class="math inline">\(W\)</span> the rest. Suppose <span class="math inline">\(v_i=e_i\)</span>. Then for <span class="math inline">\(P_t=\prod (I+\eta_{t-i}X)\)</span>, <span class="math inline">\(P_t=\smatt{\text{large}}00{\text{small}}\)</span>. Multiplying by a random initialization gives <span class="math inline">\((\text{large}\,\text{small})^T\)</span>.</p>
<p>In the online setting, let <span class="math inline">\(P_t=(I+\eta_{t-i}x_{t-i}x_{t-i}^T)\)</span>. The offdiagonal part <span class="math inline">\(W_t^TP_tV\)</span> is small compared to the diagonal part. <!--Ohad strts with something close in Frobenius norm. Once you get in neighborhood, it doesn't wander away. --></p>
<ol type="1">
<li>Show the minimal singular value of <span class="math inline">\(V^TP_tV\)</span> is large. This is not easy to bound. (The maximal one can be bounded by trace norm; exchange <span class="math inline">\(\E\)</span> and <span class="math inline">\(\Tr\)</span>.) <!--larger than singular value: martingale concentration-->
<ol type="1">
<li>Show <span class="math inline">\(\Tr[(V^TP_tV)^{-T}(V^TP_tV)^{-1}]\)</span> is small. This requires a lot of computation. (Use Sherman-Morrison formula for inverse for rank 1 update.) But this is suboptimal; lose an <span class="math inline">\(\eta\)</span> factor. <!--use Sherman-Morrison, formula for inverse for rank 1 update--></li>
<li><p>Attempt 2: show concentration. But it doesn’t work because it’s a product, not a sum. It doesn’t even concentrate in the multiplicative scale. <!--The world is against us, lower bounds are so hard --Sanjeev --></p>
<p>Why? Suppose <span class="math inline">\(k=1\)</span>. Think of <span class="math inline">\(X\)</span> as diagonal matrix with first eigenvale large and rest small. <span class="math inline">\((I+\eta e_1e_1^T)e_1e_1^T\)</span>. There is some chance that <span class="math inline">\(x_t\)</span> has some weight in the first and last entry. Then the <span class="math inline">\((1,d)\)</span> entry of the product is <span class="math inline">\(\rc{4}\eta_t\)</span>. <!-- In k=1, do epoching. This is not memory efficient? --></p>
Unlike the offline setting, <span class="math inline">\(x_t\)</span> is not perfectly aligned with the eigenspace of <span class="math inline">\(X\)</span>; it can grab something from the eigenvector.</li>
<li><p>Try induction. Suppose <span class="math inline">\(P_tP_t^T\succ c_tI\)</span>. Then <span class="math inline">\(\E (P_{t+1}P_{t+1}^T) &gt; c_t(I + \eta_{t+1}X)\)</span>. <span class="math inline">\(\si_{\min}\)</span> is not concave; we don’t have <span class="math inline">\(\E[\si_{\min}(A)]&gt;\si_{\min}[\E(A)]\)</span>. (Counterexample: take <span class="math inline">\(A=e_ie_i^T\)</span> w.p. <span class="math inline">\(\rc d\)</span>.) <!-- after taking expected value sigma min is good --> Min singular value is not a good induction hypothesis. <!--hypnosis--> Summarize <span class="math inline">\(P_t\)</span> with a number. Let <span class="math inline">\(f:\)</span> matrix <span class="math inline">\(\to\)</span> value.</p>
<p>When you try to solve a hard problem, just write down the conditions. –Yin-Tat.</p>
“The ideal girlfriend”: <span class="math inline">\(f(P_t)\)</span>
<ul>
<li>is a lower bound on the minimal singular value of <span class="math inline">\(P_t\)</span>,</li>
<li>takes care of all singular values of <span class="math inline">\(P_t\)</span>,</li>
<li>has diminishing margin (larger singular value have smaller effect)</li>
<li>easy to compute and update The magical <span class="math inline">\(\al\)</span>-min-root barrier: unique solution <span class="math inline">\(\la\)</span> of <span class="math inline">\(\sumo in \rc{\si_i - \la} = \al\)</span>, <span class="math inline">\(\la\le \si_i\)</span>. <!-- monotone decreasing. ZAZ: equivalent to regularizer. barrier view. \la is like normalization parameter. s. entropy for q=1. --> It satisfies:</li>
<li>lower bounds the minimum singular value,</li>
<li>when <span class="math inline">\(\al\to \iy\)</span>, the barrier approaches <span class="math inline">\(\si_{\min}\)</span>.</li>
<li>diminishing influence: larger <span class="math inline">\(\si\)</span> have smaller contribution</li>
<li>monotone: <span class="math inline">\(A\succ B\implies f(A)\succ f(B)\)</span>. <!-- solution of crazy function--> <!-- consider distribution of singular values--> Formula: <span class="math inline">\(\sumo in \rc{\si_i-\la} = -\pdd{\la}\log \det(A-\la I)\)</span>. Matrix-determinant lemma (from Sherman-Morrison): <span class="math display">\[\log \det(A-\la I + uv^T) = \pdd{\la} \log \det(A_\la I) + \fc{\pdd{\la}v^T(A-\la I)^{-1}u}{1+v^T(A-\la I)^{-1}u}\]</span> Key observation: If <span class="math inline">\(A=U\Si U^T\)</span>, then <span class="math inline">\(\pdd{\la}v^T (A-\la I)^{-1} u = \sumo in \fc{(Uuv^TU^T)_{ii}}{(\si-\la)^2}\)</span>. <!-- barrier function increases--> <!-- von Neumann entropy more standard. (Elad and Tengyu uses characteric polynomial)--></li>
</ul>
<p><strong>Rank 1 update lemma</strong>: Let <span class="math inline">\(A\)</span> be a symmetric matrix. Then <span class="math inline">\(\E[f(A+uu^T)]&gt;f(A) + \si_{\min}(\E(uu^T))\)</span>.</p>
Apply this on <span class="math inline">\(P_{t+1}\)</span> vs. <span class="math inline">\(P_t\)</span>. Prove the theorem by martingale concentration. <!-- Matrix doesn't concentrate at all, but min singular value improves. --></li>
</ol></li>
<li><p>Bound <span class="math inline">\(W^TP_tW\)</span> term. Using trace? But <span class="math inline">\(W^TP_tW\)</span> is not small at all. <span class="math inline">\(W^TP_tW\)</span> “mixes” with <span class="math inline">\(V^TP_tV\)</span>; it can’t interlace.</p>
<p>Remove influence of <span class="math inline">\(V^TP_tV\)</span> from <span class="math inline">\(W^TP_tW\)</span>.</p>
<p>But they are not the same dimension.</p>
<p>Use random initialization: whp <span class="math inline">\(W^TP_tW(W^TQ)(V^TP_tQ)^{-1}\)</span> is small. <!-- Easiest to explain. PMI, weighted low rank approx, optimal regret for BCO, tight bound for approx convex opt, max-entropy disribution for calc partition, NMF without separability, SVD without being cool.--></p></li>
</ol>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class="st_facebook_large" displayText="Facebook"></span>
    <span class="st_twitter_large" displayText="Tweet"></span>
    <span class="st_googleplus_large" displayText="Google +"></span>
    <span class="st_reddit_large" displayText="Reddit"></span>
    <span class="st__large" displayText></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>


<!-- Footer -->
<div id="footer">
  <div class="container">
    Built with
    <a href="http://jaspervdj.be/hakyll">Hakyll</a> 
    using 
    <a href="http://www.getbootstrap.com">Bootstrap</a>, 
    <a href="http://www.disqus.com">Disqus</a>,
    <a href="http://ignorethecode.net/blog/2010/04/20/footnotes/">Footnotes.js</a>,
    <a href="http://highlightjs.org/">Highlight.js</a>, 
    <a href="http://www.mathjax.org">MathJax</a>, 
    and <a href="http://www.sharethis.com">ShareThis</a>.
  </div>
</div>
</body>

</html>

<!-- SCRIPTS -->
<!-- jQuery-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="../../../../bootstrap/js/bootstrap.min.js"></script>

<!-- Extension : Highlight.js @ https://highlightjs.org/ -->
<!-- Syntax highlighting tomorrow-night-bright, agate-->
<link rel="stylesheet" href="../../../../highlight/css/tomorrow-night-bright.css">
<script src="../../../../highlight/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- Extension : MathJax @ https://docs.mathjax.org/en/v2.5-latest/tex.html -->
<!-- MathJax/config/local/local.js contains macros. Need to provide entire URL-->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,http://holdenlee.github.io/notebook/MathJax/config/local/local"></script>

<!-- Extension : Footnotes @ http://ignorethecode.net/blog/2010/04/20/footnotes/ -->
<script src="../../../../footnotes/js/footnotes.js"></script>

<!-- Extension : Disqus @ http://disqus.com -->
<!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->

<script src="../../../../disqus/js/disqus.js"></script>



<!-- Extension : Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73261814-1', 'auto');
  ga('send', 'pageview');

</script>

