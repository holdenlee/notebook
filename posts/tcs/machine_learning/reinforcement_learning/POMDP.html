<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

  <meta name="description" content="Holden Lee's Research Notebook">
  <meta name="author" content="Holden Lee">
    
  <title>POMDPs</title>

  <!-- Bootstrap core CSS -->
  <link href="../../../../bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

  <!-- Custom styles for this template -->
  <link href="../../../../css/blog.css" rel="stylesheet">
  <link href="../../../../css/default.css" rel="stylesheet">

  <!-- Extension : Footnotes -->
  <link href="../../../../footnotes/css/footnotes.css" rel="stylesheet">

  <!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->
  

  <!-- Extension : Collapsible lists @ http://code.stephenmorley.org/javascript/collapsible-lists/-->
  <link href="../../../../collapsible_lists/css/collapsible.css" rel="stylesheet">
  <script type="text/javascript" src="../../../../collapsible_lists/js/CollapsibleLists.js"></script>

</head>

<body>

<!-- Navigation bar. navbar-inverse is black, navbar-default is white.-->
<!-- To make a button active (pressed), use <li class="active"> -->
<div id="header">
  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../../">Notebook</a>
      </div>
      <div id="navbar" class="collapse navbar-collapse">
        <ul class="nav navbar-nav">
          <li><a href="../../../../">Home</a></li>
          <li><a href="../../../../sitemap.html">Sitemap</a></li>
<!-- TODO: Distinguish between PAPERS, RESEARCH QUESTIONS, BOOKS -->
<!-- TODO: make this part a for loop over main pages -->
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </nav>
</div>
<!-- Content -->
<!--div id="content">
  <h1>Mental Wilderness</h1>-->



<div class="container">
  <div id="content">
    <div class="page header">
      <h1>POMDPs</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-11-29 
          , Modified: 2016-11-29 
	</p>
      
       <p>Tags: <a href="../../../../tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#np-hardness">NP-hardness</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>[ALA16] Open problem - approximate planning of POMDPs in the class of memoryless policies</p>
<p>The relevant matrices/tensors are <span class="math inline">\(f_T(x'|x,a) = T_{xx'a}\)</span>, <span class="math inline">\(f_R(r|x,a) = \Ga_{xar}\)</span>, and <span class="math inline">\(f_O(y|x) = O_{xy}\)</span>. View it two ways.</p>
<ol type="1">
<li>As an extended MDP, with transition matrix <span class="math inline">\(P_{(x,a), (y,b)} = T_{yxa}\Pi_{ay}\)</span>, <span class="math inline">\(\om'\)</span> the stationary vector. (I think it’s <span class="math inline">\(\om' = (I-P^T(PP^T)^{-1}P)e_1\)</span>.) We try to maximize <span class="math display">\[\max_{\Pi\ge 0, \sum_a \pi(a|y)=1} \sum_{x,a} \om_\pi(x) \sum_y O_{xy} \Pi_{ay} \ol r(x,a).\]</span></li>
<li>As a MDP with restricted class of policies - a linear subspace, <span class="math display">\[ \Pi_{ax} = O_{xy} \Pi_{ay}.\]</span></li>
</ol>
<p>We are maximizing over a linear subspace, but the value is not convex in the policy. We can easily get to local min using policy gradient (this is easier than policy gradient because we know all parameters).</p>
<p>Questions:</p>
<ol type="1">
<li>Why is this nonconvex? Since we are maximizing, actually the question is “why is this nonconcave”? Come up with a simple concrete example where this is nonconvex. Answer (this is not full-rank, but you can change it infinitesimally): Have a state machine that rewards you only if you choose LL or RR, not if you switch. Then the optimal strategy is to always play L or always play R, rather than somewhere in the middle. More complicatedly, you can set up something which rewards the player from playing probabilities close to <span class="math inline">\(p_1,\ldots, p_k\)</span>, so there can be multiple local optima.</li>
<li>Simpler question: Suppose there are only 2 actions each step, and there is no view. Find the best strategy. Bellman’s equation gives <span class="math inline">\(Q=R + PQ\coltwo{p}{1-p}\)</span>. This is a linear equation in <span class="math inline">\(2|S|\)</span> variables, so the value is a rational function of degree <span class="math inline">\(\le 2|S|\)</span> in <span class="math inline">\(p\)</span>. I think in general you get rational functions also, but in more variables! Can we reduce from something hard like tensor decomposition to this? The difficulty is writing polynomials as solutions to systems of equations… (The denominator is multilinear if you treat all states as the same… but perhaps you can get a broader class.) Or maybe we can’t just perturb the non-full-rank case, we actually need something far enough from low-rank so that the strategy is not oblivious… (ALA16 don’t restrict to full rank.)</li>
</ol>
<p>See also Bertsekas on combining states.</p>
<p>The projection in the expression seems messy to deal with.</p>
<h2 id="np-hardness">NP-hardness</h2>
<p>Finding the optimal memoryless policy for a POMDP is NP-hard. (Proof: We can reduce from any polynomial optimization problem over the simplex to a POMDP as follows. The observation space is trivial, <span class="math inline">\(\{1\}\)</span>. Then a memoryless policy is just a vector of probabilities for different actions, summing to 1. Note a deterministic POMDP is a finite state machine where the actions correspond to characters read in by the machine. Given a polynomial is degree <span class="math inline">\(n\)</span>, make a POMDP that is stratified in <span class="math inline">\(n\)</span> layers. For each monomial <span class="math inline">\(a_{i_1\ldots i_n} x_{i_1}\cdots x_{i_n}\)</span> create a path in the finite state machine that reads in actions <span class="math inline">\(i_1,\ldots, i_n\)</span>, ends in a reward of <span class="math inline">\(a_{i_1\ldots i_n}\)</span> and then resets. Then the average reward is <span class="math inline">\(1/n\)</span> times the polynomial.)</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class="st_facebook_large" displayText="Facebook"></span>
    <span class="st_twitter_large" displayText="Tweet"></span>
    <span class="st_googleplus_large" displayText="Google +"></span>
    <span class="st_reddit_large" displayText="Reddit"></span>
    <span class="st__large" displayText></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>


<!-- Footer -->
<div id="footer">
  <div class="container">
    Built with
    <a href="http://jaspervdj.be/hakyll">Hakyll</a> 
    using 
    <a href="http://www.getbootstrap.com">Bootstrap</a>, 
    <a href="http://www.disqus.com">Disqus</a>,
    <a href="http://ignorethecode.net/blog/2010/04/20/footnotes/">Footnotes.js</a>,
    <a href="http://highlightjs.org/">Highlight.js</a>, 
    <a href="http://www.mathjax.org">MathJax</a>, 
    and <a href="http://www.sharethis.com">ShareThis</a>.
  </div>
</div>
</body>

</html>

<!-- SCRIPTS -->
<!-- jQuery-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="../../../../bootstrap/js/bootstrap.min.js"></script>

<!-- Extension : Highlight.js @ https://highlightjs.org/ -->
<!-- Syntax highlighting tomorrow-night-bright, agate-->
<link rel="stylesheet" href="../../../../highlight/css/tomorrow-night-bright.css">
<script src="../../../../highlight/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- Extension : MathJax @ https://docs.mathjax.org/en/v2.5-latest/tex.html -->
<!-- MathJax/config/local/local.js contains macros. Need to provide entire URL-->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,http://holdenlee.github.io/notebook/MathJax/config/local/local"></script>

<!-- Extension : Footnotes @ http://ignorethecode.net/blog/2010/04/20/footnotes/ -->
<script src="../../../../footnotes/js/footnotes.js"></script>

<!-- Extension : Disqus @ http://disqus.com -->
<!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->

<script src="../../../../disqus/js/disqus.js"></script>



<!-- Extension : Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73261814-1', 'auto');
  ga('send', 'pageview');

</script>

