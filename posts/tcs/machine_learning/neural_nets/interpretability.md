---
title: Interpretable neural nets
published: 2016-10-12
modified: 2016-10-12
tags: neural nets
type: topic
showTOC: True
---

Enforce sparsity or nonnegativity.

* [NTPV13] Learning Parts-based Representations with Nonnegative Restricted Boltzmann Machine [paper](http://www.jmlr.org/proceedings/papers/v33/min14.pdf)
    * Enforce nonnegativity in RBM by having a regularizer that's a quadratic barrier function $\min(0,x)^2$.
* [MNCG14] Interpretable Sparse High-Order Boltzmann Machines [paper](http://jmlr.org/proceedings/papers/v29/Nguyen13.pdf)
* [KV16] Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models [paper](https://arxiv.org/abs/1606.05320)

link to an article on extracting interpretable features out of deep nets? I forget the authors. Maybe Salakhutdinov was one of them.
