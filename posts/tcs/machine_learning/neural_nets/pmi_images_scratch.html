<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

  <meta name="description" content="Holden Lee's Research Notebook">
  <meta name="author" content="Holden Lee">
    
  <title>PMI for images (scratch)</title>

  <!-- Bootstrap core CSS -->
  <link href="../../../../bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

  <!-- Custom styles for this template -->
  <link href="../../../../css/blog.css" rel="stylesheet">
  <link href="../../../../css/default.css" rel="stylesheet">

  <!-- Extension : Footnotes -->
  <link href="../../../../footnotes/css/footnotes.css" rel="stylesheet">

  <!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->
  

  <!-- Extension : Collapsible lists @ http://code.stephenmorley.org/javascript/collapsible-lists/-->
  <link href="../../../../collapsible_lists/css/collapsible.css" rel="stylesheet">
  <script type="text/javascript" src="../../../../collapsible_lists/js/CollapsibleLists.js"></script>

</head>

<body>

<!-- Navigation bar. navbar-inverse is black, navbar-default is white.-->
<!-- To make a button active (pressed), use <li class="active"> -->
<div id="header">
  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../../">Notebook</a>
      </div>
      <div id="navbar" class="collapse navbar-collapse">
        <ul class="nav navbar-nav">
          <li><a href="../../../../">Home</a></li>
          <li><a href="../../../../sitemap.html">Sitemap</a></li>
<!-- TODO: Distinguish between PAPERS, RESEARCH QUESTIONS, BOOKS -->
<!-- TODO: make this part a for loop over main pages -->
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </nav>
</div>
<!-- Content -->
<!--div id="content">
  <h1>Mental Wilderness</h1>-->



<div class="container">
  <div id="content">
    <div class="page header">
      <h1>PMI for images (scratch)</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-01 
          , Modified: 2016-09-05 
	</p>
      
       <p>Tags: <a href="../../../../tags/PMI.html">PMI</a>, <a href="../../../../tags/neural%20nets.html">neural nets</a>, <a href="../../../../tags/vision.html">vision</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#results">Results</a><ul>
 <li><a href="#pmi-distribution">PMI distribution</a></li>
 <li><a href="#thresholded-pmi-distribution">Thresholded PMI distribution</a></li>
 <li><a href="#thresholded-pmi-distribution-1">Thresholded PMI distribution</a></li>
 <li><a href="#thresholded-pmi-distribution-2">Thresholded PMI distribution</a></li>
 <li><a href="#thresholding-by-psitr">Thresholding by psiTr</a></li>
 <li><a href="#svd-svm">SVD-SVM</a></li>
 <li><a href="#weighted-svd-svm">Weighted SVD-SVM</a></li>
 <li><a href="#pmi-suitability">PMI suitability</a></li>
 <li><a href="#pmi-and-geometry">PMI and geometry</a></li>
 <li><a href="#unorganized">Unorganized</a></li>
 </ul></li>
 <li><a href="#notes">Notes</a></li>
 <li><a href="#theory">Theory</a></li>
 <li><a href="#thoughts">Thoughts</a></li>
 <li><a href="#todo">Todo</a><ul>
 <li><a href="#coding">Coding</a></li>
 <li><a href="#theory-1">Theory</a></li>
 </ul></li>
 <li><a href="#scratch">Scratch</a></li>
 <li><a href="#steps-experiments">Steps, Experiments</a><ul>
 <li><a href="#guide-to-data">Guide to data</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="results">Results</h2>
<h3 id="pmi-distribution">PMI distribution</h3>
<p>Over all pairs of features <span class="math inline">\(i,j\)</span>, this is the distribution of <span class="math inline">\(PMI(v_i,v_j)\)</span>:</p>
<p><img src="../../../../images/pmi/pmi_histogram_1.jpg"></p>
<p>(Question: is this what you would expect if the distribution were “random” or are the tails different than what we would expect?)</p>
<p>Is there a correlation between PMI and distance between features? (Distance ranges from <span class="math inline">\(0\)</span> to <span class="math inline">\(5\sqrt 2\)</span>.) It seems not.</p>
<p><img src="../../../../images/pmi/pmi_distance.jpg"></p>
<p>Conditioned on the digit being a specific value, how does the PMI distribution look? Still about the same… except that 1 has much larger activation. These are digits 0-4.</p>
<p><img src="../../../../images/pmi/hist_P_1_0.jpg"> <img src="../../../../images/pmi/hist_P_1_1.jpg"> <img src="../../../../images/pmi/hist_P_1_2.jpg"> <img src="../../../../images/pmi/hist_P_1_3.jpg"> <img src="../../../../images/pmi/hist_P_1_4.jpg"></p>
<h3 id="thresholded-pmi-distribution">Thresholded PMI distribution</h3>
<p>See script <code>pmi_thresholded.m</code>.</p>
<p>Remove all features that do not have PMI <span class="math inline">\(&gt;1\)</span> with another feature. Now do the same thing.</p>
<p>This is the distribution of <span class="math inline">\(PMI(v_i,v_j)\)</span>:</p>
<p><img src="../../../../images/pmi/hist_thres05_1.jpg" width="500"></p>
<p>These are digits 0-4.</p>
<p><img src="../../../../images/pmi/hist_thres05_1_0.jpg" width="500"> <img src="../../../../images/pmi/hist_thres05_1_1.jpg" width="500"> <img src="../../../../images/pmi/hist_thres05_1_2.jpg" width="500"> <img src="../../../../images/pmi/hist_thres05_1_3.jpg" width="500"> <img src="../../../../images/pmi/hist_thres05_1_4.jpg" width="500"></p>
<p>Percentiles of the distributions (<span class="math inline">\(i\)</span>th row is 0, 10, 20, … 100% for the CPMI’s of digit <span class="math inline">\(i\)</span>, last row is PMI over all digits).</p>
<pre><code>  Columns 1 through 7

   -0.9746   -0.0588   -0.0140    0.0112    0.0306    0.0488    0.0681
   -3.1365   -0.3102   -0.0940    0.0050    0.0609    0.1144    0.1812
   -0.9300   -0.0626   -0.0130    0.0134    0.0333    0.0526    0.0740
   -0.7660   -0.0462   -0.0022    0.0210    0.0403    0.0602    0.0835
   -2.0740   -0.0565   -0.0071    0.0188    0.0397    0.0611    0.0855
   -0.8674   -0.0628   -0.0077    0.0229    0.0471    0.0705    0.0966
   -2.3770   -0.0457   -0.0007    0.0228    0.0428    0.0639    0.0886
   -2.4824   -0.0928   -0.0235    0.0121    0.0379    0.0633    0.0925
   -0.9435   -0.0521   -0.0066    0.0161    0.0347    0.0545    0.0785
   -2.1549   -0.0505   -0.0020    0.0231    0.0446    0.0679    0.0961
   -0.8693   -0.0515    0.0008    0.0344    0.0640    0.0949    0.1309

  Columns 8 through 11

    0.0910    0.1215    0.1730    1.5181
    0.2772    0.4314    0.7470    5.3609
    0.1007    0.1393    0.2117    1.5297
    0.1141    0.1604    0.2521    1.6011
    0.1169    0.1635    0.2537    2.2486
    0.1293    0.1773    0.2773    2.0363
    0.1215    0.1727    0.2771    2.5443
    0.1305    0.1882    0.3019    2.9006
    0.1111    0.1639    0.2788    2.1102
    0.1344    0.1957    0.3210    2.7701
    0.1761    0.2406    0.3557    1.7908
	</code></pre>
<h3 id="thresholded-pmi-distribution-1">Thresholded PMI distribution</h3>
<p>See script <code>pmi_thresholded.m</code>.</p>
<p>Remove all features that do not have PMI <span class="math inline">\(&gt;1\)</span> with another feature. Now do the same thing.</p>
<p>This is the distribution of <span class="math inline">\(PMI(v_i,v_j)\)</span>:</p>
<p><img src="../../../../images/pmi/hist_thres05_1.jpg" width="500"></p>
<p>These are digits 0-4.</p>
<p><img src="../../../../images/pmi/hist_thres05_1_0.jpg" width="500"> <img src="../../../../images/pmi/hist_thres05_1_1.jpg" width="500"> <img src="../../../../images/pmi/hist_thres05_1_2.jpg" width="500"> <img src="../../../../images/pmi/hist_thres05_1_3.jpg" width="500"> <img src="../../../../images/pmi/hist_thres05_1_4.jpg" width="500"></p>
<h3 id="thresholded-pmi-distribution-2">Thresholded PMI distribution</h3>
<h3 id="thresholding-by-psitr">Thresholding by psiTr</h3>
<p>CODE: Run <code>pmi_thresholded</code> and <code>check_top_pmis</code>.</p>
<p>Remove all features that do not have activation <span class="math inline">\(&gt;1\)</span> in some image. Now do the same thing. (NOTE: the maximum PMI is less because the max PMI’s are between features that have small activation. Try this again thresholding by PMI?)</p>
<p>This is the distribution of <span class="math inline">\(PMI(v_i,v_j)\)</span>:</p>
<p><img src="../../../../images/pmi/hist_thres05_1.jpg" width="500"></p>
<p>These are digits 0-9.</p>
<p><img src="../../../../images/pmi/hist_thres05_1_0.jpg" width="500"> <img src="../../../../images/pmi/hist_thres05_1_1.jpg" width="500"> <img src="../../../../images/pmi/hist_thres05_1_2.jpg" width="500"> <img src="../../../../images/pmi/hist_thres05_1_3.jpg" width="500"> <img src="../../../../images/pmi/hist_thres05_1_4.jpg" width="500"> <img src="../../../../images/pmi/hist_thres05_1_5.jpg" width="500"> <img src="../../../../images/pmi/hist_thres05_1_6.jpg" width="500"> <img src="../../../../images/pmi/hist_thres05_1_7.jpg" width="500"> <img src="../../../../images/pmi/hist_thres05_1_8.jpg" width="500"> <img src="../../../../images/pmi/hist_thres05_1_9.jpg" width="500"></p>
<p>Percentiles of the distributions (<span class="math inline">\(i\)</span>th row is 0, 10, 20, … 100% for the CPMI’s of digit <span class="math inline">\(i\)</span>, last row is PMI over all digits). (Note the last column is different)</p>
<pre><code>  Columns 1 through 7

   -0.9746   -0.0588   -0.0140    0.0112    0.0306    0.0488    0.0681
   -3.1365   -0.3102   -0.0940    0.0050    0.0609    0.1144    0.1812
   -0.9300   -0.0626   -0.0130    0.0134    0.0333    0.0526    0.0740
   -0.7660   -0.0462   -0.0022    0.0210    0.0403    0.0602    0.0835
   -2.0740   -0.0565   -0.0071    0.0188    0.0397    0.0611    0.0855
   -0.8674   -0.0628   -0.0077    0.0229    0.0471    0.0705    0.0966
   -2.3770   -0.0457   -0.0007    0.0228    0.0428    0.0639    0.0886
   -2.4824   -0.0928   -0.0235    0.0121    0.0379    0.0633    0.0925
   -0.9435   -0.0521   -0.0066    0.0161    0.0347    0.0545    0.0785
   -2.1549   -0.0505   -0.0020    0.0231    0.0446    0.0679    0.0961
   -0.8693   -0.0515    0.0008    0.0344    0.0640    0.0949    0.1309

  Columns 8 through 11

    0.0910    0.1215    0.1730    1.5181
    0.2772    0.4314    0.7470    5.3609
    0.1007    0.1393    0.2117    1.5297
    0.1141    0.1604    0.2521    1.6011
    0.1169    0.1635    0.2537    2.2486
    0.1293    0.1773    0.2773    2.0363
    0.1215    0.1727    0.2771    2.5443
    0.1305    0.1882    0.3019    2.9006
    0.1111    0.1639    0.2788    2.1102
    0.1344    0.1957    0.3210    2.7701
    0.1761    0.2406    0.3557    1.7908
	</code></pre>
<p>TODO: add data from <code>check_top_pmis</code>. This produces a graph showing which features are close to each other.</p>
<h3 id="svd-svm">SVD-SVM</h3>
<p>Run <code>qsub svd_svm.cmd</code> which runs <code>svd_testing2.m</code>, or <code>sbatch slurm_svd.cmd</code>. The results are saved in <code>accs_1.mat</code>.</p>
<p>Results:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Dimension</th>
<th style="text-align: left;">Last</th>
<th style="text-align: left;">Best</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">500</td>
<td style="text-align: left;">99.26</td>
<td style="text-align: left;">99.44</td>
</tr>
<tr class="even">
<td style="text-align: left;">100</td>
<td style="text-align: left;">97.5</td>
<td style="text-align: left;">98.82</td>
</tr>
<tr class="odd">
<td style="text-align: left;">50</td>
<td style="text-align: left;">96.84</td>
<td style="text-align: left;">96.84</td>
</tr>
</tbody>
</table>
<h3 id="weighted-svd-svm">Weighted SVD-SVM</h3>
<p>Do weighted SVD (as in the NLP paper) and then train a SVM. Note this does worse than just SVD! Why?</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Dimension</th>
<th style="text-align: left;">Last</th>
<th style="text-align: left;">Best</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">50</td>
<td style="text-align: left;">95.44</td>
<td style="text-align: left;">96.8</td>
</tr>
</tbody>
</table>
<h3 id="pmi-suitability">PMI suitability</h3>
<blockquote>
<p>Is PMI the right measure? Are the feature vectors close to sparse? if the average image there is <span class="math inline">\(&gt;20\%\)</span> activation, then no.</p>
</blockquote>
<p>For the first 10 images, this is what the percentiles look like:</p>
<pre><code>percentiles:

pcs =

   100    99    95    90    80    70

Image 1 (label 5)
    2.3230
    1.2966
    0.8840
    0.6766
    0.4381
    0.3140

Image 2 (label 0)
    2.1275
    1.3604
    0.9482
    0.7418
    0.5036
    0.3769

Image 3 (label 4)
    1.7975
    0.9874
    0.6185
    0.4704
    0.3145
    0.2247

Image 4 (label 1)
    2.2506
    1.2399
    0.7160
    0.4949
    0.3029
    0.1992

Image 5 (label 9)
    2.0837
    1.3970
    0.8726
    0.6461
    0.3937
    0.2707

Image 6 (label 2)
    2.4831
    1.4978
    1.0466
    0.7880
    0.5002
    0.3544

Image 7 (label 1)
    2.5064
    1.3688
    0.7591
    0.5203
    0.3089
    0.1935

Image 8 (label 3)
    3.3030
    1.6943
    1.1588
    0.8658
    0.5756
    0.4070

Image 9 (label 1)
    2.2263
    0.9396
    0.4905
    0.3279
    0.1944
    0.1178

Image 10 (label 4)
    2.2420
    1.3486
    0.8494
    0.6169
    0.3751
    0.2586
	</code></pre>
<p>This looks pretty good—the top 1% of features are much larger than the rest.</p>
<blockquote>
<p>Look for discriminative features, those that occur with one label and rarely for others. (I arbitrarily define this as: the maximum average activation is <span class="math inline">\(&gt;1.5\)</span> times the second largest, and the maximum is at least 0.1. I exclude the digit 1 (indexed as 2).)</p>
</blockquote>
<div class="sourceCode"><pre class="sourceCode matlab"><code class="sourceCode matlab">sig_act_inds = intersect(find(m1&gt;=<span class="fl">1.5</span>*m2), find(m1&gt;<span class="fl">0.1</span>));
sig_act_inds2 = intersect(sig_act_inds, find(am1 ~=<span class="fl">2</span>));</code></pre></div>
<p>115/7200 features fit this criterion.</p>
<blockquote>
<p>Do SVM coefficients have negative weights?</p>
</blockquote>
<h3 id="pmi-and-geometry">PMI and geometry</h3>
<pre><code>collect, same patch; no collect, same patch; horiz collect; horiz no collect; vert collect; vert no collect; all

ans =

  Columns 1 through 7

   -0.9605   -0.3457   -0.2067   -0.1000   -0.0103    0.0783    0.1698
   -1.0872   -0.0690   -0.0062    0.0281    0.0563    0.0880    0.1277
   -1.5676   -0.3858   -0.2386   -0.1310   -0.0454    0.0409    0.1303
   -0.9235   -0.0241    0.0307    0.0621    0.0919    0.1266    0.1695
   -1.3763   -0.3810   -0.2237   -0.1053   -0.0072    0.0839    0.1708
   -0.9163   -0.0108    0.0284    0.0530    0.0796    0.1093    0.1458
   -1.1984   -0.0727   -0.0094    0.0253    0.0534    0.0847    0.1241

  Columns 8 through 11

    0.2735    0.3922    0.5678    1.3093
    0.1821    0.2624    0.4080    2.4518
    0.2220    0.3257    0.4557    1.1007
    0.2240    0.3054    0.4534    2.6332
    0.2607    0.3593    0.4863    0.9118
    0.1942    0.2660    0.3970    2.6332
    0.1778    0.2587    0.4034    2.6897</code></pre>
<h3 id="unorganized">Unorganized</h3>
<p>I sorted pairs of features by PMI value and took the largest 10000 pairs. The majority of the pairs (~77%) have strongest activation with the digit 1. (The digit 1 has conditional PMI values that are much larger than all the other digits - don’t know what to do about this.) 85% of the pairs have highest activations with the same digit.</p>
<p>If the pair has strong activation with 1, then it’s often much larger than the other activations.</p>
<p>Otherwise this is not necessarily true. A typical non-1 feature has average activations with the 10 digits that looks something like this:</p>
<pre><code>  Columns 1 through 7

    0.0708    0.2673    0.0940    0.1537    0.2122    0.1116    0.2627

  Columns 8 through 10

    0.2947    0.1448    0.3361
</code></pre>
<h2 id="notes">Notes</h2>
<p>Experiments:</p>
<ul>
<li><code>mnist_testing.m</code>: Prep data.</li>
<li><code>svd_testing.m</code>: How does reducing the number of features through SVD affect classification accuracy?</li>
<li><code>pairwise_mi.m</code>: Tests related to mutual information</li>
<li><code>mi.m</code></li>
<li><code>MI_GG.m</code></li>
<li><code>rsvd.m</code></li>
</ul>
<pre><code>	         nlayers: 2
             layer: {2x1 cell}
    type_zerolayer: 2
             ndesc: 7200


                         numlayer: 1
                        centering: 0
    median_contrast_normalization: 0
                           npatch: 5
                      subsampling: 2
                             smap: 50
                   type_zerolayer: 2
                            sigma: 0.7459
                                Z: [25x50 double]
                                w: [50x1 double]</code></pre>
<p>In <code>pairwise_mi.m</code> there’s no normalization by scaling (<span class="math inline">\(-2\log\pat{scale}\)</span>).</p>
<p>https://en.wikipedia.org/wiki/Conditional_mutual_information</p>
<h2 id="theory">Theory</h2>
<p>The right PMI measure to use here is <span class="math display">\[
\ln \pf{\an{v,w}}{\an{v,\one}\an{w,\one}}.
\]</span> Because if we assume the activations are like <span class="math inline">\((e^{\chi,v})_\chi\)</span>, then we still get that the expected dot product of these is <span class="math inline">\(\int e^{\an{\chi,v}}e^{\an{\chi,w}}\,d\chi\propto \exp\pa{\fc{\ve{v+w}^2}{2}}\)</span>.</p>
<h2 id="thoughts">Thoughts</h2>
<ul>
<li>Is a picture more like a context or a document? We’re treating it like context (looking at all pairs of features there). But its size makes it seem more like a document. Or better: a sentence, because sentences are big enough to incorporate different features and small enough to still have a vector associated with it.</li>
<li>How to incorporate convolutional ideas? Ex. if features <span class="math inline">\(f_1,f_2\)</span> are in the same relationship (translationally) as <span class="math inline">\(f_1',f_2'\)</span> then we expect PMI to be similar, so we should we really be looking at a <span class="math inline">\(7200\times 7200\)</span> matrix? How about look at PMI of adjacent features? (But they shouldn’t overlap…) Or look at PMI pre-convolution by another layer?</li>
<li>What happens if you apply (convolutional?) DL to the learned features? Then apply SVD to the dimension-reduced vectors?</li>
</ul>
<h2 id="todo">Todo</h2>
<h3 id="coding">Coding</h3>
<ul>
<li>Completed</li>
<li>Compute PMI matrix.</li>
<li>Plot histogram of PMI’s.</li>
<li>Plot histogram of conditional PMI’s. (They don’t look like they have higher values… Try thresholding first? Look at features with largest PMI? Calculate entropies?)</li>
<li>Look at correlations between PMI’s and distances between pairs. (There doesn’t seem to be any correlation.)</li>
<li>Train on feature vectors.</li>
<li><p>Do weighted SVD on PMI matrix.</p></li>
<li>Todo:</li>
<li>Do dictionary learning on feature vectors/PMI vectors (?).</li>
<li>Do unsupervised learning: k-means on feature vectors and on original (baseline).</li>
<li>Try clustering based on graph diffusion with PMI’s giving distances.</li>
<li>Try taking SVD of features for each patch first (ex. reduce from 200 to 10; this gives 360-dimensional). How does this compare to SVD after of everything taken together? (Probably worse.)</li>
<li>Do SVD/weighted SVD of each patch only comparing with neighboring patches.</li>
<li><p>Geometry</p></li>
</ul>
<p>(Compare to purely random.)</p>
<h3 id="theory-1">Theory</h3>
<p>Understand loss function for PMI.</p>
<h2 id="scratch">Scratch</h2>
<p>Qs</p>
<ul>
<li>Why is the normalization <code>nrm=mean(sqrt(sum(compTr.^2)))</code>?</li>
</ul>
<h2 id="steps-experiments">Steps, Experiments</h2>
<p>Run experiments.</p>
<pre><code>nohup nice -19 matlab -nodisplay -nodesktop -nojvm -nosplash &lt; run_experiments2.m &gt; output_2016-08-07.txt 2&gt;&amp;1 &amp;</code></pre>
<div class="sourceCode"><pre class="sourceCode matlab"><code class="sourceCode matlab"><span class="co">%Get psiTr</span>
pmi_experiments(<span class="st">'logs/model_mnist_5_2_0_2_2_0_50_200_0_1.000000e-01_1.000000e-01_1.000000e-01_2.mat'</span>);
<span class="co">%get percentiles for each feature</span>
pc= get_percentiles(psiTr);
save(<span class="st">'pc_1.mat'</span>,<span class="st">'pc'</span>,<span class="st">'-v7.3'</span>);
<span class="co">%calculate PMI after normalizing and deleting zero rows.</span>
[P, inds] = calculate_pmi(psiTr); <span class="co">%inds give the indices that are kept (indices of nonzero rows)</span>
<span class="co">%exploratory analysis</span>
<span class="co">%pmi_experiments(P);</span>
[all_pmi_pairs, positions, dists] = pmi_experiments2(P, inds');
save(<span class="st">'all_pmi_pairs_1.mat'</span>,<span class="st">'all_pmi_pairs'</span>);
save(<span class="st">'positions_1.mat'</span>,<span class="st">'positions'</span>);
save(<span class="st">'dists_1.mat'</span>,<span class="st">'dists'</span>);

<span class="co">%cpmi matrices</span>
Ps = calculate_cpmi(psiTr, Ytr, <span class="st">'1'</span>);
cpmi_experiments(<span class="st">'P_1'</span>);</code></pre></div>
<h3 id="guide-to-data">Guide to data</h3>
<ul>
<li><code>psiTr_1.mat</code>, <code>psiTe_1.mat</code> etc. are the feature vectors for the training/test sets and the different architectures. The architectures are
<ol type="1">
<li><code>test_ckn([5 2 0],[2 2 0],[50 200 0],2,'mnist');</code> (7200, 0.57%)</li>
<li><code>test_ckn([1 3 0],[2 2 0],[12 50 0],3,'mnist');</code> (1800, 0.63%)</li>
<li><code>test_ckn([1 3 0],[2 4 0],[12 400 0],3,'mnist');</code> (3600, 0.41%)</li>
</ol></li>
<li></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class="st_facebook_large" displayText="Facebook"></span>
    <span class="st_twitter_large" displayText="Tweet"></span>
    <span class="st_googleplus_large" displayText="Google +"></span>
    <span class="st_reddit_large" displayText="Reddit"></span>
    <span class="st__large" displayText></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>


<!-- Footer -->
<div id="footer">
  <div class="container">
    Built with
    <a href="http://jaspervdj.be/hakyll">Hakyll</a> 
    using 
    <a href="http://www.getbootstrap.com">Bootstrap</a>, 
    <a href="http://www.disqus.com">Disqus</a>,
    <a href="http://ignorethecode.net/blog/2010/04/20/footnotes/">Footnotes.js</a>,
    <a href="http://highlightjs.org/">Highlight.js</a>, 
    <a href="http://www.mathjax.org">MathJax</a>, 
    and <a href="http://www.sharethis.com">ShareThis</a>.
  </div>
</div>
</body>

</html>

<!-- SCRIPTS -->
<!-- jQuery-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="../../../../bootstrap/js/bootstrap.min.js"></script>

<!-- Extension : Highlight.js @ https://highlightjs.org/ -->
<!-- Syntax highlighting tomorrow-night-bright, agate-->
<link rel="stylesheet" href="../../../../highlight/css/tomorrow-night-bright.css">
<script src="../../../../highlight/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- Extension : MathJax @ https://docs.mathjax.org/en/v2.5-latest/tex.html -->
<!-- MathJax/config/local/local.js contains macros. Need to provide entire URL-->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,http://holdenlee.github.io/notebook/MathJax/config/local/local"></script>

<!-- Extension : Footnotes @ http://ignorethecode.net/blog/2010/04/20/footnotes/ -->
<script src="../../../../footnotes/js/footnotes.js"></script>

<!-- Extension : Disqus @ http://disqus.com -->
<!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->

<script src="../../../../disqus/js/disqus.js"></script>



<!-- Extension : Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73261814-1', 'auto');
  ga('send', 'pageview');

</script>

