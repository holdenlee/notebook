<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

  <meta name="description" content="Holden Lee's Research Notebook">
  <meta name="author" content="Holden Lee">
    
  <title>Neural net learning, Anthony and Bartlett</title>

  <!-- Bootstrap core CSS -->
  <link href="../../../../bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

  <!-- Custom styles for this template -->
  <link href="../../../../css/blog.css" rel="stylesheet">
  <link href="../../../../css/default.css" rel="stylesheet">

  <!-- Extension : Footnotes -->
  <link href="../../../../footnotes/css/footnotes.css" rel="stylesheet">

  <!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->
  

  <!-- Extension : Collapsible lists @ http://code.stephenmorley.org/javascript/collapsible-lists/-->
  <link href="../../../../collapsible_lists/css/collapsible.css" rel="stylesheet">
  <script type="text/javascript" src="../../../../collapsible_lists/js/CollapsibleLists.js"></script>

</head>

<body>

<!-- Navigation bar. navbar-inverse is black, navbar-default is white.-->
<!-- To make a button active (pressed), use <li class="active"> -->
<div id="header">
  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../../">Notebook</a>
      </div>
      <div id="navbar" class="collapse navbar-collapse">
        <ul class="nav navbar-nav">
          <li><a href="../../../../">Home</a></li>
          <li><a href="../../../../sitemap.html">Sitemap</a></li>
<!-- TODO: Distinguish between PAPERS, RESEARCH QUESTIONS, BOOKS -->
<!-- TODO: make this part a for loop over main pages -->
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </nav>
</div>
<!-- Content -->
<!--div id="content">
  <h1>Mental Wilderness</h1>-->



<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Neural net learning, Anthony and Bartlett</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-02-28 
          , Modified: 2017-02-28 
	</p>
      
       <p>Tags: <a href="../../../../tags/neural%20nets.html">neural nets</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#classification-with-real-valued-functions">9 Classification with real-valued functions</a><ul>
 <li><a href="#intro">9.1 Intro</a></li>
 <li><a href="#large-margin-classifiers">9.2 Large margin classifiers</a></li>
 </ul></li>
 <li><a href="#covering-numbers-and-uniform-convergence">10 Covering numbers and uniform convergence</a><ul>
 <li><a href="#uniform-convergence">10.3 Uniform convergence</a></li>
 </ul></li>
 <li><a href="#pseudo-dimension-and-fat-shattering-dimension">11 Pseudo-dimension and fat-shattering dimension</a><ul>
 <li><a href="#pseudo-dimension">11.2 Pseudo-dimension</a></li>
 <li><a href="#fat-shattering-dimension">11.3 Fat-shattering dimension</a></li>
 </ul></li>
 <li><a href="#bounding-covering-numbers-with-dimensions">12 Bounding covering numbers with dimensions</a></li>
 <li><a href="#section">13</a></li>
 <li><a href="#dimensions-of-neural-networks">14 Dimensions of neural networks</a><ul>
 <li><a href="#in-terms-of-number-of-parameters">14.3 In terms of number of parameters</a></li>
 <li><a href="#on-fat-shattering-dimensions-independent-of-number-of-parameters">14.4 On Fat-shattering dimensions, independent of number of parameters</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="classification-with-real-valued-functions">9 Classification with real-valued functions</h2>
<h3 id="intro">9.1 Intro</h3>
<ul>
<li>VC dim grows with number of parameters</li>
<li>Training set <span class="math inline">\(\ll\)</span> number of parameters, but still avoids overfitting.</li>
<li>7.1: arbitrarily small modification to activation makes VC-dimension infinite</li>
<li>In theo theory model, learning algorithm is required to perform well for any probability distribution.</li>
<li>Learning model not appropriate for NN learning algorithms. Many learning algorithms take into account the real output.</li>
<li>Take into account the margin</li>
<li>Get bounds on misclassification <span class="math inline">\(\pat{error}\le \pat{estimate of error} + \pat{complexity penalty}\)</span>. Complexity parameter depends on size of parameters in the network.</li>
<li>Interpret: value of the function as an estimate of the probability that the correct label is 1</li>
</ul>
<h3 id="large-margin-classifiers">9.2 Large margin classifiers</h3>
<ul>
<li><span class="math inline">\(Z=X\times \{0,1\}\)</span>. Margin of <span class="math inline">\(f\)</span> on <span class="math inline">\((x,y)\in Z\)</span> is <span class="math inline">\(\sign(y) (f(x)-\rc 2)\)</span>.</li>
<li>Error <span class="math inline">\(err_P^\ga(f) = \Pj(\text{margin}(f(x),y)&lt;\ga)\)</span>.</li>
<li>Classification learning algorithm for real-valued functions: <span class="math inline">\(err_P(L(\ga,z)) &lt; \inf_{g\in F}err_P^\ga(g)+\ep\)</span>.</li>
<li><span class="math inline">\(\wh{err}_z^\ga(f) = \rc m |\set{i}{\text{margin}(f(x_i),y_i)&lt;\ga}|\)</span>.</li>
<li>Chebyshev: if <span class="math inline">\(\ep\)</span> is <span class="math inline">\(L_2\)</span> error, <span class="math inline">\(\wh{err}_z^\ga (f) &lt; \fc{\ep}{(\rc2-\ga)^2}\)</span>.</li>
</ul>
<h2 id="covering-numbers-and-uniform-convergence">10 Covering numbers and uniform convergence</h2>
<ul>
<li><span class="math inline">\(N(\ep, W, d_\iy)\)</span> is the minimum cardinality of a cover of <span class="math inline">\(W\)</span> by <span class="math inline">\(\ep\)</span>-balls in <span class="math inline">\(L^{\iy}\)</span>.</li>
<li>Ex. if <span class="math inline">\(F|_{x_{1:k}}\)</span> is contained in a linear subspace of dimension <span class="math inline">\(d\)</span>, then grows as <span class="math inline">\(\pf k\ep^d\)</span>.</li>
<li><span class="math inline">\(N_\iy(\ep, F, k) =\max_{x\in X^k} N(\ep, F|_x, d_\iy)\)</span>.</li>
</ul>
<h3 id="uniform-convergence">10.3 Uniform convergence</h3>
<p><span class="math display">\[
\Pj^m [\exists f\in F, err_P(f) \ge \wh{err}_z^\ga (f) + \ep] \le 2N_\iy(\fc \ga2, F, 2m) e^{-\fc{\ep^2m}{8}}.
\]</span> (<span class="math inline">\(\max\)</span> here signals that <span class="math inline">\(L^\iy\)</span> is the right covering.)</p>
<p>If covering number grows as <span class="math inline">\(\poly(m)\)</span>, then this approaches 0 exponentially.</p>
<h2 id="pseudo-dimension-and-fat-shattering-dimension">11 Pseudo-dimension and fat-shattering dimension</h2>
<p>Is there a combo measure like the VC dimension to bound covering numbers?</p>
<h3 id="pseudo-dimension">11.2 Pseudo-dimension</h3>
<ul>
<li><span class="math inline">\(F\)</span> is a set of functions <span class="math inline">\(X\to \R\)</span>, <span class="math inline">\(S=\{x_1,\ldots, x_m\}\subeq X\)</span>. <span class="math inline">\(S\)</span> is <strong>pseudoshattered</strong> by <span class="math inline">\(F\)</span> if there are <span class="math inline">\(r_i\)</span>, s.t. for all <span class="math inline">\(b\in \{0,1\}^m\)</span>, there is <span class="math inline">\(f_b\in F\)</span>, <span class="math inline">\(\sign(f_b(x_i)-r_i)=b_i\)</span>. (Achieve all possible above/below combinations.)</li>
<li><strong>Pseudo-dimension</strong> is maximum <span class="math inline">\(|S|\)</span> pseudo-shattered by <span class="math inline">\(F\)</span>.</li>
<li>The pseudodimension of <span class="math inline">\(F\)</span> is the VC dimension of <span class="math inline">\(\set{\sign(f(x)-y)}{f\in F}\)</span>.</li>
<li>Pseudoshattered iff some translate of <span class="math inline">\(F|_x\)</span> intersects all orthants.</li>
<li>Composing with nondecreasing function doesnâ€™t increase.</li>
<li>If <span class="math inline">\(F\)</span> is vector space, <span class="math inline">\(Pdim(F)=\dim(F)\)</span>.</li>
<li><span class="math inline">\(F=\set{w_0+w^Tx}{w\in \R^n}\)</span>: <span class="math inline">\(P\dim(F)=n+1\)</span>.</li>
<li>Poly transformations of degree <span class="math inline">\(\le k\)</span>: <span class="math inline">\(P\dim(F) = \binom{n+k}k\)</span>.</li>
<li>Poly transformations on <span class="math inline">\(\{0,1\}^k\)</span> of degree <span class="math inline">\(\le k\)</span>: <span class="math inline">\(\sumz ik \binom ni\)</span>.</li>
</ul>
<h3 id="fat-shattering-dimension">11.3 Fat-shattering dimension</h3>
<ul>
<li><span class="math inline">\(S\)</span> is <span class="math inline">\(\ga\)</span>-shattered by <span class="math inline">\(F\)</span> if there are <span class="math inline">\(r_i\)</span> such that for each <span class="math inline">\(b\in \{0,1\}^m\)</span>, there is <span class="math inline">\(f_b\in F\)</span>,
<span class="math display">\[\begin{align}
f_b(x_i) &amp; \ge r_i+\ga,&amp;b_i=1\\
f_b(x_i) &amp;\le r_i - \ga,&amp;b_i=0.
\end{align}\]</span></li>
<li><span class="math inline">\(\ga\)</span>-dimension is max cardinality of <span class="math inline">\(S\)</span> that is <span class="math inline">\(\ga\)</span>-shattered by <span class="math inline">\(F\)</span>, <span class="math inline">\(fat_F(\ga)\)</span>.</li>
<li>This is a scale-sensitive dimension.</li>
<li>Ex. <span class="math inline">\(F\)</span> set of functions <span class="math inline">\([0,1]\to [0,1]\)</span> with total variation <span class="math inline">\(\le V\)</span>. <span class="math inline">\(fat_F(\ga) = 1+\ff{V}{2\ga}\)</span>.</li>
<li><span class="math inline">\(fat_F(\ga)\le P\dim(F)\)</span>. <span class="math inline">\(P\dim(F) = \lim_{\ga\to 0} fat_F(\ga)\)</span>.</li>
<li>If closed under scalar multiplication, <span class="math inline">\(fat_F(\ga) = Pdim(F)\)</span>.</li>
</ul>
<h2 id="bounding-covering-numbers-with-dimensions">12 Bounding covering numbers with dimensions</h2>
<ul>
<li><span class="math inline">\(W\subeq A\)</span> is <span class="math inline">\(\ep\)</span>-separated/packing if <span class="math inline">\(\forall x,y\in P\)</span> distinct, <span class="math inline">\(d(x,y)\ge \ep\)</span>.</li>
<li><span class="math inline">\(\ep\)</span>-packing number <span class="math inline">\(M(\ep,W,d)\)</span> max cardinality of <span class="math inline">\(\ep\)</span>-separated subset.</li>
<li>Uniform packing number <span class="math inline">\(M_p(\ep, H, k) = \max_{x\in X^k} M(\ep, H|_x, d_p)\)</span>.</li>
<li><span class="math inline">\(M(2\ep, W, d)\le N(\ep, W, d)\le M(\ep, W,d)\)</span>.</li>
<li>If <span class="math inline">\(F:\{X\to [0,B]\}\)</span>, <span class="math inline">\(N_\iy(\ep, F, m)\le \sumo id \binom mi \pf B\ep^i\le \pf{emB}{\ep d}^d\)</span>.</li>
<li>(Bound covering number by fat-shattering dimension)
<span class="math display">\[\begin{align}
M_\iy(\ep, F, m) &amp;&lt; 2(mb^2)^{\ce{\lg y}}\\
b&amp;=\ff{2B}{\ep}\\
d&amp;=fat_F\pf{\ep}4\\
y&amp;=\sumo id \binom mi b^i.
\end{align}\]</span></li>
<li>Fat-shattering dimension determines in a fairly precise manner the covering numbers of a function class: <span class="math inline">\(F:\{X\to [0,B]\}\)</span>. <span class="math inline">\(m\ge fat_F\pf{\ep}4\ge 1\)</span>.
<span class="math display">\[\begin{align}
\fc{\lg e}8 fat_F(16\ep) \le \lg N_1(\ep, F, m) \le \lg N_\iy(\ep, F, m) \le 3fat_F\pf{\ep}4 \lg\pf{4e Bm}{\ep}^2.
\end{align}\]</span></li>
</ul>
<h2 id="section">13</h2>
<p>Large margin SEM algorithm: gives <span class="math inline">\(\wh{err}_z^\ga (L(\ga, z)) = \min_{f\in F}\wh{err}_z^\ga (f)\)</span>.</p>
<h2 id="dimensions-of-neural-networks">14 Dimensions of neural networks</h2>
<p>We bound the covering numbers and fat-shattering dimensions for net- works that are fully connected between adjacent layers, that have units with a bounded activation function satisfying a Lipschitz constraint, and that have all weights (or all weights in certain layers) constrained to be small.</p>
<p>Two types of bounds. Two notions of complexity: parameters and size of parameters</p>
<h3 id="in-terms-of-number-of-parameters">14.3 In terms of number of parameters</h3>
<ul>
<li><span class="math inline">\(F:\{X\to Y_1\}\)</span>, <span class="math inline">\(G:\{Y_1\to \R\}\)</span>, <span class="math inline">\(G\)</span> is <span class="math inline">\(L\)</span>-lipschitz. Let <span class="math inline">\(d_\iy^\rh(y,z) = \max_{1\le i\le m}\rh(y_i,z_i)\)</span>. Then <span class="math display">\[N_\iy(\ep, G\circ F_1, m)\le \max_{x\in X^m} N\pa{\fc{\ep}{2L}, F_1|_x, d_\iy^\rh} N(\pf \ep2, G, d_{L_\iy}).\]</span></li>
<li>Note finite <span class="math inline">\(L_\iy\)</span> cover is very strong.</li>
<li>Neural network
<ul>
<li><span class="math inline">\(l\)</span> layers</li>
<li><span class="math inline">\(W\)</span> weights</li>
<li>Computaion maps into <span class="math inline">\([-b,b]\)</span></li>
<li>For each unit, <span class="math inline">\(\ve{w}_1\le V\)</span>.</li>
<li>Activation functions <span class="math inline">\(L\)</span>-Lipschitz</li>
</ul></li>
<li><span class="math inline">\(N_\iy (\ep, F, m) \le \pf{4embW(LV)^l}{\ep(LV-1)}^W\)</span>.</li>
</ul>
<h3 id="on-fat-shattering-dimensions-independent-of-number-of-parameters">14.4 On Fat-shattering dimensions, independent of number of parameters</h3>
<p><span class="math inline">\(fat_F(\ga)\)</span> increases with <span class="math inline">\(\rc{\ga}\)</span>, weight bound, and number of layers.</p>
<ul>
<li>Neural network
<ul>
<li><span class="math inline">\(B=b\)</span> bound on magnitude of input values and activation function</li>
<li><span class="math inline">\(l\)</span>-layer</li>
<li><span class="math inline">\(\ve{w}_1\le V\)</span>.</li>
<li><span class="math inline">\(s\)</span> is <span class="math inline">\(L\)</span>-Lipschitz</li>
</ul></li>
<li><span class="math inline">\(l\ge 1, b\ge 1, V\ge \rc{2L}, \ep\le VbL\)</span>. Then <span class="math inline">\(\lg N_2(\ep, F_l,m) \le \rc2 \pf{2b}{\ep}^{2l} (2VL)^{l(l+1)} \lg (2n+2)\)</span>.</li>
<li><span class="math inline">\(fat_{F_l}(\ep) \le 4 \pf{32b}{\ep}^{2l} (2VL)^{l(l+1)} \ln (2n+2)\)</span>.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class="st_facebook_large" displayText="Facebook"></span>
    <span class="st_twitter_large" displayText="Tweet"></span>
    <span class="st_googleplus_large" displayText="Google +"></span>
    <span class="st_reddit_large" displayText="Reddit"></span>
    <span class="st__large" displayText></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>


<!-- Footer -->
<div id="footer">
  <div class="container">
    Built with
    <a href="http://jaspervdj.be/hakyll">Hakyll</a> 
    using 
    <a href="http://www.getbootstrap.com">Bootstrap</a>, 
    <a href="http://www.disqus.com">Disqus</a>,
    <a href="http://ignorethecode.net/blog/2010/04/20/footnotes/">Footnotes.js</a>,
    <a href="http://highlightjs.org/">Highlight.js</a>, 
    <a href="http://www.mathjax.org">MathJax</a>, 
    and <a href="http://www.sharethis.com">ShareThis</a>.
  </div>
</div>
</body>

</html>

<!-- SCRIPTS -->
<!-- jQuery-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="../../../../bootstrap/js/bootstrap.min.js"></script>

<!-- Extension : Highlight.js @ https://highlightjs.org/ -->
<!-- Syntax highlighting tomorrow-night-bright, agate-->
<link rel="stylesheet" href="../../../../highlight/css/tomorrow-night-bright.css">
<script src="../../../../highlight/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- Extension : MathJax @ https://docs.mathjax.org/en/v2.5-latest/tex.html -->
<!-- MathJax/config/local/local.js contains macros. Need to provide entire URL-->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,http://holdenlee.github.io/notebook/MathJax/config/local/local"></script>

<!-- Extension : Footnotes @ http://ignorethecode.net/blog/2010/04/20/footnotes/ -->
<script src="../../../../footnotes/js/footnotes.js"></script>

<!-- Extension : Disqus @ http://disqus.com -->
<!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->

<script src="../../../../disqus/js/disqus.js"></script>



<!-- Extension : Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73261814-1', 'auto');
  ga('send', 'pageview');

</script>

