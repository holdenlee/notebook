<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

  <meta name="description" content="Holden Lee's Research Notebook">
  <meta name="author" content="Holden Lee">
    
  <title>Kernels</title>

  <!-- Bootstrap core CSS -->
  <link href="../../../bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

  <!-- Custom styles for this template -->
  <link href="../../../css/blog.css" rel="stylesheet">
  <link href="../../../css/default.css" rel="stylesheet">

  <!-- Extension : Footnotes -->
  <link href="../../../footnotes/css/footnotes.css" rel="stylesheet">

  <!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->
  

  <!-- Extension : Collapsible lists @ http://code.stephenmorley.org/javascript/collapsible-lists/-->
  <link href="../../../collapsible_lists/css/collapsible.css" rel="stylesheet">
  <script type="text/javascript" src="../../../collapsible_lists/js/CollapsibleLists.js"></script>

</head>

<body>

<!-- Navigation bar. navbar-inverse is black, navbar-default is white.-->
<!-- To make a button active (pressed), use <li class="active"> -->
<div id="header">
  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../">Notebook</a>
      </div>
      <div id="navbar" class="collapse navbar-collapse">
        <ul class="nav navbar-nav">
          <li><a href="../../../">Home</a></li>
          <li><a href="../../../sitemap.html">Sitemap</a></li>
<!-- TODO: Distinguish between PAPERS, RESEARCH QUESTIONS, BOOKS -->
<!-- TODO: make this part a for loop over main pages -->
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </nav>
</div>
<!-- Content -->
<!--div id="content">
  <h1>Mental Wilderness</h1>-->



<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Kernels</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-12-29 
          , Modified: 2016-12-29 
	</p>
      
       <p>Tags: <a href="../../../tags/kernels.html">kernels</a>, <a href="../../../tags/RKHS.html">RKHS</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#kernels">Kernels</a></li>
 <li><a href="#equivalences">Equivalences</a></li>
 <li><a href="#rkhs">RKHS</a></li>
 <li><a href="#learning">Learning</a></li>
 <li><a href="#fourier-properties">Fourier properties</a></li>
 <li><a href="#efficient-computation">Efficient computation</a><ul>
 <li><a href="#random-features">Random features</a></li>
 <li><a href="#nystrom">Nystrom</a></li>
 </ul></li>
 <li><a href="#universality">Universality</a></li>
 <li><a href="#rkhs-embedding-of-probability-distributions">RKHS embedding of probability distributions</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>Mostly from Percy Liang’s 229T notes.</p>
<h2 id="kernels">Kernels</h2>
<p>If <span class="math inline">\(w=\sumo in \al_i \phi(x^{(i)})\)</span> then <span class="math inline">\(\an{w,\phi(x)} = \sumo in \al_i \an{\phi(x^{(i)}), \phi(x)}\)</span>.</p>
<h2 id="equivalences">Equivalences</h2>
<p>The following are equivalent (although multiple feature maps can correspond to the same kernel/RKHS):</p>
<ul>
<li>Feature map <span class="math inline">\(\phi:X\to H\)</span>.</li>
<li>Symmetric, positive (semi)definite kernel <span class="math inline">\(k:X\times X\to \R\)</span>. (for all <span class="math inline">\(S\)</span>, <span class="math inline">\((k(x_i,x_j))_{i,j\in S}\)</span> is PSD.)</li>
<li>RKHS <span class="math inline">\(\{f:X\to \R\}, \ved_H\)</span>.</li>
</ul>
<p>Proof:</p>
<ul>
<li>Feature map to kernel: <span class="math inline">\(k(x,x') = \an{\phi(x), \phi(x')}\)</span>.</li>
<li>RKHS to kernel: Riesz representation (see below). <span class="math inline">\(L_x(f) = \an{R_x,f}\)</span>.</li>
<li>Kernel to RKHS: Moore-Aronszajn. <span class="math inline">\(f=\sumo in \al_i k(x_i,x)\)</span>, <span class="math inline">\(\an{f,g} - \sumo in \sumo j{n'} \al_i\be_j k(x_i,x_j')\)</span>. (See below.)</li>
</ul>
<h2 id="rkhs">RKHS</h2>
<p>A <a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space">reproducing kernel Hilbert space (RKHS)</a> is a Hilbert space of functions <span class="math inline">\(X\to \R\)</span> in which point evaluation <span class="math inline">\(L_x[f] = f(x)\)</span> is a continuous linear functional. (Equivalently, it is bounded: <span class="math inline">\(f(x)\le M_x\ve{f}_H\)</span>.)</p>
<ul>
<li><span class="math inline">\(L^2\)</span> is not a RKHS—it consists of equivalence classes of functions.</li>
<li><span class="math inline">\(H=\set{f\in L_2(\R)}{\Supp(\wh f)\subeq [-a,a]}\)</span> is RKHS. Here <span class="math inline">\(K_x(y) = \fc{\sin(a(y-x))}{\pi(y-x)}\)</span> (bandlimited Dirac delta).</li>
</ul>
<p><strong>Theorem</strong> A Hilbert space of functions is a RKHS iff for every <span class="math inline">\(x\in X\)</span>, there exists <span class="math inline">\(K_x\)</span> such that <span class="math inline">\(g(x) = \an{g, K_x}\)</span>.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<p><em>Proof</em>. <span class="math inline">\(\Leftarrow\)</span>: inner product is continuous. <span class="math inline">\(\Rightarrow\)</span>: Riesz representation.</p>
<p>Note <span class="math inline">\(K_x(y) = \an{K_x,K_y}=:K(x,y)\)</span>. <span class="math inline">\(K\)</span> is symmetric and positive definite. (Think of <span class="math inline">\(K\)</span> as <span class="math inline">\(X_0^\R\times X_0^\R\to \R\)</span> where <span class="math inline">\(X_0^\R\)</span> is the set of functions <span class="math inline">\(X\to \R\)</span> nonzero only at a finite number of <span class="math inline">\(x\)</span>’s. I.e. formal span of <span class="math inline">\(X\)</span>.)</p>
<p><strong>Theorem (Moore-Aronszajn)</strong>. Suppose <span class="math inline">\(K\)</span> is a symmetric, positive definite kernel on a set <span class="math inline">\(X\)</span>. Then there is a unique Hilbert space of functions on <span class="math inline">\(X\)</span> for which <span class="math inline">\(K\)</span> is a reproducing kernel.</p>
<p><em>Proof</em>. Extend <span class="math inline">\(K\)</span> by bilinearity to <span class="math inline">\(\spn(X)\)</span>. Take the completion. (Extend to functions <span class="math inline">\(f=\sumo i{\iy} a_i K_{x_i}(x)\)</span> where <span class="math inline">\(\sumo i{\iy} a_i^2 K(x_i,x_i)\)</span>.)</p>
<p>(Question: how to realize this inner product as an integral?)</p>
<p>See also: Mercer’s theorem</p>
<h2 id="learning">Learning</h2>
<p><a href="https://en.wikipedia.org/wiki/Representer_theorem">Representer theorem</a> states that every function in an RKHS that minimises an empirical risk function can be written as a linear combination of the kernel function evaluated at the training points.</p>
<p>If <span class="math display">\[
f^*\in \amin_{f\in H} L(\{(x_i,y_i,f(x_i))\}) + Q(\ve{f}_H^2)
\]</span> where <span class="math inline">\(Q:[0,\iy)\to \R\)</span> strictly increasing is a regularizer, then <span class="math display">\[
f^*\in \spn(\set{k(x_i, \cdot)}{i\in [n]}).
\]</span></p>
<p>Example: kernel ridge regression <span class="math inline">\(\al = (K+\la I)^{-1}Y\)</span>.</p>
<h2 id="fourier-properties">Fourier properties</h2>
<p><span class="math inline">\(k:X\times X\to \R\)</span>, <span class="math inline">\(X\subeq \R^b\)</span> is shift-invariant if <span class="math inline">\(k(x,x') = h(x-x')\)</span>. Let <span class="math inline">\(t=x-x'\)</span>.</p>
<p><strong>Bochner’s theorem</strong>. Let <span class="math inline">\(k(x,x') = h(x-x')\)</span> be a continuous shift-invariant kernel (<span class="math inline">\(x\in \R^b\)</span>). There exists a unique finite <em>nonnegative</em> measure <span class="math inline">\(\mu\)</span> (spectral measure) such that <span class="math display">\[
h(t) = \int e^{-i\an{t,\om}}\mu(d\om).
\]</span> (<span class="math inline">\(h\)</span> is the characteristic function of <span class="math inline">\(\mu\)</span>. For <span class="math inline">\(\mu(d\om)=s(\om)d\om\)</span>, call <span class="math inline">\(s\)</span> the spectral density.) (See probability notes.)</p>
<h2 id="efficient-computation">Efficient computation</h2>
<ul>
<li>Random features: We will write the kernel function as an integral, and using Monte Carlo approximations of this integral. These approximations are of the kernel function and are data-independent.</li>
<li>Nystrom method: We will sample a subset of the n points and use these points to approximate the kernel matrix. These approximations are of the kernel matrix and are data-dependent.</li>
</ul>
<p>Ex. when points are clustered into <span class="math inline">\(m\)</span> clusters, kernel matrix looks like block diagonal with <span class="math inline">\(m\)</span> blocks <span class="math inline">\(J\)</span>, so rank <span class="math inline">\(m\)</span> approximation is effective.</p>
<h3 id="random-features">Random features</h3>
<p>Draw <span class="math inline">\(\om_i\sim \mu\)</span>, and estimate <span class="math display">\[
\wh k(x,x') = \rc m \sumo im \phi_{\om_i}(x)\ol{\phi_{\om_i}(x)}.
\]</span></p>
<strong>Theorem</strong>. Let
<span class="math display">\[\begin{align}
\mathcal F &amp;= \set{x\mapsto \int \al(\om)\phi_\om(x)\mu(d\om)}{\forall \om, |\al(\om)|\le C}\\
\wh{\mathcal F} &amp;= \set{x\mapsto \rc m\sumo im \al(\om_i)\phi_{\om_i}(x)\mu(d\om)}{\forall \om, |\al(\om)|\le C}
\end{align}\]</span>
<p>where <span class="math inline">\(\om_i\sim \mu\)</span>. Let <span class="math inline">\(\an{f,g}=\EE_{x\sim p^*}[fg]\)</span>. Then <span class="math display">\[
\Pj\pa{
\exists \wh f\in \wh{\mathcal F},
\ve{\wh f- f^*}\le \fc{C}{\sqrt m} 
\pa{1+\sqrt{2\ln(1/\de)}}
}.
\]</span></p>
<p><em>Proof</em>. McDiarmid and Jensen. (p. 133-5, omitted.)</p>
<p>(Q: what about generalization guarantee? See p. 134)</p>
<p>For other kernels, note <span class="math display">\[\an{x,x'}=\E[\an{\om, x}\an{\om,x'}].\]</span> For <span class="math inline">\(\an{x,x'}^p\)</span>, take <span class="math inline">\(p\)</span> independent draws and multiply together. For general <span class="math inline">\(f\)</span>, expand in Taylor series.</p>
<p>Comparison to neural nets: “the random features view of kernels defines a function class where <span class="math inline">\(\om_{1:m}\)</span> is chosen randomly rather than optimized, while <span class="math inline">\(\alpha_{1:m}\)</span> are optimized. The fact that random features approximates a kernel suggests that even the random initialization is quite sensible starting point.”</p>
For <span class="math inline">\(\phi_\om(x) = \one_{\om^Tx\ge 0}(\om^Tx)^q\)</span>, as <span class="math inline">\(m\to \iy\)</span>, get the kernel
<span class="math display">\[\begin{align}
k(x,x') &amp;= 2\int\phi_\om(x)\phi_\om(x')p(\om)d\om\\
&amp;= \rc\pi \ve{x}^q\ve{x'}^q J_q\ub{\pa{\cos^{-1}\pf{x^Tx'}{\ve{x}\ve{x'}}}}{\te}\\
J_0(\te) &amp;= \pi - \te\\
J_1(\te) &amp;= \sin\te + (\pi - \te)\cos\te.
\end{align}\]</span>
<p>(Bessel function?) Decouples magnitude from angle. (Cho/Saul 2009)</p>
<h3 id="nystrom">Nystrom</h3>
<p><span class="math display">\[K \approx \matt{K_{II}}{K_{IJ}}{K_{JI}}{K_{JI}K_{II}^{+}K_{IJ}}.\]</span></p>
<p><span class="math inline">\(K_{JJ} - K_{JI}K_{II}^{+}K_{IJ}\)</span> is the Schur complement.</p>
<p>Drineas/Mahoney: Choose <span class="math inline">\(I\)</span> by sampling <span class="math inline">\(i\)</span> with probability <span class="math inline">\(\fc{K_{ii}}{\sumo jn K_{jj}}\)</span>. (p. 138)</p>
<h2 id="universality">Universality</h2>
<p>Let <span class="math inline">\(X\)</span> be locally compact Hausdorff. <span class="math inline">\(k\)</span> is a <span class="math inline">\(c_0\)</span>-universal kernel if <span class="math inline">\(H\)</span> with reproducing kernel <span class="math inline">\(k\)</span> is dense in <span class="math inline">\(C_0(X)\)</span> (continuous bounded functions wrt uniform (<span class="math inline">\(L^{\iy}\)</span>) norm).</p>
<p>Carmeli2010: Let <span class="math inline">\(k\)</span> be shift-invariant with spectral measure <span class="math inline">\(\mu\)</span>. If <span class="math inline">\(\Supp(\mu) = \R^b\)</span>, then <span class="math inline">\(k\)</span> is universal.</p>
<h2 id="rkhs-embedding-of-probability-distributions">RKHS embedding of probability distributions</h2>
<p>Kernels can be used to represent and answer questions about probability distributions without having to explicitly estimate them.</p>
<p><strong>Maximum mean discrepancy</strong> <span class="math display">\[
D(P,Q,F) := \sup_{f\in\mathcal F}
\pa{
\EE_{x\sim P}[f(x)] - \EE_{x\sim Q}[f(x)]
}.
\]</span></p>
<p>If <span class="math inline">\(\mathcal F=C_0(X)\)</span>, then <span class="math inline">\(D(P,Q,\mathcal F)=0\implies P=Q\)</span>. Better: Can let <span class="math inline">\(\mathcal F = \set{f\in H}{\ve{f}_H\le 1}\)</span> where <span class="math inline">\(k\)</span> is universal. (Check proof p. 140)</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Letting <span class="math inline">\(f_x=\delta_x\)</span> is cheating—we want actual functions, not distributions.<a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class="st_facebook_large" displayText="Facebook"></span>
    <span class="st_twitter_large" displayText="Tweet"></span>
    <span class="st_googleplus_large" displayText="Google +"></span>
    <span class="st_reddit_large" displayText="Reddit"></span>
    <span class="st__large" displayText></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>


<!-- Footer -->
<div id="footer">
  <div class="container">
    Built with
    <a href="http://jaspervdj.be/hakyll">Hakyll</a> 
    using 
    <a href="http://www.getbootstrap.com">Bootstrap</a>, 
    <a href="http://www.disqus.com">Disqus</a>,
    <a href="http://ignorethecode.net/blog/2010/04/20/footnotes/">Footnotes.js</a>,
    <a href="http://highlightjs.org/">Highlight.js</a>, 
    <a href="http://www.mathjax.org">MathJax</a>, 
    and <a href="http://www.sharethis.com">ShareThis</a>.
  </div>
</div>
</body>

</html>

<!-- SCRIPTS -->
<!-- jQuery-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="../../../bootstrap/js/bootstrap.min.js"></script>

<!-- Extension : Highlight.js @ https://highlightjs.org/ -->
<!-- Syntax highlighting tomorrow-night-bright, agate-->
<link rel="stylesheet" href="../../../highlight/css/tomorrow-night-bright.css">
<script src="../../../highlight/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- Extension : MathJax @ https://docs.mathjax.org/en/v2.5-latest/tex.html -->
<!-- MathJax/config/local/local.js contains macros. Need to provide entire URL-->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,http://holdenlee.github.io/notebook/MathJax/config/local/local"></script>

<!-- Extension : Footnotes @ http://ignorethecode.net/blog/2010/04/20/footnotes/ -->
<script src="../../../footnotes/js/footnotes.js"></script>

<!-- Extension : Disqus @ http://disqus.com -->
<!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->

<script src="../../../disqus/js/disqus.js"></script>



<!-- Extension : Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73261814-1', 'auto');
  ga('send', 'pageview');

</script>

