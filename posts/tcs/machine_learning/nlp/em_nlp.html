<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

  <meta name="description" content="Holden Lee's Research Notebook">
  <meta name="author" content="Holden Lee">
    
  <title>Embedding methods in NLP</title>

  <!-- Bootstrap core CSS -->
  <link href="../../../../bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

  <!-- Custom styles for this template -->
  <link href="../../../../css/blog.css" rel="stylesheet">
  <link href="../../../../css/default.css" rel="stylesheet">

  <!-- Extension : Footnotes -->
  <link href="../../../../footnotes/css/footnotes.css" rel="stylesheet">

  <!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->
  

  <!-- Extension : Collapsible lists @ http://code.stephenmorley.org/javascript/collapsible-lists/-->
  <link href="../../../../collapsible_lists/css/collapsible.css" rel="stylesheet">
  <script type="text/javascript" src="../../../../collapsible_lists/js/CollapsibleLists.js"></script>

</head>

<body>

<!-- Navigation bar. navbar-inverse is black, navbar-default is white.-->
<!-- To make a button active (pressed), use <li class="active"> -->
<div id="header">
  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../../">Notebook</a>
      </div>
      <div id="navbar" class="collapse navbar-collapse">
        <ul class="nav navbar-nav">
          <li><a href="../../../../">Home</a></li>
          <li><a href="../../../../sitemap.html">Sitemap</a></li>
<!-- TODO: Distinguish between PAPERS, RESEARCH QUESTIONS, BOOKS -->
<!-- TODO: make this part a for loop over main pages -->
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </nav>
</div>
<!-- Content -->
<!--div id="content">
  <h1>Mental Wilderness</h1>-->



<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Embedding methods in NLP</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-12-16 
          , Modified: 2016-12-16 
	</p>
      
       <p>Tags: <a href="../../../../tags/nlp.html">nlp</a>, <a href="../../../../tags/word%20vectors.html">word vectors</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#emnlp">EMNLP</a><ul>
 <li><a href="#methods">Methods</a></li>
 <li><a href="#details">Details</a></li>
 <li><a href="#embeddings-for-multi-relational-data">Embeddings for multi-relational data</a></li>
 </ul></li>
 <li><a href="#spwc-recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank">[SPWC] Recursive deep models for semantic compositionality over a sentiment treebank</a></li>
 <li><a href="#scmn16-reasoning-with-neural-tensor-networks-for-knowledge-base-completion">[SCMN16] Reasoning With Neural Tensor Networks for Knowledge Base Completion</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="emnlp">EMNLP</h2>
<p><a href="http://emnlp2014.org/tutorials.html#embedding">EMNLP 2014 tutorial</a></p>
<h3 id="methods">Methods</h3>
<ul>
<li>Ranking/retrieval
<ul>
<li>LSI</li>
<li>SSI</li>
</ul></li>
<li>Language modeling
<ul>
<li>NNLM</li>
<li>Word2vec</li>
<li>GloVe</li>
</ul></li>
<li>Supervised prediction
<ul>
<li>RNN(LM)</li>
<li>Convolutional nets for tagging (SENNA)</li>
</ul></li>
</ul>
<h3 id="details">Details</h3>
<ul>
<li>Bag of words (baseline)</li>
<li>LSI <span class="math inline">\(\phi(d) = Ud\)</span> (<span class="math inline">\(d\)</span> document as sum of words)</li>
<li>SSI Similarity of query <span class="math inline">\(q\)</span> to document <span class="math inline">\(d\)</span> given by <span class="math inline">\(f(q,d) = q^TWd\)</span>, <span class="math inline">\(W\)</span> from data.
<ul>
<li><span class="math inline">\(W\)</span> is big.</li>
<li>Make <span class="math inline">\(W\)</span> low rank. <span class="math inline">\(W=U^TV+I\)</span>.</li>
<li>Maximize AUC. Want <span class="math inline">\(f(q,d^+)&gt;f(q,d^-)\)</span>. (14)</li>
<li>Ex. cross-language retrieval</li>
<li>Better: replace AUC with rank weighted loss</li>
</ul></li>
<li><span class="math inline">\(n\)</span>-gram (baseline)</li>
<li>NNLM
<ul>
<li>Arrange in tree. Cluster dictionary according to semantics or frequency.</li>
</ul></li>
<li>Word2Vec
<ul>
<li>CBOW: predict current word from neighbors</li>
<li>Skip-gram: predict neighbors from current word</li>
<li>Compositionality (addition)</li>
</ul></li>
<li>NLP tasks: POS tagging, chunking (NP, VP), named entity recognition, semantic role labeling</li>
<li>Hand-made features, SVM. ASSERT. Pipeline
<ul>
<li>POS tagger, parser</li>
<li>NER tagging</li>
<li>Put together for SRL labeler</li>
</ul></li>
<li>Deep learning way
<ul>
<li>Avoid parse tree</li>
<li>Avoid hand-built features</li>
<li>Joint train NLP tasks on common embedding</li>
<li>Window approach: feed fixed-size window around each word</li>
<li>Sentence: feed whole sentence, tag 1 word at a time, convolutions handle variable length inputs. Max pool over time</li>
<li>Deep SRL
<ul>
<li>Softmax: Train for word tag likelihood, or</li>
<li>Sentence tag likelihood. Sentence score is sum of likelihoods for individual words and transition score</li>
</ul></li>
<li>Bad for rare words</li>
</ul></li>
<li>? semi-supervised (61)</li>
<li>Recursive NN’s
<ul>
<li>Socher ICML13, EMNLP13</li>
<li>Build sentence representations using parse tree <span class="math inline">\(f(W[c_1;c_2]+b)\)</span></li>
</ul></li>
<li>Paragraph vector predicts words in doc, <span class="math inline">\(n\)</span>-grams in the doc.</li>
<li>Words are combined with linear matrices dependendent on the P.O.S.: G. Dinu and M. Baroni. How to make words with vectors: Phrase generation in distributional semantics. ACL ’14.</li>
<li>Previous common belief in NLP: engineering syntactic features necessary for semantic tasks. One can do well by engineering a model/algorithm rather than features.</li>
</ul>
<h3 id="embeddings-for-multi-relational-data">Embeddings for multi-relational data</h3>
<ul>
<li>Embeddings for multi-relational data
<ul>
<li>Relation (sub, rel, obj)</li>
<li>WordNet: dictionary where each entity is a sense (synset).</li>
<li>Freebase</li>
<li>KB’s are hard to manipulate.
<ul>
<li>Large</li>
<li>Sparse</li>
<li>Noisy/incomplete</li>
</ul></li>
<li>Solutions:
<ul>
<li>Encode into low-dimensional vector spaces</li>
<li>Use representations to complete/visualize, in applications</li>
</ul></li>
<li>Link prediction
<ul>
<li>RESCAL (<span class="math inline">\(X_k\)</span> are matrices) <span class="math display">\[
\min_{A,R} \rc2 \pa{\sum_k \ve{X_k - AR_k A^T}_F^2} + \la_A \ve{A}_F^2 + \la_R \sum_k \ve{R_k}_F^2.
\]</span>
<ul>
<li>Downsides: many parameters, bad scalability, bad reconstruction criteria for binary data</li>
</ul></li>
<li>Ideas
<ul>
<li>Low-dimensionsal continuous vector embedding trained on similarity</li>
<li>Stochastic training based on ranking loss</li>
</ul></li>
<li><span class="math inline">\(d(sub, rel, obj) = -\ve{R^{left}s^T - R^{right} o^T}_1\)</span>. (Note: this is projection onto a common space, rather than something quadratic!)
<ul>
<li>SGD with negative examples</li>
<li>Still have problems</li>
</ul></li>
<li>Neural tensor networks
<ul>
<li><span class="math inline">\(d(sub, rel, obj) = u_r^T \tanh (h^TW_r t + V_r^1 h + V_3^2 t + b_r).\)</span></li>
<li><span class="math inline">\(d^3\)</span> parameters</li>
</ul></li>
<li>Model relations as translations
<ul>
<li>$d = _2^2.</li>
<li>there may exist embedding spaces in which relationships among concepts are all decribed by translations</li>
<li>Much fewer parameters. Does better!</li>
<li>Refinements (108)</li>
</ul></li>
</ul></li>
<li>Embeddings for information extraction
<ul>
<li>Entity linking: Identify mentions</li>
<li>Relation extraction: extract facts</li>
<li>Word-sense disambiguation is key</li>
<li>Embedding-based classifier to predict relation (Weston13, 11) (predict based on other text besides subj, obj. Can combine with KB</li>
<li>Universal schemas (I don’t get this.)</li>
</ul></li>
<li>Question answering
<ul>
<li>Subgraph embeddings: learn embeddings of questions and candidate answers (Bordes14) (Q: How to embed subgraph???)</li>
<li>Convert Freebase to Q&amp;A pairs.</li>
<li>Paraphrase questions</li>
</ul></li>
</ul></li>
<li>Issues
<ul>
<li>Must train embeddings together</li>
<li>Compression, blurring (How to one-shot learn new symbols?)</li>
<li>Sequential models suffer from long-term memory</li>
<li>Need many updates</li>
<li>Negative example sampling inefficient</li>
<li>How to use nonlinearity?</li>
</ul></li>
<li>Code
<ul>
<li><a href="www.torch.ch">Torch</a></li>
<li><a href="ronan.collobert.com/senna">SENNA</a></li>
<li><a href="www.fit.vutbr.cz/~imikolov/rnnlm">RNNLM</a></li>
<li><a href="code.google.com/p/word2vec">Word2vec</a></li>
<li><a href="nlp.stanford.edu/sentiment">Recursive NN</a></li>
<li><a href="github.com/glorotxa/sme">SME (multi-relational data)</a></li>
</ul></li>
</ul>
<h2 id="spwc-recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank">[SPWC] Recursive deep models for semantic compositionality over a sentiment treebank</h2>
<p>Put triplet (subj, rel, obj) through tensor network to predict. <span class="math display">\[
g(e_1,R,e_2) = 
u_R^T f\pa{
e_1^T W_R^{[1:k]} e_2 + 
V_R\coltwo{e_1}{e_2} + b_R
}
\]</span></p>
<!--
(Q: how about subj, obj -> rel, etc.? Joint probability distribution makes more sense? What is relationship between prob and NN models?)
-->
<h2 id="scmn16-reasoning-with-neural-tensor-networks-for-knowledge-base-completion">[SCMN16] Reasoning With Neural Tensor Networks for Knowledge Base Completion</h2>
<p>Recursive neural tensor model</p>
<p>Sentiment Treebank: sentiment labels for phrases in parse trees of sentences. <a href="http://nlp.stanford.edu/sentiment">Website</a></p>
<p>Captures effects of negation and its scope.</p>
<ul>
<li>RNN with a classifier at each node in the parse tree that predicts sentiment in one of 5 classes. (<span class="math inline">\(W\)</span> is the same matrix.) (Q: how do you make it a binary tree?)</li>
<li>MV-RNN (matrix-vector): represent every word and longer phrase in parse tree as both vector and matrix (initialized as <span class="math inline">\(I_d+\)</span>noise). Matrix of one multiplied by vector of other. (parameters is large and depends on vocab)</li>
<li>RNTN allows quadratic interaction: <span class="math inline">\(p = f\pa{(b\,c)V^{[1:d]}\coltwo bc + W \coltwo bc}\)</span>.</li>
</ul>
<p>Train: Classification at each node. Minimize KL-divergence.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class="st_facebook_large" displayText="Facebook"></span>
    <span class="st_twitter_large" displayText="Tweet"></span>
    <span class="st_googleplus_large" displayText="Google +"></span>
    <span class="st_reddit_large" displayText="Reddit"></span>
    <span class="st__large" displayText></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>


<!-- Footer -->
<div id="footer">
  <div class="container">
    Built with
    <a href="http://jaspervdj.be/hakyll">Hakyll</a> 
    using 
    <a href="http://www.getbootstrap.com">Bootstrap</a>, 
    <a href="http://www.disqus.com">Disqus</a>,
    <a href="http://ignorethecode.net/blog/2010/04/20/footnotes/">Footnotes.js</a>,
    <a href="http://highlightjs.org/">Highlight.js</a>, 
    <a href="http://www.mathjax.org">MathJax</a>, 
    and <a href="http://www.sharethis.com">ShareThis</a>.
  </div>
</div>
</body>

</html>

<!-- SCRIPTS -->
<!-- jQuery-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="../../../../bootstrap/js/bootstrap.min.js"></script>

<!-- Extension : Highlight.js @ https://highlightjs.org/ -->
<!-- Syntax highlighting tomorrow-night-bright, agate-->
<link rel="stylesheet" href="../../../../highlight/css/tomorrow-night-bright.css">
<script src="../../../../highlight/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- Extension : MathJax @ https://docs.mathjax.org/en/v2.5-latest/tex.html -->
<!-- MathJax/config/local/local.js contains macros. Need to provide entire URL-->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,http://holdenlee.github.io/notebook/MathJax/config/local/local"></script>

<!-- Extension : Footnotes @ http://ignorethecode.net/blog/2010/04/20/footnotes/ -->
<script src="../../../../footnotes/js/footnotes.js"></script>

<!-- Extension : Disqus @ http://disqus.com -->
<!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->

<script src="../../../../disqus/js/disqus.js"></script>



<!-- Extension : Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73261814-1', 'auto');
  ga('send', 'pageview');

</script>

