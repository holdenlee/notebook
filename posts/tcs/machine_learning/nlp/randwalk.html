<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

  <meta name="description" content="Holden Lee's Research Notebook">
  <meta name="author" content="Holden Lee">
    
  <title>[ALLMR16] RAND-WALK: A latent variable model approach to word embeddings</title>

  <!-- Bootstrap core CSS -->
  <link href="../../../../bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

  <!-- Custom styles for this template -->
  <link href="../../../../css/blog.css" rel="stylesheet">
  <link href="../../../../css/default.css" rel="stylesheet">

  <!-- Extension : Footnotes -->
  <link href="../../../../footnotes/css/footnotes.css" rel="stylesheet">

  <!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->
  

</head>

<body>

<!-- Navigation bar. navbar-inverse is black, navbar-default is white.-->
<!-- To make a button active (pressed), use <li class="active"> -->
<div id="header">
  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../../">Notebook</a>
      </div>
      <div id="navbar" class="collapse navbar-collapse">
        <ul class="nav navbar-nav">
          <li><a href="../../../../">Home</a></li>
          <li><a href="../../../../sitemap.html">Sitemap</a></li>
<!-- TODO: Distinguish between PAPERS, RESEARCH QUESTIONS, BOOKS -->
<!-- TODO: make this part a for loop over main pages -->
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </nav>
</div>
<!-- Content -->
<!--div id="content">
  <h1>Mental Wilderness</h1>-->



<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[ALLMR16] RAND-WALK: A latent variable model approach to word embeddings</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-15 
          , Modified: 2016-03-15 
	</p>
      
       <p>Tags: <a href="../../../../tags/nlp.html">nlp</a>, <a href="../../../../tags/word%20embeddings.html">word embeddings</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#task">Task</a></li>
 <li><a href="#previous-work-observed-phenomena">Previous work, observed phenomena</a><ul>
 <li><a href="#mysteries">Mysteries</a></li>
 </ul></li>
 <li><a href="#model">Model</a></li>
 <li><a href="#explanation">Explanation</a></li>
 <li><a href="#algorithm">Algorithm</a></li>
 <li><a href="#followup">Followup</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="task">Task</h2>
<p>Given a corpus (a long sequence of words, e.g. the text of Wikipedia) to learn from, answer analogy questions such as ?:queen::man:woman.</p>
<h2 id="previous-work-observed-phenomena">Previous work, observed phenomena</h2>
<p>The usual approach is to learn “word vectors.”</p>
<p>Define a context of a word <span class="math inline">\(w\)</span> in the corpus to be the two words (say) on either side of <span class="math inline">\(w\)</span>. (Thus, for a context <span class="math inline">\(\chi=(w_{-2},w_{-1},w_1,w_2)\)</span>, <span class="math inline">\(\Pj(\chi|w)\)</span> means the probability of observing the words of <span class="math inline">\(\chi\)</span> in a window of length 2, given that the middle word is <span class="math inline">\(w\)</span>.) (Mikolov)</p>
<p>Pennington et al. and Levy and Goldberg posit the following approach.</p>
<ul>
<li>Model: If <span class="math inline">\(a:b::c:d\)</span> is an analogy, then <span class="math display">\[ \fc{\Pj(\chi|a)}{\Pj(\chi|b)} \approx \fc{\Pj(\chi|a)}{\Pj(\chi|b)}.\]</span></li>
<li>Thus, the solution to the analogy <span class="math inline">\(?:b::c:d\)</span>$ is <span class="math display">\[ \amin_w \sum_\chi \pa{ \ln \pf{\Pj(\chi|w)}{\Pj(\chi|b)} - \ln \pf{\Pj(\chi|c)}{\Pj(\chi|d)} }^2.\]</span></li>
<li>Define probability-mutual-information <span class="math display">\[ PMI(w,\chi) = \ln \pf{\Pj(w,\chi)}{\Pj(w)\Pj(\chi)} = \ln \pf{\Pj(\chi|w)}{\Pj(\chi)}.\]</span> Let <span class="math inline">\(C\)</span> be the set of possible contexts. For a word <span class="math inline">\(w\)</span>, let <span class="math inline">\(v_w\)</span> be the vector indexed by contexts <span class="math inline">\(\chi \in C\)</span>, <!--containing the empirical estimate of PMI from the corpus,
    $$ v_w(\chi)= \wh{PMI}(w,\chi) = \ln \pf{\wh \Pj(w,\chi)}{\wh \Pj(w)\wh \Pj(\chi)} = \ln \pf{\wh \Pj(\chi|w)}{\wh \Pj(\chi)}.$$--> <span class="math display">\[ v_w(\chi)= {PMI}(w,\chi) = \ln \pf{ \Pj(w,\chi)}{\Pj(w) \Pj(\chi)} = \ln \pf{ \Pj(\chi|w)}{\Pj(\chi)}.\]</span> Under this embedding, the summand equals <span class="math inline">\(v_a-v_b-v_c+v_d\)</span>. To find <span class="math inline">\(a\)</span>, solve <span class="math display">\[ \amin_a \ve{v_a-v_b-v_c+v_d}^2.\]</span></li>
<li>Algorithm: GloVe (global vector) method (Pennington) Let <span class="math inline">\(X_{w,w'}\)</span> be the co-occurrence for words <span class="math inline">\(w,w'\)</span>. Find low-dimensional <span class="math inline">\(\wt v_w, \wt v_{w'}, \wt b_w, \wt b_{w'}\)</span> to minimize <span class="math display">\[ \sum_{w,w'} f(X_{w,w'}) (\an{\wt v_w, \wt v_{w'}} - \wt b_w- \wt b_{w'} - \ln X_{w,w'})^2\]</span> for some function <span class="math inline">\(f\)</span>. They choose <span class="math inline">\(f(x) = \min\bc{\pf{x}{x_{\max}}^{.75}, 1}, x_{\max}=100\)</span> from experiments.</li>
</ul>
<p>(How to optimize this?)</p>
<h3 id="mysteries">Mysteries</h3>
<ol type="1">
<li>There is a disconnect between the definition of <span class="math inline">\(v_w\)</span> and the estimate <span class="math inline">\(\wt v_w\)</span>. Namely, the <span class="math inline">\(v_w\)</span> are vectors giving the PMI with all <em>contexts</em> while <span class="math inline">\(\wt v_w\)</span> are the vectors such that <span class="math inline">\(\an{\wt v_w,\wt v_{w'}}\)</span> give the <em>word-word co-occurences</em>. Why do the learned <span class="math inline">\(\wt v_w\)</span> help in solving analogies? Why is the optimization problem in the algorithm a good proxy?</li>
<li>Why is this method stable to noise? <!--Why would we suspect noise is bad?--></li>
</ol>
<h2 id="model">Model</h2>
<p>Let the dimension of the underlying space be <span class="math inline">\(d\)</span>.</p>
<ul>
<li>There are <span class="math inline">\(n\)</span> words <span class="math inline">\(w\)</span> in the dictionary <span class="math inline">\(W\)</span>, and they are associated with vectors <span class="math inline">\(v_w\in \R^d\)</span>.
<ul>
<li>The vectors <span class="math inline">\(v_w\)</span> are iid generated by <span class="math inline">\(v=s\cdot \wh v\)</span> where <!--$\wh v$ is uniform on the sphere and -->
<ul>
<li><span class="math inline">\(\wh v\sim N(0,I_d)\)</span>,</li>
<li><span class="math inline">\(s\)</span> is a random scalar with <span class="math inline">\(\si^2\le d\)</span> and <span class="math inline">\(|s|\le \ka \sqrt d\)</span> for some constant <span class="math inline">\(\ka\)</span>.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></li>
</ul></li>
</ul></li>
<li>At each time <span class="math inline">\(t\in \N\)</span>, there is a context vector <span class="math inline">\(c_t\in \R^d\)</span>.
<ul>
<li>The vectors <span class="math inline">\(c_t\)</span> follow a random walk, satisfying the following:
<ul>
<li>(Uniform stationary distribution) The stationary distribution is <span class="math inline">\([-\rc{\sqrt d},\rc{\sqrt d}]^d\)</span>.</li>
<li>(Small drift) The drift in the context vector is <span class="math inline">\(\ve{c_{t+1}-c_t}_1\le \rc{\ln n}\)</span>. (There are more complicated general conditions under which the theorems work; take this for simplicity.) <!--  * (small drift) $\ve{\De_t}_1\le \rc{\ln n}$.
*--></li>
</ul></li>
</ul></li>
<li>At each time <span class="math inline">\(t\)</span>, word <span class="math inline">\(w\)</span> is emitted with probability
\begin{align}
\Pj[w|c_t] &amp;= \rc{Z_{c_t}} \exp(\an{v_w,c_t})\\
\text{where} Z_{c}:&amp;= \sum_w \exp(\an{v_w,c}).
\end{align}</li>
</ul>
<h2 id="explanation">Explanation</h2>
<p>Let <span class="math inline">\(\Pj(w,w')\)</span> be the probability that <span class="math inline">\(w,w'\)</span> appear consecutively at time <span class="math inline">\(t,t+1\)</span> when <span class="math inline">\(c_t\sim U_{[-\rc{\sqrt d},\rc{\sqrt d}]^d}\)</span> is drawn from the stationary distribution. Then the following hold.</p>
<strong>Theorem 1</strong>: With high probability over choice of <span class="math inline">\(v_w\)</span>’s,
\begin{align}
\forall &amp;w,w', &amp;
\ln \Pj (w,w')&amp;\approx \rc{2d}|v_w+v_w'|^2 - 2\lg Z - o(1)\\
\forall &amp; w,&amp;
\lg \Pj(w) &amp;\approx \rc{2d} |v_w|^2 - \lg Z -o(1)\\
\therefore &amp;&amp; \lg \fc{\Pj[w,w']}{\Pj[w]\Pj[w']} &amp;\approx \rc d \an{v_w,v_w'}\pm o(1).
\end{align}
<p>This is exactly the PMI, so the theorem “explains” Mystery 1.</p>
<p><em>Proof idea</em>:</p>
<p>Let <span class="math inline">\(c\)</span> be the context vector at time <span class="math inline">\(t\)</span> and <span class="math inline">\(c'\)</span> be the vector at time <span class="math inline">\(t+1\)</span>.</p>
<ol type="1">
<li>The main difficulty is that <span class="math inline">\(Z_c\)</span> is intractable to compute. However, because the <span class="math inline">\(v_w\)</span> are random (or isotropic), so <span class="math inline">\(Z_c\)</span> concentrates around its mean, and we can approximate it by a constant <span class="math inline">\(Z\)</span> (Theorem 2 in the paper).</li>
<li>Because drift is small, we can make the approximation <span class="math inline">\(c'\approx c\)</span>. Then
\begin{align}
\Pj(w,w') &amp;= \int_{c,c'} \Pj(w|c)\Pj(w'|c_{t+1})\Pj(c,c')\,dc\,dc'\\
&amp;\sim \rc{Z^2} \EE_{c} \exp(\an{v_w+v_w',c})\\
&amp;\sim \rc{Z^2}\exp\pf{|v_w+v_{w'}|^2}{2}
\end{align}
using some calculus in the last step (exercise). <!--check this--></li>
</ol>
<p>The calculation for <span class="math inline">\(\Pj(w)\)</span> is even simpler.</p>
<h2 id="algorithm">Algorithm</h2>
<p>What algorithm does the theory suggest to estimate the <span class="math inline">\(v_w\)</span>’s?</p>
<p>It suggests minimizing an objective as in GloVe. The weights <span class="math inline">\(f_{w,w'}\)</span> re selected to compensate noise in <span class="math inline">\(X_{w,w'}\)</span>; when <span class="math inline">\(X_{w,w'}\)</span> is on the average larger, it has lower variance, and the weight <span class="math inline">\(f_{w,w'}\)</span> is larger.</p>
<p>(What improvement does it suggest?)</p>
<h2 id="followup">Followup</h2>
<ul>
<li>Polysemy</li>
<li>Weighted SVD (Yuanzhi Li)</li>
</ul>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>The results in the paper are stated as “with high probability over the choice of <span class="math inline">\(v_w\)</span>”. This can probably be relaxed to “For all <span class="math inline">\(v_w\)</span> that are isotropic”, where <strong>isotropic</strong> means <span class="math inline">\(\EE_w [v_wv_w^T]\)</span> has all eigenvalues in <span class="math inline">\([1,1+\de]\)</span>.<a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class="st_facebook_large" displayText="Facebook"></span>
    <span class="st_twitter_large" displayText="Tweet"></span>
    <span class="st_googleplus_large" displayText="Google +"></span>
    <span class="st_reddit_large" displayText="Reddit"></span>
    <span class="st__large" displayText></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>


<!-- Footer -->
<div id="footer">
  <div class="container">
    Built with
    <a href="http://jaspervdj.be/hakyll">Hakyll</a> 
    using 
    <a href="http://www.getbootstrap.com">Bootstrap</a>, 
    <a href="http://www.disqus.com">Disqus</a>,
    <a href="http://ignorethecode.net/blog/2010/04/20/footnotes/">Footnotes.js</a>,
    <a href="http://highlightjs.org/">Highlight.js</a>, 
    <a href="http://www.mathjax.org">MathJax</a>, 
    and <a href="http://www.sharethis.com">ShareThis</a>.
  </div>
</div>
</body>

</html>

<!-- SCRIPTS -->
<!-- jQuery-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="../../../../bootstrap/js/bootstrap.min.js"></script>

<!-- Extension : Highlight.js @ https://highlightjs.org/ -->
<!-- Syntax highlighting tomorrow-night-bright, agate-->
<link rel="stylesheet" href="../../../../highlight/css/tomorrow-night-bright.css">
<script src="../../../../highlight/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- Extension : MathJax @ https://docs.mathjax.org/en/v2.5-latest/tex.html -->
<!-- MathJax/config/local/local.js contains macros. Need to provide entire URL-->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,http://holdenlee.github.io/notebook/MathJax/config/local/local"></script>

<!-- Extension : Footnotes @ http://ignorethecode.net/blog/2010/04/20/footnotes/ -->
<script src="../../../../footnotes/js/footnotes.js"></script>

<!-- Extension : Disqus @ http://disqus.com -->
<!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->

<script src="../../../../disqus/js/disqus.js"></script>



<!-- Extension : Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73261814-1', 'auto');
  ga('send', 'pageview');

</script>

