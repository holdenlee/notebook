<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

  <meta name="description" content="Holden Lee's Research Notebook">
  <meta name="author" content="Holden Lee">
    
  <title>[AGMM15] Simple, efficient, and neural algorithms for sparse coding</title>

  <!-- Bootstrap core CSS -->
  <link href="../../../bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

  <!-- Custom styles for this template -->
  <link href="../../../css/blog.css" rel="stylesheet">
  <link href="../../../css/default.css" rel="stylesheet">

  <!-- Extension : Footnotes -->
  <link href="../../../footnotes/css/footnotes.css" rel="stylesheet">

  <!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->
  

</head>

<body>

<!-- Navigation bar. navbar-inverse is black, navbar-default is white.-->
<!-- To make a button active (pressed), use <li class="active"> -->
<div id="header">
  <nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../">Notebook</a>
      </div>
      <div id="navbar" class="collapse navbar-collapse">
        <ul class="nav navbar-nav">
          <li><a href="../../../">Home</a></li>
          <li><a href="../../../sitemap.html">Sitemap</a></li>
<!-- TODO: Distinguish between PAPERS, RESEARCH QUESTIONS, BOOKS -->
<!-- TODO: make this part a for loop over main pages -->
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </nav>
</div>
<!-- Content -->
<!--div id="content">
  <h1>Mental Wilderness</h1>-->



<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[AGMM15] Simple, efficient, and neural algorithms for sparse coding</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-25 
          , Modified: 2016-03-25 
	</p>
      
       <p>Tags: <a href="../../../tags/dictionary%20learning.html">dictionary learning</a>, <a href="../../../tags/sparse%20coding.html">sparse coding</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#problemmodel-and-assumptions">Problem/Model and Assumptions</a></li>
 <li><a href="#algorithm">Algorithm</a><ul>
 <li><a href="#instantiation">Instantiation</a><ul>
 <li><a href="#version-1">Version 1</a></li>
 <li><a href="#version-2">Version 2</a></li>
 <li><a href="#instantiation-1">Instantiation</a></li>
 </ul></li>
 </ul></li>
 <li><a href="#approximate-gradient-descent">Approximate gradient descent</a></li>
 <li><a href="#theorems">Theorems</a></li>
 <li><a href="#proofs">Proofs</a><ul>
 <li><a href="#initialization">Initialization</a></li>
 </ul></li>
 <li><a href="#birds-eye-view">Bird’s eye view</a></li>
 <li><a href="#questions">Questions</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="problemmodel-and-assumptions">Problem/Model and Assumptions</h2>
<p>Given many independent samples <span class="math inline">\(y=A^*x^*+N\)</span>, recover <span class="math inline">\((A^*,x^*)\)</span>. Here,</p>
<ol type="1">
<li><span class="math inline">\(A^*\in \R^{n\times m}\)</span> is a fixed matrix. We assume:
<ul>
<li><span class="math inline">\(A\)</span> has unit norm columns.</li>
<li>(Norm is like that of a random matrix) <span class="math inline">\(\ve{A^*}=O\pa{\sfc{m}{n}}\)</span>.</li>
<li><span class="math inline">\(A\)</span> is <span class="math inline">\(\mu\)</span>-incoherent, i.e., <span class="math inline">\(\an{A_i,A_j}\le \fc{\mu}{\sqrt n}\)</span>.</li>
</ul></li>
<li><span class="math inline">\(x^*\)</span> is drawn from a sparse distribution. Specifically, <span class="math inline">\(x^*\sim D\)</span> where
<ul>
<li>Always, <span class="math inline">\(\Supp(x^*)\le k\)</span>.</li>
<li><span class="math inline">\(\Pj(i\in S)=\Te\pf{k}{m}\)</span>.</li>
<li><span class="math inline">\(\Pj(i,j\in S) = \Te\pf{k^2}{m^2}\)</span>.</li>
<li><span class="math inline">\(\E[x_i^*|x_j^*\ne 0]=0\)</span> (why do we care about the conditioning here?) and <span class="math inline">\(\E[x_i^{*2}| x_i^*\ne0]=1\)</span>.</li>
<li>When <span class="math inline">\(x_i^*\ne 0\)</span>, <span class="math inline">\(|x_i^*|\ge C\)</span> for some constant <span class="math inline">\(C\le 1\)</span>. (This is an unreasonable assumption, but it makes sure the thresholding doesn’t cut off actual coordinates.)</li>
<li>Conditioned on the support, the nonzero entries are pairwise independent and subgaussian. Subgaussian means <span class="math inline">\(\exists C,v&gt;0, \Pj(|X|&gt;t)\le Ce^{-vt^2}\)</span> or equivalently <span class="math inline">\(\exists b, \E e^{tX}\le e^{b^2t^2/2}\)</span>.</li>
</ul></li>
<li>The noise <span class="math inline">\(N\)</span> is Gaussian and independent.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></li>
</ol>
<h2 id="algorithm">Algorithm</h2>
<p>The algorithm is alternating minimization. In alternating minimization, we want to minimize a function <span class="math inline">\(f(x,y)\)</span>. It is difficult to minimize with respect to <span class="math inline">\((x,y)\)</span> together (e.g., because <span class="math inline">\(f\)</span> is nonconvex when taken as a joint function of <span class="math inline">\(x,y\)</span>) but easy to minimize with respect to <span class="math inline">\(x\)</span> or <span class="math inline">\(y\)</span> while keeping the other fixed (e.g., because <span class="math inline">\(f\)</span> is convex in <span class="math inline">\(x\)</span> and in <span class="math inline">\(y\)</span>). Alternate between</p>
<ul>
<li>minimizing with respect to <span class="math inline">\(x\)</span>, and</li>
<li>minimizing with respect to <span class="math inline">\(y\)</span>.</li>
</ul>
<p>For example, we can take gradient steps.</p>
<p>There are no results for general nonconvex <span class="math inline">\(f\)</span>; theoretical results tend to be ad hoc (for specific <span class="math inline">\(f\)</span>). AGMM make a general technique that can be used to analyze alternating minimization algorithms, <strong>approximate gradient descent</strong>.</p>
<h3 id="instantiation">Instantiation</h3>
<h4 id="version-1">Version 1</h4>
<p>The algorithm is</p>
<ul>
<li>Input: initial guess <span class="math inline">\(A^0\)</span>, step size <span class="math inline">\(\eta\)</span>.</li>
<li>At step <span class="math inline">\(s\)</span>, take <span class="math inline">\(p\)</span> samples <span class="math inline">\(y^{(i)}\)</span> and let
<ul>
<li>(Decode) Let <span class="math inline">\(x^{(i)}=1_{\ge \fc C2}((A^s)^Ty^{(i)})\)</span>.</li>
<li>(Update)
\begin{align}
\wh g^s &amp;=\rc p \sumo ip (y^{(i)} - A^{(s)}x^{(i)}) \sign(x^{(i)})^T\\
A^{s+1} &amp;= A^s - \eta \wh g^s
\end{align}</li>
</ul></li>
</ul>
<p>The decoding step is doing a minimization with respect to <span class="math inline">\(x\)</span><a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>. Intuitively, since <span class="math inline">\(A^*\)</span> is incoherent (random-like), <span class="math inline">\((A^*)^TA^*\approx I\)</span>, so multiplying by <span class="math inline">\((A^*)^T\)</span> has the effect of inverting. The threshold acts as a denoiser.</p>
<p>Note that we don’t quite take the gradient step: the gradient is <span class="math inline">\(2(y^{(i)} - Ax^{(i)})^T x^{(i)}\)</span>; we only take the sign of <span class="math inline">\(x^{(i)}\)</span>.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p>
<p>(There’s no regularization here?)</p>
<h4 id="version-2">Version 2</h4>
<p>A better version of the above is to use gradient descent (Olshausen-Field), rather than just the sign of the difference. Let <span class="math inline">\(A'=A - \eta g\)</span> where <span class="math inline">\(g=\E [(y-Ax) x^T]\)</span>. The complication is that this may not prserver <span class="math inline">\((\de,2)\)</span>-nearness, so the update rule is replaced by</p>
\begin{align}
\wh g^s &amp;=\rc p \sumo ip (y^{(i)} - A^{(s)}x^{(i)}) \sign(x^{(i)})^T\\
A^{s+1} &amp;= \Proj_B(A^s - \eta \wh g^s)
\end{align}
<p>where <span class="math inline">\(B\)</span> is the set of <span class="math inline">\((\de_0,2)\)</span>-close matrices to <span class="math inline">\(A^*\)</span>.</p>
<h4 id="instantiation-1">Instantiation</h4>
<p>The algorithms above require a matrix that is <span class="math inline">\(O^*\prc{\log n}\)</span>-close to the true matrix. Algorithm 3 gives such a matrix.</p>
<ul>
<li>Set <span class="math inline">\(L=\phi\)</span>.</li>
<li>While <span class="math inline">\(|L|&lt;m\)</span> choose samples <span class="math inline">\(u,v\)</span>. (We will need <span class="math inline">\(p_1\)</span> of these samples.)
<ul>
<li>Set <span class="math inline">\(\wh M_{u,v} - \EE_{i\in p_2}\)</span> y<sup>{(i)}y</sup>{(i)T}$. (Average over <span class="math inline">\(p_2\)</span> fresh samples.)</li>
<li>Compute the top two singular values <span class="math inline">\(\si_1,\si_2\)</span> and top singular vector <span class="math inline">\(z\)</span> of <span class="math inline">\(\wh M_{u,v}\)</span>.</li>
<li>If <span class="math inline">\(\si_1\ge \Om\pf{k}{m}, \si_2&lt;O^*\pf{k}{m\log m}\)</span>, and <span class="math inline">\(\pm z\)</span> is not within distance <span class="math inline">\(\rc{\log m}\)</span> of any vector in <span class="math inline">\(L\)</span>, add <span class="math inline">\(z\)</span> to <span class="math inline">\(L\)</span>.</li>
</ul></li>
<li>Let <span class="math inline">\(z\in L\)</span> be the columns of <span class="math inline">\(\wt A\)</span>. Let <span class="math inline">\(A=\Proj_B\wt A\)</span>.</li>
</ul>
<h2 id="approximate-gradient-descent">Approximate gradient descent</h2>
<p>A general condition for a “descent”-type algorithm to work is that we make a step correlated with the direction to the optimum each time.</p>
<p><strong>Definition</strong>: <span class="math inline">\(g\)</span> is <span class="math inline">\((\al,\be,\ep)\)</span>-correlated with <span class="math inline">\(z^*\)</span> if <span class="math display">\[\an{g,z-z^*} \ge \al\ve{z-z^*}^2 + \be \ve{g}^2 - \ep.\]</span></p>
<p>An algorithm which makes correlated steps has geometric convergence. For example, gradient descent on strongly convex, smooth functions gives geometric convergence.</p>
<p><strong>Proposition</strong>: If <span class="math inline">\(f\)</span> is <span class="math inline">\(2\al\)</span>-strongly convex and <span class="math inline">\(\rc{2\be}\)</span>-smooth, then <span class="math inline">\(\nb f(z)\)</span> is <span class="math inline">\((\al,\be,0)\)</span>-correlated with <span class="math inline">\(z^*\)</span>.</p>
<em>Proof</em>: Note that <span class="math inline">\(L\)</span>-smooth means <span class="math inline">\(f(x) - f(x^*) \ge \rc{2L} \ve{\nb f(x)}^2\)</span> (cf. gradient descent lemma). Then
\begin{align}
\an{g,x-x^*} &amp;\ge f(x)-f(x^*) + \al\ve{x-x^*}^2\\
&amp;\ge \be \ve{\nb f}^2 + \al\ve{x-x^*}^2.
\end{align}
<p><strong>Theorem</strong> (Theorem 6): If <span class="math inline">\(g\)</span> is <span class="math inline">\((\al,\be,\ep)\)</span>-correlated with <span class="math inline">\(z^*\)</span> and <span class="math inline">\(z'=z-\eta g\)</span> and <span class="math inline">\(0&lt;\eta\le 2\be\)</span>, then <span class="math display">\[\ve{z'-z^*}^2 \le (1-2\al \eta)\ve{z-z^*}^2 + 2\eta \ep.\]</span> If <span class="math inline">\(z_{t+1}=z_t - \eta g_t\)</span> and each <span class="math inline">\(g_t\)</span> is <span class="math inline">\((\al,\be,\ep)\)</span>-correlated with <span class="math inline">\(z^*\)</span>, <span class="math display">\[\ve{z_t-z^*}^2\le (1-\al \eta)^t \ve{z_0-z^*}^2.\]</span></p>
<p>(Note the usual gradient step size for <span class="math inline">\(\rc{2\be}\)</span>-smooth functions is <span class="math inline">\(2\be\)</span>.)</p>
<em>Proof</em>: Break up <span class="math inline">\(z'-z^* = (z-z^*) - \eta g\)</span> in order to use the correlation.
\begin{align}
\ve{z'-z^*}^2 &amp;=\ve{z-z^*} - 2\an{z-z^*, \eta g} + \eta^2\ve{g}^2\\
&amp;\le \ve{z-z^*}^2 - 2\eta (\al\ve{z-z^*}^2+\be \ve{g}^2-\ep) + \eta^2\ve{g}^2\\
&amp;= (1-2\eta \al)\ve{z-z^*}^2 +\ub{ \eta (\eta - 2\be) \ve{g}^2}{\le 0} + 2\eta \ep.
\end{align}
<p>In actuality, we’ll need a weaker form of this, “<span class="math inline">\((\al,\be,\ep_s)\)</span>-correlated with high probability” (Definition 38). There is an analogue of the theorem in this setting (Theorem 40).</p>
<h2 id="theorems">Theorems</h2>
<ul>
<li>Theorem 2, 9 give the convergence of the main alternating minimization algorithm for sparse coding, given good initialization. Theorem 14 is the simpler case where we have an infinite number of samples in each step (so we can actually minimize in <span class="math inline">\(A\)</span> at each step, rather than just take a step towards the minimum). We show Theorem 14.</li>
<li>Theorem 3, 13 give the convergence of a more sophisticated algorithm which achieves better additive error.</li>
<li>Theorem 4, 19 give an algorithm to initialize <span class="math inline">\(A\)</span> (using SVD).</li>
</ul>
<p>Say <span class="math inline">\(A\)</span> is <span class="math inline">\((\de,\ka)\)</span>-near to <span class="math inline">\(A^*\)</span> if</p>
<ul>
<li>there is a permutation <span class="math inline">\(\pi\in S_m\)</span> and <span class="math inline">\(\si:[m]\to \{\pm1\}\)</span> such that <span class="math inline">\(\ve{\si(i)A_{\pi(i)}-A_i^*}\le \de\)</span> for all <span class="math inline">\(i\)</span>.</li>
<li><span class="math inline">\(\ve{A-A^*}\le \ka \ve{A^*}\)</span>.</li>
</ul>
<p>In detail: Let <span class="math inline">\(\de = O^*\prc{\log n}\)</span>. (Here <span class="math inline">\(x=O^*(y)\)</span> means there exists <span class="math inline">\(c\)</span> such that the statement is true for <span class="math inline">\(x\le cy\)</span>.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a>)</p>
<ul>
<li>(Theorem 14, 9) Suppose <span class="math inline">\(A^0\)</span> is <span class="math inline">\((2\de,2)\)</span>-near to <span class="math inline">\(A^*\)</span> and <span class="math inline">\(\eta = \Te\pf{m}{k}\)</span>. If each update step uses an infinite (<span class="math inline">\(\wt \Om(mk)\)</span>) samples, there is <span class="math inline">\(0&lt;\tau&lt;\rc2\)</span> such that <span class="math display">\[\ve{A_i^s - A_i^*}^2\le \ve{A_i^0 - A_i^*}^2 + \begin{cases}O\pf{k^2}{n^2}&amp;\\
O\pf{k}n&amp;\\\end{cases},\]</span> respectively.<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a></li>
<li>(Theorem 13) We can improve the algorithm to get additive error <span class="math inline">\(n^{-\om(1)}\)</span>.</li>
<li>(Theorem 19) If the instantiation algorithm is given <span class="math inline">\(p_1=\wt\Om(m)\)</span> and <span class="math inline">\(p_2=\wt\Om(mk)\)</span> fresh samples, and
<ul>
<li><span class="math inline">\(A^*\)</span> is <span class="math inline">\(\mu=O^*\pf{\sqrt n}{k(\log n)^3}\)</span>-incoherent,</li>
<li><span class="math inline">\(m=O(n)\)</span></li>
<li><span class="math inline">\(\ve{A^*} \le O\pa{\sfc{m}{n}}\)</span>, then w.h.p. <span class="math inline">\(A\)</span> is <span class="math inline">\((O^*\prc{\log n},2)\)</span>-near to <span class="math inline">\(A^*\)</span>.</li>
</ul></li>
</ul>
<h2 id="proofs">Proofs</h2>
<p>We cheat in this proof; for the correct version see Appendix D. We calculate <span class="math inline">\(\E g\)</span>, the expected value of the gradient step, and pretend that is the actual gradient step (this is OK for infinite sample size). For the real version, we’d have to consider concentration around <span class="math inline">\(\E g\)</span>.</p>
<p>The strategy is to show that step is correlated with the actual minimum, and then use the theorem on approximate gradient descent. First, we need to show that the minimum <span class="math inline">\(x\)</span> will have the right support with high probability.</p>
<ul>
<li>(Lemma 10, 23) Suppose <span class="math inline">\(A\)</span> is <span class="math inline">\(\de\)</span>-close to <span class="math inline">\(A^*\)</span>. Then with high probability over <span class="math inline">\(y=A^*x^*\)</span> (randomness is over <span class="math inline">\(x\sim D\)</span>), <span class="math display">\[\sign(1_{\ge \fc C2} (A^Ty)) = \sign(x^*).\]</span> (Decoding gives the right sign. This uses crucially that <span class="math inline">\(A\)</span> is already close enough.) We rely on the assumption that when <span class="math inline">\(x_i\ne 0\)</span>, it is bounded away on 0. (It would be interesting to remove this condition.)
<ul>
<li><strong>Lemma</strong>: If <span class="math inline">\(\fc{\mu}{\sqrt{n}}\le \rc{2k}\)</span> and <span class="math inline">\(k=\Om^*(\log m)\)</span> and <span class="math inline">\(\de=O^*\prc{\sqrt{\log m}}\)</span>, then w.h.p over <span class="math inline">\(x^*\)</span>, <span class="math inline">\(S:=\Supp(x^*)=\set{i}{|\an{A_i, y}|&gt;\fc C2}\)</span>, and <span class="math inline">\(\sign(\an{A_i,y})=\sign(x_i^*)\)</span>. <em>Proof</em>: Write <span class="math display">\[\an{A_i, A^*x^*}=\ub{\an{A_i,A_i^*}x_i^*}{\ad\ge 1-\fc{\de^2}2} + \sum_{j\ne i} \ub{\an{A_i,A_j^*}}{(\cdot)^2 \le 2\mu^2 + \an{A_i-A_i^*,A_j^*}}x_j^*\]</span> where we use closeness of <span class="math inline">\(A_i,A_i^*\)</span> and incoherence of <span class="math inline">\(A^*\)</span>. The sum is <span class="math display">\[\le 2\mu^2 + \ve{A_{S\be \{i\}}^{*T}(A_i-A_i^*)}^2.\]</span> Bound <span class="math inline">\(\ve{A_{S\bs\{i\}}}^2\)</span> by bounding <span class="math display">\[\ve{A_{S\bs\{i\}}}^2 =\sqrt{\ve{A_{S\bs\{i\}}^TA_{S\bs\{i\}}}} \le \sqrt{1+k\fc{\mu}{2\sqrt n}}.\]</span></li>
</ul></li>
<li>(Lemma 11) Now derive an expression for the expected value of the next value of <span class="math inline">\(A\)</span>. Let <span class="math inline">\(A'=A-\eta g\)</span>. We write it in the form <span class="math inline">\(\al (A-A^*)+\pat{error terms}\)</span>.
<ul>
<li><strong>Lemma</strong>: Let <span class="math inline">\(A'=A-\eta g\)</span>. Then <span class="math display">\[\E g_i = p_iq_i (\ub{\an{A_i' , A_i^*}}{=:\la_i}A_i - A_i^*+O\pf{k}n).\]</span><a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> <em>Proof</em>: With high probability, <span class="math inline">\(g\)</span> has the right sign as <span class="math inline">\(x\)</span>, so in calculating the expected value, we can assume it always has the right sign—its support is always <span class="math inline">\(\Supp(x)\)</span>. Then we can restrict to the support of <span class="math inline">\(x\)</span>. We separate the term <span class="math inline">\(i\)</span> because that will have the main contribution (recall incoherence means <span class="math inline">\(AA^T \approx I\)</span>).
\begin{align}
\E g_i &amp;= \E[(y-Ax)\sgn(x_i)]\\
&amp;\approx \E[(y-A_SA_S^Ty) \sgn(x_i)]\\
&amp;=\E[(I-A_SA_S^T)A^* x \sgn(x_i)]\\
&amp;=\E[(I-A_iA_i^T)A^* x \sgn(x_i)] - \E[A_{S\bs\{i\}} A_{S\bs\{i\}}^T A^* x \sgn(x_i)]\\
&amp;= \ub{\E[x_i^*\sgn(x_i^*)|S]}{p_i} \ub{\Pj(i\in S)}{q_i} (I-A_iA_i^T) A_i^* - p_i A_{-i} \diag(\ub{\Pj(i,j\in S)}{q_{i,j}}) A_{-i} A_i^*.
\end{align}
The error is <span class="math inline">\(\ve{A_{-i}}^2\max\fc{q_{ij}}{q_i} = \fc mn O\pf km = O\pf kn\)</span>, as needed.</li>
</ul></li>
<li>(Lemma 15-18) Bound the error term to show that at each step of the algorithm, the step is correlated, and maintains nearness.
<ul>
<li>Lemma 11 gives the following. Let <span class="math inline">\(k=p_iq_i\)</span>. We show the bound indicated below. <span class="math display">\[\E g_i  = \ub{p_iq_i}{=:k}(A_i - A^*) + \ub{p_iq_i ((1-\la_i) A_i + \ep_i + \ga)}{\ved \le \fc k4 \ve{A_i - A_i^*} + \ze}\]</span> where <span class="math inline">\(\ze=O\pf{k^2}{mn}\)</span>. Then <span class="math display">\[\an{g_i,A_i-A_i^*} \ge k\ve{A_i - A_i^*}^2 - \fc k4 \ve{A_i - A_i^*}^2 - \ze\ve{A_i - A_i^*}.\]</span> Using the inequality <span class="math inline">\(ax^2-bx \ge cx^2 + \fc{b^2}{4(a-c)}\)</span> with <span class="math inline">\(a=\fc 34k\)</span>, <span class="math inline">\(b=\ze\)</span>, <span class="math inline">\(c=\fc k4\)</span>, we get this is <span class="math inline">\(\ge \rc 4 k\ve{A_i-A_i^*}^2 + \fc{\ze^2}{2k}\)</span>.</li>
<li>Thus <span class="math inline">\(\E g_i\)</span> is <span class="math inline">\((\Om\pf km, \Om\pf mk, O\pf{k^3}{mn^2})\)</span>-correlated with <span class="math inline">\(A^*\)</span>. The last term comes from <span class="math inline">\(\fc{\ep}{\al} = \fc{k^3/(mn^2)}{k/m}\)</span>.</li>
<li>Note we need to maintain nearness in order to apply the estimates that gave correlation. (Where do we use 2-nearness in Lemma 11?) We show nearness is preserved. The natural way to decompose <span class="math inline">\(A_i'\)</span> is as follows.
\begin{align}
\E(A_i' - A_i^*) &amp;= \E[(A_i' - A_i)+(A_i-A_i^*)]\\
&amp;=-\eta p_iq_i (\la_i^s A_i^s - A_i^*+\ep_i +\ga) + A_i - A_i^*\\
&amp;=\diag(1-\eta p_iq_i) (A_i-A_i^*) + (1-\la_i) \eta p_iq_i A_i - \eta p_iq_i(\ep_i+\ga).
\end{align}
Now bound the two error terms by bounding the spectral norm.</li>
</ul></li>
<li>Finish.
<ul>
<li>At each step the step is <span class="math inline">\((\Om\pf{k}m,\Om \pf mk, O\pf{k^3}{mn^2})\)</span>-correlated with <span class="math inline">\(A_i^*\)</span>. By the approximate gradient descent theorem, this gives the result.</li>
</ul></li>
</ul>
<h3 id="initialization">Initialization</h3>
<ul>
<li>(Lemma 20) Write <span class="math inline">\(M_{u,v}=\EE[\an{u,y}\an{v,y}yy^T]\)</span> as a main term plus error terms. (Why does this only give up to <span class="math inline">\(\rc{\log n}\)</span>?) <span class="math display">\[ M_{u,v} = \sum_{i\in U\cap V} q_ic_i\be_i\be_i' A_i^*,A_i^{*T}+E_1+E_2+E_3\]</span> where <span class="math inline">\(\be = A^{*T},\be'=A^{*T}v,q_i=\Pj(i\in S), c_i=\E[x_i^{*4}|i\nin S]\)</span>, <span class="math inline">\(U=\Supp(u),V=\Supp(v)\)</span>. The main term comes from the indices in ths support of both vectors. To calculate this, expand in terms of <span class="math inline">\(x_i^*\)</span>, and note that because they have mean 0, the terms that contribute are <span class="math inline">\(x_i^{*4},x_i^{*2}x_j^{*2}\)</span>. Bound the spectral norm of <span class="math inline">\(E_1+E_2+E_3\)</span> by <span class="math inline">\(O^*\pf{k}{m\log n}\)</span>.
<ul>
<li>(Lemma 36) If <span class="math inline">\(U\cap V=\{i\}\)</span>, then the top singular vector of <span class="math inline">\(M_{u,v}\)</span> is <span class="math inline">\(O^*\prc{\log n}\)</span>-close to <span class="math inline">\(A_i^*\)</span>. Proof: Use Wedin’s Theorem. (Let <span class="math inline">\(v_1(A)\)</span> be the top eigenvector of <span class="math inline">\(A\)</span>. If <span class="math inline">\(\de=|\la_1(A)-\la_2(A)|\)</span>, then <span class="math inline">\(\sin(\angle (v_1(A), v_1(A+E)))\le \fc{\ve{E}_2}{\de}\)</span>.)</li>
<li>(Lemma 37) If there is a gap between the largest and second largest singular values as in the algorithm, then w.h.p. <span class="math inline">\(u,v\)</span> share a unique dictionary element. (Proof uses Weyl’s Theorem, HJ90—check this out.)</li>
<li>Finally, <span class="math inline">\(\Pj(|U\cap V|=\{i\}) = \Om\pf{k^2}{m^2}\)</span>.</li>
</ul></li>
</ul>
<h2 id="birds-eye-view">Bird’s eye view</h2>
<ul>
<li>Main technique: Alternating minimization. Crystallize out a weaker form of gradient descent, what is <em>actually</em> needed.</li>
<li>Threshold to denoise and cancel out the vectors that are 0.</li>
<li>For initialization, try to find the columns as the largest eigenvector of some matrix (SVD). Which matrix? <span class="math inline">\(\E\an{y,u}\an{y,v}yy^T\)</span>, because intuitively the large singular values will correspond to indices shared by <span class="math inline">\(u,v\)</span>, and with good probability <span class="math inline">\(u,v\)</span> have only one index in common in their support.</li>
</ul>
<h2 id="questions">Questions</h2>
<ul>
<li>Presence of multiple local minima?</li>
</ul>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>What is the magnitude? I think this is responsible for the <span class="math inline">\(\ga\)</span>’s that appear, but which are swept under the rug.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Is it the minimum?<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Is this important, or just for simplicity of analysis?<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>This is to disambiguate from the other meaning of <span class="math inline">\(O\)</span>, which is “whenever <span class="math inline">\(x\le cy\)</span> for some <span class="math inline">\(c\)</span>, there are constants such that the result holds.”<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>Are we implicitly reordering the columns here?<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>Do we need concentration too?<a href="#fnref6">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class="st_facebook_large" displayText="Facebook"></span>
    <span class="st_twitter_large" displayText="Tweet"></span>
    <span class="st_googleplus_large" displayText="Google +"></span>
    <span class="st_reddit_large" displayText="Reddit"></span>
    <span class="st__large" displayText></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>


<!-- Footer -->
<div id="footer">
  <div class="container">
    Built with
    <a href="http://jaspervdj.be/hakyll">Hakyll</a> 
    using 
    <a href="http://www.getbootstrap.com">Bootstrap</a>, 
    <a href="http://www.disqus.com">Disqus</a>,
    <a href="http://ignorethecode.net/blog/2010/04/20/footnotes/">Footnotes.js</a>,
    <a href="http://highlightjs.org/">Highlight.js</a>, 
    <a href="http://www.mathjax.org">MathJax</a>, 
    and <a href="http://www.sharethis.com">ShareThis</a>.
  </div>
</div>
</body>

</html>

<!-- SCRIPTS -->
<!-- jQuery-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="../../../bootstrap/js/bootstrap.min.js"></script>

<!-- Extension : Highlight.js @ https://highlightjs.org/ -->
<!-- Syntax highlighting tomorrow-night-bright, agate-->
<link rel="stylesheet" href="../../../highlight/css/tomorrow-night-bright.css">
<script src="../../../highlight/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- Extension : MathJax @ https://docs.mathjax.org/en/v2.5-latest/tex.html -->
<!-- MathJax/config/local/local.js contains macros. Need to provide entire URL-->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,http://holdenlee.github.io/notebook/MathJax/config/local/local"></script>

<!-- Extension : Footnotes @ http://ignorethecode.net/blog/2010/04/20/footnotes/ -->
<script src="../../../footnotes/js/footnotes.js"></script>

<!-- Extension : Disqus @ http://disqus.com -->
<!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->

<script src="../../../disqus/js/disqus.js"></script>



<!-- Extension : Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73261814-1', 'auto');
  ga('send', 'pageview');

</script>

