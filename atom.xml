<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2016-04-13T00:00:00Z</updated>
    <entry>
    <title>Perfect LCCs</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/coding/perfect_lcc.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/coding/perfect_lcc.html</id>
    <published>2016-04-13T00:00:00Z</published>
    <updated>2016-04-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Perfect LCCs</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-13 
          , Modified: 2016-04-13 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#proof-1">Proof 1</a></li>
 <li><a href="#proof-2">Proof 2</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>There are many proofs showing we must have <span class="math inline">\(n=2^k\)</span> for perfect 2 LCCs. Can we extend these methods to give</p>
<ul>
<li>exponential lower bounds for LCCs?</li>
<li>lower bounds for perfect <span class="math inline">\(q\)</span>-LCCs, <span class="math inline">\(q&gt;2\)</span>?</li>
</ul>
<p>A perfect <span class="math inline">\(q\)</span>-LCC of input length <span class="math inline">\(k\)</span> in <span class="math inline">\(n\)</span> dimensions is a set of <span class="math inline">\(2^k\)</span> points in <span class="math inline">\(\{\pm 1\}^n\)</span>, together with <span class="math inline">\(n\)</span> unions of perfect <span class="math inline">\(q\)</span>-matchings <span class="math inline">\(M_i\)</span> (or just matchings) and a sign <span class="math inline">\(s_m\)</span> for each <span class="math inline">\(m\in M_i\)</span>, such that on codewords, the decoding process defined by taking any <span class="math inline">\(m\in M_i\)</span> and taking <span class="math inline">\(s_m \prod_{j\in m}x_i\)</span> recovers <span class="math inline">\(x_i\)</span> with probability 1. In other words, <span class="math display">\[\EE_{m\in M_i} s_m\prod_{j\in m} x_j = 1.\]</span></p>
<h2 id="proof-1">Proof 1</h2>
<p>For <span class="math inline">\(q=2\)</span>, these are quadratic forms <span class="math inline">\(Q_i\)</span>, with associated matrices <span class="math inline">\(A_i\)</span>. The fact that they are matchings means that <span class="math inline">\(A_i = \rc nS_i\)</span> where <span class="math inline">\(S_i\)</span> is (doubly) stochastic. (In fact, we can deal more generally with perfectly smooth LCCs that recover perfectly on codewords.) The codewords are those with <span class="math display">\[Q_i(x)=x \iff \an{x,A_ix}=x_i.\]</span> Now <span class="math inline">\(S\)</span>, being stochastic, satisfies <span class="math inline">\(\ve{S}_{\iy\to \iy} \le 1\)</span>. Now <span class="math inline">\(\ve{x}_{\iy}\le 1\)</span> so <span class="math inline">\(\ve{A_i x}_{\iy}\le \rc{n}\)</span>. We have <span class="math display">\[n=\ve{x}_1 =\sum_i |\an{x,A_ix}|\le \sum_i \ve{A_ix}_{\iy}\ve{x}_1=n,\]</span> so equality holds and <span class="math inline">\(A_ix = x_ix\)</span>.</p>
<p>This means the <span class="math inline">\(x\in C\)</span> are simultaneous eigenvalues for the <span class="math inline">\(A_i\)</span>. The sequences of eigenvectors are different, so <span class="math inline">\(|C|\le n\)</span>, i.e., <span class="math inline">\(2^k\le n\)</span>. Equality is acheived for the Hadamard code.</p>
<p>To extend this: some notion of “approximate eigenvector,” “well-conditioned linear dependency”?</p>
<h2 id="proof-2">Proof 2</h2>
<p>For LDCs whose matching correspond to a group action, every <span class="math inline">\(i\)</span> corresponds to a matching <span class="math inline">\(M_i=\{(y, y+x_i)\}\)</span>. Now for any <span class="math inline">\(\ep\in \{-1,1\}^k\)</span>, there must exist a set <span class="math inline">\(S\)</span>, the support of the codeword <span class="math inline">\(x=C(\ep)\)</span>, for which <span class="math inline">\(M_i\)</span> only has edges in <span class="math inline">\(S\)</span> if <span class="math inline">\(\ep_i=1\)</span>, and <span class="math inline">\(M_i\)</span> only has edges between <span class="math inline">\(S,S^c\)</span> if <span class="math inline">\(\ep_i=-1\)</span>. We must have <span class="math inline">\(|S|=\fc n2\)</span>.</p>
<p>Now if <span class="math inline">\(x_k=\sum_{i=1}^{k-1} a_ix_i\)</span>, consider the set <span class="math inline">\(S\)</span> where <span class="math inline">\(\ep_i=1\)</span> iff <span class="math inline">\(a_i=1\)</span>, but <span class="math inline">\(\ep_{k}=-1\)</span>. Then <span class="math inline">\(S = S+x_i\)</span> for all <span class="math inline">\(i\)</span>, and <span class="math inline">\(S\ne S+x_k\)</span>, contradiction.</p>
<p>This is very much related to the first proof. There, one can use a linear dependency argument to show there can’t be more than <span class="math inline">\(n\)</span> eigenvectors with distinct sequences of eigenvalues. (Make this relationship more explicit?)</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Notes index</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/notes_index.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/notes_index.html</id>
    <published>2016-04-08T00:00:00Z</published>
    <updated>2016-04-08T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Notes index</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-08 
          , Modified: 2016-04-08 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Martingales</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/probability/martingales.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/probability/martingales.html</id>
    <published>2016-04-08T00:00:00Z</published>
    <updated>2016-04-08T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Martingales</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-08 
          , Modified: 2016-04-08 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#conditional-expectation">Conditional expectation</a></li>
 <li><a href="#martingales-almost-sure-convergence">Martingales, Almost Sure Convergence</a></li>
 <li><a href="#examples">Examples</a></li>
 <li><a href="#doobs-inequality-lp-convergence">Doob’s inequality, <span class="math inline"><em>L</em><sup><em>p</em></sup></span> convergence</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>Durrett Ch. 5</p>
<h2 id="conditional-expectation">Conditional expectation</h2>
<h2 id="martingales-almost-sure-convergence">Martingales, Almost Sure Convergence</h2>
<h2 id="examples">Examples</h2>
<h2 id="doobs-inequality-lp-convergence">Doob’s inequality, <span class="math inline">\(L^p\)</span> convergence</h2>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Matrix factorizations</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/matrix-factorization.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/matrix-factorization.html</id>
    <published>2016-04-04T00:00:00Z</published>
    <updated>2016-04-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Matrix factorizations</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-04 
          , Modified: 2016-04-04 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#flavors">Flavors</a></li>
 <li><a href="#previous-work">Previous work</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="flavors">Flavors</h2>
<ul>
<li><strong>Sparse coding/Dictionary learning</strong>: Write <span class="math inline">\(M=AX\)</span> where the columns of <span class="math inline">\(X\)</span> are sparse. Assume some properties on <span class="math inline">\(A\)</span> (e.g., incoherence) and on the distribution of <span class="math inline">\(X\)</span>.</li>
<li><strong>Nonnegative matrix factorization</strong>: Sparse coding with the caveat that <span class="math inline">\(A,X\)</span> are positive. Write <span class="math inline">\(M=AW\)</span> where <span class="math inline">\(A,W\)</span> have nonnegative entries. <span class="math inline">\(A\)</span> is fixed (assume some properties on <span class="math inline">\(A\)</span>) and <span class="math inline">\(W\)</span> consists of random samples. I.e., we are given many samples <span class="math inline">\(y=Ax\)</span>.</li>
<li><strong>Topic modeling</strong>: Given <span class="math inline">\(x\)</span>, we see samples from the probability vector <span class="math inline">\(Ax\)</span> (rather than <span class="math inline">\(Ax\)</span>). Learn <span class="math inline">\(A\)</span> and infer the <span class="math inline">\(x\)</span>’s. ??</li>
</ul>
<p>NMF is the least understood.</p>
<h2 id="previous-work">Previous work</h2>
<ul>
<li>NMF
<ul>
<li>Hardness: If there exists a <span class="math inline">\(n^{o(r)}\)</span> time algorithm, then there exists a <span class="math inline">\(2^{o(n)}\)</span> algorithm for 3SAT.</li>
<li>[AGKM12, M14] Given that the rank equals the nonnegative rank, there is a <span class="math inline">\(n^{O(r)}\)</span> time algorithm for NMF. (Relies on solving polynomial inequalities.)</li>
<li>[AGKM12] Under separability, NMF can be solved in polynomial time in <span class="math inline">\(n\)</span>. (Use the geometry.)</li>
<li>Inference: <a href="arora-topic-models.html">A16</a> Given <span class="math inline">\(A\)</span>, recover <span class="math inline">\(x\)</span> from <span class="math inline">\(Ax\)</span> if <span class="math inline">\(x\)</span> is sparse and <span class="math inline">\(A\)</span> has small <span class="math inline">\(\ell_\iy\to \ell_1\)</span> condition number. (“Parameter learning” is learning <span class="math inline">\(A\)</span>, “inference” is finding <span class="math inline">\(x\)</span>.)</li>
</ul></li>
<li>Topic modeling
<ul>
<li>… see p. 23 in new_thread.pdf.</li>
<li>[AGHMMSWZ12] A Practical Algorithm for Topic Modeling with Provable Guarantees:</li>
</ul></li>
<li>Sparse coding/ Dictionary learning
<ul>
<li><a href="SW08.html">SW08</a>: Algorithm for full-rank matrices (no noise).
<ul>
<li>Apply a <span class="math inline">\(\ved_0\to \ved_1\)</span> relaxation.</li>
</ul></li>
<li><a href="AGM14.html">AGM14</a>: Fixed-parameter tractable algorithm for overcomplete (incoherent) dictionaries, up to sparsity <span class="math inline">\(n^{\rc 2-\ep}\)</span>.
<ul>
<li>Use the fact that high dot product between two vectors indicates intersection of supports to reduce to a <strong>overlapping community detection</strong> problem.</li>
<li>Run <strong>SVD</strong> within communities.</li>
</ul></li>
<li><a href="AGMM15.html">AGMM15</a>: Efficient polytime algorithm for overcomplete (incoherent) dictionaries, that works up to sparsity <span class="math inline">\(\fc{\sqrt{n}}{\mu\poly\log n}\)</span>.
<ul>
<li>Initialize with <strong>SVD</strong>, noting that with good probability the <strong>intersection of supports</strong> of two vectors <span class="math inline">\(x,x'\)</span> will have one index in common.</li>
<li>Apply <strong>alternating minimization</strong> and analyze using <strong>approximate gradient descent</strong> (correlation with the right direction).</li>
<li>To get a sparse decoding, apply <strong>thresholding</strong> in the decoding step of AM.</li>
</ul></li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[A16] Provable algorithms for inference in topic models</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/arora-topic-models.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/arora-topic-models.html</id>
    <published>2016-04-04T00:00:00Z</published>
    <updated>2016-04-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[A16] Provable algorithms for inference in topic models</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-04 
          , Modified: 2016-04-04 
	</p>
      
       <p>Tags: <a href="/tags/topic%20models.html">topic models</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#model">Model</a><ul>
 <li><a href="#definitions">Definitions</a></li>
 <li><a href="#model-1">Model</a></li>
 </ul></li>
 <li><a href="#algorithm">Algorithm</a></li>
 <li><a href="#analysis">Analysis</a><ul>
 <li><a href="#thresholded-linear-inverse-algorithm">Thresholded linear inverse algorithm</a></li>
 <li><a href="#mle-estimate">MLE estimate</a></li>
 <li><a href="#sample-complexity-lower-bounds">Sample complexity lower bounds</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="model">Model</h2>
<h3 id="definitions">Definitions</h3>
<p>Define</p>
<ul>
<li>the <span class="math inline">\(\ell_1\)</span> condition number <span class="math display">\[\ka(A) = \min \set{\ka}{\forall x\in \R^k, \ve{Ax}_1\ge \rc{\ka}\ve{x}_1}\]</span></li>
<li>the <span class="math inline">\(\ell_\iy\to \ell_1\)</span> condition number <span class="math display">\[\la(A) = \min \set{\la}{\forall x\in \R^k, \ve{Ax}_1\ge \rc{\la}\ve{x}_\iy}\]</span></li>
<li>the <span class="math inline">\(\de\)</span>-biased <span class="math inline">\(\ell_\iy\to \ell_1\)</span> condition number
\begin{align}
\la_\de&amp;=\min_{|BA-I_k|\le \de, B\in \R^{k\times D}} |B|_{\iy}\\
&amp;=\max_{\ve{Ax}_1\le 1} \ve{x}_{\iy} - \de\ve{x}_1.
\end{align}
(The equality follows from a duality calculation.)</li>
<li>the restricted <span class="math inline">\(\ell_1\to \ell_1\)</span> condition number is <span class="math display">\[\ol{\ka}_r(A) = \min\set{\ol{\ka}}{\forall \ve{v}_0, \ve{Av}_1\ge \rc{\ol \ka}\ve{v}}.\]</span></li>
</ul>
<p>Note <span class="math display">\[\la_\de\le \la_0=\la \le \ka.\]</span> (To see this, take <span class="math inline">\(x=By\)</span>.)</p>
<h3 id="model-1">Model</h3>
<ul>
<li><span class="math inline">\(A\)</span> is a fixed <span class="math inline">\(n\times k\)</span> matrix with <span class="math inline">\(\de\)</span>-biased condition number <span class="math inline">\(\la_\de(A)\)</span>. (We want this to be small.)</li>
<li><span class="math inline">\(x\in \De_k\)</span> is <span class="math inline">\(r\)</span>-sparse.</li>
<li><span class="math inline">\(y\sim Ax\)</span> (<span class="math inline">\(Ax\)</span> is treated as a probability vector).</li>
</ul>
<h2 id="algorithm">Algorithm</h2>
<p>Given <span class="math inline">\(y\sim Ax\)</span> and <span class="math inline">\(A\)</span>,</p>
<ul>
<li>Thresholded linear inverse
<ul>
<li>Let <span class="math inline">\(B=\amin_{|BA-I_k|\le \de, B\in \R^{k\times D}} |B|_{\iy}\)</span> be a <span class="math inline">\(\de\)</span>-biased minimum variance inverse.</li>
<li>Compute <span class="math inline">\(\wh x = \rc n By\)</span>.</li>
<li>Let <span class="math display">\[x_i = \wh x_i (\wh x_i \ge \ub{2\la_\de(A) \sfc{\ln k}{n} + \de}{\tau}).\]</span></li>
</ul></li>
<li>TLI finds the support of <span class="math inline">\(x^*\)</span> with high probability. Now find the MLE <span class="math inline">\(x^*\)</span> given the support. (This is a convex problem.)</li>
</ul>
<h2 id="analysis">Analysis</h2>
<h3 id="thresholded-linear-inverse-algorithm">Thresholded linear inverse algorithm</h3>
<p>We have <span class="math display">\[ \E \wh x_i  =x_j^* + \sumo jk ((BA)_{ij} - \de_{i,j}) x_j^*.\]</span> <em>This is why it’s natural to consider the <span class="math inline">\(\de\)</span>-biased inverse</em>: we don’t need <span class="math inline">\(B=A^+\)</span> exactly, we can relax this to each <span class="math inline">\((BA)_{ij} - \de_{i,j}\)</span> being small. Now use Bernstein’s inequality to get concentration on the order of <span class="math inline">\(\tau\)</span>. Finally use union bound. <!--check this--></p>
<h3 id="mle-estimate">MLE estimate</h3>
<p>If</p>
<ul>
<li><span class="math inline">\(x^*\in \De_k\)</span> is <span class="math inline">\(r\)</span>-sparse</li>
<li><span class="math inline">\(x_i^*\ge \fc{\tau}r\)</span> for any <span class="math inline">\(i\in R\)</span>,</li>
<li><span class="math inline">\(A\)</span> has <span class="math inline">\(\le \ol{\ka}\)</span> restricted <span class="math inline">\(\ell_1\to \ell_1\)</span> condition number (a bound on <span class="math inline">\(\ka\)</span> is sufficient as <span class="math inline">\(\ol\ka\le \ka\)</span>)</li>
<li><span class="math inline">\(n\ge c\ol{\ka}^2 r^2 \fc{\log k}{\tau^2}\)</span></li>
</ul>
then with high probability
\begin{align}
\ve{Ax_{MLE}-Ax^*}_1 &amp;\le \wt O\pa{\sfc rn}\\
\ve{x_{MLE}-x^*}_1 &amp;\le \wt O\pa{\ol \ka \sfc rn}
\end{align}
<p>The proof is like the <a href="../../math/statistics/fisher-info.html">proof of asymptotic normality of MLE</a>, but with matrix concentration to get a finite sample bound.</p>
<h3 id="sample-complexity-lower-bounds">Sample complexity lower bounds</h3>
<p>These exist!</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 4-9-16</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-04-09.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-04-09.html</id>
    <published>2016-04-04T00:00:00Z</published>
    <updated>2016-04-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 4-9-16</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-04 
          , Modified: 2016-04-04 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Maximum entropy distributions</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/statistics/max-entropy.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/statistics/max-entropy.html</id>
    <published>2016-04-04T00:00:00Z</published>
    <updated>2016-04-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Maximum entropy distributions</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-04 
          , Modified: 2016-04-04 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#references">References</a><ul>
 <li><a href="#constraint-distribution-entropy-base-e">Constraint | Distribution | Entropy (base e)</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="references">References</h2>
<ul>
<li><a href="http://www.math.uconn.edu/~kconrad/blurbs/analysis/entropypost.pdf">Keith Conrad’s notes</a></li>
<li><a href="http://www.ski.org/Rehab/Coughlan_lab/General/TutorialsandReference/MaxEnt.pdf">ML vs. ME</a></li>
<li><a href="https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution">Wikipedia</a></li>
</ul>
<p>Many naturally occurring distributions are the maximal entropy distribution under some constraint. Here is a table.</p>
<h3 id="constraint-distribution-entropy-base-e">Constraint | Distribution | Entropy (base e)</h3>
<p>Mean <span class="math inline">\(\mu\)</span>, variance <span class="math inline">\(\si^2\)</span> | Normal<span class="math inline">\((\mu,\si^2)\)</span> | <span class="math inline">\(\rc2 (1+\ln (2\pi \si^2))\)</span> Support <span class="math inline">\([0,\iy)\)</span>, mean <span class="math inline">\(la\)</span> | Exponential <span class="math inline">\(\rc{\la} e^{-\fc x\la}\)</span> | <span class="math inline">\(1+\ln \la\)</span> <span class="math inline">\(\E X= \mu\)</span>, <span class="math inline">\(\E |X - \E X| = \la\)</span> | Laplace<span class="math inline">\((\mu,2\la^2)\)</span> | <span class="math inline">\(1+\ln(2\la)\)</span> Energy <span class="math inline">\(\sum p_iE_i = \ol E\)</span> | Boltzmann <span class="math inline">\(\Pj(i) = \fc{e^{-\be E_i}}{Z}\)</span>, <span class="math inline">\(Z=\sum_i e^{-\be E_i}\)</span> | <span class="math inline">\(\E(-\be E) - \ln Z\)</span></p>
<p>Note that in the continuous case, the Boltzmann formula encompasses everything! For example, for the normal distribution, energy is <span class="math inline">\((x-\mu)^2\)</span>.</p>
<p>A systematic way to show this is Lagrange multipliers.</p>
<p>A more elegant way is to do the following:</p>
<ul>
<li>Note that by nonnegativity of KL divergence, <span class="math display">\[\int_{\Om} p \ln p \ge \int_{\Om} p \ln q.\]</span></li>
<li>For <span class="math inline">\(F(p)\)</span> the property of the distribution you’re interested in and <span class="math inline">\(q\)</span> equal to the maximizing distribution, find that <span class="math display">\[-\int_{\Om} p \ln q = g(F(p))\]</span> for some function <span class="math inline">\(g\)</span>.</li>
<li>Conclude that if <span class="math inline">\(F(p)=F(q)\)</span> then <span class="math inline">\(-\int_{\Om} p\ln q = -\int_{\Om} q\ln q\)</span>. Hence <span class="math display">\[H(p) = -\int p\ln p \ge -\int p\ln q = -\int q\ln q = H(q).\]</span></li>
</ul>
<p>Here is an example. For <span class="math inline">\(q=\rc{\sqrt{2\pi}}e^{-\fc{x^2}{2\si^2}}\)</span>, <span class="math display">\[-\int_{\R} p \ln q \dx = \rc2 \ln(2\pi \si^2) + \int_{\R}p\cdot \rc2 \pf{x}{\si^2}^2\dx = \rc2 \ln(2\pi \si^2) + \rc{2}\fc{\Var(q)}{\si^2}.\]</span></p>
<p>Question: why is the maximum entropy distribution the best choice in statistical problems?</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Fisher information</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/statistics/fisher-info.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/statistics/fisher-info.html</id>
    <published>2016-04-04T00:00:00Z</published>
    <updated>2016-04-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Fisher information</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-04 
          , Modified: 2016-04-04 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#definitions">Definitions</a></li>
 <li><a href="#intuition">Intuition</a></li>
 <li><a href="#theorems">Theorems</a><ul>
 <li><a href="#cramer-rao">Cramer-Rao</a></li>
 <li><a href="#asymptotic-normality">Asymptotic normality</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="definitions">Definitions</h2>
Define the <strong>score</strong> and <strong>Fisher information</strong> by
\begin{align}
s(X;\te)&amp;=\pd{\ln f}{\te}\\
I(\te)&amp;=\Var_\te(s(X;\te))
\end{align}
<p>(This generalizes immediately to the multivariate case; for simplicity we consider the univariate case.)</p>
<p>The expecation of score is 0: <span class="math display">\[\E s=\int_{-\iy}^{\iy} s(X;\te) f\,dx=\int_{-\iy}^{\iy} \fc{\ln f}{f}f\dx=(\int_{-\iy}^{\iy}f\,dx)_{\te}=0.\]</span> Thus <span class="math display">\[I(\te) = \Var(s(X;\te)) = \E [(\ln f)_\te^2] = -\E((\ln f)_{\te\te}).\]</span></p>
<h2 id="intuition">Intuition</h2>
<p>Suppose a data point <span class="math inline">\(x\)</span> is observed. What is the posterior distribution on the parameter <span class="math inline">\(\te\)</span>? Consider the log of this probability, the log-likelihood. The Fisher information measures how curved the log-likelihood is at <span class="math inline">\(x\)</span>.</p>
<p>Consider the Fisher information at the MLE. If <span class="math inline">\(I(\te)\)</span> is large, then we are reasonably certain of the value of <span class="math inline">\(\te\)</span> (changing <span class="math inline">\(\te\)</span> by a bit decreases the log-probability of observing <span class="math inline">\(x\)</span> a lot).<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> If <span class="math inline">\(I(\te)\)</span> is small, then we are not certain.</p>
<h2 id="theorems">Theorems</h2>
<h3 id="cramer-rao">Cramer-Rao</h3>
<p><a href="https://en.wikipedia.org/wiki/Cramer-Rao_inequality">Wikipedia</a></p>
<p>This intuition is formalized by Cramer-Rao: the variance of any unbiased estimator for <span class="math inline">\(\te\)</span> is lower-bounded by the inverse of the Fisher information.</p>
<p><strong>Theorem (Cramer-Rao)</strong>: Suppose <span class="math inline">\(T(X)\)</span> is an unbiased estimator of <span class="math inline">\(\te\)</span>. Then <span class="math inline">\(\Var(T) \ge \rc{I(\te)}\)</span>. More generally, <span class="math inline">\(\Var(\psi(T)) \ge \fc{\psi'(\te)}{I(\te)}\)</span>.</p>
<p>In higher dimensions, <span class="math display">\[
Var(T) \succeq \pd{\psi}{\te} I^{-1}\pd{\psi}{\te}.
\]</span></p>
<p><em>Proof</em>: Suppose <span class="math inline">\(T=t(x)\)</span>. By Cauchy-Schwarz, <span class="math display">\[
\Var(T) \ge \fc{\text{Covar}(T,s_\te)^2}{\Var(s_\te)} = \fc{\int t(x)f (\ln f_\te)}{I(\te)} = \fc{\int g(x) f_{\te}(x)}{I(\te)}
=\fc{(\E g)_\te}{I(\te)}= \rc{I(\te)}.
\]</span> <!--randomized? --></p>
<h3 id="asymptotic-normality">Asymptotic normality</h3>
<!-- 3/16 p. 197-->
<p>Define the <strong>standard error</strong> by <span class="math inline">\(\se=\sqrt{\Var_\te(\wh{\te_n})}\)</span>.</p>
<p><strong>Theorem (Asymptotic normality of MLE)</strong>: <span class="math inline">\(\se\sim \sfc1{nI(\te)}\)</span> and <span class="math inline">\(\fc{\wh{\te_n}-\te}{\se}\to N(0,1)\)</span>.</p>
<p>(With a little more work, we can replace se by <span class="math inline">\(\wh{se}\)</span> (estimated standard error).)</p>
<p><em>Proof</em>: Denoting the log-likelihood by <span class="math inline">\(\ell(\te):= \ln \Pj(x^n|\te) = \sum_{i=1}^n \ln f(x_i;\te)\)</span>, linearize to find that <span class="math display">\[
\ell'(\wh \te)-\ell'(\te)\approx (\wh \te-\te)(\ell''(\te))\implies -\fc{\ell'}{\ell''}(\te)\approx \wh{\te}-\te.
\]</span> Now <span class="math display">\[
\sqrt n(\wh{\te_n}-\te)=\fc{\rc{\sqrt n}\ell'(\te)}{-\rc n\ell''(\te)}\to \fc{N(0,I(\te))}{I(\te)}\to N(0,1),
\]</span> the top in distribution, the bottom in probability. (The top uses CLT on <span class="math inline">\(\sum (\ln f)_\te\)</span>; the bottom uses LoLN on <span class="math inline">\(\sum -(\ln f)_{\te\te}\)</span>.)</p>
<!--
Define some quantities first.
\begin{df}
\begin{enumerate}
\item
\textbf{KL distance}
\[
D(f,g)=\int f(x)\ln \pf{f}{g}\,dx.
\]
Why do we care about this? Maximizing $\ell_n(\te)$ is equivalent to maximizing 
\[
M_n(\te)=\rc n\sum_i\ln \fc{f(X_i;\te)}{f(X_i;\te_*)}
\]
which has the nice property that the maximum is 0. (Without the $\rc n$ it would blow up.) By LLN the expected value of this is exactly $-D(\te_*,\te)$.
\item 
\textbf{score function} $s(X;\te)=\pd{\ln f}{\te}$.

Important property: $\E s=\int_{-\iy}^{\iy} s(X;\te) f\,dx=(\int_{-\iy}^{\iy}f\,dx)_{\te}=0$. 
\item
\textbf{Fisher information} $I(\te)=\Var_\te(s(X;\te))$, $I_n(\te)=nI(\te)$.
I.e., $I(\te)=-\E((\ln f)_{\te\te})$.
\end{enumerate}
\end{df}

\begin{enumerate}
\item
\begin{thm}[Convergence of MLE]
Suppose 
\begin{enumerate}
\item
$\sup_{\te\in \Te}|M_n(\te)-M(\te)|\xra{P}0$,
\item
for all $\ep>0$, $\sup_{|\te-\te_*|\ge\ep} M(\te)<M(\te_*)$.
\end{enumerate}
Then the MLE $\wh{\te_n}\xra P\te_*$.
\end{thm}
\begin{proof}
First show that $M(\te_*)-M(\wh{\te_n})\xra P0$. Then use continuity of $M$.
\end{proof}
\item
\begin{thm}[Asymptotic normality of MLE]
\begin{enumerate}
\item
$\se\sim \sfc1{nI(\te)}$ and $\fc{\wh{\te_n}-\te}{\se}\to N(0,1)$.
\item \fixme{$\wh{\se}=\sfc{1}{nI(\wh{\te_n})}$: why are we redefining $\wh{\se}$? We defined it a different way before. Do these definitions coincide?}
$\fc{\wh{\te_n}-\te}{\wh{\se}}\to N(0,1)$.
\end{enumerate}
\end{thm}
\begin{proof}
\begin{enumerate}
\item
Linearize to find that
\[
\ell'(\wh \te)-\ell'(\te)\approx (\wh \te-\te)(\ell''(\te))\implies -\fc{\ell'}{\ell''}(\te)\approx \wh{\te}-\te.
\]
Now
\[
\sqrt n(\wh{\te_n}-\te)=\fc{\rc{\sqrt n}\ell'(\te)}{-\rc n\ell''(\te)}\to \fc{N(0,I(\te))}{I(\te)}\to N(0,1),
\]
the top in distribution, the bottom in probability. (The top uses CLT on $\sum (\ln f)_\te$; the bottom uses LoLN on $\sum -(\ln f)_{\te\te}$.)
\item
Show that $\sfc{I(\wh{\te_n})}{I(\te)}\xra P 1$.
\end{enumerate}
\end{proof}
\item Think of this as a chain rule.
\begin{thm}
If $\tau=g(\te)$ and $g'(\te)\ne 0$, then $\fc{\wh{\tau_n}-\tau}{\wh{\se}(\wh{\tau})}\to N(0,1)$ where $\wh{\tau_n}=g(\wh{\te_n}),\wh{\se}(\wh{\tau_n})=|g'(\wh{\tau})|\wh{\se}(\wh{\tau_n})$.
\end{thm}
Proof: just expand $g$ using $g'$.
\item (Equivariance) If $\tau=g(\te)$ is 1-to-1, then $\wh{\tau_n}=g(\wh{\te_n})$. Follow definitions!
\end{enumerate}
-->
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>For sake of discussion suppose the log-likelihood function is convex in <span class="math inline">\(\te\)</span>, so there aren’t other local minima.<a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Haskell</title>
    <link href="http://holdenlee.github.io/notebook/posts/cs/functional_programming/haskell.html" />
    <id>http://holdenlee.github.io/notebook/posts/cs/functional_programming/haskell.html</id>
    <published>2016-04-03T00:00:00Z</published>
    <updated>2016-04-03T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Haskell</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-03 
          , Modified: 2016-04-03 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#parsec">Parsec</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="parsec">Parsec</h2>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">type</span> <span class="dt">LeafTree</span> <span class="fu">=</span> <span class="dt">Free</span> []

leaf <span class="fu">=</span> <span class="dt">Pure</span>
node <span class="fu">=</span> <span class="dt">Free</span>

genWord <span class="fu">=</span> many1 (noneOf <span class="st">&quot; (),\n&quot;</span>)

<span class="ot">parseExpr ::</span> <span class="dt">Parser</span> a <span class="ot">-&gt;</span> <span class="dt">Parser</span> (<span class="dt">LeafTree</span> a)
parseExpr p <span class="fu">=</span> (spaces <span class="fu">&gt;&gt;</span> (p <span class="fu">&gt;&gt;=</span> (return <span class="fu">.</span> leaf))) <span class="fu">&lt;|&gt;</span>
  <span class="kw">do</span> {
    char <span class="ch">'('</span>;
    trees <span class="ot">&lt;-</span> sepEndBy (parseExpr p) spaces;
    char <span class="ch">')'</span>;
    return <span class="fu">$</span> node trees;
  }

<span class="ot">parseLISP' ::</span> <span class="dt">Parser</span> (<span class="dt">LeafTree</span> <span class="dt">String</span>)
parseLISP' <span class="fu">=</span> parseExpr genWord

<span class="ot">parseLISP ::</span> <span class="dt">String</span> <span class="ot">-&gt;</span> <span class="dt">Maybe</span> (<span class="dt">LeafTree</span> <span class="dt">String</span>)
parseLISP <span class="fu">=</span> fromRight <span class="fu">.</span> parse parseLISP' <span class="st">&quot;&quot;</span></code></pre></div>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[AGM14] New algorithms for learning incoherent and overcomplete dictionaries</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/AGM14.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/AGM14.html</id>
    <published>2016-04-02T00:00:00Z</published>
    <updated>2016-04-02T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[AGM14] New algorithms for learning incoherent and overcomplete dictionaries</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-02 
          , Modified: 2016-04-02 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#model">Model</a></li>
 <li><a href="#theorem">Theorem</a></li>
 <li><a href="#algorithm">Algorithm</a></li>
 <li><a href="#analysis">Analysis</a><ul>
 <li><a href="#graph-construction">Graph construction</a></li>
 <li><a href="#overlapping-communities">Overlapping communities</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <p>Back to <a href="matrix-factorization.html">Matrix factorization</a></p>
<h2 id="model">Model</h2>
<ul>
<li><span class="math inline">\(A\)</span> is a <span class="math inline">\(n\times m\)</span> matrix.</li>
<li><span class="math inline">\(\E X_i=0\)</span> and <span class="math inline">\(X_i\in [-C,1]\cup [1,C]\)</span> when <span class="math inline">\(X_i\ne 0\)</span>.</li>
<li><span class="math inline">\(X_i\)</span> is independent conditioned on <span class="math inline">\(X_i\ne 0\)</span> for <span class="math inline">\(i\in S\)</span>.</li>
<li>“Bounded <span class="math inline">\(l\)</span>th moments” for some <span class="math inline">\(l\)</span>: For <span class="math inline">\(|S|=l\)</span>, <span class="math inline">\(\Pj(\forall i\in S,X_i\ne 0)\le c^l \prod_{i\in S} \Pj(X_i\ne 0)\)</span>.</li>
</ul>
<h2 id="theorem">Theorem</h2>
<ul>
<li>The algorithm does the following: If the sparsity is <span class="math inline">\(k\le c\min\pa{m^{\fc 25}, \fc{\sqrt n}{\mu\log n}}\)</span> and <span class="math inline">\(X\)</span> has bounded 3-wise moments, taking <span class="math inline">\(p_1=\poly(m,\rc k, \log\prc{\ep})\)</span> samples and time <span class="math inline">\(\wt O(p_1^2n)\)</span>, w.h.p. the algorithm recovers <span class="math inline">\(A\)</span> up to <span class="math inline">\(\ep\)</span> column-wise error.</li>
<li>Same is true for <span class="math inline">\(m^{\fc 25}\)</span> replaced by <span class="math inline">\(m^{\fc{l-1}{2l-1}}\)</span> and <span class="math inline">\(c\)</span> replaced by <span class="math inline">\(c_l\)</span>.</li>
<li>The same is true when the samples are subject to noise: <span class="math inline">\(y=Ax+n\)</span> where <span class="math inline">\(n\)</span> is independent Gaussian, <span class="math inline">\(\si=o(\sqrt n)\)</span>. The sample complexity becomes <span class="math inline">\(\poly(m^l,\rc{k^l}, \log\prc{\ep})\poly\prc{\si^2}{\ep^2}\)</span>.</li>
</ul>
<h2 id="algorithm">Algorithm</h2>
<ul>
<li>Create a graph with an edge between <span class="math inline">\(i,j\)</span> if <span class="math inline">\(|\an{y^i, y^j}|&gt;\rc2\)</span>.</li>
<li>Run the following <strong>overlapping community detection</strong> algorithm.
<ul>
<li>Repeat <span class="math inline">\(\Om(m^2\log m)\)</span> times.
<ul>
<li>Take an edge <span class="math inline">\(uv\)</span>.</li>
<li>Count the number of vertices that are connected to both <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span>.</li>
<li>Let <span class="math inline">\(T=\fc{k/(2m)}{5} = \fc{k}{10m}\)</span>. If the number of common neighbors is <span class="math inline">\(\ge T\)</span>, add to <span class="math inline">\(S_{uv}\)</span>. (I.e., <span class="math inline">\(S_{uv} = \set{w}{|\Ga(u)\cap \Ga(v)\cap \Ga(w)|\ge T}\cup \{u,v\}\)</span>.</li>
</ul></li>
<li>Take the minimal sets (with respect to set inclusion). These will be (whp) be the collection <span class="math inline">\(\{C_i\}\)</span> where <span class="math inline">\(C_i\)</span> consists of the vectors having <span class="math inline">\(i\)</span> in the support.</li>
</ul></li>
<li>To estimate <span class="math inline">\(A_i\)</span>, find the largest singular value of <span class="math inline">\(\wh{\Si}_i = \rc{|C_i|} \sum_{y\in C_i} yy^T\)</span>. (Alternatively, run the “Overlapping average” algorithm, which is not SVD but is similar in spirit.)</li>
<li>Iteratively improve the estimate. (Wrks when <span class="math inline">\(Y=AX\)</span> exactly. Omitted here.)</li>
</ul>
<p>For the <span class="math inline">\(m^{\fc{l-1}{2l-1}}\)</span> bound, look at common neighbors of <span class="math inline">\(l\)</span>-tuple of samples.</p>
<h2 id="analysis">Analysis</h2>
<h3 id="graph-construction">Graph construction</h3>
<p>We have <span class="math inline">\(\an{Y^{(i)}, Y^{(j)}} = \sum \an{A_p,A_q} X_p^{(i)}X_q^{(j)}\)</span>.</p>
<p>We want</p>
<ul>
<li>Completeness (if <span class="math inline">\(\Supp(Y^i)\cap \Supp(Y^j)=\{i\}\)</span>, then with high probability <span class="math inline">\(\an{Y^i,Y^j}&gt;\rc2\)</span>).</li>
<li>Soundness (if <span class="math inline">\(\Supp(Y^i)\cap \Supp(Y^j)=\phi\)</span> then whp <span class="math inline">\(\an{Y^i,Y^j}&lt;\rc2\)</span>.</li>
</ul>
Both come from the same matrix concentration bound. We show soundness. Condition on the supports. Let <span class="math inline">\(S_i = \Supp(X^i)\)</span>. Write
\begin{align}
\an{Y^i,Y^j} &amp;= X^T M X\\
M_{(S_1\cup S_2)^2} :&amp;= \matt 0{\rc 2N^T}{\rc 2N}0\\
N :&amp;= (A^TA)_{S_1\times S_2}.
\end{align}
<p>Now use Hanson-Wright to show <span class="math inline">\(X^TMX\approx \Tr(M)\)</span> with high probability.</p>
<h3 id="overlapping-communities">Overlapping communities</h3>
<p>We need to show</p>
<ul>
<li>if <span class="math inline">\(\Om^i\cap \Om^j\cap \Om^k\ne \phi\)</span> then likely <span class="math inline">\(i,j,k\)</span> have many common neighbors</li>
<li>if <span class="math inline">\(\Om^i\cap \Om^j\cap \Om^k=\phi\)</span>, then likely <span class="math inline">\(i,j,k\)</span> have few common neighbors</li>
</ul>
<p>and find the right threshold.</p>
<ul>
<li>If <span class="math inline">\(\Om^i\cap \Om^j\cap \Om^k\ne \phi\)</span> then <span class="math inline">\(\Pj_y(\forall j=1,2,3, |\an{y,y^j}|&gt;\rc 2)\ge \fc{k}{2m}\)</span>.</li>
<li>If <span class="math inline">\(\Om^i\cap \Om^j\cap \Om^k\ne \phi\)</span> then letting <span class="math inline">\(a=|\Om^1\cap \Om^2|\)</span> and similarly for <span class="math inline">\(b,c\)</span>, <span class="math display">\[\Pj_y(\forall j=1,2,3, |\an{y,y^j}|&gt;\rc 2|\Om^{(1)}, \Om^{(2)}, \Om^{(3)})\le \fc{k^6}{m^3} + \fc{3k^3(a+b+c)}{m^2}.\]</span>
<ul>
<li><span class="math inline">\(\pf{k^2}{m}^3\)</span> comes from intersecting each set separately.</li>
<li><span class="math inline">\(\fc{3k^3(a+b+c)}{m^2}\)</span> comes from intersecting two together and the other separately.</li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
