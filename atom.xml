<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2016-08-31T00:00:00Z</updated>
    <entry>
    <title>Transductive learning</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/transduction.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/transduction.html</id>
    <published>2016-08-31T00:00:00Z</published>
    <updated>2016-08-31T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Transductive learning</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-31 
          , Modified: 2016-08-31 
	</p>
      
       <p>Tags: <a href="/tags/self-taught%20learning.html">self-taught learning</a>, <a href="/tags/dictionary%20learning.html">dictionary learning</a>, <a href="/tags/sparse%20coding.html">sparse coding</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p>From <a href="https://en.wikipedia.org/wiki/Transduction_(machine_learning)">wikipedia</a></p>
<blockquote>
<p>In logic, statistical inference, and supervised learning, transduction or transductive inference is reasoning from observed, specific (training) cases to specific (test) cases. In contrast, induction is reasoning from observed training cases to general rules, which are then applied to the test cases. The distinction is most interesting in cases where the predictions of the transductive model are not achievable by any inductive model. Note that this is caused by transductive inference on different test sets producing mutually inconsistent predictions.</p>
</blockquote>
<p>Ex. Semi-supervised clustering</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Self-taught learning ([RBLPN07] Self-taught Learning: Transfer Learning from Unlabeled Data)</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/self_taught_learning.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/self_taught_learning.html</id>
    <published>2016-08-31T00:00:00Z</published>
    <updated>2016-08-31T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Self-taught learning ([RBLPN07] Self-taught Learning: Transfer Learning from Unlabeled Data)</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-31 
          , Modified: 2016-08-31 
	</p>
      
       <p>Tags: <a href="/tags/self-taught%20learning.html">self-taught learning</a>, <a href="/tags/dictionary%20learning.html">dictionary learning</a>, <a href="/tags/sparse%20coding.html">sparse coding</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#definition">Definition</a></li>
 <li><a href="#approach">Approach</a></li>
 <li><a href="#discussion">Discussion</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="http://www.andrewng.org/portfolio/self-taught-learning-transfer-learning-from-unlabeled-data/">website</a></p>
<h2 id="definition">Definition</h2>
<p>Unlabeled data (ex. random Internet images, unlimited audio) need not have the same class labels or generative distribution as labeled data. This is different from</p>
<ul>
<li>semi-supervised learning setting (same distribution/classes)</li>
<li>transfer learning (requires labels for both groups)</li>
</ul>
<p>Makes ML easier and cheaper. Much of human learning is believed to be from unlabeled data. (Order of magnitude: <span class="math inline">\(10^{14}\pat{synapses}/10^9s=10^5\)</span>bits/s)</p>
<!-- (? Ex. maybe just overlap in dictionary features?)-->
<h2 id="approach">Approach</h2>
<ol type="1">
<li><p>Use sparse coding to construct higher-level features using unlabeled data. (cf. representation learning)</p>
They use <span class="math inline">\(L^1\)</span> norm regularization. <span class="math inline">\(\min_{A,x, \ve{A_{\cdot j}}\le 1} \sum_i \ve{y^{(i)} - Ax}_2^2 + \be \ve{x}_1\)</span>. Use AM.</li>
<li>For each input, do sparse recovery (same objective, fixing the <span class="math inline">\(A\)</span> now). This is a convex problem.</li>
<li><p>Now apply supervised learning algorithm, e.g. SVM.</p></li>
</ol>
<h2 id="discussion">Discussion</h2>
<p>?? Sparse coding model also suggests a specific specialized similarity function (kernel) for the learned representations. Once the bases b have been learned using unlabeled data, we obtain a complete generative model for the input x. Thus, we can compute the Fisher kernel to measure the similarity between new inputs.</p>
<p>(Disadvantages of PCA: linear and undercomplete)</p>
<p>What about auto-encoders and NMF?</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Nash equilibrium</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/algorithmic_game_theory/nash_equilibrium.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/algorithmic_game_theory/nash_equilibrium.html</id>
    <published>2016-08-31T00:00:00Z</published>
    <updated>2016-08-31T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Nash equilibrium</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-31 
          , Modified: 2016-08-31 
	</p>
      
       <p>Tags: <a href="/tags/game%20theory.html">game theory</a>, <a href="/tags/Nash.html">Nash</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#definitions">Definitions</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="definitions">Definitions</h2>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Ellipsoid method</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/ellipsoid.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/ellipsoid.html</id>
    <published>2016-08-30T00:00:00Z</published>
    <updated>2016-08-30T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Ellipsoid method</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-30 
          , Modified: 2016-08-30 
	</p>
      
       <p>Tags: <a href="/tags/linear%20programming.html">linear programming</a>, <a href="/tags/algorithms.html">algorithms</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <ul>
<li><a href="http://ocw.mit.edu/courses/mathematics/18-433-combinatorial-optimization-fall-2003/lecture-notes/l1617.pdf">OCW 18.433</a></li>
<li><a href="http://www.cs.princeton.edu/courses/archive/fall15/cos521/lecnotes/lec17.pdf">Arora’s course</a> (<a href="http://www.cs.princeton.edu/courses/archive/fall15/cos521/">course site</a>)</li>
<li>LSW15 <a href="http://arxiv.org/abs/1508.04874">paper</a></li>
</ul>
<h2 id="ellipsoid-method">Ellipsoid method</h2>
<p>This is a very general <em>polynomial time</em> algorithm for convex optimization. We can use it to solve convex optimization problems that are even too large to write down. <!--faster than gradient descent.--></p>
<ol type="1">
<li>LP: <span class="math inline">\(f(x)=c^T\cdot x\)</span>.</li>
<li>SDP: Infinitely many constraints <span class="math inline">\(a^TXa\ge 0\)</span>.</li>
<li>Held-Karp relaxation for traveling salesman: <span class="math inline">\(\min \sum_{i,j}c_{i,j}X_{i,j}\)</span> under conditions
\begin{align}
0 \le X_{ij} &amp;\le 1\\
\forall S\ne \phi, V, \quad \sum_{i\in S, j\in \ol S} X_{ij} &amp;\ge 2
\end{align}
(Last constraint is subtour elimination constraints. We can find a violation by finding a minimum cut with capacity <span class="math inline">\(&lt;2\)</span>.)</li>
</ol>
<!--
Recall convex optimization. In general there is no succinct description for $K$.
\begin{enumerate}
\item
LP: $f(x)=c^T\cdot x$.
\item
SDP: Think of $a^TXa\ge 0$ as infinitely many linear constraints. 
\item
Held-Karp relaxation for traveling salesman: $\min \sum_{i,j}c_{i,j}X_{i,j}$; for all $\sum_j X_{ij}=2$. 
\fixme{Can't require eigenvalue $>$something?}
To prevent disjoint cycles, for all $S\subeq [n],S\ne \phi,[n]$, $\sum_{i\in S,j\in \ol S} X_{ij}\ge 2$. (Exponentially many constraints. Nevertheless we can solve it!)
\end{enumerate}
We only need to be able to project to the convex body.-->
<p><strong>Separation oracle</strong> for convex <span class="math inline">\(K\)</span>: given <span class="math inline">\(x\)</span>, gives a plane that separates <span class="math inline">\(K\)</span> from <span class="math inline">\(x\)</span> in polynomial time. Think of hyperplane as “feedback” on why <span class="math inline">\(x\nin K\)</span>.</p>
<p><strong>Farkas’s Lemma</strong>: If <span class="math inline">\(K\)</span> is convex, for all <span class="math inline">\(x\nin K\)</span>, there is a hyperplane <span class="math inline">\(a^T\cdot x=b\)</span> such that <span class="math inline">\(K\)</span> lies on one side and <span class="math inline">\(y\)</span> on the other.</p>
<p>Ex. 1. PSD: Compute eigenvalues of <span class="math inline">\(Y\)</span>. Say there is eigenvectors <span class="math inline">\(u\)</span> such that <span class="math inline">\(u^TYu&lt;0\)</span>. Use this hyperplane. 2. Traveling salesman: Cut with <span class="math inline">\(&lt;2\)</span>.</p>
<p>An ellipsoid is <span class="math inline">\((X-a)^TBB^T(X-a)\le 1\)</span> where <span class="math inline">\(B\)</span> is PSD. The ellipsoid algorithm: given: an ellipsoid <span class="math inline">\(\cal E\)</span> containing <span class="math inline">\(K\)</span> and <span class="math inline">\(K\)</span> has a poly-time separation oracle.</p>
<p>To find a point of <span class="math inline">\(K\)</span>, recurse:</p>
<ol type="1">
<li>Is the center <span class="math inline">\(x\)</span> in <span class="math inline">\(K\)</span>? If so, done.</li>
<li>Else, find a separating hyperplane <span class="math inline">\(H\)</span> going through <span class="math inline">\(x\)</span>. Find the smallest ellipsoid containing the half cut by <span class="math inline">\(H\)</span>. (<span class="math inline">\(E_{i+1}=E_i\cap \set{x}{a^Tx\le b}\)</span>) This can be found in poly time with ellipsoids.</li>
</ol>
<p><strong>Theorem</strong>: <span class="math inline">\(\Vol(E_{i+1})\le \pa{1-\rc{2n}}\Vol(E_i)\)</span>.</p>
<p>What we need: 1. Rephrase optimization as feasibility. (Binary search.) 2. Find a “reasonably snug” bounding ellipsoid for <span class="math inline">\(K\)</span>. 3. Implement separation oracle for <span class="math inline">\(K\)</span>. 4. Implement computation to find <span class="math inline">\(E_{i+1}\)</span> given <span class="math inline">\(E_i\)</span> and separation oracle.</p>
<strong>Lemma</strong>. The minimum volume ellipsoid containing <span class="math inline">\(Ell(D,z)\cap \set{x}{a\cdot x\le a\cdot z}\)</span> is <span class="math inline">\(E'=Ell(D',z')\)</span> where
\begin{align}
z' &amp;= z-\rc{n+1} \fc{Da}{\sqrt{a^TDa}}\\
D' &amp;= \fc{n^2}{n^2-1} \pa{D-\fc{2}{n+1}\fc{Daa^TD}{a^TDa}}\\
\fc{\Vol(E')}{\Vol(E)} &amp;\le e^{-\rc{2n+2}}.
\end{align}
<p><em>Proof</em>. It suffices to prove this for <span class="math inline">\(D=I\)</span>, <span class="math inline">\(a=e_1\)</span>. Here <span class="math display">\[ D' = \fc{n^2}{n^2-1} \mattn{1-\fc{2}{n+1}}0{\cdots}0{1}{\cdots}{\vdots}{\vdots}{\ddots}.\]</span> Volume bound follows from determinant calculation.</p>
<p>The number of steps needed is <span class="math inline">\(n\ln \pf{V_1}{V_0}\)</span> where <span class="math inline">\(V_1\)</span> is the volume of the smallest ellipsoid containing the body and <span class="math inline">\(V_0\)</span> is volume of the starting ellipsoid. Ex. If <span class="math inline">\(P\)</span> is a polyhedron, and <span class="math inline">\(\nu\)</span> is the number of bits required to write down any <span class="math inline">\(n\times n\)</span> subset of <span class="math inline">\((A,b)\)</span>, then <span class="math inline">\(\Vol(P)\ge 2^{-2n\nu}\)</span>. (Use Cramer’s rule to get expressions for vertices of <span class="math inline">\(Ax\le b\)</span>.) Then the number of iterations is <span class="math inline">\(O(n^2)\)</span>. ?? Each step takes <span class="math inline">\(O(n^2L)O(mn)\)</span> time (<span class="math inline">\(L\)</span>-bit numbers, check validity of point) for a total of <span class="math inline">\(O(mn^5L)\)</span>.</p>
<blockquote>
<p>How do you find a lion in the Sahara? Split it in half and recurse.</p>
</blockquote>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Tensor decomposition</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/tensor_decomposition.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/tensor_decomposition.html</id>
    <published>2016-08-30T00:00:00Z</published>
    <updated>2016-08-30T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Tensor decomposition</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-30 
          , Modified: 2016-08-30 
	</p>
      
       <p>Tags: <a href="/tags/tensors.html">tensors</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#basics">Basics</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="basics">Basics</h2>
<p>See “Tensor methods” in <code>new_thread.pdf</code>.</p>
<p>Review questions:</p>
<ol type="1">
<li>Define tensor and tensor decomposition. When is it unique? Discuss the intractability of tensor problems. Why would we work with them? Give an example where matrix methods fail but tensor methods work because of uniqueness.</li>
<li>What properties of matrix rank fail for tensor rank?</li>
<li>(*) Give Jeinrich’s algorithm. When does it work, and what is its complexity?
<ul>
<li>Suppose <span class="math inline">\(T=\sum^r u_i\ot v_i\ot w_i\)</span>, where <span class="math inline">\(\{u_i\}\)</span> is linearly independent, <span class="math inline">\(\{v_i\}\)</span> is l.i., and every pair in <span class="math inline">\(\{w_i\}\)</span> is l.i. Then this decomposition is unique and there is an efficient algorithm to find it.</li>
<li>For a tensor <span class="math inline">\(T\in V^{\ot 3}\)</span>, view <span class="math inline">\(T_{\bullet}\in V\ot V^*\ot V^*\)</span>, with <span class="math inline">\(\bullet\)</span> corresponding to the last <span class="math inline">\(V^*\)</span>. The slices are sums of rank 1 matrices <span class="math inline">\(u_iv_i^*\)</span> with coefficients given by the projections of <span class="math inline">\(w_i\)</span> onto <span class="math inline">\(a\)</span>.
\begin{align}
T_a &amp;=\sum \an{w_i,a}u_iv_i^* =: UD_aV^*
T_aT_b^+ &amp;= UD_aD_b^+ U^+\\
T_b^+T_a &amp;= VD_b^+D_a V^+
\end{align}
WHP diagonalization recovers <span class="math inline">\(U,V\)</span>.</li>
</ul></li>
<li>Give a perturbation bound for finding eigenvalues and eigenvectors of a matrix. What does the bound depend on, in what way (polynomial?)? Make it quantitative. Use this to give a bound on convergence in tensor decomposition.</li>
<li>What is the Kruskal rank, why is it important?</li>
<li>Describe the phylogenetic tree/HMM setup. How can you solve it with tensor methods?</li>
<li>What about the non-full-rank case? (Give a reduction from noisy parity.)</li>
<li>Give the block stochastic model. When is it information theoretically posible to find the groups (is this exact, or up to some error)? Compare spectral algorithms with tensor methods. (What are the assumptions? Why are tensor methods more flexible?)</li>
<li>(*) Describe and prove the tensor algorithm for the block stochastic model.</li>
<li>(*) Give a tensor algorithm for a pure topic model, and then generalize under latent dirichlet allocation. Why is the Dirichlet distribution particularly nice here? Find the complexity and error in terms of parameters.</li>
<li>(*) Give a tensor algorithm for independent component analysis. There is a dependence on non-Gaussianlity: explain.</li>
<li>Define the cumulants of a distribution. Why are they nice to work with, and how do they help in algorithms?</li>
</ol>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[BKS15] Dictionary Learning and Tensor Decomposition via the Sum-of-Squares Method</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/BKS15.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/BKS15.html</id>
    <published>2016-08-30T00:00:00Z</published>
    <updated>2016-08-30T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[BKS15] Dictionary Learning and Tensor Decomposition via the Sum-of-Squares Method</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-30 
          , Modified: 2016-08-30 
	</p>
      
       <p>Tags: <a href="/tags/dictionary%20learning.html">dictionary learning</a>, <a href="/tags/tensor.html">tensor</a>, <a href="/tags/sparse%20coding.html">sparse coding</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#noisy-tensor-decomposition">Noisy tensor decomposition</a><ul>
 <li><a href="#reduction">Reduction</a></li>
 <li><a href="#algorithm">Algorithm</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <p><strong>Theorem</strong>. Given <span class="math inline">\(\ep&gt;0,\si\ge 1, \de&gt;0\)</span> there exists <span class="math inline">\(d\)</span> and a polytime algorithm that given input</p>
<ul>
<li>a <span class="math inline">\(\si\)</span>-dictionary <span class="math inline">\(n\times m\)</span>,</li>
<li><span class="math inline">\((d,\tau=n^{-\de})\)</span>-nice <span class="math inline">\(\{x\}\)</span>,</li>
<li><span class="math inline">\(n^{O(1)}\)</span> samples from <span class="math inline">\(y=Ax\)</span>,</li>
</ul>
<p>outputs with probability <span class="math inline">\(\ge 0.9\)</span> a set <span class="math inline">\(\ep\)</span>-close to columns of <span class="math inline">\(A\)</span>.</p>
<ul>
<li>A <span class="math inline">\(\si\)</span>-dictionary has <span class="math inline">\(AA^T\preceq \si I\)</span> (analytic proxy for overcompleteness <span class="math inline">\(\fc mn\)</span>).</li>
<li>A distribution is <span class="math inline">\((d,\tau)\)</span>-nice if <span class="math inline">\(\E x_i^{d/2}x_j^{d/2}\le \tau\)</span> for all <span class="math inline">\(i\ne j\)</span>, and <span class="math inline">\(\E x^\al=0\)</span> for all <span class="math inline">\(|\al|\le d\)</span>. (Ex. Bernoulli(<span class="math inline">\(\tau\)</span>) times Gaussian with <span class="math inline">\(\E z_i^d = \rc\tau\)</span>.)</li>
</ul>
<p>Note on running time:</p>
<ul>
<li><span class="math inline">\(\ep\)</span>-accuracy with running time depending on <span class="math inline">\(\poly\prc{\ep}\)</span> in the exponent. So this is better for giving an initial solution for local search methods.</li>
</ul>
<h2 id="noisy-tensor-decomposition">Noisy tensor decomposition</h2>
<p>Given noisy version of <span class="math inline">\(\sumo im \an{a^i, u}^d = \ve{A^Tu}_d^d\)</span>, recover <span class="math inline">\(\{a^1,\ldots, a^m\}\)</span>.</p>
<p><strong>Theorem</strong> (Noisy tensor decomposition). For every <span class="math inline">\(\ep&gt;0,\si\ge 1\)</span> there exist <span class="math inline">\(d,\tau\)</span> such that a probabilistic <span class="math inline">\(n^{O(\ln n)}\)</span>-time agorithm, given <span class="math inline">\(P\)</span> with <span class="math display">\[\ve{A^T u}_d^d - \tau \ve{u}_2^d \preceq P \preceq \ve{A^Tu}_d^d + \tau \ve{u}_2^d,\]</span> outputs with probability 0.9 <span class="math inline">\(S\)</span> that is <span class="math inline">\(\ep\)</span>-close to <span class="math inline">\(\{a^1,\ldots, a^m\}\)</span>. (<span class="math inline">\(\preceq\)</span> means the difference is a sum of squares.)</p>
<p>Note this allows significant noise since we expect <span class="math inline">\(\ve{A^Tu}_d^d\sim mn^{-\fc d2}\ll \tau\)</span>.</p>
<h3 id="reduction">Reduction</h3>
<p>Take <span class="math display">\[ P = \EE_{i} \an{y_i, u}^{2d}.\]</span></p>
<h3 id="algorithm">Algorithm</h3>
<ol type="1">
<li>Use SoS to find degree-<span class="math inline">\(k\)</span> pseudo-distribution <span class="math inline">\(\{u\}\)</span> that maximizes <span class="math inline">\(P(u)\)</span> under <span class="math inline">\(\ve{u}^2=1\)</span>.</li>
<li>Let <span class="math inline">\(W\)</span> be a product of <span class="math inline">\(O(\ln n)\)</span> rndom linear functions.</li>
<li>Output top eigenvector of <span class="math inline">\(M\)</span>, <span class="math inline">\(M_{ij} = \wt \E W(u)^2 u_iu_j\)</span>.</li>
</ol>
<p>SoS Algorithm: given <span class="math inline">\(\ep&gt;0, k,n,M\)</span>, <span class="math inline">\(P_1,\ldots, P_m\in \R[x_1,\ldots, x_n]\)</span> with coefficients in <span class="math inline">\([0,M]\)</span>, if there exists a degree <span class="math inline">\(k\)</span> pseudo-distribution with <span class="math inline">\(P_i=0,i\in [m]\)</span>, then we can find <span class="math inline">\(u'\)</span> satisfying <span class="math inline">\(P_i\le \ep, P_i\ge -\ep\)</span> for every <span class="math inline">\(i\in [m]\)</span> in time <span class="math inline">\((n\polylog\prc{M}{\ep})^{O(k)}\)</span>.</p>
<p>(This is a feasibility problem? Use ellipsoid method?)</p>
<p>Steps</p>
<ol type="1">
<li>If <span class="math inline">\(u\)</span> is an actual distribution, the the output is close to a column.</li>
<li>Generalize to pseudo-distributions.</li>
<li>Generalize to getting all columns.</li>
</ol>
<p>Idea: If not correlated, then <span class="math inline">\(P(u)\)</span> is small. The inequalities can be turned into polynomial inequalities provable by SoS.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2016-09-03</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-09-03.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-09-03.html</id>
    <published>2016-08-30T00:00:00Z</published>
    <updated>2016-08-30T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2016-09-03</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-30 
          , Modified: 2016-08-30 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#threads">Threads</a></li>
 <li><a href="#summary">Summary</a></li>
 <li><a href="#conversation-with-arora">Conversation with Arora</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="threads">Threads</h2>
<ul>
<li>Relaxing sparsity assumption: Independent sparsity is an unrealistic assumption. For example, many features tend to co-occur with each other. Re-analyze algorithms that rely on independent sparsity under looser conditions on the distribution.
<ul>
<li>Two ways to loosen the conditions:
<ul>
<li>Drop the condition of independence.
<ul>
<li>A first step is “group sparsity”. See <a href="/posts/tcs/machine_learning/representation.html">representation learning</a> for references.</li>
<li>A next step is to do away with independence entirely. (Certain distributions may be intractable… if so, obtain a hardness result and then add some looser assumption.)</li>
</ul></li>
<li>Drop the condition of sparsity.
<ul>
<li>Instead have some condition on moments/tails so that each vector is well-approximated by a sparse vector, cf. “flatness”, ex. 99% of weight is on <span class="math inline">\(o\prc{\sqrt n}\)</span> of coordinates.</li>
<li>The difference with adding noise is that
<ul>
<li>Noise is added to <span class="math inline">\(x\)</span> (pre-coding) rather than <span class="math inline">\(y\)</span> (post-coding)</li>
<li>Noise can be correlated with the vector, not independent. (if it were independent, there’s not that much difference from adding on the <span class="math inline">\(y\)</span>-side)</li>
</ul></li>
</ul></li>
</ul></li>
<li>Sparse recovery. Ex. basis pursuit.</li>
<li>Dictionary learning
<ul>
<li>Q (<span class="citation" data-cites="Arora">@Arora</span>): How does dictionary learning perform in real life? What is it applied to? Applying it to problems traditionally solved with SVM’s and neural nets, how does it compare? How do AGM14, AGMM15 perform?</li>
<li>Re-analyze AGM14, AGMM15 with group sparsity.</li>
<li>Re-analyze with arbitrary sparselike distribution.</li>
</ul></li>
</ul></li>
<li>Representation learning: general problem
<ul>
<li>The main bottleneck is that the problem “sort-of” reduces to tensor decomposition, but I don’t understand much of how TD performs in theory and in practice.</li>
<li>Read papers on TD (Ge, Anandkumar).</li>
</ul></li>
<li>PMI for images
<ul>
<li>I don’t have any positive results when I build on top of the CKN.
<ul>
<li>May try some alternatives: work with CIFAR instead, do some thresholding thing, do things in middle layer… (but unlikely to work if first thing didn’t work?)</li>
</ul></li>
<li>Do experiments on a more basic level.
<ul>
<li>Try unsupervised multiclass SVM on both the original image and on the CKN features. How does this compare to clustering? (Look at the actual pics.)</li>
<li>Have a programming setup where you can visualize clusters and features.</li>
</ul></li>
<li>DL for images
<ul>
<li>If you do DL, do you obtain the categories? (ex. digits)</li>
<li>If you do DL, is training on top of the DL easier?</li>
<li>How does (H)DL compare to the CKN and other unsupervised methods? Make DL convolutional.</li>
<li>What is the typical sparsity of a trained NN? (cf. dropout?)</li>
</ul></li>
<li>Use DL/weighted SVD with the natural proximity/distance in images. Get “context vectors” for different parts of the image. (Hierarchical fashion?) A global context vector would be the classification?</li>
<li>(images not just DL/SVM because adding something in perpendicular direction wrecks things?)</li>
</ul></li>
</ul>
<p>@Arora: in what way is DL “just” tensor decomposition?</p>
<p>(Is there a relationship between weighted SVD for PMI and DL? It’s not at all clear to me. Somehow neural nets do something “like” DL (does the CKN do this though?) but then we’re trying to see if the features can be SVD’d with PMI. Learning with 2 “layers”:</p>
<ol type="1">
<li>Representation - with NN/DL</li>
<li>Classification - (on top) with SVD/PMI.</li>
</ol>
<p>Also how do hierarchical methods come in?)</p>
<p>Things meriting a further look</p>
<ul>
<li>Traditional learning theory (ex. learning SAT)</li>
<li>Probabilistic view of dictionary learning (LDA, hierarchical DP, etc.) <span class="citation" data-cites="Bianca">@Bianca</span></li>
<li>Pattern theory—how they model images. Traditional image things—wavelets etc.</li>
</ul>
<h2 id="summary">Summary</h2>
<ul>
<li>Reading
<ul>
<li><a href="../tcs/machine_learning/self_taught_learning.html">Self-taught learning</a></li>
<li><a href="../tcs/machine_learning/transduction.html">Transduction</a></li>
<li><a href="../tcs/machine_learning/tensor/BKS15.html">BKS15 Dictionary Learning and Tensor Decomposition via the Sum-of-Squares Method</a></li>
</ul></li>
</ul>
<h2 id="conversation-with-arora">Conversation with Arora</h2>
<ul>
<li>“Show that backprop works.”
<ul>
<li>Assume a generative distribution: <span class="math inline">\(x\)</span> from sparse distribution, observe <span class="math inline">\(y\approx Ax\)</span>, there is a SVM classifier depending on <span class="math inline">\(x\)</span>. Show that backprop for a 2-layer NN works. (<span class="citation" data-cites="Tengyu">@Tengyu</span>)</li>
</ul></li>
<li>PMI: restrict to features that co-occur.</li>
<li>SoS for TD + AM for DL gives <span class="math inline">\(n^{O(\log n)}\)</span> algorithm for sparsity <span class="math inline">\(n^{1-\ep}\)</span>. Check this (don’t need sparsity after initialization).</li>
<li>(Hard) Kernel SVM’s can’t learn DL + classifier (essentially 2-layer NN).</li>
</ul>
<!--1-layer NN is like sparse recovery.-->

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Matrix perturbation</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/linear_algebra/matrix_analysis/perturbation.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/linear_algebra/matrix_analysis/perturbation.html</id>
    <published>2016-08-30T00:00:00Z</published>
    <updated>2016-08-30T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Matrix perturbation</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-30 
          , Modified: 2016-08-30 
	</p>
      
       <p>Tags: <a href="/tags/matrices.html">matrices</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#davis-kahan">Davis-Kahan</a></li>
 <li><a href="#weyls-theorem">Weyl’s Theorem</a></li>
 <li><a href="#wedins-theorem">Wedin’s Theorem</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="davis-kahan">Davis-Kahan</h2>
<p>(Theorem 4.8 <a href="http://ocw.mit.edu/courses/mathematics/18-s997-high-dimensional-statistics-spring-2015/lecture-notes/MIT18_S997S15_Chapter4.pdf">here</a>)</p>
<p>Distance between eigenvectors.</p>
<h2 id="weyls-theorem">Weyl’s Theorem</h2>
<h2 id="wedins-theorem">Wedin’s Theorem</h2>
<p>Let <span class="math inline">\(v_1(A)\)</span> be the top eigenvector of <span class="math inline">\(A\)</span>. If <span class="math inline">\(\de=|\la_1(A)-\la_2(A)|\)</span>, then <span class="math inline">\(\sin(\angle (v_1(A), v_1(A+E)))\le \fc{\ve{E}_2}{\de}\)</span>.</p>
<p>?? What’s the analogue of this for subspaces? Ex. <span class="math inline">\(\la_1,\ldots, \la_c\)</span> large and <span class="math inline">\(\la_{c+1}\)</span> small.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2016-08-27</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-08-27.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-08-27.html</id>
    <published>2016-08-27T00:00:00Z</published>
    <updated>2016-08-27T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2016-08-27</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-27 
          , Modified: 2016-08-27 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#goals">Goals</a></li>
 <li><a href="#summary">Summary:</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="goals">Goals</h2>
<ul>
<li>Review/learning
<ul>
<li>HDP Ch. 4</li>
<li>Index-card AI safety notes</li>
<li>Convex optimization, bandit optimization</li>
</ul></li>
<li>PMI
<ul>
<li>Organize code, data; write up what I’ve found so far.</li>
</ul></li>
<li>Representation learning
<ul>
<li>Experiment?</li>
</ul></li>
<li>Tensorflow
<ul>
<li>Get basic LSTM running.</li>
<li>Play with parameters in MNIST.</li>
</ul></li>
</ul>
<h2 id="summary">Summary:</h2>
<ul>
<li>PMI
<ul>
<li>Run weighted SVD and compared</li>
</ul></li>
<li>HDP started Ch. 4</li>
<li>Reviewed convex optimization.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Interior point methods</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/ipm.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/ipm.html</id>
    <published>2016-08-26T00:00:00Z</published>
    <updated>2016-08-26T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Interior point methods</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-26 
          , Modified: 2016-08-26 
	</p>
      
       <p>Tags: <a href="/tags/convex%20optimization.html">convex optimization</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#introduction">Introduction</a></li>
 <li><a href="#analysis-and-algorithm-explanation">Analysis and algorithm explanation</a><ul>
 <li><a href="#place-to-start">1 Place to start</a><ul>
 <li><a href="#termination-near-phase-ii-central-path">Termination near phase II central path</a></li>
 </ul></li>
 <li><a href="#inner-steps">2 Inner steps</a></li>
 <li><a href="#outer-steps">3 Outer steps</a></li>
 <li><a href="#total">Total</a></li>
 <li><a href="#variations">Variations</a></li>
 </ul></li>
 <li><a href="#primal-dual-ipm">Primal-dual IPM</a></li>
 <li><a href="#intuitions">Intuitions</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="introduction">Introduction</h2>
<p>The idea: Given <span class="math inline">\(\min_{f_i\le 0} f\)</span>,</p>
<ol type="1">
<li>Find a feasible point.</li>
<li>Turn into soft constraints: let <span class="math inline">\(F(t,x) = \min f+ \sum -\rc t \ln (-f_i)\)</span>. (Add a barrier function.) As <span class="math inline">\(t\to \iy\)</span>, <span class="math inline">\(F(t,x)\)</span> becomes steeper and <span class="math inline">\(\to f(x)\)</span> pointwise.</li>
<li>Solve iteratively. Start at some <span class="math inline">\((x_0,t_0)\)</span>: Find a value within <span class="math inline">\(\ep\)</span> of <span class="math inline">\(\min F(t_i,x)\)</span>. (Centering)</li>
<li>Set <span class="math inline">\(t_{i+1}=\ga t_i\)</span> (updating schedule).</li>
<li>Go back to 3. Repeat until <span class="math inline">\(t\)</span> is large enough. (<span class="math inline">\(m/t&lt;\ep\)</span>)</li>
</ol>
<p>Define the central path. What are the properties of the central path. Explain the relationship to the dual problem.</p>
<p><span class="math display">\[
C = \set{(t,x)}{x=\amin f(x) + \sum -\rc t \ln (-f_i(x))}.
\]</span> Now <span class="math inline">\(f(c(t))\to f(x^*)\)</span>. For <span class="math inline">\((t,x')\in C\)</span>, we have <span class="math display">\[ \nb f + \sum - \fc{\nb f_i}{tf_i}=0.\]</span> This is the first KKT condition with <span class="math inline">\(\la_i = -\rc{tf_i(x')}\)</span>. (Complementarity is not satisfied though: we have <span class="math inline">\(-\la_if_i=\rc t\)</span>, not 0.)</p>
Let <span class="math inline">\(x^*=\amin_{f_i\le 0} f\)</span>, <span class="math inline">\(x'=\amin f - \sum \rc t \ln (-f_i)\)</span>. Substituting <span class="math inline">\(x',\la_i\)</span> in the dual problem, we get
\begin{align}
f(x^*) &amp;\ge \min_x f(x) - \sum \rc{t f_i(x')} f_i(x) \\
&amp; = f(x') - \fc mt &amp; x=x'\\
\implies f(x^*) \le f(x') &amp;\le f(x^*) + \fc mt.
\end{align}
<p>Now add in the condition <span class="math inline">\(Ax=b\)</span>, and the term <span class="math inline">\(\nu^T(Ax-b)\)</span>. The analysis stays the same.</p>
<p>(<span class="math inline">\(\ln(-f_i)\)</span> has the magic that its gradient is <span class="math inline">\(\nb f_i\)</span> times something.)</p>
<p>What questions do we need to address?</p>
<ol type="1">
<li>How do we find a point to start—find a feasible point?</li>
<li>Given the previous almost-optimal solution, what’s the complexity of finding the next one, given ratio <span class="math inline">\(\mu\)</span>? (Number of inner steps)</li>
<li>How many outer steps do we need? When to stop? (We’ve shown the gap is <span class="math inline">\(\fc mt\)</span>, so stop when <span class="math inline">\(t&gt;\fc m\ep\)</span>.</li>
</ol>
<p>What assumptions do we need?</p>
<ul>
<li><span class="math inline">\(tf_0+\phi\)</span> (<span class="math inline">\(\phi = -\sumo im \ln(-f_i)\)</span>) closed (??) and self-concordant for all <span class="math inline">\(t\ge t^{(0)}\)</span>.</li>
<li>Sublevel sets of <span class="math inline">\(f_0\)</span> (<span class="math inline">\(f_i\le 0\)</span>, <span class="math inline">\(Ax=b\)</span>) are bounded. (This implies that <span class="math inline">\(\nb^2(tf_0+\phi)\)</span> is PD—why?)</li>
</ul>
<p>This is reasonable because it is self-concordant if all <span class="math inline">\(f_i\)</span> are linear or quadratic, and <span class="math inline">\(\ln\)</span> is good at making functions self-concordant.</p>
<p>Give an example of a problem that can be reformulated to be self-concordant. Technique is to add redundant constraints or extra variables.</p>
<ul>
<li>For <span class="math inline">\(\min_{Fx\le g, Ax=b} \sumo in x_i\ln x_i\)</span>, <span class="math inline">\(tf_0(x)+\phi(x)\)</span> is not closed (?) or self-concordant. Using that <span class="math inline">\(ty\ln y - \ln y\)</span> is self-concordant, we add in the redundant constraint <span class="math inline">\(x\ge 0\)</span> to get <span class="math display">\[tf_0+\phi = \sumo in (tx_i\ln x_i - \ln x_i) - \sumo im \ln (g_i-f_i^Tx).\]</span></li>
<li>For <span class="math inline">\(\min_{\ln(\sumo k{K_i}\exp(a_{ik}^T + b_{ik}))\le 0} \ln \pa{\sumo k{K_0} \exp(a_{0k}^Tx+b_{0k})}\)</span>, introduce the variables <span class="math inline">\(y_{ik}\)</span>. Change the problem to
\begin{align}
\min \sumo k{K_0} y_{0k}\\
\sumo k{K_i} y_{ik} &amp;\le 1\\
a_{ik}^T x+b_{ik} - \ln y_{ik} &amp;\le 0\\
y_{ik} &amp;\ge 0.
\end{align}</li>
</ul>
<h2 id="analysis-and-algorithm-explanation">Analysis and algorithm explanation</h2>
<h3 id="place-to-start">1 Place to start</h3>
<p>How to find a feasible point? A feasibility problem can be transformed into an optimization problem</p>
<p><span class="math display">\[
\exists x, f_i\le 0,  \iff \min_{f_i\le m} m\le 0.
\]</span> <!-- Ax = b --> Assume <span class="math inline">\(\{\forall i, f_i\le 0\}\subeq B_R(0)\)</span>. Let <span class="math inline">\(\ol p^*\)</span> be the optimal value of this optimization problem.</p>
Actually add an extra constraint (<span class="math inline">\(a\)</span> satisfies <span class="math inline">\(\ve{a}_2\le \rc R\)</span> so is redundant). <span class="math display">\[
\min_{f_i\le s, a^Tx \le 1} s.
\]</span> Choose <span class="math inline">\(a,s_0\)</span> so that <span class="math inline">\((x=0,s=s_0, t_0)\)</span> is on the central path (to make analysis easier), i.e., so <span class="math inline">\(x=0,s=s_0\)</span> minimizes <span class="math display">\[t^{(0)}s- \sumo im \ln (s-f_i(x)) - \ln (1-a^T x).\]</span> Set the gradient to 0 to see that we require
\begin{align}
t^{(0)} &amp;= \sumo im \rc{s_0 - f_i(0)}\\
a &amp;= -\sumo im \rc{s_0-f_i(0)} \nb f_i(0).
\end{align}
<p>What to choose for <span class="math inline">\(s_0\)</span>? We need <span class="math inline">\(s_0&gt;F:=\max_i f_i(0)\)</span> and <span class="math inline">\(\ve{a}_2\le \rc{R}\)</span>. Upper bound <span class="math inline">\(\ve{a}_2\)</span> by <span class="math display">\[
\ve{a}_2\le \sumo im \rc{s_0-f_i(0)}\ve{\nb f_i(0)} \le \fc{mG}{s_0-F},\quad G=\max_i \ve{\nb f_i(0)}_2,
\]</span> so we can take <span class="math inline">\(\boxed{s_0=mGR+F}\)</span>. Then <span class="math inline">\(t^{(0)} \ge \rc{mGR}\)</span> so the initial duality gap is <span class="math inline">\(\fc{m+1}{t^{(0)}} \le (m+1) mGR\)</span>. Use the barrier method until the duality gap is <span class="math inline">\(&lt;|\ol p^*|\)</span>, so that we can determine <span class="math inline">\(\sgn(\ol p^*)\)</span>. This requires (take <span class="math inline">\(\mu = 1+\rc{\sqrt{m+1}}\)</span>) <span class="math display">\[
\le \ce{\sqrt{m+1} \log_2 \fc{m(m+1)GR}{|\ol p^*|}}\pa{\rc{2\ga} + c}
\]</span> Newton steps. (Interpret <span class="math inline">\(\lg\pf{GR}{|\ol p^*|}\)</span> as how close the feasibility problem is to the boundary between feasibility and infeasibility.)</p>
<p>(Equality constraints don’t change things too much. ?? <span class="math inline">\(G\)</span>, <span class="math inline">\(R\)</span> refer to reduced/eliminated problem.)</p>
Why did we add in <span class="math inline">\(a^Tx\le 1\)</span>? Otherwise, we minimize <span class="math inline">\(ts-\sum \ln (-(f_i-s))\)</span>, and
\begin{align}
\ddd{s} &amp;= t+\sum \rc{f_i-s} = 0\\
\nb_x &amp;= \sum \fc{-\nb f_i}{f_i-s} =0.
\end{align}
<p>We can’t choose <span class="math inline">\(x=0\)</span> because we need <span class="math inline">\(s&gt;\max f_i\)</span>, and <span class="math inline">\(\nb_x&gt;0\)</span>.</p>
<p>(Note if we add the constraint <span class="math inline">\(a^Tx\le 1\)</span> for phase 1, we have to include it for phase II as well.)</p>
<h4 id="termination-near-phase-ii-central-path">Termination near phase II central path</h4>
<p>During phase I, add in the extra constraint <span class="math inline">\(f_0(x)\le M\)</span> to make it intersect the phase II central path. (We can add the constraint <span class="math inline">\(a^Tx\le 1\)</span> below.) We want the point on the central path for phase I corresponding to <span class="math inline">\(s=0\)</span> to also be on the phase II central path. (I think you won’t get to <span class="math inline">\(s=0\)</span> exactly, but you get close—then the duality gap is <span class="math inline">\(m(M-f_0)+\)</span>(something small).)</p>
\begin{align}
\min_{f_i\le s, f_0\le M, Ax=b} s:&amp;&amp; \nb(ts + (\sum-\ln (s-f_i)) - \ln (M-f_0) + A^T \nu) &amp;=0\\
\iff &amp;&amp; t&amp;=\sum \rc{s-f_i} = \sum -\rc{f_i}\\
&amp;&amp; \rc{M-f_0} \nb f_0 + \sum\rc{s-f_i} \nb f_i + A^T\nu &amp;=0\\
\min_{f_i\le 0, Ax=b}f_0:&amp;&amp; \nb(tf_0+\sum - \ln (-f_i) + A^T \nu) &amp;=0\\
\iff &amp;&amp;t\nb f + \fc{\nb f_i}{f_i} + A^T \nu &amp;=0
\end{align}
<p>Make these match by setting <span class="math inline">\(t=\rc{M-f_0}\)</span>. I.e., the initial duality gap for phase 2 is <span class="math inline">\(\fc{m}{t} = m(M-f_0)\)</span>.</p>
<h3 id="inner-steps">2 Inner steps</h3>
<p>Given <span class="math inline">\(x^*(t)\)</span>, how many steps does it take to compute <span class="math inline">\(x^*(\mu t)\)</span>?</p>
<p>How can you use the fact that <span class="math inline">\(x\)</span> is optimal for <span class="math inline">\(tf_0(x) + \phi(x)\)</span> to prove how optimal it is for <span class="math inline">\(\mu t f_0(x)+\phi(x)\)</span>?</p>
<p>Let <span class="math inline">\(x=x^*(t)\)</span>, <span class="math inline">\(x^+=x^*(\mu t)\)</span>.</p>
<p>It suffices to bound <span class="math display">\[\mu t f_0(x) + \phi(x) - \mu tf_0(x^+) - \phi(x^+).\]</span></p>
<p>We can’t bound <span class="math inline">\(\ln(-f_i)\)</span> directly. We can hope to bound the dual function <span class="math inline">\(\cL\)</span> (we have info from the KKT conditions). Use the linear approximation to <span class="math inline">\(\ln\)</span>.</p>
\begin{align}
\mu t f_0(x) + \phi(x) - \mu tf_0(x^+) - \phi(x^+)
&amp;= \mu t f_0(x) - \mu t f_0(x^+) + \sum \ln \pf{\mu f_i(x^+)}{\mu f_i(x^+)}\\
&amp;\le \mu t f_0(x) - \mu t f_0(x^+) + \sum \pa{\fc{\mu f_i(x^+)}{f_i(x)} - 1 -\ln \mu}\\
&amp;= \mu t f_0(x) - \mu t f_0(x^+) + t\pa{\sum \rc{tf_i} \mu f_i(x^+)} - m - m\ln \mu\\
&amp;= \mu t f_0(x) - \mu t \cL(x^+, \la, \nu) - m - m \ln \mu &amp;Ax^+=b, \la_i  = \rc{tf_i}\\
&amp;\le \mu tf_0(x) - \mu t g(\la, \nu) - m - m\ln \mu\\
&amp;=m(\mu-1 - \ln \mu).
\end{align}
<p>Thus, the number of inner steps is <span class="math display">\[
\fc{f(x)=p^*}{\ga}+c = \fc{m(\mu - 1 - \ln \mu)}{\ga} + c
\]</span> (See <a href="second-order.html">Newton</a> for definition of <span class="math inline">\(\ga\)</span>. We approximate <span class="math inline">\(\ln\ln\)</span> by a constant.) This is approximately quadratic for small <span class="math inline">\(\mu\)</span> (<span class="math inline">\(O(m(\mu-1)^2)\)</span>), linear (<span class="math inline">\(O(m\mu)\)</span>) for large <span class="math inline">\(\mu\)</span>. (This does not depend on <span class="math inline">\(n,p\)</span>.)</p>
<h3 id="outer-steps">3 Outer steps</h3>
<p>The number of outer steps needed is <span class="math inline">\(\ce{\fc{\ln (m/(t^{(0)}\ep))}{\ln \mu}}\)</span> so the total number of Newton steps needed is <span class="math display">\[\ce{\log_\mu \pf{m}{t^{(0)}\ep)}} \pa{\fc{m(\mu - 1 - \ln \mu)}{\ga} + c}. \]</span></p>
<ul>
<li>Choosing <span class="math inline">\(\mu\)</span> constant, get <span class="math inline">\(O\pf{\pa{\ln\pf{m}\ep}m}{\ga}\)</span>.</li>
<li>Set <span class="math inline">\(\mu\)</span> small to do better. Balance <span class="math inline">\((\mu-1-\ln \mu)m=O(m(\mu-1)^2)\)</span> and <span class="math inline">\(c\)</span> by setting <span class="math inline">\(\mu = 1+\rc{\sqrt m}\)</span>, and get <span class="math inline">\(O(\sqrt m)\)</span>. (Recall <span class="math inline">\(m\)</span> is number of constraints.)</li>
<li>In practice, though, choose <span class="math inline">\(\mu\)</span> constant.</li>
</ul>
<h3 id="total">Total</h3>
\begin{align}
N_I &amp;= O(\sqrt m \log \pf{mGR}{|\ol p^*|} \rc{\ga})\\
N_{II} &amp;= O(\sqrt m \log \pf{m(M-p^*)}{\ep}) \prc{\ga}.
\end{align}
<p>Explanation: The point at the end of phase I has duality gap <span class="math inline">\(\le (m+1)(M-p^*)\)</span>.</p>
<h3 id="variations">Variations</h3>
<p>What are other ways to do Phase I?</p>
<ul>
<li>Sum of infeasibilities <span class="math inline">\(\min_{f_i\le s_i, Ax=b, s\ge0} \one^Ts\)</span>. Why use this? When infeasible, the optimal point often violates a small unber of inequalities (cf. <span class="math inline">\(l_1\)</span>-regression, basis pursuit). Here the penalty is <span class="math inline">\(l_1\)</span> not <span class="math inline">\(l_\iy\)</span>.</li>
<li>Use infeasible start Newton to solve the barrier formulation <span class="math display">\[\min_{f_i\le s, Ax=b, s=0} f_0\qquad \min_{Ax=b, s=0} t^{(0)} f_0-\sum \ln (s-f_i).\]</span> (Infeasible start means you can start with points violating equality constraints.)</li>
</ul>
<p>What if we don’t know a point in <span class="math inline">\(\bigcap dom(f_i)\)</span>? Add a translation, <span class="math inline">\(\min_{..., z_i=0} t^{(0)} f_0(x+z_0) - \sum \ln (s-f_i(x+z_i))\)</span>.</p>
<p>Disadvantage: There is no good stopping criterion when infeasible.</p>
<h2 id="primal-dual-ipm">Primal-dual IPM</h2>
<p>Review <a href="constrained.html">constrained optimization</a>. Here there is no distinction between inner and outer iterations.</p>
<p>Applying the infeasible start Newton method to the barrier problem gives <span class="math display">\[ \matt{t\nb^2 f_0+\nb^2 \phi A^T}{A^T}{A}{0} \coltwo{\De x_{nt}}{\nu_{nt}}  = -\coltwo{t\nb f_0+\nb \phi}{0}. \]</span> The residual is <span class="math inline">\((\nb f + \nb \phi + A^T\nu, Ax-b)\)</span>, <span class="math inline">\(\phi = -\sum \rc{t}\ln (-f_i)\)</span>.</p>
<p>But here <span class="math inline">\(t\)</span> is fixed. We want to treat <span class="math inline">\(t\)</span> as a variable. Actually, we introduce <span class="math inline">\(\la\)</span>.</p>
<p>Recall that a point on the central path gives a dual feasible <span class="math inline">\((\la, \nu)\)</span> with <span class="math inline">\(\la_if_i=-\rc t\)</span>. We write everything in terms of the primal <span class="math inline">\(x\)</span> and dual <span class="math inline">\((\la,\nu)\)</span>. Introduce <span class="math inline">\(\la\)</span> as a variable that we want to satisfy <span class="math inline">\(\la_if_i=-\rc t\)</span> (the modified KKT equation) to get the (dual, centrality, primal) residual <span class="math display">\[r_t(x,\la,\nu) = \colthree{\nb f + Df^T\la + A^T\nu}{-\diag(\la) f - \rc t \one}{Ax-b} =: \colthree{\De x}{\De \la}{\De \nu} = -\colthree{r_{dual}}{r_{cent}}{r_{pri}}.\]</span></p>
The Newton step for solving the modified KKT equations
\begin{align}
\nb f_0 + Df^T \la + A^T\nu &amp;=0\\
-\la_i f_i &amp;=\rc t\\
Ax-b &amp;=0
\end{align}
is hence (set the gradients of these equations to the negative residuals)
\begin{align}
\mattn{\nb^2 f_0+\sum \la_i \nb^2 f_i}{Df^T}{A^T}{-\diag(\la)Df}{-\diag(f)}{0}{A}00 \colthree{\De x}{\De \la}{\De \nu} = -\colthree{r_{dual}}{r_{cent}}{r_{pri}}.
\end{align}
<p>(<span class="math inline">\(Df\)</span> has <span class="math inline">\(\nb f_i\)</span> as rows.) The solution is the primal-dual search direction. It is not the same as the search direction in the barrier method (because here we’re changing <span class="math inline">\(\la\)</span>).</p>
Solving for <span class="math inline">\(\De \la\)</span> and substituting gives
\begin{align}
\matt{\nb^2 f_0 + \sumo im \la_i \nb^2 f_i + \sumo im -\fc{\la_i}{f_i}\nb f_i \nb f_i^T}{A^T}A0 \coltwo{\De x_{pd}}{\De \nu_{pd}} &amp;= 
\coltwo{r_{dual} - \sum \la_i \nb f_i - \sum \fc{\nb f_i}{tf_i}}{r_{pri}}\\
&amp;=-\coltwo{\nb f_0 + \rc t \sumo im -\rc{f_i}\nb f_i+A^T\nu}{r_{pri}}.
\end{align}
<p>Compare to the Newton step for the centering method (<span class="math inline">\(t\)</span> fixed) in the barrier method. The upper-left entry is replaced by <span class="math inline">\(t\nb^2 f_0 + \sumo im -\rc{f_i} \nb^2 f_i + \sumo \rc{f_i^2} \nb f_i f_i^T\)</span>, and the dual residual is instead <span class="math inline">\(t\nb f_0 + \sumo im -\rc{f_i} \nb f_i\)</span>.</p>
<p>The iterates are not necessarily feasible, so we can’t evaluate a duality gap. Define the <strong>surrogate duality gap</strong> for <span class="math inline">\(x,f(x)&lt;0, \la\ge 0\)</span> by <span class="math display">\[\wh \eta(x,\la) = -f^T\la.\]</span> It is the duality gap if <span class="math inline">\(x\)</span> were primal feasible and <span class="math inline">\(\la,\nu\)</span> were dual feasible (<span class="math inline">\(r_{pri}=0,r_{dual}=0\)</span>).</p>
<p>The algorithm:</p>
<p>(Feasible start) Start with <span class="math inline">\(x\)</span> such that <span class="math inline">\(f_i(x)&lt;0\)</span>, <span class="math inline">\(\la&gt;0\)</span>, <span class="math inline">\(\mu&gt;1\)</span>, <span class="math inline">\(\ep_{feas}&gt;0\)</span>, <span class="math inline">\(\ep&gt;0\)</span>.</p>
<ol type="1">
<li>Set <span class="math inline">\(t=\mu m/\wh eta\)</span>.</li>
<li>Compute primal-dual search direction <span class="math inline">\(\De y_{pd}\)</span>.</li>
<li>Line search and update.</li>
<li>Repeat until <span class="math inline">\(\ve{r_{pri}}_2\le \ep_{feas}\)</span>, <span class="math inline">\(\ve{r_{dual}}_2\le \ep_{feas}\)</span>, and <span class="math inline">\(\wh \eta \le \ep\)</span>.</li>
</ol>
<p>(Note <span class="math inline">\(r_{cent}\)</span> means that we stick close to the central path.)</p>
<h2 id="intuitions">Intuitions</h2>
<ul>
<li>Each constraint has the force <span class="math inline">\(\rc{f_i(x)}\nb f_i(x)\)</span> and the objective force field is <span class="math inline">\(-t\nb f_0(x)\)</span>.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
