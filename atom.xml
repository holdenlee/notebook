<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2016-04-04T00:00:00Z</updated>
    <entry>
    <title>Matrix factorizations</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/matrix-factorization.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/matrix-factorization.html</id>
    <published>2016-04-04T00:00:00Z</published>
    <updated>2016-04-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Matrix factorizations</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-04 
          , Modified: 2016-04-04 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#flavors">Flavors</a></li>
 <li><a href="#previous-work">Previous work</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="flavors">Flavors</h2>
<ul>
<li><strong>Sparse coding/Dictionary learning</strong>: Write <span class="math inline">\(M=AX\)</span> where the columns of <span class="math inline">\(X\)</span> are sparse. Assume some properties on <span class="math inline">\(A\)</span> (e.g., incoherence) and on the distribution of <span class="math inline">\(X\)</span>.</li>
<li><strong>Nonnegative matrix factorization</strong>: Sparse coding with the caveat that <span class="math inline">\(A,X\)</span> are positive. Write <span class="math inline">\(M=AW\)</span> where <span class="math inline">\(A,W\)</span> have nonnegative entries. <span class="math inline">\(A\)</span> is fixed (assume some properties on <span class="math inline">\(A\)</span>) and <span class="math inline">\(W\)</span> consists of random samples. I.e., we are given many samples <span class="math inline">\(y=Ax\)</span>.</li>
<li><strong>Topic modeling</strong>: Given <span class="math inline">\(x\)</span>, we see samples from the probability vector <span class="math inline">\(Ax\)</span> (rather than <span class="math inline">\(Ax\)</span>). Learn <span class="math inline">\(A\)</span> and infer the <span class="math inline">\(x\)</span>’s. ??</li>
</ul>
<p>NMF is the least understood.</p>
<h2 id="previous-work">Previous work</h2>
<ul>
<li>NMF
<ul>
<li>Hardness: If there exists a <span class="math inline">\(n^{o(r)}\)</span> time algorithm, then there exists a <span class="math inline">\(2^{o(n)}\)</span> algorithm for 3SAT.</li>
<li>[AGKM12, M14] Given that the rank equals the nonnegative rank, there is a <span class="math inline">\(n^{O(r)}\)</span> time algorithm for NMF. (Relies on solving polynomial inequalities.)</li>
<li>[AGKM12] Under separability, NMF can be solved in polynomial time in <span class="math inline">\(n\)</span>. (Use the geometry.)</li>
<li>Inference: <a href="arora-topic-models.html">A16</a> Given <span class="math inline">\(A\)</span>, recover <span class="math inline">\(x\)</span> from <span class="math inline">\(Ax\)</span> if <span class="math inline">\(x\)</span> is sparse and <span class="math inline">\(A\)</span> has small <span class="math inline">\(\ell_\iy\to \ell_1\)</span> condition number. (“Parameter learning” is learning <span class="math inline">\(A\)</span>, “inference” is finding <span class="math inline">\(x\)</span>.)</li>
</ul></li>
<li>Topic modeling
<ul>
<li>… see p. 23 in new_thread.pdf.</li>
<li>[AGHMMSWZ12] A Practical Algorithm for Topic Modeling with Provable Guarantees:</li>
</ul></li>
<li>Sparse coding/ Dictionary learning
<ul>
<li><a href="SW08.html">SW08</a>: Algorithm for full-rank matrices (no noise).
<ul>
<li>Apply a <span class="math inline">\(\ved_0\to \ved_1\)</span> relaxation.</li>
</ul></li>
<li><a href="AGM14.html">AGM14</a>: Fixed-parameter tractable algorithm for overcomplete (incoherent) dictionaries, up to sparsity <span class="math inline">\(n^{\rc 2-\ep}\)</span>.
<ul>
<li>Use the fact that high dot product between two vectors indicates intersection of supports to reduce to a <strong>overlapping community detection</strong> problem.</li>
<li>Run <strong>SVD</strong> within communities.</li>
</ul></li>
<li><a href="AGMM15.html">AGMM15</a>: Efficient polytime algorithm for overcomplete (incoherent) dictionaries, that works up to sparsity <span class="math inline">\(\fc{\sqrt{n}}{\mu\poly\log n}\)</span>.
<ul>
<li>Initialize with <strong>SVD</strong>, noting that with good probability the <strong>intersection of supports</strong> of two vectors <span class="math inline">\(x,x'\)</span> will have one index in common.</li>
<li>Apply <strong>alternating minimization</strong> and analyze using <strong>approximate gradient descent</strong> (correlation with the right direction).</li>
<li>To get a sparse decoding, apply <strong>thresholding</strong> in the decoding step of AM.</li>
</ul></li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[A16] Provable algorithms for inference in topic models</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/arora-topic-models.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/arora-topic-models.html</id>
    <published>2016-04-04T00:00:00Z</published>
    <updated>2016-04-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[A16] Provable algorithms for inference in topic models</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-04 
          , Modified: 2016-04-04 
	</p>
      
       <p>Tags: <a href="/tags/topic%20models.html">topic models</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#model">Model</a><ul>
 <li><a href="#definitions">Definitions</a></li>
 <li><a href="#model-1">Model</a></li>
 </ul></li>
 <li><a href="#algorithm">Algorithm</a></li>
 <li><a href="#analysis">Analysis</a><ul>
 <li><a href="#thresholded-linear-inverse-algorithm">Thresholded linear inverse algorithm</a></li>
 <li><a href="#mle-estimate">MLE estimate</a></li>
 <li><a href="#sample-complexity-lower-bounds">Sample complexity lower bounds</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="model">Model</h2>
<h3 id="definitions">Definitions</h3>
<p>Define</p>
<ul>
<li>the <span class="math inline">\(\ell_1\)</span> condition number <span class="math display">\[\ka(A) = \min \set{\ka}{\forall x\in \R^k, \ve{Ax}_1\ge \rc{\ka}\ve{x}_1}\]</span></li>
<li>the <span class="math inline">\(\ell_\iy\to \ell_1\)</span> condition number <span class="math display">\[\la(A) = \min \set{\la}{\forall x\in \R^k, \ve{Ax}_1\ge \rc{\la}\ve{x}_\iy}\]</span></li>
<li>the <span class="math inline">\(\de\)</span>-biased <span class="math inline">\(\ell_\iy\to \ell_1\)</span> condition number
\begin{align}
\la_\de&amp;=\min_{|BA-I_k|\le \de, B\in \R^{k\times D}} |B|_{\iy}\\
&amp;=\max_{\ve{Ax}_1\le 1} \ve{x}_{\iy} - \de\ve{x}_1.
\end{align}
(The equality follows from a duality calculation.)</li>
<li>the restricted <span class="math inline">\(\ell_1\to \ell_1\)</span> condition number is <span class="math display">\[\ol{\ka}_r(A) = \min\set{\ol{\ka}}{\forall \ve{v}_0, \ve{Av}_1\ge \rc{\ol \ka}\ve{v}}.\]</span></li>
</ul>
<p>Note <span class="math display">\[\la_\de\le \la_0=\la \le \ka.\]</span> (To see this, take <span class="math inline">\(x=By\)</span>.)</p>
<h3 id="model-1">Model</h3>
<ul>
<li><span class="math inline">\(A\)</span> is a fixed <span class="math inline">\(n\times k\)</span> matrix with <span class="math inline">\(\de\)</span>-biased condition number <span class="math inline">\(\la_\de(A)\)</span>. (We want this to be small.)</li>
<li><span class="math inline">\(x\in \De_k\)</span> is <span class="math inline">\(r\)</span>-sparse.</li>
<li><span class="math inline">\(y\sim Ax\)</span> (<span class="math inline">\(Ax\)</span> is treated as a probability vector).</li>
</ul>
<h2 id="algorithm">Algorithm</h2>
<p>Given <span class="math inline">\(y\sim Ax\)</span> and <span class="math inline">\(A\)</span>,</p>
<ul>
<li>Thresholded linear inverse
<ul>
<li>Let <span class="math inline">\(B=\amin_{|BA-I_k|\le \de, B\in \R^{k\times D}} |B|_{\iy}\)</span> be a <span class="math inline">\(\de\)</span>-biased minimum variance inverse.</li>
<li>Compute <span class="math inline">\(\wh x = \rc n By\)</span>.</li>
<li>Let <span class="math display">\[x_i = \wh x_i (\wh x_i \ge \ub{2\la_\de(A) \sfc{\ln k}{n} + \de}{\tau}).\]</span></li>
</ul></li>
<li>TLI finds the support of <span class="math inline">\(x^*\)</span> with high probability. Now find the MLE <span class="math inline">\(x^*\)</span> given the support. (This is a convex problem.)</li>
</ul>
<h2 id="analysis">Analysis</h2>
<h3 id="thresholded-linear-inverse-algorithm">Thresholded linear inverse algorithm</h3>
<p>We have <span class="math display">\[ \E \wh x_i  =x_j^* + \sumo jk ((BA)_{ij} - \de_{i,j}) x_j^*.\]</span> <em>This is why it’s natural to consider the <span class="math inline">\(\de\)</span>-biased inverse</em>: we don’t need <span class="math inline">\(B=A^+\)</span> exactly, we can relax this to each <span class="math inline">\((BA)_{ij} - \de_{i,j}\)</span> being small. Now use Bernstein’s inequality to get concentration on the order of <span class="math inline">\(\tau\)</span>. Finally use union bound. <!--check this--></p>
<h3 id="mle-estimate">MLE estimate</h3>
<p>If</p>
<ul>
<li><span class="math inline">\(x^*\in \De_k\)</span> is <span class="math inline">\(r\)</span>-sparse</li>
<li><span class="math inline">\(x_i^*\ge \fc{\tau}r\)</span> for any <span class="math inline">\(i\in R\)</span>,</li>
<li><span class="math inline">\(A\)</span> has <span class="math inline">\(\le \ol{\ka}\)</span> restricted <span class="math inline">\(\ell_1\to \ell_1\)</span> condition number (a bound on <span class="math inline">\(\ka\)</span> is sufficient as <span class="math inline">\(\ol\ka\le \ka\)</span>)</li>
<li><span class="math inline">\(n\ge c\ol{\ka}^2 r^2 \fc{\log k}{\tau^2}\)</span></li>
</ul>
then with high probability
\begin{align}
\ve{Ax_{MLE}-Ax^*}_1 &amp;\le \wt O\pa{\sfc rn}\\
\ve{x_{MLE}-x^*}_1 &amp;\le \wt O\pa{\ol \ka \sfc rn}
\end{align}
<p>The proof is like the <a href="../../math/statistics/fisher-info.html">proof of asymptotic normality of MLE</a>, but with matrix concentration to get a finite sample bound.</p>
<h3 id="sample-complexity-lower-bounds">Sample complexity lower bounds</h3>
<p>These exist!</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 4-9-16</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-04-09.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-04-09.html</id>
    <published>2016-04-04T00:00:00Z</published>
    <updated>2016-04-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 4-9-16</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-04 
          , Modified: 2016-04-04 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Maximum entropy distributions</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/statistics/max-entropy.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/statistics/max-entropy.html</id>
    <published>2016-04-04T00:00:00Z</published>
    <updated>2016-04-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Maximum entropy distributions</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-04 
          , Modified: 2016-04-04 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#references">References</a><ul>
 <li><a href="#constraint-distribution-entropy-base-e">Constraint | Distribution | Entropy (base e)</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="references">References</h2>
<ul>
<li><a href="http://www.math.uconn.edu/~kconrad/blurbs/analysis/entropypost.pdf">Keith Conrad’s notes</a></li>
<li><a href="http://www.ski.org/Rehab/Coughlan_lab/General/TutorialsandReference/MaxEnt.pdf">ML vs. ME</a></li>
<li><a href="https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution">Wikipedia</a></li>
</ul>
<p>Many naturally occurring distributions are the maximal entropy distribution under some constraint. Here is a table.</p>
<h3 id="constraint-distribution-entropy-base-e">Constraint | Distribution | Entropy (base e)</h3>
<p>Mean <span class="math inline">\(\mu\)</span>, variance <span class="math inline">\(\si^2\)</span> | Normal<span class="math inline">\((\mu,\si^2)\)</span> | <span class="math inline">\(\rc2 (1+\ln (2\pi \si^2))\)</span> Support <span class="math inline">\([0,\iy)\)</span>, mean <span class="math inline">\(la\)</span> | Exponential <span class="math inline">\(\rc{\la} e^{-\fc x\la}\)</span> | <span class="math inline">\(1+\ln \la\)</span> <span class="math inline">\(\E X= \mu\)</span>, <span class="math inline">\(\E |X - \E X| = \la\)</span> | Laplace<span class="math inline">\((\mu,2\la^2)\)</span> | <span class="math inline">\(1+\ln(2\la)\)</span> Energy <span class="math inline">\(\sum p_iE_i = \ol E\)</span> | Boltzmann <span class="math inline">\(\Pj(i) = \fc{e^{-\be E_i}}{Z}\)</span>, <span class="math inline">\(Z=\sum_i e^{-\be E_i}\)</span> | <span class="math inline">\(\E(-\be E) - \ln Z\)</span></p>
<p>Note that in the continuous case, the Boltzmann formula encompasses everything! For example, for the normal distribution, energy is <span class="math inline">\((x-\mu)^2\)</span>.</p>
<p>A systematic way to show this is Lagrange multipliers.</p>
<p>A more elegant way is to do the following:</p>
<ul>
<li>Note that by nonnegativity of KL divergence, <span class="math display">\[\int_{\Om} p \ln p \ge \int_{\Om} p \ln q.\]</span></li>
<li>For <span class="math inline">\(F(p)\)</span> the property of the distribution you’re interested in and <span class="math inline">\(q\)</span> equal to the maximizing distribution, find that <span class="math display">\[-\int_{\Om} p \ln q = g(F(p))\]</span> for some function <span class="math inline">\(g\)</span>.</li>
<li>Conclude that if <span class="math inline">\(F(p)=F(q)\)</span> then <span class="math inline">\(-\int_{\Om} p\ln q = -\int_{\Om} q\ln q\)</span>. Hence <span class="math display">\[H(p) = -\int p\ln p \ge -\int p\ln q = -\int q\ln q = H(q).\]</span></li>
</ul>
<p>Here is an example. For <span class="math inline">\(q=\rc{\sqrt{2\pi}}e^{-\fc{x^2}{2\si^2}}\)</span>, <span class="math display">\[-\int_{\R} p \ln q \dx = \rc2 \ln(2\pi \si^2) + \int_{\R}p\cdot \rc2 \pf{x}{\si^2}^2\dx = \rc2 \ln(2\pi \si^2) + \rc{2}\fc{\Var(q)}{\si^2}.\]</span></p>
<p>Question: why is the maximum entropy distribution the best choice in statistical problems?</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Fisher information</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/statistics/fisher-info.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/statistics/fisher-info.html</id>
    <published>2016-04-04T00:00:00Z</published>
    <updated>2016-04-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Fisher information</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-04 
          , Modified: 2016-04-04 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#definitions">Definitions</a></li>
 <li><a href="#intuition">Intuition</a></li>
 <li><a href="#theorems">Theorems</a><ul>
 <li><a href="#cramer-rao">Cramer-Rao</a></li>
 <li><a href="#asymptotic-normality">Asymptotic normality</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="definitions">Definitions</h2>
Define the <strong>score</strong> and <strong>Fisher information</strong> by
\begin{align}
s(X;\te)&amp;=\pd{\ln f}{\te}\\
I(\te)&amp;=\Var_\te(s(X;\te))
\end{align}
<p>(This generalizes immediately to the multivariate case; for simplicity we consider the univariate case.)</p>
<p>The expecation of score is 0: <span class="math display">\[\E s=\int_{-\iy}^{\iy} s(X;\te) f\,dx=\int_{-\iy}^{\iy} \fc{\ln f}{f}f\dx=(\int_{-\iy}^{\iy}f\,dx)_{\te}=0.\]</span> Thus <span class="math display">\[I(\te) = \Var(s(X;\te)) = \E [(\ln f)_\te^2] = -\E((\ln f)_{\te\te}).\]</span></p>
<h2 id="intuition">Intuition</h2>
<p>Suppose a data point <span class="math inline">\(x\)</span> is observed. What is the posterior distribution on the parameter <span class="math inline">\(\te\)</span>? Consider the log of this probability, the log-likelihood. The Fisher information measures how curved the log-likelihood is at <span class="math inline">\(x\)</span>.</p>
<p>Consider the Fisher information at the MLE. If <span class="math inline">\(I(\te)\)</span> is large, then we are reasonably certain of the value of <span class="math inline">\(\te\)</span> (changing <span class="math inline">\(\te\)</span> by a bit decreases the log-probability of observing <span class="math inline">\(x\)</span> a lot).<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> If <span class="math inline">\(I(\te)\)</span> is small, then we are not certain.</p>
<h2 id="theorems">Theorems</h2>
<h3 id="cramer-rao">Cramer-Rao</h3>
<p><a href="https://en.wikipedia.org/wiki/Cramer-Rao_inequality">Wikipedia</a></p>
<p>This intuition is formalized by Cramer-Rao: the variance of any unbiased estimator for <span class="math inline">\(\te\)</span> is lower-bounded by the inverse of the Fisher information.</p>
<p><strong>Theorem (Cramer-Rao)</strong>: Suppose <span class="math inline">\(T(X)\)</span> is an unbiased estimator of <span class="math inline">\(\te\)</span>. Then <span class="math inline">\(\Var(T) \ge \rc{I(\te)}\)</span>. More generally, <span class="math inline">\(\Var(\psi(T)) \ge \fc{\psi'(\te)}{I(\te)}\)</span>.</p>
<p>In higher dimensions, <span class="math display">\[
Var(T) \succeq \pd{\psi}{\te} I^{-1}\pd{\psi}{\te}.
\]</span></p>
<p><em>Proof</em>: Suppose <span class="math inline">\(T=t(x)\)</span>. By Cauchy-Schwarz, <span class="math display">\[
\Var(T) \ge \fc{\text{Covar}(T,s_\te)^2}{\Var(s_\te)} = \fc{\int t(x)f (\ln f_\te)}{I(\te)} = \fc{\int g(x) f_{\te}(x)}{I(\te)}
=\fc{(\E g)_\te}{I(\te)}= \rc{I(\te)}.
\]</span> <!--randomized? --></p>
<h3 id="asymptotic-normality">Asymptotic normality</h3>
<!-- 3/16 p. 197-->
<p>Define the <strong>standard error</strong> by <span class="math inline">\(\se=\sqrt{\Var_\te(\wh{\te_n})}\)</span>.</p>
<p><strong>Theorem (Asymptotic normality of MLE)</strong>: <span class="math inline">\(\se\sim \sfc1{nI(\te)}\)</span> and <span class="math inline">\(\fc{\wh{\te_n}-\te}{\se}\to N(0,1)\)</span>.</p>
<p>(With a little more work, we can replace se by <span class="math inline">\(\wh{se}\)</span> (estimated standard error).)</p>
<p><em>Proof</em>: Denoting the log-likelihood by <span class="math inline">\(\ell(\te):= \ln \Pj(x^n|\te) = \sum_{i=1}^n \ln f(x_i;\te)\)</span>, linearize to find that <span class="math display">\[
\ell'(\wh \te)-\ell'(\te)\approx (\wh \te-\te)(\ell''(\te))\implies -\fc{\ell'}{\ell''}(\te)\approx \wh{\te}-\te.
\]</span> Now <span class="math display">\[
\sqrt n(\wh{\te_n}-\te)=\fc{\rc{\sqrt n}\ell'(\te)}{-\rc n\ell''(\te)}\to \fc{N(0,I(\te))}{I(\te)}\to N(0,1),
\]</span> the top in distribution, the bottom in probability. (The top uses CLT on <span class="math inline">\(\sum (\ln f)_\te\)</span>; the bottom uses LoLN on <span class="math inline">\(\sum -(\ln f)_{\te\te}\)</span>.)</p>
<!--
Define some quantities first.
\begin{df}
\begin{enumerate}
\item
\textbf{KL distance}
\[
D(f,g)=\int f(x)\ln \pf{f}{g}\,dx.
\]
Why do we care about this? Maximizing $\ell_n(\te)$ is equivalent to maximizing 
\[
M_n(\te)=\rc n\sum_i\ln \fc{f(X_i;\te)}{f(X_i;\te_*)}
\]
which has the nice property that the maximum is 0. (Without the $\rc n$ it would blow up.) By LLN the expected value of this is exactly $-D(\te_*,\te)$.
\item 
\textbf{score function} $s(X;\te)=\pd{\ln f}{\te}$.

Important property: $\E s=\int_{-\iy}^{\iy} s(X;\te) f\,dx=(\int_{-\iy}^{\iy}f\,dx)_{\te}=0$. 
\item
\textbf{Fisher information} $I(\te)=\Var_\te(s(X;\te))$, $I_n(\te)=nI(\te)$.
I.e., $I(\te)=-\E((\ln f)_{\te\te})$.
\end{enumerate}
\end{df}

\begin{enumerate}
\item
\begin{thm}[Convergence of MLE]
Suppose 
\begin{enumerate}
\item
$\sup_{\te\in \Te}|M_n(\te)-M(\te)|\xra{P}0$,
\item
for all $\ep>0$, $\sup_{|\te-\te_*|\ge\ep} M(\te)<M(\te_*)$.
\end{enumerate}
Then the MLE $\wh{\te_n}\xra P\te_*$.
\end{thm}
\begin{proof}
First show that $M(\te_*)-M(\wh{\te_n})\xra P0$. Then use continuity of $M$.
\end{proof}
\item
\begin{thm}[Asymptotic normality of MLE]
\begin{enumerate}
\item
$\se\sim \sfc1{nI(\te)}$ and $\fc{\wh{\te_n}-\te}{\se}\to N(0,1)$.
\item \fixme{$\wh{\se}=\sfc{1}{nI(\wh{\te_n})}$: why are we redefining $\wh{\se}$? We defined it a different way before. Do these definitions coincide?}
$\fc{\wh{\te_n}-\te}{\wh{\se}}\to N(0,1)$.
\end{enumerate}
\end{thm}
\begin{proof}
\begin{enumerate}
\item
Linearize to find that
\[
\ell'(\wh \te)-\ell'(\te)\approx (\wh \te-\te)(\ell''(\te))\implies -\fc{\ell'}{\ell''}(\te)\approx \wh{\te}-\te.
\]
Now
\[
\sqrt n(\wh{\te_n}-\te)=\fc{\rc{\sqrt n}\ell'(\te)}{-\rc n\ell''(\te)}\to \fc{N(0,I(\te))}{I(\te)}\to N(0,1),
\]
the top in distribution, the bottom in probability. (The top uses CLT on $\sum (\ln f)_\te$; the bottom uses LoLN on $\sum -(\ln f)_{\te\te}$.)
\item
Show that $\sfc{I(\wh{\te_n})}{I(\te)}\xra P 1$.
\end{enumerate}
\end{proof}
\item Think of this as a chain rule.
\begin{thm}
If $\tau=g(\te)$ and $g'(\te)\ne 0$, then $\fc{\wh{\tau_n}-\tau}{\wh{\se}(\wh{\tau})}\to N(0,1)$ where $\wh{\tau_n}=g(\wh{\te_n}),\wh{\se}(\wh{\tau_n})=|g'(\wh{\tau})|\wh{\se}(\wh{\tau_n})$.
\end{thm}
Proof: just expand $g$ using $g'$.
\item (Equivariance) If $\tau=g(\te)$ is 1-to-1, then $\wh{\tau_n}=g(\wh{\te_n})$. Follow definitions!
\end{enumerate}
-->
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>For sake of discussion suppose the log-likelihood function is convex in <span class="math inline">\(\te\)</span>, so there aren’t other local minima.<a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Haskell</title>
    <link href="http://holdenlee.github.io/notebook/posts/cs/functional_programming/haskell.html" />
    <id>http://holdenlee.github.io/notebook/posts/cs/functional_programming/haskell.html</id>
    <published>2016-04-03T00:00:00Z</published>
    <updated>2016-04-03T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Haskell</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-03 
          , Modified: 2016-04-03 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#parsec">Parsec</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="parsec">Parsec</h2>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">type</span> <span class="dt">LeafTree</span> <span class="fu">=</span> <span class="dt">Free</span> []

leaf <span class="fu">=</span> <span class="dt">Pure</span>
node <span class="fu">=</span> <span class="dt">Free</span>

genWord <span class="fu">=</span> many1 (noneOf <span class="st">&quot; (),\n&quot;</span>)

<span class="ot">parseExpr ::</span> <span class="dt">Parser</span> a <span class="ot">-&gt;</span> <span class="dt">Parser</span> (<span class="dt">LeafTree</span> a)
parseExpr p <span class="fu">=</span> (spaces <span class="fu">&gt;&gt;</span> (p <span class="fu">&gt;&gt;=</span> (return <span class="fu">.</span> leaf))) <span class="fu">&lt;|&gt;</span>
  <span class="kw">do</span> {
    char <span class="ch">'('</span>;
    trees <span class="ot">&lt;-</span> sepEndBy (parseExpr p) spaces;
    char <span class="ch">')'</span>;
    return <span class="fu">$</span> node trees;
  }

<span class="ot">parseLISP' ::</span> <span class="dt">Parser</span> (<span class="dt">LeafTree</span> <span class="dt">String</span>)
parseLISP' <span class="fu">=</span> parseExpr genWord

<span class="ot">parseLISP ::</span> <span class="dt">String</span> <span class="ot">-&gt;</span> <span class="dt">Maybe</span> (<span class="dt">LeafTree</span> <span class="dt">String</span>)
parseLISP <span class="fu">=</span> fromRight <span class="fu">.</span> parse parseLISP' <span class="st">&quot;&quot;</span></code></pre></div>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[AGM14] New algorithms for learning incoherent and overcomplete dictionaries</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/AGM14.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/AGM14.html</id>
    <published>2016-04-02T00:00:00Z</published>
    <updated>2016-04-02T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[AGM14] New algorithms for learning incoherent and overcomplete dictionaries</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-02 
          , Modified: 2016-04-02 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#model">Model</a></li>
 <li><a href="#theorem">Theorem</a></li>
 <li><a href="#algorithm">Algorithm</a></li>
 <li><a href="#analysis">Analysis</a><ul>
 <li><a href="#graph-construction">Graph construction</a></li>
 <li><a href="#overlapping-communities">Overlapping communities</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <p>Back to <a href="matrix-factorization.html">Matrix factorization</a></p>
<h2 id="model">Model</h2>
<ul>
<li><span class="math inline">\(A\)</span> is a <span class="math inline">\(n\times m\)</span> matrix.</li>
<li><span class="math inline">\(\E X_i=0\)</span> and <span class="math inline">\(X_i\in [-C,1]\cup [1,C]\)</span> when <span class="math inline">\(X_i\ne 0\)</span>.</li>
<li><span class="math inline">\(X_i\)</span> is independent conditioned on <span class="math inline">\(X_i\ne 0\)</span> for <span class="math inline">\(i\in S\)</span>.</li>
<li>“Bounded <span class="math inline">\(l\)</span>th moments” for some <span class="math inline">\(l\)</span>: For <span class="math inline">\(|S|=l\)</span>, <span class="math inline">\(\Pj(\forall i\in S,X_i\ne 0)\le c^l \prod_{i\in S} \Pj(X_i\ne 0)\)</span>.</li>
</ul>
<h2 id="theorem">Theorem</h2>
<ul>
<li>The algorithm does the following: If the sparsity is <span class="math inline">\(k\le c\min\pa{m^{\fc 25}, \fc{\sqrt n}{\mu\log n}}\)</span> and <span class="math inline">\(X\)</span> has bounded 3-wise moments, taking <span class="math inline">\(p_1=\poly(m,\rc k, \log\prc{\ep})\)</span> samples and time <span class="math inline">\(\wt O(p_1^2n)\)</span>, w.h.p. the algorithm recovers <span class="math inline">\(A\)</span> up to <span class="math inline">\(\ep\)</span> column-wise error.</li>
<li>Same is true for <span class="math inline">\(m^{\fc 25}\)</span> replaced by <span class="math inline">\(m^{\fc{l-1}{2l-1}}\)</span> and <span class="math inline">\(c\)</span> replaced by <span class="math inline">\(c_l\)</span>.</li>
<li>The same is true when the samples are subject to noise: <span class="math inline">\(y=Ax+n\)</span> where <span class="math inline">\(n\)</span> is independent Gaussian, <span class="math inline">\(\si=o(\sqrt n)\)</span>. The sample complexity becomes <span class="math inline">\(\poly(m^l,\rc{k^l}, \log\prc{\ep})\poly\prc{\si^2}{\ep^2}\)</span>.</li>
</ul>
<h2 id="algorithm">Algorithm</h2>
<ul>
<li>Create a graph with an edge between <span class="math inline">\(i,j\)</span> if <span class="math inline">\(|\an{y^i, y^j}|&gt;\rc2\)</span>.</li>
<li>Run the following <strong>overlapping community detection</strong> algorithm.
<ul>
<li>Repeat <span class="math inline">\(\Om(m^2\log m)\)</span> times.
<ul>
<li>Take an edge <span class="math inline">\(uv\)</span>.</li>
<li>Count the number of vertices that are connected to both <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span>.</li>
<li>Let <span class="math inline">\(T=\fc{k/(2m)}{5} = \fc{k}{10m}\)</span>. If the number of common neighbors is <span class="math inline">\(\ge T\)</span>, add to <span class="math inline">\(S_{uv}\)</span>. (I.e., <span class="math inline">\(S_{uv} = \set{w}{|\Ga(u)\cap \Ga(v)\cap \Ga(w)|\ge T}\cup \{u,v\}\)</span>.</li>
</ul></li>
<li>Take the minimal sets (with respect to set inclusion). These will be (whp) be the collection <span class="math inline">\(\{C_i\}\)</span> where <span class="math inline">\(C_i\)</span> consists of the vectors having <span class="math inline">\(i\)</span> in the support.</li>
</ul></li>
<li>To estimate <span class="math inline">\(A_i\)</span>, find the largest singular value of <span class="math inline">\(\wh{\Si}_i = \rc{|C_i|} \sum_{y\in C_i} yy^T\)</span>. (Alternatively, run the “Overlapping average” algorithm, which is not SVD but is similar in spirit.)</li>
<li>Iteratively improve the estimate. (Wrks when <span class="math inline">\(Y=AX\)</span> exactly. Omitted here.)</li>
</ul>
<p>For the <span class="math inline">\(m^{\fc{l-1}{2l-1}}\)</span> bound, look at common neighbors of <span class="math inline">\(l\)</span>-tuple of samples.</p>
<h2 id="analysis">Analysis</h2>
<h3 id="graph-construction">Graph construction</h3>
<p>We have <span class="math inline">\(\an{Y^{(i)}, Y^{(j)}} = \sum \an{A_p,A_q} X_p^{(i)}X_q^{(j)}\)</span>.</p>
<p>We want</p>
<ul>
<li>Completeness (if <span class="math inline">\(\Supp(Y^i)\cap \Supp(Y^j)=\{i\}\)</span>, then with high probability <span class="math inline">\(\an{Y^i,Y^j}&gt;\rc2\)</span>).</li>
<li>Soundness (if <span class="math inline">\(\Supp(Y^i)\cap \Supp(Y^j)=\phi\)</span> then whp <span class="math inline">\(\an{Y^i,Y^j}&lt;\rc2\)</span>.</li>
</ul>
Both come from the same matrix concentration bound. We show soundness. Condition on the supports. Let <span class="math inline">\(S_i = \Supp(X^i)\)</span>. Write
\begin{align}
\an{Y^i,Y^j} &amp;= X^T M X\\
M_{(S_1\cup S_2)^2} :&amp;= \matt 0{\rc 2N^T}{\rc 2N}0\\
N :&amp;= (A^TA)_{S_1\times S_2}.
\end{align}
<p>Now use Hanson-Wright to show <span class="math inline">\(X^TMX\approx \Tr(M)\)</span> with high probability.</p>
<h3 id="overlapping-communities">Overlapping communities</h3>
<p>We need to show</p>
<ul>
<li>if <span class="math inline">\(\Om^i\cap \Om^j\cap \Om^k\ne \phi\)</span> then likely <span class="math inline">\(i,j,k\)</span> have many common neighbors</li>
<li>if <span class="math inline">\(\Om^i\cap \Om^j\cap \Om^k=\phi\)</span>, then likely <span class="math inline">\(i,j,k\)</span> have few common neighbors</li>
</ul>
<p>and find the right threshold.</p>
<ul>
<li>If <span class="math inline">\(\Om^i\cap \Om^j\cap \Om^k\ne \phi\)</span> then <span class="math inline">\(\Pj_y(\forall j=1,2,3, |\an{y,y^j}|&gt;\rc 2)\ge \fc{k}{2m}\)</span>.</li>
<li>If <span class="math inline">\(\Om^i\cap \Om^j\cap \Om^k\ne \phi\)</span> then letting <span class="math inline">\(a=|\Om^1\cap \Om^2|\)</span> and similarly for <span class="math inline">\(b,c\)</span>, <span class="math display">\[\Pj_y(\forall j=1,2,3, |\an{y,y^j}|&gt;\rc 2|\Om^{(1)}, \Om^{(2)}, \Om^{(3)})\le \fc{k^6}{m^3} + \fc{3k^3(a+b+c)}{m^2}.\]</span>
<ul>
<li><span class="math inline">\(\pf{k^2}{m}^3\)</span> comes from intersecting each set separately.</li>
<li><span class="math inline">\(\fc{3k^3(a+b+c)}{m^2}\)</span> comes from intersecting two together and the other separately.</li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>LCC lower bounds by geometry</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/coding/lcc_geometry.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/coding/lcc_geometry.html</id>
    <published>2016-04-02T00:00:00Z</published>
    <updated>2016-04-02T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>LCC lower bounds by geometry</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-02 
          , Modified: 2016-04-02 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#general-plan">General plan</a></li>
 <li><a href="#convex-hull-of-image-is-large">Convex hull of image is large</a></li>
 <li><a href="#image-close-to-convex">Image close to convex</a></li>
 <li><a href="#volume-of-image-small">Volume of image small</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="general-plan">General plan</h2>
<p>See <a href="ldc.html">LDCs</a> for basics.</p>
<p>Suppose the LCC is large.</p>
<ul>
<li>The image of the decoding map <span class="math inline">\(D:[-1,1]^n\to [0,1]^n\)</span> contains many points that are far apart, corresponding to the codewords. This gives that the convex hull of the image has large volume or a large <span class="math inline">\(\ep\)</span>-net.</li>
<li>The image is close to convex, so the image is also large.</li>
<li>However, the Jacobian of the map is small, so the volume of the image is small, contradiction.</li>
</ul>
<h2 id="convex-hull-of-image-is-large">Convex hull of image is large</h2>
<p>?</p>
<p>Intuitively this can work: consider a simplexification; if each simplex is large the volume grows rapidly in terms of the number of vertices.</p>
<h2 id="image-close-to-convex">Image close to convex</h2>
<p>The idea is from <a href="Bar13.html">Bar13</a>.</p>
<p>We modify it to give a bound on <span class="math inline">\(L^2\)</span> distance instead of KL-divergence, and to look at the image of the unit ball instead. (Normalize the domain by <span class="math inline">\(\rc{\sqrt{n}}\)</span>.) The image is of size on the order <span class="math inline">\(\sqrt{n}\)</span>, and we get <span class="math inline">\(\ep\sqrt{n}\)</span> approximations.</p>
<p><em>Problem</em>: Is this enough?</p>
<ul>
<li>This compares the volume of the convex hull to the volume of the <span class="math inline">\(\ep\)</span>-expanded set, which may be much larger than the volume of the set.</li>
<li>To show it’s not much larger, it seems necessary to show the surface area is small, and inductively the surface area in each dimension is small.</li>
<li>If we use “<span class="math inline">\(\ep\)</span>-net size” instead of volume, we don’t have this problem—but then we have to bound the size of an <span class="math inline">\(\ep\)</span>-net of the convex hull in the first step.</li>
</ul>
<h2 id="volume-of-image-small">Volume of image small</h2>
<p>For <span class="math inline">\(q=2\)</span>, get <span class="math inline">\(\pf{C}{\sqrt n}^n\)</span>.</p>
<p>For <span class="math inline">\(q&gt;2\)</span>, use the matrix Efron-Stein inequality.</p>
<p>We might want to look at surface areas, etc. I think 1-dimensional length between code-words gives a ECC condition.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[Bar13] Convexity of the image of a quadratic map via the relative entropy distance</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/coding/Bar13.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/coding/Bar13.html</id>
    <published>2016-04-02T00:00:00Z</published>
    <updated>2016-04-02T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[Bar13] Convexity of the image of a quadratic map via the relative entropy distance</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-02 
          , Modified: 2016-04-02 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#dines-theorem">Dines Theorem</a></li>
 <li><a href="#proof-sketch">Proof sketch</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><strong>Theorem</strong>: Let <span class="math inline">\(q_1,\ldots, q_k:\R^n\to \R\)</span> be positive definite quadratic forms, <span class="math inline">\(\psi = (q_1,\ldots, q_k)\)</span>. Let <span class="math inline">\(a\in \conv(\psi(\R^n))\cap \De\)</span>, where <span class="math inline">\(\De\)</span> is the probability simplex. There exists <span class="math inline">\(b\in \psi(\R^n)\cap \De\)</span> that is at most 4.8 away in KL divergence.</p>
<h2 id="dines-theorem">Dines Theorem</h2>
<p>(Just for some motivation here)</p>
<p>When <span class="math inline">\(k=2\)</span>, <span class="math inline">\(\psi(\R^n)\)</span> is convex. Proof: When system of linear equations in <span class="math inline">\(X\succeq 0\)</span> have a solution, and the number of equations is small, there is in fact a low-rank solution. Here, look for a rank 1 solution.</p>
<h2 id="proof-sketch">Proof sketch</h2>
<p>Note KL-divergence is easier to deal with—the logs mean that scaling comes out as constants—and is stronger than <span class="math inline">\(L^p\)</span> norms.</p>
<ol type="1">
<li>Write down what it means for <span class="math inline">\(a\)</span> to be in the convex hull. This means there is <span class="math inline">\(\ga_1,\ldots, \ga_n\)</span>, <span class="math inline">\(x_1,\ldots, x_n\)</span> such that <span class="math display">\[a_i = \sum_j \ga_j \an{Q_i, x_jx_j^T}, \quad \sum\ga_j=1.\]</span> Let <span class="math inline">\(X=\sum_j \ga_j x_jx_j^T\)</span>. This is <span class="math display">\[a_i = \an{Q_i, X}.\]</span> Let <span class="math inline">\(b=\psi(x)\)</span> be the approximation we want to find. Then the KL divergence is <span class="math display">\[\sum_i a_i\ln \fc{a_i}{q_i(x)},\]</span> so we want to maximize <span class="math display">\[\sum_i a_i \ln q_i(x).\]</span> We want a bound that looks like something like the following <span class="math display">\[\sum_i a_i \ln q_i(x)\ge \pa{\max_{X\succeq 0} \sumo ik a_i\ln a_i} - \be. \]</span> However, we need to make sure we normalize everything consistently. A simple way to do this is to use a similarity transformation to make <span class="math inline">\(\sum Q_i = I\)</span>, possible since the <span class="math inline">\(Q_i\)</span> are positive definite. Then the condition <span class="math inline">\(\sum a_i=1\)</span> becomes <span class="math inline">\(1=\sum_i \an{Q_i, X}= \Tr(X)\)</span>. We maximize over all <span class="math inline">\(X\)</span> satisfying this. We would like the <span class="math inline">\(b=\psi(x)\)</span> we get to have <span class="math inline">\(\sum b_i=x^Tx = 1\)</span> as well. Thus, we would like to show (Theorem 3.2) <span class="math display">\[\max_{x\in \bS^{n-1}} \sum_i a_i \ln q_i(x)\ge \pa{\max_{X\succeq 0, \Tr(X)=1} \sumo ik a_i\ln \an{Q_i,X}} - \be.\]</span></li>
<li><p>Prove Theorem 3.2 by SDP rounding. Let <span class="math inline">\(X\)</span> be the optimal solution, write <span class="math inline">\(X=T^2\)</span>. In SDP rounding, take <span class="math inline">\(x\sim N(0,I)\)</span>, <span class="math inline">\(y=Tx\)</span>, and hope that <span class="math inline">\(y\)</span> is a good solution to the original problem. (If <span class="math inline">\(X=xx^T\)</span>, then <span class="math inline">\(T\)</span> would be a projection.) We use the Quadratic Sampling Lemma—the expectation of <span class="math inline">\(q(y)\)</span> for any quadratic <span class="math inline">\(q\)</span> is the same as the <span class="math inline">\(\an{Q,X}\)</span>.</p>
<p>Thus <span class="math inline">\(\E q_i(y) = \an{Q_i,X}=1\)</span>.</p>
Now do some calculations (Lemma 2.1). Calculate that
\begin{align}
\E |\ln q_i(y)| &amp; \le C_1 \implies \E\ab{\sumo ik a_i \ln q_i(y)}\le C\\
\Pj( \ve{y}^2 \ge C_2) &amp; \le \ep_3
\end{align}
<p>for some <span class="math inline">\(C_1,C_2,\ep_3\)</span>. Use Markov’s inequality on the first inequality. Find there is a point with <span class="math inline">\(\ve{y}^2&lt;C_2\)</span>, <span class="math inline">\(\sumo ik a_i\ln q_i(y)&gt;-C_4\)</span>. Let <span class="math inline">\(y' = \wh y\)</span> be the solution.</p></li>
</ol>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Goemans-Williamson</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/algorithms/gw.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/algorithms/gw.html</id>
    <published>2016-04-02T00:00:00Z</published>
    <updated>2016-04-02T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Goemans-Williamson</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-02 
          , Modified: 2016-04-02 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <p>See also <a href="../complexity/sos.html">SoS</a>.</p>
<p>(From Algorithms 15)</p>
<p>Relaxing a combinatorial optimization to a SDP: let <span class="math inline">\(x_i\)</span> be unit vectors instead of <span class="math inline">\(-1,1\)</span>. Recover by partitioning with a random hyperplane. Ex. for min-cut, <span class="math inline">\(\sum_{\{i,j\}\in E}\rc 4|v_i-v_j|^2\)</span>.</p>
<p>We find <span class="math display">\[
\E(\text{cut edges})=\sum_{ij\in E}\fc{\te_{ij}}{\pi}\ge\ub{\min_{\te\in [0,\pi]}\fc{2\te}{\pi(1-\cos\te)}}{\approx .878} \sum_{ij\in E} \rc2(1-\cos\te_{ij}).
\]</span></p>
For MAX-2SAT, have a dummy vector <span class="math inline">\(u_0\)</span> for true (so that variable assignments correspond to <span class="math inline">\(u_i=\pm u_0\)</span>), maximize
\begin{align}
OPT=\sum1-\rc4(u_0-u_{k1})(u_0-u_{k2})&amp;=\sum \rc 4(1+u_0\cdot u_{k1})+\rc 4(1+u_0\cdot u_{k2})+\rc 4(1 - u_{k1}\cdot u_{k2})\\
&amp;=\sum \rc4(1 +\cos \fc{\te_{0,k1}}{\pi})+\cdots \\
\E&amp;=\sum\rc 4(1-\fc{\te_{0,k1}}{\pi})+\cdots\\
&amp;\ge \min_{\te\in[0,\pi]}\fc{2(\pi-\te)}{\pi(1+\cos\te)}\cdot OPT,
\end{align}
<p>so we get .878-approximation.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
