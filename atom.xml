<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2016-12-14T00:00:00Z</updated>
    <entry>
    <title>Linear convex regulator</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/lcr.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/lcr.html</id>
    <published>2016-12-14T00:00:00Z</published>
    <updated>2016-12-14T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Linear convex regulator</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-12-14 
          , Modified: 2016-12-14 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#linear-quadratic-regulator">Linear quadratic regulator</a></li>
 <li><a href="#linear-convex-regulator">Linear convex regulator</a></li>
 <li><a href="#problem">Problem</a></li>
 <li><a href="#email">Email</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>Project evolved from <a href="continuous.html">Continuous MDP</a>.</p>
<h2 id="linear-quadratic-regulator">Linear quadratic regulator</h2>
The linear quadratic regulator is
<span class="math display">\[\begin{align}
\dot x &amp; = Ax + Ba = f(x,a)\\
r(x,a) &amp; = -c(x,a) = -\rc2 x^TQx - \rc 2 a^Ta\\
\pat{Total reward} &amp;= \int_0^T r(x(t), a(t))\,dt.
\end{align}\]</span>
<p>The optimal action (for fixed horizon <span class="math inline">\(T\)</span>, take <span class="math inline">\(T= \iy\)</span> to get the infinite-horizon case) is given by <span class="math display">\[
a = -P_{t}x
\]</span> where <span class="math inline">\(P_{t}\)</span> satisfies the matrix Riccati equation. For the infinite horizon case, take <span class="math inline">\(T=\iy\)</span> and find that <span class="math inline">\(a=-Px\)</span> where <!-- , and $P_{-\iy}$ is the steady-state solution, which can be solved by solving --> <span class="math inline">\(P\)</span> is the solution to an algebraic Riccati equation, which has an expression in terms of eigenvalues of <span class="math inline">\(\matt{A}{BB^T}{Q}{-A^T}\)</span>.</p>
<h2 id="linear-convex-regulator">Linear convex regulator</h2>
Generalize this to
<span class="math display">\[\begin{align}
\dot x &amp; = Ax + Ba = f(x,a)\\
r(x,a) &amp; = -c(x,a) = -g(x) - a^Ta\\
\pat{Total reward} &amp;= \int_0^T r(x(t), a(t))\,dt.
\end{align}\]</span>
<p>For example, take <span class="math inline">\(g\)</span> to be a convex function.</p>
<p>Idea: Approximate <span class="math inline">\(g\)</span> locally with a quadratic function and then make the action that optimizes the linear quadratic system.</p>
<h2 id="problem">Problem</h2>
The starting point is the linear quadratic regulator, which is well-understood. The linear quadratic regulator is (here <span class="math inline">\(x\in \mathbb R^n, a\in \mathbb R^k\)</span>)
<span class="math display">\[\begin{align}
\dot x &amp; = Ax + Ba = f(x,a)\\
r(x,a) &amp; = -c(x,a) = -\frac12 x^TQx - \rc2 a^Ta\\
x(0) &amp;= x_0\\
V_t(x,a(\cdot)) &amp;= \int_t^T r(x(t), a(t))\,dt, \quad x(t) = x, \dot x = f(x,a).
\end{align}\]</span>
<p>The problem to find the function <span class="math inline">\(a(t)\)</span> that maximizes <span class="math inline">\(V_{t}(x,a)\)</span>. Especially we care about when <span class="math inline">\(T\to \iy\)</span>. There are two ways to solve this problem:</p>
<ol type="1">
<li><p>Use Pontryagrin’s maximal principle to write this as a Hamiltonian system. One can show the optimal control has the form <span class="math inline">\(a(t) = -P(t)x(t)\)</span>. Get a differential equation in <span class="math inline">\(P(t)\)</span> which is the matrix Riccati equation.</p>
For infinite-horizon, the optimal control is <span class="math inline">\(a= -Px\)</span> independent of time. (Reparameterizing <span class="math inline">\([0,\iy)\)</span> as <span class="math inline">\((-\iy, 0]\)</span>, the solution <span class="math inline">\(P\)</span> is the limit <span class="math inline">\(\lim_{t\to \iy}P(-t)\)</span>. <!-- can be solved for explicitly. Then $a=-Px$ is the optimal action when the time horizon is infinite.--></li>
<li><p>Use the Bellman-Jacobi-Hamilton equation. Let <span class="math inline">\(v(x,t) = \max_{a(\cdot)} V_{t}(x,a(\cdot))\)</span>. Then <span class="math inline">\(v\)</span> satisfies <span class="math inline">\(v_t + \max_a [r(x,a) + (\nabla_x v)^T f(x,a)]=0\)</span>. <span class="math inline">\(v(x,t)=-x^TK(t)x\)</span> is quadratic in <span class="math inline">\(x\)</span>, <span class="math inline">\(a\)</span> is linear in <span class="math inline">\(x\)</span>, and we also get a matrix Riccati equation in <span class="math inline">\(K\)</span>.</p>
<p>For infinite-horizon, reparameterizing <span class="math inline">\([0,\iy)\)</span> as <span class="math inline">\((-\iy, 0]\)</span>, the solution <span class="math inline">\(v\)</span> is a steady-state solution, <span class="math inline">\(\max_a [r(x,a) + (\nabla_x v)^T f(x,a)]=0\)</span>.</p></li>
</ol>
<p>We want to generalize <span class="math inline">\(\rc 2 x^TQx\)</span>, e.g. to <span class="math inline">\(g(x)\)</span>. Suppose that <span class="math inline">\(g\)</span> is convex. In general it’s infeasible to solve for the optimal action (you can only numerically approximate the solution, which is infeasible in high dimension); the question is whether we can find a simple policy <span class="math inline">\(a\)</span> that approximates the optimal action (e.g. up to a constant factor depending on properties of <span class="math inline">\(g\)</span>). For example <span class="math inline">\(a(x)\)</span> could be chosen based on a quadratic approximation of <span class="math inline">\(g\)</span>. For a particular choice of <span class="math inline">\(a(x)\)</span>, the value satisfies <span class="math display">\[ -g(x) - \rc2 a^Ta + (\nb_x v)^T f(x,a) = 0
\]</span> and for the optimal choice <span class="math inline">\(a(x) = B^T(\nb_x v)\)</span> the value satisfies <span class="math display">\[
-g(x) + \rc 2 (\nb_x v^*)^T BB^T (\nb_x v^*) + (\nb_x v^*)^T Ax=0
\]</span> We’d have to relate <span class="math inline">\(v\)</span> and <span class="math inline">\(v^*\)</span> in some way.</p>
<h2 id="email">Email</h2>
<p>Hi …,</p>
<p>I’m trying to develop a way to approximate an optimal control for some general class of control problems. This involves understanding some differential equations. I don’t really have intuition or have a good idea of whether what we’re trying to do is feasible. Would you have time to talk? Just getting some qualitative understanding or knowing what tools are available would be great!</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2016-12-10</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-12-10.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-12-10.html</id>
    <published>2016-12-10T00:00:00Z</published>
    <updated>2016-12-10T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2016-12-10</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-12-10 
          , Modified: 2016-12-10 
	</p>
      
       <p>Tags: <a href="/tags/reinforcement%20learning.html">reinforcement learning</a>, <a href="/tags/control%20theory.html">control theory</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#rl-project">RL project</a></li>
 <li><a href="#alexanlp-experiments">Alexa/NLP experiments</a></li>
 <li><a href="#directions">Directions</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>See also <a href="2016-12-03.html">last week</a> for unfinished threads.</p>
<h2 id="rl-project">RL project</h2>
<ul>
<li>Learned <a href="../tcs/machine_learning/reinforcement_learning/control_theory.html">control theory</a>. (Ch. 1-5)</li>
<li>Wrote up
<ul>
<li><a href="https://onedrive.live.com/view.aspx?cid=5f18ce7c7533d083&amp;id=documents&amp;resid=5F18CE7C7533D083%21465&amp;app=OneNote&amp;authkey=!ADwaKCUz_Q1nRSM&amp;&amp;wd=target%28%2F%2FResearch.one%7C3dc2fa9e-847a-4ff2-ba10-094a253311df%2FLinear%20quadratic%20regulator%7Cd61f5b02-9dac-4a89-b019-11c61dc2baa2%2F%29">Discrete LQR</a></li>
<li><a href="../tcs/machine_learning/reinforcement_learning/lqr.html">Continuous LQR</a></li>
</ul></li>
</ul>
<h2 id="alexanlp-experiments">Alexa/NLP experiments</h2>
<ul>
<li><a href="https://workflowy.com/#/ca56745b6f0e">Minutes</a></li>
<li><a href="../tcs/machine_learning/nlp/emnlp.html">EMNLP</a></li>
</ul>
<h2 id="directions">Directions</h2>
<ul>
<li>Learning the LQR: Suppose we get a noisy observation of reward, and possibly there is noise in the dynamical system. We don’t know the parameters (matrices). Can we “PAC learn” the optimal policy?
<ul>
<li>Have to define “PAC learn” in this context.</li>
<li>I think so. cf. Contextual bandits, UCRL, LinUCB. There will be a lot of details to work out and a lot of possible ways to state a result. The question is whether this is actually valuable to work on.</li>
<li>This is more the optimization standpoint, not the “existence of good solution” standpoint.</li>
</ul></li>
<li>Replace the quadratic function by a convex function <span class="math inline">\(f\)</span>.
<ul>
<li>Suppose we know the dynamics and <span class="math inline">\(f\)</span>. Approximate the optimal policy. (Ex. Find a simple class of functions such that optimizing within this class will give an approximate solution.)</li>
<li>So here we just care about “existence of good solution” (in a convenient class).</li>
<li>Start with linear controls first. How do they do?</li>
<li>Optimization standpoint: given a (nice, ex. linear) class of functions (for value or for policy) find the optimal within that class knowing dynamics including <span class="math inline">\(f\)</span>.</li>
</ul></li>
<li>Combine the previous two: don’t know dynamics or <span class="math inline">\(f\)</span>, and learn it!</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Linear quadratic regulator</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/lqr.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/lqr.html</id>
    <published>2016-12-09T00:00:00Z</published>
    <updated>2016-12-09T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Linear quadratic regulator</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-12-09 
          , Modified: 2016-12-09 
	</p>
      
       <p>Tags: <a href="/tags/control%20theory.html">control theory</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#solve-the-hamiltonian">Solve the Hamiltonian</a></li>
 <li><a href="#the-algebraic-riccati-equation">The algebraic Riccati equation</a></li>
 <li><a href="#solve-the-riccati-equation">Solve the Riccati equation</a></li>
 <li><a href="#choose-the-stable-solution">Choose the stable solution</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>We derive the solution to the linear quadratic regulator problem in the continuous, infinite-horizon setting. <em>Handwavy parts are in italics</em>.</p>
<p>References: Ch. 5 of <a href="https://math.berkeley.edu/~evans/control.course.pdf">course notes</a> and <a href="http://www.kybernetika.cz/content/1973/1/42/paper.pdf">Riccati equation paper</a>. Wikipedia on <a href="https://en.wikipedia.org/wiki/Linear%E2%80%93quadratic_regulator">LQR</a> and <a href="https://en.wikipedia.org/wiki/Algebraic_Riccati_equation">Riccati</a>.</p>
<p>The problem:</p>
<span class="math display">\[\begin{align}
\dot x &amp; = Ax + Ba = f(x,a)\\
r(x,a) &amp; = -c(x,a) = -\rc2 x^TQx - a^Ta\\
\pat{Total reward} &amp;= \iiy r(x(t), a(t))\,dt.
\end{align}\]</span>
<p>Here <span class="math inline">\(Q\)</span> is positive definite. Write <span class="math inline">\(Q=C^TC\)</span>.</p>
<h2 id="solve-the-hamiltonian">Solve the Hamiltonian</h2>
We write down the corresponding Hamiltonian system and then solve it. (See <a href="control_theory.html">Control theory</a> Section 5.)
<span class="math display">\[\begin{align}
H(x,a) &amp;= f(x,a)^T p + r(x,a) = f(x,a)^T p - \rc2 x^TQx - a^Ta\\
\dot x &amp;= \nb_p H(x(t),p(t),a(t)) = Ax + Ba \\
\dot p &amp;= -\nb_x H(x(t),p(t),a(t)) = -A^T p + Qx\\
a &amp;= \amax_a H(x(t),p(t),a(t))\\
\iff 0 &amp;= \nb_a H(x(t),p(t),a(t))\\
\iff B^T p - a &amp;=0\\
\iff a &amp;= B^T p
\end{align}\]</span>
We get
<span class="math display">\[\begin{align}
\dot x &amp;= Ax + BB^Tp\\
\dot p &amp;= -Qx + A^Tp\\
\ddd t \coltwo xp &amp;= \matt{A}{BB^T}{-Q}{A^T} \coltwo xp.
\end{align}\]</span>
<h2 id="the-algebraic-riccati-equation">The algebraic Riccati equation</h2>
<p><em>Look for a solution in the form <span class="math inline">\(p=-Px\)</span>.</em> (Note that in the finite-horizon setting, solving the Hamiltonian system with the appropriate boundary condition <span class="math inline">\(p(T)=0\)</span> gives that the solution is <span class="math inline">\(p=-P(t)x\)</span> where <span class="math inline">\(P(t)\)</span> satisfies a differential equation called the Matrix Riccati differential equation. In the infinite-horizon setting, <span class="math inline">\(P\)</span> doesn’t change with time and we get a single <span class="math inline">\(P\)</span>.)</p>
Substituting <span class="math inline">\(p=-Px\)</span>, the system becomes
<span class="math display">\[\begin{align}
-(PA - PBB^TP) x &amp;= -P \dot x = \dot p = (Q+A^TP)x\\
-PBB^TP + PA + A^TP + Q &amp;= 0 
\end{align}\]</span>
<p>This is called the <a href="https://en.wikipedia.org/wiki/Algebraic_Riccati_equation">algebraic Riccati equation</a>.</p>
<h2 id="solve-the-riccati-equation">Solve the Riccati equation</h2>
Write the Riccati equation as (set <span class="math inline">\(Q=C^TC\)</span>) <span class="math display">\[
(-P\; I) \matt{A}{-BB^T}{-CC^T}{-A^T}\coltwo IP = 0
\]</span> Let <span class="math inline">\(K=A-BB^TP\)</span>. This is <span class="math display">\[
\ub{\matt{A}{-BB^T}{-C^TC}{-A^T}}M\coltwo IP = \coltwo K{PK} = \coltwo IP K.
\]</span> <em>Assume <span class="math inline">\(K\)</span> is diagonalizable</em>. Write <span class="math inline">\(K=XDX^{-1}\)</span>.
<span class="math display">\[\begin{align}
M\coltwo IP &amp;= \coltwo X{PX} DX^{-1}\\
M\coltwo{X}{PX} &amp;= \coltwo{X}{PX}D.
\end{align}\]</span>
<p>Hence the columns of <span class="math inline">\(\coltwo{X}{PX}\)</span> are eigenvalues of <span class="math inline">\(M\)</span>.</p>
<p>So the <span class="math inline">\(P\)</span> that solve the algebraic Riccati equation are <span class="math inline">\(P=YX^{-1}\)</span> where the columns of <span class="math inline">\(\coltwo XY\)</span> are <span class="math inline">\(n\)</span> of the eigenvectors of <span class="math inline">\(M\)</span>.</p>
<h2 id="choose-the-stable-solution">Choose the stable solution</h2>
<p>First some definitions.</p>
<ul>
<li><span class="math inline">\(\la\)</span> is <strong>stable</strong> if <span class="math inline">\(\Re \la&lt;0\)</span>. <span class="math inline">\(A\)</span> is stable if all eigenvalues of <span class="math inline">\(A\)</span> are stable. (This means that any solution to <span class="math inline">\(\dot x = Ax\)</span> has <span class="math inline">\(x(t)\to 0\)</span>.)</li>
<li><span class="math inline">\(\la\)</span> is a <strong>controllable</strong> eigenvalue for <span class="math inline">\((A,B)\)</span> if for all row eigenvectors of <span class="math inline">\(A\)</span> corresponding to <span class="math inline">\(\la\)</span> (<span class="math inline">\(wA=\la w, w\ne 0\)</span>), <span class="math inline">\(wB\ne 0\)</span>. <span class="math inline">\((A,B)\)</span> is controllable if all its eigenvalues are controllable. (Equivalently, <span class="math inline">\((A^T,B^T)\)</span> is observable.)</li>
<li><span class="math inline">\((A,C)\)</span> is <strong>observable</strong> if for all (column) eigenvectors of <span class="math inline">\(A\)</span>, <span class="math inline">\(Cz\ne 0\)</span>.</li>
<li><span class="math inline">\((A,B)\)</span> is <strong>stabilizable</strong> if every unstable eigenvalue is controllable. (This is true for e.g. random matrices, because w.p. 1 a random vector is not in the left nullspace of <span class="math inline">\(B\)</span>.) Equivalently, there exists <span class="math inline">\(L\)</span> (with real entries) such that <span class="math inline">\(A+BL\)</span> is stable.</li>
</ul>
<p><strong>Theorem</strong>. The following are equivalent.</p>
<ol type="1">
<li>There exists a stable solution <span class="math inline">\(P\)</span> to the algebraic Riccati equation, giving the solution <span class="math inline">\(p = -Px\)</span>, <span class="math inline">\(\boxed{a = B^Tp = -B^TPx}\)</span>.</li>
<li>All eigenvalues satisfy <span class="math inline">\(\Re \la \ne 0\)</span> and <span class="math inline">\((A,B)\)</span> is stabilizable.</li>
</ol>
<p>The stable solution is unique.</p>
<p><em>Proof</em>.</p>
<p>From the previous section, a solution <span class="math inline">\(P\)</span> has to be in the form <span class="math inline">\(Y^{-1}X\)</span> where <span class="math inline">\(X, Y\)</span> are such that <span class="math inline">\(\coltwo XY\)</span> are the eigenvectors corresponding to the stable eigenvalues. Note that the eigenvectors come in <span class="math inline">\(\pm\)</span> pairs because letting <span class="math inline">\(J=\matt O{-I}{I}O\)</span>, <span class="math inline">\(M^T = J M J\)</span>, <span class="math inline">\(M^T \coltwo{-y}x = -JM\coltwo xy\)</span>. (<span class="math inline">\(M\)</span> is a <a href="https://en.wikipedia.org/wiki/Hamiltonian_matrix">Hamiltonian matrix</a>, i.e. <span class="math inline">\(JM\)</span> is symmetric.) So exactly <span class="math inline">\(n\)</span> of the <span class="math inline">\(2n\)</span> eigenvalues are stable, which is the unique choice for <span class="math inline">\(\coltwo XY\)</span> up to permutation.</p>
<ol type="1">
<li><span class="math inline">\(\Rightarrow\)</span>: Take <span class="math inline">\(L=-B^TP\)</span>. Then <span class="math inline">\(A+B(-B^TP)=K\)</span> is stable. (Its eigenvalues are exactly the eigenvalues of <span class="math inline">\(M\)</span>.)</li>
<li><span class="math inline">\(\Leftarrow\)</span>: We just have to check that <span class="math inline">\(Y\)</span> is invertible, which is implied by the stabilizable condition. <em>Proof omitted</em>.</li>
</ol>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Control theory</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/control_theory.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/control_theory.html</id>
    <published>2016-12-06T00:00:00Z</published>
    <updated>2016-12-06T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Control theory</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-12-06 
          , Modified: 2016-12-06 
	</p>
      
       <p>Tags: <a href="/tags/control%20theory.html">control theory</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#introduction">1 Introduction</a></li>
 <li><a href="#controllability">2 Controllability</a><ul>
 <li><a href="#observations">Observations</a></li>
 <li><a href="#bang-bang">Bang-bang</a></li>
 </ul></li>
 <li><a href="#section">3</a><ul>
 <li><a href="#examples">Examples</a></li>
 </ul></li>
 <li><a href="#the-pontryagin-maximum-principle">4 The Pontryagin Maximum Principle</a><ul>
 <li><a href="#maximal-principle">Maximal principle</a></li>
 </ul></li>
 <li><a href="#dynamic-programming">Dynamic programming</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="https://math.berkeley.edu/~evans/control.course.pdf">Course notes</a></p>
<h2 id="introduction">1 Introduction</h2>
Setup
<span class="math display">\[\begin{align}
\dot x(t) &amp;= f(x(t),\al(t))\\
x(0) &amp;= x^0\\
P[\al] &amp;= \int_0^T r(x(t),\al(t))\,dt + g(x(T))
\end{align}\]</span>
<p>where <span class="math inline">\(\al: [0,\iy)\to A\)</span> is the control, <span class="math inline">\(r:\R^n\times A\to \R\)</span> is the reward and <span class="math inline">\(g:\R^n\to \R\)</span> is the terminal reward. The goal is to find the optimal <span class="math inline">\(\al\)</span>. (We can think of <span class="math inline">\(x\)</span> as a function of <span class="math inline">\(t, \al, x^0\)</span>, <span class="math inline">\(x(t, \al, x^0)\)</span>.)</p>
Example: Economics (investment) - <span class="math inline">\(x\)</span> is output and <span class="math inline">\(\al\)</span> is proportion to reinvest.
<span class="math display">\[\begin{align}
\dot x &amp;= k\al x\\
x(0)&amp;=x^0\\
P(\al)&amp;=\int_0^T (1-\al(t))x(t)\,dt.
\end{align}\]</span>
Example: Try to stop a train with rockets on both sides - Here <span class="math inline">\(T\)</span> is not fixed, but the <span class="math inline">\(\tau\)</span>, the first time that <span class="math inline">\((x,\dot x)=0\)</span>. <span class="math inline">\(\al\in [-1,1]\)</span>, <span class="math inline">\(x=\coltwo qv\)</span>.
<span class="math display">\[\begin{align}
\dot x &amp; = \matt 0100 x + \coltwo 01 \al\\
P(\al) &amp; = -\tau
\end{align}\]</span>
<h2 id="controllability">2 Controllability</h2>
<p>Let <span class="math inline">\(C(t)=\set{x}{\exists \al, x(t, \al, x^0)= x}\)</span> and <span class="math inline">\(C=\bigcup_{t\ge 0} C(t)\)</span>.</p>
Consider linear systems with solution
<span class="math display">\[\begin{align}
\dot x &amp;= Mx + \ub{N\al}{f}\\
X&amp;=e^{tM}\\
x(t) &amp;= X(t) x^0 + X(t) \int_0^t X^{-1}(s) f(s)\,ds.
\end{align}\]</span>
<ul>
<li><span class="math inline">\(C, C(t)\)</span> are symmetric and convex, and <span class="math inline">\(C(t)\)</span> is increasing.
<ul>
<li><em>Proof.</em> If <span class="math inline">\(x^0\in C(t), \wh x^0\in C(\wh t)\)</span> with <span class="math inline">\(\al,\wh \al\)</span>, <span class="math inline">\(t\le\wh t\)</span>, then <span class="math inline">\(\la x^0 + (1-\la)\wh x^0\in C(\wh t)\)</span> with <span class="math inline">\(\la \al \one_{\le t} + (1-\la)\wh \al\)</span>.</li>
</ul></li>
<li>Define the <span class="math inline">\(n\times mn\)</span> <strong>control matrix</strong> <span class="math inline">\(G(M,N) = [N, MN, \ldots, M^{n-1}N]\)</span>. Then <span class="math inline">\(\rank G = n \iff 0\in C^{\circ}\)</span> (interior of <span class="math inline">\(C\)</span>).
<ul>
<li><em>Proof.</em> If <span class="math inline">\(b^TG=0\)</span>, then <span class="math inline">\(b^TM^kN=0\)</span> for all <span class="math inline">\(k\)</span> by Cayley-Hamilton.</li>
<li><span class="math inline">\(b^TX^{-1}=0\)</span> by Taylor expansion.</li>
<li>If <span class="math inline">\(x^0\in C(t)\)</span>, <span class="math inline">\(b^Tx^0=0\)</span>, then <span class="math inline">\(C\perp b\)</span>.</li>
<li>If <span class="math inline">\(0\nin C^0\)</span>, then there is a separating hyperplane, <span class="math inline">\(\forall x^0, b^Tx^0\le 0\)</span>.
<ul>
<li><em>Lemma.</em> If for all <span class="math inline">\(\al\)</span>, <span class="math inline">\(\int_0^T b^T X^{-1}N\al\ge0\)</span> then <span class="math inline">\(b^TX^{-1}N\equiv 0\)</span>.
<ul>
<li><em>Proof.</em> Let <span class="math inline">\(v=b^TX^{-1}N\)</span>. If <span class="math inline">\(v\ne 0\)</span> on <span class="math inline">\(I\)</span>, define <span class="math inline">\(\al = \fc{v}{|v|}\)</span> on <span class="math inline">\(I\)</span>.</li>
</ul></li>
<li>From lemma, <span class="math inline">\(bX^{-1} N = 0\)</span>. Differentiate to get <span class="math inline">\(b^T M^k N=0\)</span>.</li>
</ul></li>
</ul></li>
<li>If <span class="math inline">\(A=[-1,1]^n\)</span>, and <span class="math inline">\(\rank G=n\)</span> and <span class="math inline">\(\Re \la \le 0\)</span> for all eigenvalues <span class="math inline">\(\la\)</span> of <span class="math inline">\(M\)</span>, then <span class="math inline">\(C=\R^n\)</span>.
<ul>
<li>If <span class="math inline">\(C\ne \R^n\)</span>, find a separating hyperplane <span class="math inline">\(b^T x^0\le \mu\)</span>.</li>
<li>We aim to get <span class="math inline">\(b^Tx^0&gt;\mu\)</span> for contradiction.</li>
<li><span class="math inline">\(v=b^TX^{-1}N\nequiv 0\)</span> because the rank of <span class="math inline">\(G\)</span> is <span class="math inline">\(n\)</span>.</li>
<li>Let <span class="math inline">\(\al = -\fc{v}{|v|}\one_{v\ne 0}\)</span>. Then <span class="math inline">\(p\pa{-\ddd t}v = 0\)</span> by CH, so <span class="math inline">\(\ddd t p\pa{-\ddd t}\phi = 0\)</span>. So <span class="math inline">\(\phi(t) = \sum p_i e^{\mu_i t}\not \to 0\)</span> where <span class="math inline">\(\mu_{n+1}=0\)</span>, <span class="math inline">\(\mu_k=-\la_k\)</span>. So <span class="math inline">\(\int_0^\iy|v|=\iy\)</span> and <span class="math inline">\(b^Tx^0\to \iy\)</span>.</li>
</ul></li>
</ul>
<p>(If <span class="math inline">\(A=\R^n\)</span>, then <span class="math inline">\(\rank G=n \iff C=\R^n\)</span>.)</p>
<h3 id="observations">Observations</h3>
<p>Suppose we observe <span class="math inline">\(y=Nx\)</span> where <span class="math inline">\(N\in \R^{m\times n}\)</span>. Think of <span class="math inline">\(m&lt;n\)</span>.</p>
<p>Say the system is observable if <span class="math inline">\(Nx_1\equiv Nx_2\)</span> on <span class="math inline">\([0,t]\)</span> implies <span class="math inline">\(x_1\equiv x_2\)</span>.</p>
<p><strong>Duality</strong>. <span class="math inline">\(\dot x = Mx\)</span>, <span class="math inline">\(y=Nx\)</span> is observable iff <span class="math inline">\(\dot z = M^Tz + N^T \al\)</span>, <span class="math inline">\(\al\in \R^m\)</span> is controllable.</p>
<p><em>Proof</em>.</p>
<ul>
<li>Assume not observable.
<ul>
<li><span class="math inline">\(x=x_1-x_2\)</span>, <span class="math inline">\(\dot x = Mx\)</span>, <span class="math inline">\(Nx=0\)</span>. Then <span class="math inline">\(\rank G&lt;n\)</span>, so not controllable.</li>
</ul></li>
<li>Assume not controllable.
<ul>
<li>Then <span class="math inline">\(\rank G&lt;n\)</span>, take <span class="math inline">\(x\)</span> such that <span class="math inline">\(x^TG=0\)</span>.</li>
</ul></li>
</ul>
<h3 id="bang-bang">Bang-bang</h3>
<p><strong>Theorem</strong>. Any extreme point of the set of admissible controls <span class="math inline">\(\set{\al:\R^n\to [-1,1]^n}{x(t,\al,x^0)=x}\)</span> has, for each <span class="math inline">\(t\ge 0\)</span>, <span class="math inline">\(i\)</span>, <span class="math inline">\(|\al^i|=1\)</span> (is “bang-bang”). In particular, there always exists a bang-bang solution.</p>
<p><em>Proof</em>.</p>
<ul>
<li>The set of admissible controls is convex in <span class="math inline">\(L^{\iy}\)</span> and <span class="math inline">\(w^*\)</span> compact (show sequential compactness using Alaoglu.).</li>
<li>If an extreme point has <span class="math inline">\(|\al^i|&lt;1-\ep\)</span> on <span class="math inline">\(F\)</span>, then write as combination of <span class="math inline">\(\al^*\pm \ep \be e_i \one_F\)</span>.</li>
<li>Extreme points exist by Krein-Milman.</li>
</ul>
<h2 id="section">3</h2>
<p>For the linear system and <span class="math inline">\(A=[-1,1]^n\)</span>, there exist a time-optimal bang-bang solution. I.e. <span class="math inline">\(\tau^*=\inf \set{t}{x^0\in C(t)}\)</span> is attainable.</p>
<p><em>Proof.</em> Take <span class="math inline">\(t_n\to t\)</span>, <span class="math inline">\(\al_n\)</span>. Use Alaoglu.</p>
<p>Let the reachable set be <span class="math inline">\(K(t,x^0) = \set{x^1}{\exists \al, x(x^0, \al, t) = x^1}\)</span>. It is convex and closed (Pf. Alaoglu).</p>
<p><strong>Theorem</strong>. There is <span class="math inline">\(h\)</span> (depending on <span class="math inline">\(x_0\)</span>, but not on <span class="math inline">\(t\)</span>) such that the optimal action is <span class="math display">\[
\al^*(t) = \max_{a\in A} [h^T X^{-1}(t)Na].
\]</span></p>
<p><em>Proof.</em></p>
<ol type="1">
<li>By convexity of <span class="math inline">\(K(\tau, x)\)</span>, <span class="math inline">\(0\in \pl K(\tau^*,x^0)\)</span>. Take <span class="math inline">\(g\)</span> such that <span class="math inline">\(g^T x_1\le 0\)</span> for <span class="math inline">\(x_1\in K(\tau^*,x^0)\)</span>.</li>
<li>Write the trajectories <span class="math inline">\(\al, \al^*\)</span> ending in <span class="math inline">\(x^1, 0\)</span>. Dot with <span class="math inline">\(g\)</span>. Het <span class="math inline">\(h^T = g^T X(\tau^*)\)</span>, get <span class="math inline">\(\int_0^{\tau^*} h^T X^{-1}(s) N(\al^*-\al)\,ds\ge 0\)</span>.</li>
<li>If <span class="math inline">\(h^TX^{-1}N\al^* \le \max_{a\in A}h^T X^{-1}Na\)</span> on some set then we can replace <span class="math inline">\(\al^*\)</span> on that set and get something larger, contradiction.</li>
</ol>
<strong>Corollary</strong>. For <span class="math inline">\(H(x,p,a) = (Mx + Na)^Tp\)</span>, the optimal trajectory solves
<span class="math display">\[\begin{align}
\cdot x &amp;= \nb_p H\\
\cdot p &amp;= -\nb_x H\\
\al &amp;= \max_a\in A H.
\end{align}\]</span>
<p>(Take <span class="math inline">\(p(0) = h\)</span>, <span class="math inline">\(p=h^TX^{-1}\)</span>.)</p>
<h3 id="examples">Examples</h3>
<ul>
<li>Rocket train: <span class="math inline">\(\al^* = \sgn(-th_1+h_2)\)</span> so switch at most once.</li>
<li>Pendulum <span class="math inline">\(\ddot x + x = \al\)</span>. <span class="math inline">\(\al^* = \sign(\sin(t+\de))\)</span>, switch every <span class="math inline">\(\pi\)</span> (between 2 circles).</li>
</ul>
<h2 id="the-pontryagin-maximum-principle">4 The Pontryagin Maximum Principle</h2>
<p>Let <span class="math inline">\(L:\R^n\times \R^r\to \R\)</span> (a Lagrangian). Suppose we want to solve (action equation) <span class="math display">\[
\min I[x], \quad I[x] = \int_0^T L(x,\dot x)\,dt.
\]</span> Assume that <span class="math inline">\(p=\nb_v L(x,v)\)</span> can be solved for <span class="math inline">\(v\)</span>. (How important is this?) The solution satisfies the Euler-Lagrange equation <span class="math display">\[
\ddd t \ub{[\nb_v L(x^*, \dot x^*)]}{p} = \nb_x L(x^*, \dot x^*).
\]</span></p>
<p><em>Proof.</em> Consider “differentiating” in directoin <span class="math inline">\(y:[0,T]\to \R^n\)</span>, <span class="math inline">\(y(0)=y(T) = 0\)</span>. Consider <span class="math inline">\(i(\tau) = I[x+\tau y]\)</span>. <span class="math inline">\(i(\tau)\ge i(0)\)</span> so <span class="math inline">\(i'(0)=0\)</span>. <span class="math display">\[
i'(0) = \sumo in \int_0^T L_x(x,\dot x)y_i + L_{v_i} (x,\dot x) \dot y_i\,dt.
\]</span> Choose <span class="math inline">\(y = \psi(t) e_j\)</span>. IbP gives <span class="math inline">\(L_{x_j} - (L_{v_j})_t=0\)</span>.</p>
The solution to EL satisfies Hamiltonian system: let <span class="math inline">\(H=p^Tv - L(x, v(x,p))\)</span>,
<span class="math display">\[\begin{align} 
\dot x &amp;= \nb_p H\\
\dot p &amp;= -\nb_x H.
\end{align}\]</span>
<em>Proof.</em>
<span class="math display">\[\begin{align}
\nb_x H &amp;= p\nb_x v - \nb_x L - \nb_v L \nb_x v = -\nb_xL\\
\nb_p H &amp;= v(p) + p^T \fc{Dv}{Dp} - \nb_p L \\
&amp;= \dot x + p^T \fc{Dv}{Dp} - (\nb_v L)^T\fc{Dp}{Dv}=\dot x.
\end{align}\]</span>
<p>(This is pretty confusing. <span class="math inline">\(v\)</span> is implicitly defined in terms of <span class="math inline">\(p\)</span>, the value such that <span class="math inline">\(p=\nb_v L(x,v)\)</span>.)</p>
Example:
<span class="math display">\[\begin{align}
L &amp;= \fc{m|v|^2}{2} - V(x)\\
m\ddot x &amp;= -\nb V(x(t))\\
p &amp;= \nb_v L = mv\\
H(x,p) &amp;= \fc{|p|^2}{2m} + V.
\end{align}\]</span>
<p>4.2. Constraints create Lagrange multipliers, which contain valuable information. If <span class="math inline">\(x^*\in \pl R\)</span>, <span class="math inline">\(R=\{g\le 0\}\)</span>, <span class="math inline">\(x^*=\amax f\)</span>, then <span class="math inline">\(\nb f = \nb g\)</span>, <span class="math inline">\(\mu \nb f(x^*) = \la \nb g(x^*)\)</span>.</p>
<h3 id="maximal-principle">Maximal principle</h3>
The control theory Hamiltonian corresponding to
<span class="math display">\[\begin{align}
\dot x &amp; = f(x(t),a(t))\\
P[\al] &amp;=\int_0^T r(x(t),a(t))\,dt + g(x(T))
\end{align}\]</span>
<p>is <span class="math display">\[ H(x,p,a) = f(x,a)^Tp+r(x,a)\]</span></p>
<ol type="1">
<li>Fixed time, free endpoint
<span class="math display">\[\begin{align}
\dot x &amp;= \nb_p H\\
\dot p &amp;= -\nb_x H\\
H(x,p,\al) &amp;= \max_{a\in A} H(x(t),p(t),a)\\
p(T) &amp;= \nb g(x^*(T)).
\end{align}\]</span>
Moreover <span class="math inline">\(t\mapsto H(x(t), p(t),\al(t))\)</span> is constant.</li>
<li>Free time, fixed endpoint <span class="math inline">\(P[\al] = \int_0^\tau r\,dt\)</span>. Everything is same except there is no end boundary value condition, and there is <span class="math inline">\(H(x(t),p(t),\al(t))\equiv 0\)</span>.</li>
</ol>
<p>(See warning on p. 50.)</p>
<p>Methodology: solve for <span class="math inline">\(\al(x,p)\)</span>, substitute back, solve the DE, then sub <span class="math inline">\(x,p\)</span> into expression <span class="math inline">\(\al\)</span>. “Feedback controls”: set <span class="math inline">\(\al(t) = c(t)x(t)\)</span> and write equation for <span class="math inline">\(c(t)\)</span>. (Cf. eigenfunctions??)</p>
<p>Transversality: adding condition to start in <span class="math inline">\(X_0\)</span> and end in <span class="math inline">\(X_1\)</span>, we have <span class="math inline">\(p^*(\tau^*)\perp T_1\)</span>, <span class="math inline">\(p^*(0)\perp T_0\)</span>.</p>
<h2 id="dynamic-programming">Dynamic programming</h2>
<p>Adding a variable can help. Ex. <span class="math display">\[
I(\al) = \iiy \redd{e^{-\al x}} \fc{\sin x}{x} \dx,\quad I'(\al) = -\rc{\al^2+1}.
\]</span></p>
<p>Fix <span class="math inline">\(T\)</span>. Vary starting time and point: <span class="math display">\[
v(x,t) = \sup_{\al \in A} P_{x,t}[\al].
\]</span></p>
Hamilton-Jacobi-Bellman equation
<span class="math display">\[\begin{align}
v_t + \ub{\max_{a\in A}[f\cdot \nb_x v + r]}{a^*(x,\nb_x v)} &amp;= 0\\
v(x,T) &amp; = g(x).
\end{align}\]</span>
<em>Proof.</em> Taking the first equation, dividing by <span class="math inline">\(h\to 0\)</span>, using the chain rule
<span class="math display">\[\begin{align}
v_t &amp; \ge \int_t^{t+h} r\,ds + v(x(t+h), t+h) \\
v_t + \nb_x v \cdot x + r&amp;\le 0.
\end{align}\]</span>
<p>Now take the max. Equality attained at optimal <span class="math inline">\(\al^*\)</span>.</p>
<p>General procedure;:</p>
<ol type="1">
<li>Solve HJB, compute <span class="math inline">\(v\)</span>.</li>
<li>Solve for <span class="math inline">\(\al\)</span>, plug in.</li>
<li>The feedback control is <span class="math inline">\(\al^*(s) = \al(x^*(s),s)\)</span>.</li>
</ol>
General linear-quadratic regulator
<span class="math display">\[\begin{align}
\dot x &amp;= Mx + N\al\\
P[\al] &amp;= \int_t^T (x^TBx+\al^TC \al) \,ds - x(T)^T D x(T)\\
v_t + \max_{a\in \R^m} (\nb v^T Na - a^TCa) + (\nb v)^T Mx - x^TBx &amp;=0\\
v(x,T)&amp;=-x^TDx\\
a &amp; = \rc 2 C^{-1} N^T\nb_x v\\
&amp;=C^{-1} N^T Kx
\end{align}\]</span>
<p>where <span class="math inline">\(K\)</span> satisfies the matrix Riccati equation.</p>
<p>5.3. HJ equations…</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Embedding methods in NLP</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/em_nlp.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/em_nlp.html</id>
    <published>2016-12-06T00:00:00Z</published>
    <updated>2016-12-06T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Embedding methods in NLP</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-12-06 
          , Modified: 2016-12-06 
	</p>
      
       <p>Tags: <a href="/tags/nlp.html">nlp</a>, <a href="/tags/word%20vectors.html">word vectors</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#emnlp">EMNLP</a><ul>
 <li><a href="#methods">Methods</a></li>
 <li><a href="#details">Details</a></li>
 <li><a href="#embeddings-for-multi-relational-data">Embeddings for multi-relational data</a></li>
 </ul></li>
 <li><a href="#spwc-recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank">[SPWC] Recursive deep models for semantic compositionality over a sentiment treebank</a></li>
 <li><a href="#scmn16-reasoning-with-neural-tensor-networks-for-knowledge-base-completion">[SCMN16] Reasoning With Neural Tensor Networks for Knowledge Base Completion</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="emnlp">EMNLP</h2>
<p><a href="http://emnlp2014.org/tutorials.html#embedding">EMNLP 2014 tutorial</a></p>
<h3 id="methods">Methods</h3>
<ul>
<li>Ranking/retrieval
<ul>
<li>LSI</li>
<li>SSI</li>
</ul></li>
<li>Language modeling
<ul>
<li>NNLM</li>
<li>Word2vec</li>
<li>GloVe</li>
</ul></li>
<li>Supervised prediction
<ul>
<li>RNN(LM)</li>
<li>Convolutional nets for tagging (SENNA)</li>
</ul></li>
</ul>
<h3 id="details">Details</h3>
<ul>
<li>Bag of words (baseline)</li>
<li>LSI <span class="math inline">\(\phi(d) = Ud\)</span> (<span class="math inline">\(d\)</span> document as sum of words)</li>
<li>SSI Similarity of query <span class="math inline">\(q\)</span> to document <span class="math inline">\(d\)</span> given by <span class="math inline">\(f(q,d) = q^TWd\)</span>, <span class="math inline">\(W\)</span> from data.
<ul>
<li><span class="math inline">\(W\)</span> is big.</li>
<li>Make <span class="math inline">\(W\)</span> low rank. <span class="math inline">\(W=U^TV+I\)</span>.</li>
<li>Maximize AUC. Want <span class="math inline">\(f(q,d^+)&gt;f(q,d^-)\)</span>. (14)</li>
<li>Ex. cross-language retrieval</li>
<li>Better: replace AUC with rank weighted loss</li>
</ul></li>
<li><span class="math inline">\(n\)</span>-gram (baseline)</li>
<li>NNLM
<ul>
<li>Arrange in tree. Cluster dictionary according to semantics or frequency.</li>
</ul></li>
<li>Word2Vec
<ul>
<li>CBOW: predict current word from neighbors</li>
<li>Skip-gram: predict neighbors from current word</li>
<li>Compositionality (addition)</li>
</ul></li>
<li>NLP tasks: POS tagging, chunking (NP, VP), named entity recognition, semantic role labeling</li>
<li>Hand-made features, SVM. ASSERT. Pipeline
<ul>
<li>POS tagger, parser</li>
<li>NER tagging</li>
<li>Put together for SRL labeler</li>
</ul></li>
<li>Deep learning way
<ul>
<li>Avoid parse tree</li>
<li>Avoid hand-built features</li>
<li>Joint train NLP tasks on common embedding</li>
<li>Window approach: feed fixed-size window around each word</li>
<li>Sentence: feed whole sentence, tag 1 word at a time, convolutions handle variable length inputs. Max pool over time</li>
<li>Deep SRL
<ul>
<li>Softmax: Train for word tag likelihood, or</li>
<li>Sentence tag likelihood. Sentence score is sum of likelihoods for individual words and transition score</li>
</ul></li>
<li>Bad for rare words</li>
</ul></li>
<li>? semi-supervised (61)</li>
<li>Recursive NN’s
<ul>
<li>Socher ICML13, EMNLP13</li>
<li>Build sentence representations using parse tree <span class="math inline">\(f(W[c_1;c_2]+b)\)</span></li>
</ul></li>
<li>Paragraph vector predicts words in doc, <span class="math inline">\(n\)</span>-grams in the doc.</li>
<li>Words are combined with linear matrices dependendent on the P.O.S.: G. Dinu and M. Baroni. How to make words with vectors: Phrase generation in distributional semantics. ACL ’14.</li>
<li>Previous common belief in NLP: engineering syntactic features necessary for semantic tasks. One can do well by engineering a model/algorithm rather than features.</li>
</ul>
<h3 id="embeddings-for-multi-relational-data">Embeddings for multi-relational data</h3>
<ul>
<li>Embeddings for multi-relational data
<ul>
<li>Relation (sub, rel, obj)</li>
<li>WordNet: dictionary where each entity is a sense (synset).</li>
<li>Freebase</li>
<li>KB’s are hard to manipulate.
<ul>
<li>Large</li>
<li>Sparse</li>
<li>Noisy/incomplete</li>
</ul></li>
<li>Solutions:
<ul>
<li>Encode into low-dimensional vector spaces</li>
<li>Use representations to complete/visualize, in applications</li>
</ul></li>
<li>Link prediction
<ul>
<li>RESCAL (<span class="math inline">\(X_k\)</span> are matrices) <span class="math display">\[
\min_{A,R} \rc2 \pa{\sum_k \ve{X_k - AR_k A^T}_F^2} + \la_A \ve{A}_F^2 + \la_R \sum_k \ve{R_k}_F^2.
\]</span>
<ul>
<li>Downsides: many parameters, bad scalability, bad reconstruction criteria for binary data</li>
</ul></li>
<li>Ideas
<ul>
<li>Low-dimensionsal continuous vector embedding trained on similarity</li>
<li>Stochastic training based on ranking loss</li>
</ul></li>
<li><span class="math inline">\(d(sub, rel, obj) = -\ve{R^{left}s^T - R^{right} o^T}_1\)</span>. (Note: this is projection onto a common space, rather than something quadratic!)
<ul>
<li>SGD with negative examples</li>
<li>Still have problems</li>
</ul></li>
<li>Neural tensor networks
<ul>
<li><span class="math inline">\(d(sub, rel, obj) = u_r^T \tanh (h^TW_r t + V_r^1 h + V_3^2 t + b_r).\)</span></li>
<li><span class="math inline">\(d^3\)</span> parameters</li>
</ul></li>
<li>Model relations as translations
<ul>
<li>$d = _2^2.</li>
<li>there may exist embedding spaces in which relationships among concepts are all decribed by translations</li>
<li>Much fewer parameters. Does better!</li>
<li>Refinements (108)</li>
</ul></li>
</ul></li>
<li>Embeddings for information extraction
<ul>
<li>Entity linking: Identify mentions</li>
<li>Relation extraction: extract facts</li>
<li>Word-sense disambiguation is key</li>
<li>Embedding-based classifier to predict relation (Weston13, 11) (predict based on other text besides subj, obj. Can combine with KB</li>
<li>Universal schemas (I don’t get this.)</li>
</ul></li>
<li>Question answering
<ul>
<li>Subgraph embeddings: learn embeddings of questions and candidate answers (Bordes14) (Q: How to embed subgraph???)</li>
<li>Convert Freebase to Q&amp;A pairs.</li>
<li>Paraphrase questions</li>
</ul></li>
</ul></li>
<li>Issues
<ul>
<li>Must train embeddings together</li>
<li>Compression, blurring (How to one-shot learn new symbols?)</li>
<li>Sequential models suffer from long-term memory</li>
<li>Need many updates</li>
<li>Negative example sampling inefficient</li>
<li>How to use nonlinearity?</li>
</ul></li>
<li>Code
<ul>
<li><a href="www.torch.ch">Torch</a></li>
<li><a href="ronan.collobert.com/senna">SENNA</a></li>
<li><a href="www.fit.vutbr.cz/~imikolov/rnnlm">RNNLM</a></li>
<li><a href="code.google.com/p/word2vec">Word2vec</a></li>
<li><a href="nlp.stanford.edu/sentiment">Recursive NN</a></li>
<li><a href="github.com/glorotxa/sme">SME (multi-relational data)</a></li>
</ul></li>
</ul>
<h2 id="spwc-recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank">[SPWC] Recursive deep models for semantic compositionality over a sentiment treebank</h2>
<p>Put triplet (subj, rel, obj) through tensor network to predict. <span class="math display">\[
g(e_1,R,e_2) = 
u_R^T f\pa{
e_1^T W_R^{[1:k]} e_2 + 
V_R\coltwo{e_1}{e_2} + b_R
}
\]</span></p>
<!--
(Q: how about subj, obj -> rel, etc.? Joint probability distribution makes more sense? What is relationship between prob and NN models?)
-->
<h2 id="scmn16-reasoning-with-neural-tensor-networks-for-knowledge-base-completion">[SCMN16] Reasoning With Neural Tensor Networks for Knowledge Base Completion</h2>
<p>Recursive neural tensor model</p>
<p>Sentiment Treebank: sentiment labels for phrases in parse trees of sentences. <a href="http://nlp.stanford.edu/sentiment">Website</a></p>
<p>Captures effects of negation and its scope.</p>
<ul>
<li>RNN with a classifier at each node in the parse tree that predicts sentiment in one of 5 classes. (<span class="math inline">\(W\)</span> is the same matrix.) (Q: how do you make it a binary tree?)</li>
<li>MV-RNN (matrix-vector): represent every word and longer phrase in parse tree as both vector and matrix (initialized as <span class="math inline">\(I_d+\)</span>noise). Matrix of one multiplied by vector of other. (parameters is large and depends on vocab)</li>
<li>RNTN allows quadratic interaction: <span class="math inline">\(p = f\pa{(b\,c)V^{[1:d]}\coltwo bc + W \coltwo bc}\)</span>.</li>
</ul>
<p>Train: Classification at each node. Minimize KL-divergence.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>POMDPs</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/POMDP.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/POMDP.html</id>
    <published>2016-11-29T00:00:00Z</published>
    <updated>2016-11-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>POMDPs</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-11-29 
          , Modified: 2016-11-29 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#np-hardness">NP-hardness</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>[ALA16] Open problem - approximate planning of POMDPs in the class of memoryless policies</p>
<p>The relevant matrices/tensors are <span class="math inline">\(f_T(x'|x,a) = T_{xx'a}\)</span>, <span class="math inline">\(f_R(r|x,a) = \Ga_{xar}\)</span>, and <span class="math inline">\(f_O(y|x) = O_{xy}\)</span>. View it two ways.</p>
<ol type="1">
<li>As an extended MDP, with transition matrix <span class="math inline">\(P_{(x,a), (y,b)} = T_{yxa}\Pi_{ay}\)</span>, <span class="math inline">\(\om'\)</span> the stationary vector. (I think it’s <span class="math inline">\(\om' = (I-P^T(PP^T)^{-1}P)e_1\)</span>.) We try to maximize <span class="math display">\[\max_{\Pi\ge 0, \sum_a \pi(a|y)=1} \sum_{x,a} \om_\pi(x) \sum_y O_{xy} \Pi_{ay} \ol r(x,a).\]</span></li>
<li>As a MDP with restricted class of policies - a linear subspace, <span class="math display">\[ \Pi_{ax} = O_{xy} \Pi_{ay}.\]</span></li>
</ol>
<p>We are maximizing over a linear subspace, but the value is not convex in the policy. We can easily get to local min using policy gradient (this is easier than policy gradient because we know all parameters).</p>
<p>Questions:</p>
<ol type="1">
<li>Why is this nonconvex? Since we are maximizing, actually the question is “why is this nonconcave”? Come up with a simple concrete example where this is nonconvex. Answer (this is not full-rank, but you can change it infinitesimally): Have a state machine that rewards you only if you choose LL or RR, not if you switch. Then the optimal strategy is to always play L or always play R, rather than somewhere in the middle. More complicatedly, you can set up something which rewards the player from playing probabilities close to <span class="math inline">\(p_1,\ldots, p_k\)</span>, so there can be multiple local optima.</li>
<li>Simpler question: Suppose there are only 2 actions each step, and there is no view. Find the best strategy. Bellman’s equation gives <span class="math inline">\(Q=R + PQ\coltwo{p}{1-p}\)</span>. This is a linear equation in <span class="math inline">\(2|S|\)</span> variables, so the value is a rational function of degree <span class="math inline">\(\le 2|S|\)</span> in <span class="math inline">\(p\)</span>. I think in general you get rational functions also, but in more variables! Can we reduce from something hard like tensor decomposition to this? The difficulty is writing polynomials as solutions to systems of equations… (The denominator is multilinear if you treat all states as the same… but perhaps you can get a broader class.) Or maybe we can’t just perturb the non-full-rank case, we actually need something far enough from low-rank so that the strategy is not oblivious… (ALA16 don’t restrict to full rank.)</li>
</ol>
<p>See also Bertsekas on combining states.</p>
<p>The projection in the expression seems messy to deal with.</p>
<h2 id="np-hardness">NP-hardness</h2>
<p>Finding the optimal memoryless policy for a POMDP is NP-hard. (Proof: We can reduce from any polynomial optimization problem over the simplex to a POMDP as follows. The observation space is trivial, <span class="math inline">\(\{1\}\)</span>. Then a memoryless policy is just a vector of probabilities for different actions, summing to 1. Note a deterministic POMDP is a finite state machine where the actions correspond to characters read in by the machine. Given a polynomial is degree <span class="math inline">\(n\)</span>, make a POMDP that is stratified in <span class="math inline">\(n\)</span> layers. For each monomial <span class="math inline">\(a_{i_1\ldots i_n} x_{i_1}\cdots x_{i_n}\)</span> create a path in the finite state machine that reads in actions <span class="math inline">\(i_1,\ldots, i_n\)</span>, ends in a reward of <span class="math inline">\(a_{i_1\ldots i_n}\)</span> and then resets. Then the average reward is <span class="math inline">\(1/n\)</span> times the polynomial.)</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2016-12-03</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-12-03.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-12-03.html</id>
    <published>2016-11-28T00:00:00Z</published>
    <updated>2016-11-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2016-12-03</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-11-28 
          , Modified: 2016-12-03 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#hmmmdp-project">HMM/MDP project</a></li>
 <li><a href="#alexanlp-experiments">Alexa/NLP experiments</a></li>
 <li><a href="#other-experiments">Other experiments</a></li>
 <li><a href="#learning">Learning</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="hmmmdp-project">HMM/MDP project</h2>
<ul>
<li>[ALA16] open problem, simple policy optimization - is it intractable? (Tue. started)</li>
<li>[JKAL16] Contextual MDP paper (Tue. started)</li>
<li>Go through policy gradient (Tue)</li>
<li>Theory review
<ul>
<li>Ch. 3, 5 of FA</li>
<li>Ch. 6 Bertsekas</li>
</ul></li>
<li>Experiments
<ul>
<li>Program basic RL</li>
</ul></li>
</ul>
<h2 id="alexanlp-experiments">Alexa/NLP experiments</h2>
<ul>
<li>Get Yingyu’s word2vec, DL code up and running.
<ul>
<li>Try modifying with other modalities (sentiments, etc.)</li>
</ul></li>
<li>Lisa Lee’s thesis</li>
<li>Parsing
<ul>
<li>Stanford parser</li>
<li>Try training NN</li>
<li>SyntaxNet</li>
</ul></li>
<li>Train word vectors with syntax trees.</li>
<li>Read EMNLP papers.</li>
</ul>
<h2 id="other-experiments">Other experiments</h2>
<ul>
<li>Neural net learns dictionaries: run experiments.
<ul>
<li>NNDL experiment.</li>
<li>Where is the cusp?</li>
</ul></li>
</ul>
<h2 id="learning">Learning</h2>
<ul>
<li>EM algorithm</li>
<li>Nonparametric/kernel methods (cf. BCO)</li>
<li>Tensor decomposition and latent variable model</li>
<li>MDP theory (see above)</li>
<li>Summarize papers</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Policy gradient</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/policy_gradient.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/policy_gradient.html</id>
    <published>2016-11-22T00:00:00Z</published>
    <updated>2016-11-22T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Policy gradient</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-11-22 
          , Modified: 2016-11-22 
	</p>
      
       <p>Tags: <a href="/tags/reinforcement%20learning.html">reinforcement learning</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#setup">Setup</a></li>
 <li><a href="#computation">Computation</a></li>
 <li><a href="#questions-notes">Questions, notes</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>[SASM00] Policy gradient methods for RL with function approximation</p>
<p>A version of policy iteration with arbitrary differentiable function converges to a locally optimal policy.</p>
<p>Problems with the value-function approach</p>
<ul>
<li>finds deterministic policies.</li>
<li>a small change in estimated vaue of action can change the action selected. (obstacles for convergence guarantees)</li>
</ul>
<p><span class="math display">\[\De \te \approx \al \pdd{\rh}{\te}.\]</span></p>
<h2 id="setup">Setup</h2>
<p>The policy is parameterized by <span class="math inline">\(\te\)</span>, and gives probabilities of making action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span>, <span class="math display">\[
\pi_\te(a|s) = \Pj_\te(a|s).
\]</span> Before, we considered the value at every state. Here, it’s more convenient to consider the value of a fixed start state, which we’ll denote by <span class="math inline">\(\rh\)</span>.</p>
There are 2 ways we can have an expression for <span class="math inline">\(\rh\)</span>.

<p>Consider two settings.</p>
<ol type="1">
<li>Average reward setting. Here we want to maximize <span class="math inline">\(\lim_{T\to \iy} \rc{T}\sumz t{T-1} R_t\)</span>. Then let <span class="math inline">\(d_\te\)</span> be the stationary distribution: <span class="math inline">\(d_\te(s)\)</span> is the probability of being at state <span class="math inline">\(s\)</span> under policy <span class="math inline">\(\pi_\te\)</span>.</li>
<li>Discounted reward setting. Here we want to maximize <span class="math inline">\(\sumz t{\iy} \ga^t R_t\)</span>. Now <span class="math inline">\(d_\te\)</span> has to take into account the discounting, so <span class="math display">\[ d_\te (s) = \sumz t{\iy} \ga^t \Pj (s_t = s).\]</span></li>
</ol>
We also have to change our definition of <span class="math inline">\(Q\)</span> for the average reward setting - otherwise all the <span class="math inline">\(Q\)</span>’s would be the same! In the two cases, define
<span class="math display">\[\begin{align}
Q^{\pi}(s,a) &amp;= \sumz t{\iy} \E[R_t - \rh_\pi | s_0=s,a_0=a, \te]\\
Q^{\pi}(s,a) &amp;= \E\ba{\sumz k{\iy} \ga^{k} R_{t+k} | s_t=s, a_t=a, \pi}.
\end{align}\]</span>
<p>The second is the usual definition of the <span class="math inline">\(Q\)</span>-function. Think of the first as “advantage” of starting at <span class="math inline">\((s,a)\)</span>. It converges because working out with matrices, <span class="math display">\[
\E R_t = \rh_\te + O_{s,a} \pa{\la^t}
\]</span> where <span class="math inline">\(\la\)</span> is the second-largest eigenvalue of the transition matrix (besides 1). So instead of discounting to make <span class="math inline">\(Q\)</span> converge, we subtract the mean to make <span class="math inline">\(Q\)</span> converge. Define <span class="math inline">\(V^\pi(s) = \max_a Q^\pi(s,a)\)</span>.</p>
<p>Then (for continuous spaces replace <span class="math inline">\(\sum\)</span> with <span class="math inline">\(\int\)</span>) <span class="math display">\[\rh_\te = \sum_s d_\te(s) \sum_{a} Q^\te(s,a) \pi_\te(a|s) \]</span> (Whenever something depends on the policy <span class="math inline">\(\pi_\te\)</span>, we write <span class="math inline">\(\te\)</span> as shorthand: <span class="math inline">\(Q^{\te} = Q^{\pi_\te}\)</span>.</p>
<h2 id="computation">Computation</h2>
<p>We need to calculate the derivative. Naively using the product rule, we would need <span class="math inline">\(\nb_\te d_\te(s)\)</span> which we don’t have a way of estimating. But we can derive an expression that doesn’t include this term! To do this we differentiate the Bellman equation for <span class="math inline">\(V\)</span> (which implicitly includes <span class="math inline">\(\rh\)</span>).</p>
<p><strong>Lemma 1</strong>. <span class="math display">\[
\nb_\te \rh_\te = \sum_s d_\te(s) \sum_a \nb_\te \pi(s|a) Q^\te(s,a).
\]</span></p>
<em>Proof</em>. Our strategy is to write the one-step expansion of <span class="math inline">\(Q^\te\)</span> <!-- , and find there's a term that cancels with $\nb_\te \pi_\te(a|s)$.--> Let <span class="math inline">\(R(s,a)\)</span> be the one-step reward of action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span>. We write the proof for the undiscounted case.
<span class="math display">\[\begin{align}
\nb_\te V^\te (s)
&amp;= \sum_a \pa{
(\nb_\te \pi_\te(a|s)) Q^\te(s,a) 
+
\pi_\te(a|s) (\nb_te Q^\te(s,a))}
&amp;= 
\sum_a (\nb_\te \pi_\te (a|s)) Q^\te(s,a) 
+ \pi(a|s) \nb_\te\pa{\E R(s,a) - \rh_\te + \sum_{s'} \Pj(s'|s,a) V^{\te}(s')}\\
\nb_\te \rh_\te &amp;= \pa{\sum_a \nb_\te \pi_\te (a|s)) Q^\te(s,a) +  + \pi(a|s) \sum_{s'} \nb_\te V^{\te} (s') \Pj(s'|s,a) }- \nb_\te V^{\te}(s)
\end{align}\]</span>
This is unsatisfactory because we can’t estimate <span class="math inline">\(\nb_\te V^{\te}(s')\)</span> for every <span class="math inline">\(s'\)</span>. But there is a combination of these we can estimate, namely the stationary distribution. So multiply by <span class="math inline">\(d_\te(s)\)</span> and sum.
<span class="math display">\[\begin{align}
\nb_\te \rh_\te &amp;= \pa{\sum_{s} d_\te(s) \pa{\sum_a(\nb_\te \pi_\te(a|s)) Q^{\te}(s,a)} + \cancel{d_\te(s) \pa{\sum_a\pi(a|s) \sum_{s'} \nb_\te V^{\te}(s') \Pj(s'|s,a)}} - \cancel{\nb_\te V_\te(s)}}.
\end{align}\]</span>
<p>We have to replace <span class="math inline">\(Q^\te\)</span> by an estimate. This doesn’t correspond to an update rule directly, because we can’t estimate <span class="math inline">\(\nb_\te(s|a)\)</span>.</p>
We estimate of <span class="math inline">\(Q^\te\)</span> by a function approximation, updating by <span class="math inline">\(\nb_w (\wh Q^\te(s,a) - f_w(s,a))^2\)</span> given a sample <span class="math inline">\((s,a)\)</span>. This converges to a local min where (<span class="math inline">\(\wh Q^\te\)</span> is an unbiased estimator)
<span class="math display">\[\begin{align}
\E \nb_w \pa{\wh Q^{\te} (s,a) - f_w(s,a)} &amp;=0\\
\sum_s d_\te(s) \sum_a \pi_\te(s|a) (Q^\te(s,a) - f_w(s,a)) &amp;=0
\end{align}\]</span>
<p>We would like to be able to replace <span class="math inline">\(Q^\te(s,a)\)</span> in Lemma 1 with the approximation <span class="math inline">\(f_w(s,a)\)</span>. In order to do this we need the error to be 0: <span class="math display">\[
\sum_s d^{\pi}(s) \pa{
\sum_a (\nb_\te \pi_\te(s|a)) Q^\te(s,a) - \nb_\te\pi(s|a) f_w(s,a)
}
\]</span> In order for these to match up, we need <span class="math display">\[
\nb_w f_w(s,a) = \fc{\nb_\te \pi(s|a)}{\pi(s|a)} = \nb_\te(\ln \pi(s|a)).
\]</span></p>
<p><strong>Question</strong>: Do people wait until convergence, or do they do alternating minimization in practice? Does alternating minimization converge?</p>
Work out the loglinear case:
<span class="math display">\[\begin{align}
\pi(s|a) &amp;= \fc{e^{\te^T \phi_a(s)}}{\sum_b e^{\te^T\phi_b(s)}}\\
\nb_\te \ln \pi(s|a) &amp;= \phi_a(s) - \fc{\sum_b e^{\te^T\phi_b(s)} \phi_b(s)}{\sum_b e^{\te^T \phi_b(s)}}\\
&amp;= \phi_a(s) - \sum_b \pi(s|b) \phi_b(s).
\end{align}\]</span>
<p>So take <span class="math display">\[
f_w(s,a) = w^T ( \phi_a(s) - \sum_b \pi(s|b) \phi_b(s)).
\]</span></p>
<p>How well does this parametrization work, compared to Q-learning by FA?</p>
<h2 id="questions-notes">Questions, notes</h2>
<ol type="1">
<li>Does the LP formulation give a way to understand the derivative? In terms of slack constraints, etc.</li>
<li>How nonconvex is this? What does the optimization landscape look like? Ex. take 2-action, 3-action case, random transition matrices, some kind of grouping together of states. See <a href="POMDP.html">POMDP</a></li>
<li>Try to prove intractability - see POMDP. The parametrization there is a slice of the product of simplices, rather than a subspace in logspace. But you can relate it to optimizing rational functions this way. The main reason I suspect intractability is that the degree can be as large as number of parameters…</li>
</ol>
Minimizing cross-entropy for loglinear is a convex problem. Minimize perplexity for distribution <span class="math inline">\(\Pj(y|x)\)</span> with distribution <span class="math display">\[
\wh P(y|x) = \fc{e^{\te_y^T\phi(x)}}{\sum_{y'} e^{\te_{y'}^T \phi(x)}}.
\]</span> To show convexity, we need to show log of partition function is convex. Reducing to 1-variable case,
<span class="math display">\[\begin{align}
f &amp;= \ln \sum_i e^{a_i+b_i\te}\\
f_\te &amp; = \E b_i\\
f_{\te\te} &amp;= \E b_i^2 - \pa{\E b_i}^2 \ge 0.
\end{align}\]</span>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2016-11-19</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-11-19.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-11-19.html</id>
    <published>2016-11-14T00:00:00Z</published>
    <updated>2016-11-14T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2016-11-19</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-11-14 
          , Modified: 2016-11-14 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#hmmmdp-project">HMM/MDP project</a></li>
 <li><a href="#alexanlp-experiments">Alexa/NLP experiments</a></li>
 <li><a href="#other-experiments">Other experiments</a></li>
 <li><a href="#learning">Learning</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="hmmmdp-project">HMM/MDP project</h2>
<ul>
<li>Lit review
<ul>
<li>[TLF] Monte Carlo HiddenMarkov Models: Learning Non-Parametric Models of Partially Observable Stochastic Processes (Mon.)</li>
</ul></li>
<li>Theory review
<ul>
<li>Ch. 3, 5 of FA</li>
<li>Ch. 6 Bertsekas</li>
</ul></li>
<li>Experiments
<ul>
<li>Program basic RL</li>
</ul></li>
<li>Theory: things to try
<ul>
<li>Nonparametric/kernel methods (cf. BCO)</li>
</ul></li>
</ul>
<h2 id="alexanlp-experiments">Alexa/NLP experiments</h2>
<ul>
<li>Get LSTM up and running.</li>
<li>Get Yingyu’s word2vec, DL code up and running.
<ul>
<li>Try modifying with other modalities (sentiments, etc.)</li>
</ul></li>
<li>Lisa Lee’s thesis</li>
</ul>
<h2 id="other-experiments">Other experiments</h2>
<ul>
<li>Neural net learns dictionaries: run experiments.
<ul>
<li>NNDL experiment.</li>
<li>Where is the cusp?</li>
</ul></li>
</ul>
<h2 id="learning">Learning</h2>
<ul>
<li>Sum of squares: tensor decomposition (Mon.)</li>
<li>EM algorithm</li>
<li>Tensor decomposition and latent variable model</li>
<li>MDP theory (see above)</li>
<li>Summarize papers</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Probabilistic models - Ideas</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/probabilistic/ideas.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/probabilistic/ideas.html</id>
    <published>2016-11-07T00:00:00Z</published>
    <updated>2016-11-07T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Probabilistic models - Ideas</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-11-07 
          , Modified: 2016-11-07 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#non-generative-framework-for-learning-probabilistic-models">Non-generative framework for learning probabilistic models</a><ul>
 <li><a href="#hmms">HMM’s</a></li>
 <li><a href="#kernel-hmm">Kernel HMM</a></li>
 </ul></li>
 <li><a href="#markov-models-with-exponential-state-space">Markov models with exponential state space</a></li>
 <li><a href="#context-vectors">Context vectors</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="non-generative-framework-for-learning-probabilistic-models">Non-generative framework for learning probabilistic models</h2>
<p>We can adapt Hazan and Ma’s framework [HM16] to learning probabilistic models. Note that given a model, in the limit the number of bits needed to encode a sequence of samples (up to some level of accuracy) is the entropy of that sequence under the model. Thus the natural loss to look at here is the entropy.</p>
<p>We say a distribution <span class="math inline">\(D\)</span> is <span class="math inline">\(\ga\)</span> C-learnable by a hypothesis class <span class="math inline">\(\mathcal H\)</span> of probabilistic models if there is an algorithm, that given <span class="math inline">\(\ep\)</span>, with probability <span class="math inline">\(1-\de\)</span>, outputs a model <span class="math inline">\(m\)</span> such that <span class="math display">\[
\rc{l(\vec x)} \EE_{D} \ln \Pj(\vec x|m) \le \rc{l(\vec x)} \min_{m\in \mathcal H} \EE_{D}\ln \Pj(\vec x|m) + \ep + \ga
\]</span> in time polynomial in the parameters, <span class="math inline">\(\poly\pa{\rc\ep, \ln\prc{\de}, d}\)</span>. Equivalently, the <span class="math inline">\(m\)</span> that is output satisfies <span class="math display">\[
KL(D || m)\le KL(D || \mu) +\ep + \ga.
\]</span> where <span class="math inline">\(\mu\)</span> is the distribution minimizing <span class="math inline">\(KL(D||\mu)\)</span>.</p>
<p>We say the hypothesis class <span class="math inline">\(\mathcal H\)</span> is C-learnable if any distribution <span class="math inline">\(D\)</span> is C-learnable by <span class="math inline">\(\mathcal H\)</span>.</p>
<h3 id="hmms">HMM’s</h3>
<p><strong>Question</strong>: Is the class of HMM’s C-learnable?</p>
<p>The tensor algorithm for HMM’s reduces from the problem of learning from 3 independent views to learning a HMM. Learning from 3 independent views relies on 3-tensor decomposition.</p>
<p>The problem boils down to: <strong>Can we find the closest rank <span class="math inline">\(k\)</span> tensor to a given 3-tensor?</strong> (Consider the regime <span class="math inline">\(k\le n\)</span>, <span class="math inline">\(n\)</span> the dimension.) (If there are <span class="math inline">\(k\)</span> hidden states, the the tensor of probabilities is <span class="math display">\[\pa{\sum_h \Pj(h)\Pj(x_1|h)\Pj(x_2|h) \Pj(x_3|h)}_{x_1x_2x_3} = \sum \Pj(h) (p_1)_h\ot (p_2)_h\ot (p_3)_h.\]</span></p>
<p>Actually, it’s a little harder: the distance here is not the <span class="math inline">\(L^2\)</span> distance but the KL divergence. (We can also consider the <span class="math inline">\(L^2\)</span> distance.) For 2 independent views, this problem becomes: given a matrix <span class="math inline">\(A\)</span> with nonnegative entries summing to 1, find a matrix <span class="math inline">\(B\)</span> with nonnegative entries summing to 1, of rank <span class="math inline">\(k\)</span>, and such that <span class="math inline">\(KL(p_B||p_A)\)</span> is as small as possible.</p>
<h3 id="kernel-hmm">Kernel HMM</h3>
<p>Apply the kernel methods of [SBSG10] Hilbert Space Embeddings of Hidden Markov Models not to the spectral algorithm in [HKZ12] but to the tensor algorithm.</p>
<!--A spectral algorithm for learning Hidden Markov Models, we get a kernel algorithm in an esy -->
<p>This requires some calculation but is straightforward.</p>
<h2 id="markov-models-with-exponential-state-space">Markov models with exponential state space</h2>
<p>Can we learn (hidden) Markov models with exponential state space?</p>
<p>One model is that of a dynamic Bayes net (DBN). Even given pairs <span class="math inline">\((x,y)\)</span> it may be difficult to learn in general (check this…)—if the graph is bounded degree <span class="math inline">\(d\)</span> with bounded edge weights then it could be fixed parameter tractable in <span class="math inline">\(d\)</span>. (To be actually tractable probably the weights have to be <span class="math inline">\(1/d\)</span>.)</p>
<h2 id="context-vectors">Context vectors</h2>
<p>Currently we model documents with the context vector undergoing a random walk (<a href="../nlp/randwalk.html">ALLMR16</a>). I don’t think the different coordinates are so interpretable. (?) A random walk is like a DBM where the connections are <span class="math inline">\(1\to 1', 2\to 2'\)</span>, etc. Can we model the evolution of the context vector as something more complicated but still tractable?</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
