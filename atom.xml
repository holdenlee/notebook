<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2016-08-30T00:00:00Z</updated>
    <entry>
    <title>Ellipsoid method</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/ellipsoid.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/ellipsoid.html</id>
    <published>2016-08-30T00:00:00Z</published>
    <updated>2016-08-30T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Ellipsoid method</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-30 
          , Modified: 2016-08-30 
	</p>
      
       <p>Tags: <a href="/tags/linear%20programming.html">linear programming</a>, <a href="/tags/algorithms.html">algorithms</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <ul>
<li><a href="http://ocw.mit.edu/courses/mathematics/18-433-combinatorial-optimization-fall-2003/lecture-notes/l1617.pdf">OCW 18.433</a></li>
<li><a href="http://www.cs.princeton.edu/courses/archive/fall15/cos521/lecnotes/lec17.pdf">Arora’s course</a> (<a href="http://www.cs.princeton.edu/courses/archive/fall15/cos521/">course site</a>)</li>
<li>LSW15 <a href="http://arxiv.org/abs/1508.04874">paper</a></li>
</ul>
<h2 id="ellipsoid-method">Ellipsoid method</h2>
<p>This is a very general <em>polynomial time</em> algorithm for convex optimization. We can use it to solve convex optimization problems that are even too large to write down. <!--faster than gradient descent.--></p>
<ol type="1">
<li>LP: <span class="math inline">\(f(x)=c^T\cdot x\)</span>.</li>
<li>SDP: Infinitely many constraints <span class="math inline">\(a^TXa\ge 0\)</span>.</li>
<li>Held-Karp relaxation for traveling salesman: <span class="math inline">\(\min \sum_{i,j}c_{i,j}X_{i,j}\)</span> under conditions
\begin{align}
0 \le X_{ij} &amp;\le 1\\
\forall S\ne \phi, V, \quad \sum_{i\in S, j\in \ol S} X_{ij} &amp;\ge 2
\end{align}
(Last constraint is subtour elimination constraints. We can find a violation by finding a minimum cut with capacity <span class="math inline">\(&lt;2\)</span>.)</li>
</ol>
<!--
Recall convex optimization. In general there is no succinct description for $K$.
\begin{enumerate}
\item
LP: $f(x)=c^T\cdot x$.
\item
SDP: Think of $a^TXa\ge 0$ as infinitely many linear constraints. 
\item
Held-Karp relaxation for traveling salesman: $\min \sum_{i,j}c_{i,j}X_{i,j}$; for all $\sum_j X_{ij}=2$. 
\fixme{Can't require eigenvalue $>$something?}
To prevent disjoint cycles, for all $S\subeq [n],S\ne \phi,[n]$, $\sum_{i\in S,j\in \ol S} X_{ij}\ge 2$. (Exponentially many constraints. Nevertheless we can solve it!)
\end{enumerate}
We only need to be able to project to the convex body.-->
<p><strong>Separation oracle</strong> for convex <span class="math inline">\(K\)</span>: given <span class="math inline">\(x\)</span>, gives a plane that separates <span class="math inline">\(K\)</span> from <span class="math inline">\(x\)</span> in polynomial time. Think of hyperplane as “feedback” on why <span class="math inline">\(x\nin K\)</span>.</p>
<p><strong>Farkas’s Lemma</strong>: If <span class="math inline">\(K\)</span> is convex, for all <span class="math inline">\(x\nin K\)</span>, there is a hyperplane <span class="math inline">\(a^T\cdot x=b\)</span> such that <span class="math inline">\(K\)</span> lies on one side and <span class="math inline">\(y\)</span> on the other.</p>
<p>Ex. 1. PSD: Compute eigenvalues of <span class="math inline">\(Y\)</span>. Say there is eigenvectors <span class="math inline">\(u\)</span> such that <span class="math inline">\(u^TYu&lt;0\)</span>. Use this hyperplane. 2. Traveling salesman: Cut with <span class="math inline">\(&lt;2\)</span>.</p>
<p>An ellipsoid is <span class="math inline">\((X-a)^TBB^T(X-a)\le 1\)</span> where <span class="math inline">\(B\)</span> is PSD. The ellipsoid algorithm: given: an ellipsoid <span class="math inline">\(\cal E\)</span> containing <span class="math inline">\(K\)</span> and <span class="math inline">\(K\)</span> has a poly-time separation oracle.</p>
<p>To find a point of <span class="math inline">\(K\)</span>, recurse:</p>
<ol type="1">
<li>Is the center <span class="math inline">\(x\)</span> in <span class="math inline">\(K\)</span>? If so, done.</li>
<li>Else, find a separating hyperplane <span class="math inline">\(H\)</span> going through <span class="math inline">\(x\)</span>. Find the smallest ellipsoid containing the half cut by <span class="math inline">\(H\)</span>. (<span class="math inline">\(E_{i+1}=E_i\cap \set{x}{a^Tx\le b}\)</span>) This can be found in poly time with ellipsoids.</li>
</ol>
<p><strong>Theorem</strong>: <span class="math inline">\(\Vol(E_{i+1})\le \pa{1-\rc{2n}}\Vol(E_i)\)</span>.</p>
<p>What we need: 1. Rephrase optimization as feasibility. (Binary search.) 2. Find a “reasonably snug” bounding ellipsoid for <span class="math inline">\(K\)</span>. 3. Implement separation oracle for <span class="math inline">\(K\)</span>. 4. Implement computation to find <span class="math inline">\(E_{i+1}\)</span> given <span class="math inline">\(E_i\)</span> and separation oracle.</p>
<strong>Lemma</strong>. The minimum volume ellipsoid containing <span class="math inline">\(Ell(D,z)\cap \set{x}{a\cdot x\le a\cdot z}\)</span> is <span class="math inline">\(E'=Ell(D',z')\)</span> where
\begin{align}
z' &amp;= z-\rc{n+1} \fc{Da}{\sqrt{a^TDa}}\\
D' &amp;= \fc{n^2}{n^2-1} \pa{D-\fc{2}{n+1}\fc{Daa^TD}{a^TDa}}\\
\fc{\Vol(E')}{\Vol(E)} &amp;\le e^{-\rc{2n+2}}.
\end{align}
<p><em>Proof</em>. It suffices to prove this for <span class="math inline">\(D=I\)</span>, <span class="math inline">\(a=e_1\)</span>. Here <span class="math display">\[ D' = \fc{n^2}{n^2-1} \mattn{1-\fc{2}{n+1}}0{\cdots}0{1}{\cdots}{\vdots}{\vdots}{\ddots}.\]</span> Volume bound follows from determinant calculation.</p>
<p>The number of steps needed is <span class="math inline">\(n\ln \pf{V_1}{V_0}\)</span> where <span class="math inline">\(V_1\)</span> is the volume of the smallest ellipsoid containing the body and <span class="math inline">\(V_0\)</span> is volume of the starting ellipsoid. Ex. If <span class="math inline">\(P\)</span> is a polyhedron, and <span class="math inline">\(\nu\)</span> is the number of bits required to write down any <span class="math inline">\(n\times n\)</span> subset of <span class="math inline">\((A,b)\)</span>, then <span class="math inline">\(\Vol(P)\ge 2^{-2n\nu}\)</span>. (Use Cramer’s rule to get expressions for vertices of <span class="math inline">\(Ax\le b\)</span>.) Then the number of iterations is <span class="math inline">\(O(n^2)\)</span>. ?? Each step takes <span class="math inline">\(O(n^2L)O(mn)\)</span> time (<span class="math inline">\(L\)</span>-bit numbers, check validity of point) for a total of <span class="math inline">\(O(mn^5L)\)</span>.</p>
<blockquote>
<p>How do you find a lion in the Sahara? Split it in half and recurse.</p>
</blockquote>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2016-08-27</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-08-27.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-08-27.html</id>
    <published>2016-08-27T00:00:00Z</published>
    <updated>2016-08-27T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2016-08-27</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-27 
          , Modified: 2016-08-27 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#goals">Goals</a></li>
 <li><a href="#summary">Summary:</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="goals">Goals</h2>
<ul>
<li>Review/learning
<ul>
<li>HDP Ch. 4</li>
<li>Index-card AI safety notes</li>
<li>Convex optimization, bandit optimization</li>
</ul></li>
<li>PMI
<ul>
<li>Organize code, data; write up what I’ve found so far.</li>
</ul></li>
<li>Representation learning
<ul>
<li>Experiment?</li>
</ul></li>
<li>Tensorflow
<ul>
<li>Get basic LSTM running.</li>
<li>Play with parameters in MNIST.</li>
</ul></li>
</ul>
<h2 id="summary">Summary:</h2>
<ul>
<li>PMI
<ul>
<li>Run weighted SVD and compare</li>
</ul></li>
<li>HDP started Ch. 4</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Interior point methods</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/ipm.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/ipm.html</id>
    <published>2016-08-26T00:00:00Z</published>
    <updated>2016-08-26T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Interior point methods</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-26 
          , Modified: 2016-08-26 
	</p>
      
       <p>Tags: <a href="/tags/convex%20optimization.html">convex optimization</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#introduction">Introduction</a></li>
 <li><a href="#analysis-and-algorithm-explanation">Analysis and algorithm explanation</a><ul>
 <li><a href="#place-to-start">1 Place to start</a><ul>
 <li><a href="#termination-near-phase-ii-central-path">Termination near phase II central path</a></li>
 </ul></li>
 <li><a href="#inner-steps">2 Inner steps</a></li>
 <li><a href="#outer-steps">3 Outer steps</a></li>
 <li><a href="#total">Total</a></li>
 <li><a href="#variations">Variations</a></li>
 </ul></li>
 <li><a href="#primal-dual-ipm">Primal-dual IPM</a></li>
 <li><a href="#intuitions">Intuitions</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="introduction">Introduction</h2>
<p>The idea: Given <span class="math inline">\(\min_{f_i\le 0} f\)</span>,</p>
<ol type="1">
<li>Find a feasible point.</li>
<li>Turn into soft constraints: let <span class="math inline">\(F(t,x) = \min f+ \sum -\rc t \ln (-f_i)\)</span>. (Add a barrier function.) As <span class="math inline">\(t\to \iy\)</span>, <span class="math inline">\(F(t,x)\)</span> becomes steeper and <span class="math inline">\(\to f(x)\)</span> pointwise.</li>
<li>Solve iteratively. Start at some <span class="math inline">\((x_0,t_0)\)</span>: Find a value within <span class="math inline">\(\ep\)</span> of <span class="math inline">\(\min F(t_i,x)\)</span>. (Centering)</li>
<li>Set <span class="math inline">\(t_{i+1}=\ga t_i\)</span> (updating schedule).</li>
<li>Go back to 3. Repeat until <span class="math inline">\(t\)</span> is large enough. (<span class="math inline">\(m/t&lt;\ep\)</span>)</li>
</ol>
<p>Define the central path. What are the properties of the central path. Explain the relationship to the dual problem.</p>
<p><span class="math display">\[
C = \set{(t,x)}{x=\amin f(x) + \sum -\rc t \ln (-f_i(x))}.
\]</span> Now <span class="math inline">\(f(c(t))\to f(x^*)\)</span>. For <span class="math inline">\((t,x')\in C\)</span>, we have <span class="math display">\[ \nb f + \sum - \fc{\nb f_i}{tf_i}=0.\]</span> This is the first KKT condition with <span class="math inline">\(\la_i = -\rc{tf_i(x')}\)</span>. (Complementarity is not satisfied though: we have <span class="math inline">\(-\la_if_i=\rc t\)</span>, not 0.)</p>
Let <span class="math inline">\(x^*=\amin_{f_i\le 0} f\)</span>, <span class="math inline">\(x'=\amin f - \sum \rc t \ln (-f_i)\)</span>. Substituting <span class="math inline">\(x',\la_i\)</span> in the dual problem, we get
\begin{align}
f(x^*) &amp;\ge \min_x f(x) - \sum \rc{t f_i(x')} f_i(x) \\
&amp; = f(x') - \fc mt &amp; x=x'\\
\implies f(x^*) \le f(x') &amp;\le f(x^*) + \fc mt.
\end{align}
<p>Now add in the condition <span class="math inline">\(Ax=b\)</span>, and the term <span class="math inline">\(\nu^T(Ax-b)\)</span>. The analysis stays the same.</p>
<p>(<span class="math inline">\(\ln(-f_i)\)</span> has the magic that its gradient is <span class="math inline">\(\nb f_i\)</span> times something.)</p>
<p>What questions do we need to address?</p>
<ol type="1">
<li>How do we find a point to start—find a feasible point?</li>
<li>Given the previous almost-optimal solution, what’s the complexity of finding the next one, given ratio <span class="math inline">\(\mu\)</span>? (Number of inner steps)</li>
<li>How many outer steps do we need? When to stop? (We’ve shown the gap is <span class="math inline">\(\fc mt\)</span>, so stop when <span class="math inline">\(t&gt;\fc m\ep\)</span>.</li>
</ol>
<p>What assumptions do we need?</p>
<ul>
<li><span class="math inline">\(tf_0+\phi\)</span> (<span class="math inline">\(\phi = -\sumo im \ln(-f_i)\)</span>) closed (??) and self-concordant for all <span class="math inline">\(t\ge t^{(0)}\)</span>.</li>
<li>Sublevel sets of <span class="math inline">\(f_0\)</span> (<span class="math inline">\(f_i\le 0\)</span>, <span class="math inline">\(Ax=b\)</span>) are bounded. (This implies that <span class="math inline">\(\nb^2(tf_0+\phi)\)</span> is PD—why?)</li>
</ul>
<p>This is reasonable because it is self-concordant if all <span class="math inline">\(f_i\)</span> are linear or quadratic, and <span class="math inline">\(\ln\)</span> is good at making functions self-concordant.</p>
<p>Give an example of a problem that can be reformulated to be self-concordant. Technique is to add redundant constraints or extra variables.</p>
<ul>
<li>For <span class="math inline">\(\min_{Fx\le g, Ax=b} \sumo in x_i\ln x_i\)</span>, <span class="math inline">\(tf_0(x)+\phi(x)\)</span> is not closed (?) or self-concordant. Using that <span class="math inline">\(ty\ln y - \ln y\)</span> is self-concordant, we add in the redundant constraint <span class="math inline">\(x\ge 0\)</span> to get <span class="math display">\[tf_0+\phi = \sumo in (tx_i\ln x_i - \ln x_i) - \sumo im \ln (g_i-f_i^Tx).\]</span></li>
<li>For <span class="math inline">\(\min_{\ln(\sumo k{K_i}\exp(a_{ik}^T + b_{ik}))\le 0} \ln \pa{\sumo k{K_0} \exp(a_{0k}^Tx+b_{0k})}\)</span>, introduce the variables <span class="math inline">\(y_{ik}\)</span>. Change the problem to
\begin{align}
\min \sumo k{K_0} y_{0k}\\
\sumo k{K_i} y_{ik} &amp;\le 1\\
a_{ik}^T x+b_{ik} - \ln y_{ik} &amp;\le 0\\
y_{ik} &amp;\ge 0.
\end{align}</li>
</ul>
<h2 id="analysis-and-algorithm-explanation">Analysis and algorithm explanation</h2>
<h3 id="place-to-start">1 Place to start</h3>
<p>How to find a feasible point? A feasibility problem can be transformed into an optimization problem</p>
<p><span class="math display">\[
\exists x, f_i\le 0,  \iff \min_{f_i\le m} m\le 0.
\]</span> <!-- Ax = b --> Assume <span class="math inline">\(\{\forall i, f_i\le 0\}\subeq B_R(0)\)</span>. Let <span class="math inline">\(\ol p^*\)</span> be the optimal value of this optimization problem.</p>
Actually add an extra constraint (<span class="math inline">\(a\)</span> satisfies <span class="math inline">\(\ve{a}_2\le \rc R\)</span> so is redundant). <span class="math display">\[
\min_{f_i\le s, a^Tx \le 1} s.
\]</span> Choose <span class="math inline">\(a,s_0\)</span> so that <span class="math inline">\((x=0,s=s_0, t_0)\)</span> is on the central path (to make analysis easier), i.e., so <span class="math inline">\(x=0,s=s_0\)</span> minimizes <span class="math display">\[t^{(0)}s- \sumo im \ln (s-f_i(x)) - \ln (1-a^T x).\]</span> Set the gradient to 0 to see that we require
\begin{align}
t^{(0)} &amp;= \sumo im \rc{s_0 - f_i(0)}\\
a &amp;= -\sumo im \rc{s_0-f_i(0)} \nb f_i(0).
\end{align}
<p>What to choose for <span class="math inline">\(s_0\)</span>? We need <span class="math inline">\(s_0&gt;F:=\max_i f_i(0)\)</span> and <span class="math inline">\(\ve{a}_2\le \rc{R}\)</span>. Upper bound <span class="math inline">\(\ve{a}_2\)</span> by <span class="math display">\[
\ve{a}_2\le \sumo im \rc{s_0-f_i(0)}\ve{\nb f_i(0)} \le \fc{mG}{s_0-F},\quad G=\max_i \ve{\nb f_i(0)}_2,
\]</span> so we can take <span class="math inline">\(\boxed{s_0=mGR+F}\)</span>. Then <span class="math inline">\(t^{(0)} \ge \rc{mGR}\)</span> so the initial duality gap is <span class="math inline">\(\fc{m+1}{t^{(0)}} \le (m+1) mGR\)</span>. Use the barrier method until the duality gap is <span class="math inline">\(&lt;|\ol p^*|\)</span>, so that we can determine <span class="math inline">\(\sgn(\ol p^*)\)</span>. This requires (take <span class="math inline">\(\mu = 1+\rc{\sqrt{m+1}}\)</span>) <span class="math display">\[
\le \ce{\sqrt{m+1} \log_2 \fc{m(m+1)GR}{|\ol p^*|}}\pa{\rc{2\ga} + c}
\]</span> Newton steps. (Interpret <span class="math inline">\(\lg\pf{GR}{|\ol p^*|}\)</span> as how close the feasibility problem is to the boundary between feasibility and infeasibility.)</p>
<p>(Equality constraints don’t change things too much. ?? <span class="math inline">\(G\)</span>, <span class="math inline">\(R\)</span> refer to reduced/eliminated problem.)</p>
Why did we add in <span class="math inline">\(a^Tx\le 1\)</span>? Otherwise, we minimize <span class="math inline">\(ts-\sum \ln (-(f_i-s))\)</span>, and
\begin{align}
\ddd{s} &amp;= t+\sum \rc{f_i-s} = 0\\
\nb_x &amp;= \sum \fc{-\nb f_i}{f_i-s} =0.
\end{align}
<p>We can’t choose <span class="math inline">\(x=0\)</span> because we need <span class="math inline">\(s&gt;\max f_i\)</span>, and <span class="math inline">\(\nb_x&gt;0\)</span>.</p>
<p>(Note if we add the constraint <span class="math inline">\(a^Tx\le 1\)</span> for phase 1, we have to include it for phase II as well.)</p>
<h4 id="termination-near-phase-ii-central-path">Termination near phase II central path</h4>
<p>During phase I, add in the extra constraint <span class="math inline">\(f_0(x)\le M\)</span> to make it intersect the phase II central path. (We can add the constraint <span class="math inline">\(a^Tx\le 1\)</span> below.) We want the point on the central path for phase I corresponding to <span class="math inline">\(s=0\)</span> to also be on the phase II central path. (I think you won’t get to <span class="math inline">\(s=0\)</span> exactly, but you get close—then the duality gap is <span class="math inline">\(m(M-f_0)+\)</span>(something small).)</p>
\begin{align}
\min_{f_i\le s, f_0\le M, Ax=b} s:&amp;&amp; \nb(ts + (\sum-\ln (s-f_i)) - \ln (M-f_0) + A^T \nu) &amp;=0\\
\iff &amp;&amp; t&amp;=\sum \rc{s-f_i} = \sum -\rc{f_i}\\
&amp;&amp; \rc{M-f_0} \nb f_0 + \sum\rc{s-f_i} \nb f_i + A^T\nu &amp;=0\\
\min_{f_i\le 0, Ax=b}f_0:&amp;&amp; \nb(tf_0+\sum - \ln (-f_i) + A^T \nu) &amp;=0\\
\iff &amp;&amp;t\nb f + \fc{\nb f_i}{f_i} + A^T \nu &amp;=0
\end{align}
<p>Make these match by setting <span class="math inline">\(t=\rc{M-f_0}\)</span>. I.e., the initial duality gap for phase 2 is <span class="math inline">\(\fc{m}{t} = m(M-f_0)\)</span>.</p>
<h3 id="inner-steps">2 Inner steps</h3>
<p>Given <span class="math inline">\(x^*(t)\)</span>, how many steps does it take to compute <span class="math inline">\(x^*(\mu t)\)</span>?</p>
<p>How can you use the fact that <span class="math inline">\(x\)</span> is optimal for <span class="math inline">\(tf_0(x) + \phi(x)\)</span> to prove how optimal it is for <span class="math inline">\(\mu t f_0(x)+\phi(x)\)</span>?</p>
<p>Let <span class="math inline">\(x=x^*(t)\)</span>, <span class="math inline">\(x^+=x^*(\mu t)\)</span>.</p>
<p>It suffices to bound <span class="math display">\[\mu t f_0(x) + \phi(x) - \mu tf_0(x^+) - \phi(x^+).\]</span></p>
<p>We can’t bound <span class="math inline">\(\ln(-f_i)\)</span> directly. We can hope to bound the dual function <span class="math inline">\(\cL\)</span> (we have info from the KKT conditions). Use the linear approximation to <span class="math inline">\(\ln\)</span>.</p>
\begin{align}
\mu t f_0(x) + \phi(x) - \mu tf_0(x^+) - \phi(x^+)
&amp;= \mu t f_0(x) - \mu t f_0(x^+) + \sum \ln \pf{\mu f_i(x^+)}{\mu f_i(x^+)}\\
&amp;\le \mu t f_0(x) - \mu t f_0(x^+) + \sum \pa{\fc{\mu f_i(x^+)}{f_i(x)} - 1 -\ln \mu}\\
&amp;= \mu t f_0(x) - \mu t f_0(x^+) + t\pa{\sum \rc{tf_i} \mu f_i(x^+)} - m - m\ln \mu\\
&amp;= \mu t f_0(x) - \mu t \cL(x^+, \la, \nu) - m - m \ln \mu &amp;Ax^+=b, \la_i  = \rc{tf_i}\\
&amp;\le \mu tf_0(x) - \mu t g(\la, \nu) - m - m\ln \mu\\
&amp;=m(\mu-1 - \ln \mu).
\end{align}
<p>Thus, the number of inner steps is <span class="math display">\[
\fc{f(x)=p^*}{\ga}+c = \fc{m(\mu - 1 - \ln \mu)}{\ga} + c
\]</span> (See <a href="second-order.html">Newton</a> for definition of <span class="math inline">\(\ga\)</span>. We approximate <span class="math inline">\(\ln\ln\)</span> by a constant.) This is approximately quadratic for small <span class="math inline">\(\mu\)</span> (<span class="math inline">\(O(m(\mu-1)^2)\)</span>), linear (<span class="math inline">\(O(m\mu)\)</span>) for large <span class="math inline">\(\mu\)</span>. (This does not depend on <span class="math inline">\(n,p\)</span>.)</p>
<h3 id="outer-steps">3 Outer steps</h3>
<p>The number of outer steps needed is <span class="math inline">\(\ce{\fc{\ln (m/(t^{(0)}\ep))}{\ln \mu}}\)</span> so the total number of Newton steps needed is <span class="math display">\[\ce{\log_\mu \pf{m}{t^{(0)}\ep)}} \pa{\fc{m(\mu - 1 - \ln \mu)}{\ga} + c}. \]</span></p>
<ul>
<li>Choosing <span class="math inline">\(\mu\)</span> constant, get <span class="math inline">\(O\pf{\pa{\ln\pf{m}\ep}m}{\ga}\)</span>.</li>
<li>Set <span class="math inline">\(\mu\)</span> small to do better. Balance <span class="math inline">\((\mu-1-\ln \mu)m=O(m(\mu-1)^2)\)</span> and <span class="math inline">\(c\)</span> by setting <span class="math inline">\(\mu = 1+\rc{\sqrt m}\)</span>, and get <span class="math inline">\(O(\sqrt m)\)</span>. (Recall <span class="math inline">\(m\)</span> is number of constraints.)</li>
<li>In practice, though, choose <span class="math inline">\(\mu\)</span> constant.</li>
</ul>
<h3 id="total">Total</h3>
\begin{align}
N_I &amp;= O(\sqrt m \log \pf{mGR}{|\ol p^*|} \rc{\ga})\\
N_{II} &amp;= O(\sqrt m \log \pf{m(M-p^*)}{\ep}) \prc{\ga}.
\end{align}
<p>Explanation: The point at the end of phase I has duality gap <span class="math inline">\(\le (m+1)(M-p^*)\)</span>.</p>
<h3 id="variations">Variations</h3>
<p>What are other ways to do Phase I?</p>
<ul>
<li>Sum of infeasibilities <span class="math inline">\(\min_{f_i\le s_i, Ax=b, s\ge0} \one^Ts\)</span>. Why use this? When infeasible, the optimal point often violates a small unber of inequalities (cf. <span class="math inline">\(l_1\)</span>-regression, basis pursuit). Here the penalty is <span class="math inline">\(l_1\)</span> not <span class="math inline">\(l_\iy\)</span>.</li>
<li>Use infeasible start Newton to solve the barrier formulation <span class="math display">\[\min_{f_i\le s, Ax=b, s=0} f_0\qquad \min_{Ax=b, s=0} t^{(0)} f_0-\sum \ln (s-f_i).\]</span> (Infeasible start means you can start with points violating equality constraints.)</li>
</ul>
<p>What if we don’t know a point in <span class="math inline">\(\bigcap dom(f_i)\)</span>? Add a translation, <span class="math inline">\(\min_{..., z_i=0} t^{(0)} f_0(x+z_0) - \sum \ln (s-f_i(x+z_i))\)</span>.</p>
<p>Disadvantage: There is no good stopping criterion when infeasible.</p>
<h2 id="primal-dual-ipm">Primal-dual IPM</h2>
<p>Review <a href="constrained.html">constrained optimization</a>. Here there is no distinction between inner and outer iterations.</p>
<p>Applying the infeasible start Newton method to the barrier problem gives <span class="math display">\[ \matt{t\nb^2 f_0+\nb^2 \phi A^T}{A^T}{A}{0} \coltwo{\De x_{nt}}{\nu_{nt}}  = -\coltwo{t\nb f_0+\nb \phi}{0}. \]</span> The residual is <span class="math inline">\((\nb f + \nb \phi + A^T\nu, Ax-b)\)</span>, <span class="math inline">\(\phi = -\sum \rc{t}\ln (-f_i)\)</span>.</p>
<p>But here <span class="math inline">\(t\)</span> is fixed. We want to treat <span class="math inline">\(t\)</span> as a variable. Actually, we introduce <span class="math inline">\(\la\)</span>.</p>
<p>Recall that a point on the central path gives a dual feasible <span class="math inline">\((\la, \nu)\)</span> with <span class="math inline">\(\la_if_i=-\rc t\)</span>. We write everything in terms of the primal <span class="math inline">\(x\)</span> and dual <span class="math inline">\((\la,\nu)\)</span>. Introduce <span class="math inline">\(\la\)</span> as a variable that we want to satisfy <span class="math inline">\(\la_if_i=-\rc t\)</span> (the modified KKT equation) to get the (dual, centrality, primal) residual <span class="math display">\[r_t(x,\la,\nu) = \colthree{\nb f + Df^T\la + A^T\nu}{-\diag(\la) f - \rc t \one}{Ax-b} =: \colthree{\De x}{\De \la}{\De \nu} = -\colthree{r_{dual}}{r_{cent}}{r_{pri}}.\]</span></p>
The Newton step for solving the modified KKT equations
\begin{align}
\nb f_0 + Df^T \la + A^T\nu &amp;=0\\
-\la_i f_i &amp;=\rc t\\
Ax-b &amp;=0
\end{align}
is hence (set the gradients of these equations to the negative residuals)
\begin{align}
\mattn{\nb^2 f_0+\sum \la_i \nb^2 f_i}{Df^T}{A^T}{-\diag(\la)Df}{-\diag(f)}{0}{A}00 \colthree{\De x}{\De \la}{\De \nu} = -\colthree{r_{dual}}{r_{cent}}{r_{pri}}.
\end{align}
<p>(<span class="math inline">\(Df\)</span> has <span class="math inline">\(\nb f_i\)</span> as rows.) The solution is the primal-dual search direction. It is not the same as the search direction in the barrier method (because here we’re changing <span class="math inline">\(\la\)</span>).</p>
Solving for <span class="math inline">\(\De \la\)</span> and substituting gives
\begin{align}
\matt{\nb^2 f_0 + \sumo im \la_i \nb^2 f_i + \sumo im -\fc{\la_i}{f_i}\nb f_i \nb f_i^T}{A^T}A0 \coltwo{\De x_{pd}}{\De \nu_{pd}} &amp;= 
\coltwo{r_{dual} - \sum \la_i \nb f_i - \sum \fc{\nb f_i}{tf_i}}{r_{pri}}\\
&amp;=-\coltwo{\nb f_0 + \rc t \sumo im -\rc{f_i}\nb f_i+A^T\nu}{r_{pri}}.
\end{align}
<p>Compare to the Newton step for the centering method (<span class="math inline">\(t\)</span> fixed) in the barrier method. The upper-left entry is replaced by <span class="math inline">\(t\nb^2 f_0 + \sumo im -\rc{f_i} \nb^2 f_i + \sumo \rc{f_i^2} \nb f_i f_i^T\)</span>, and the dual residual is instead <span class="math inline">\(t\nb f_0 + \sumo im -\rc{f_i} \nb f_i\)</span>.</p>
<p>The iterates are not necessarily feasible, so we can’t evaluate a duality gap. Define the <strong>surrogate duality gap</strong> for <span class="math inline">\(x,f(x)&lt;0, \la\ge 0\)</span> by <span class="math display">\[\wh \eta(x,\la) = -f^T\la.\]</span> It is the duality gap if <span class="math inline">\(x\)</span> were primal feasible and <span class="math inline">\(\la,\nu\)</span> were dual feasible (<span class="math inline">\(r_{pri}=0,r_{dual}=0\)</span>).</p>
<p>The algorithm:</p>
<p>(Feasible start) Start with <span class="math inline">\(x\)</span> such that <span class="math inline">\(f_i(x)&lt;0\)</span>, <span class="math inline">\(\la&gt;0\)</span>, <span class="math inline">\(\mu&gt;1\)</span>, <span class="math inline">\(\ep_{feas}&gt;0\)</span>, <span class="math inline">\(\ep&gt;0\)</span>.</p>
<ol type="1">
<li>Set <span class="math inline">\(t=\mu m/\wh eta\)</span>.</li>
<li>Compute primal-dual search direction <span class="math inline">\(\De y_{pd}\)</span>.</li>
<li>Line search and update.</li>
<li>Repeat until <span class="math inline">\(\ve{r_{pri}}_2\le \ep_{feas}\)</span>, <span class="math inline">\(\ve{r_{dual}}_2\le \ep_{feas}\)</span>, and <span class="math inline">\(\wh \eta \le \ep\)</span>.</li>
</ol>
<p>(Note <span class="math inline">\(r_{cent}\)</span> means that we stick close to the central path.)</p>
<h2 id="intuitions">Intuitions</h2>
<ul>
<li>Each constraint has the force <span class="math inline">\(\rc{f_i(x)}\nb f_i(x)\)</span> and the objective force field is <span class="math inline">\(-t\nb f_0(x)\)</span>.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Logic of provability</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/logic/logic_of_provability.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/logic/logic_of_provability.html</id>
    <published>2016-08-25T00:00:00Z</published>
    <updated>2016-08-25T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Logic of provability</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-25 
          , Modified: 2016-08-25 
	</p>
      
       <p>Tags: <a href="/tags/logic.html">logic</a>, <a href="/tags/self-reference.html">self-reference</a>, <a href="/tags/provability.html">provability</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#systems">Systems</a></li>
 <li><a href="#questions">Questions</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>George Boolos</p>
<h2 id="systems">Systems</h2>
<p>Normal: all tautologies, distribution axioms, closed under MP, necessitation (???), substitution.</p>
<p>(I’m confused about necessitation. Aren’t there true things without proof? Yes, but you couldn’t have written them down in the first place! The fact that you can write down <span class="math inline">\(A\)</span> as a result of some deductions means you proved <span class="math inline">\(A\)</span>, so you might as well say you proved <span class="math inline">\(\square A\)</span> too! Note <span class="math inline">\(A\implies \square A\)</span> is a deduction rule, but <span class="math inline">\(A\to \square A\)</span> is NOT TRUE! The difference is that in the deduction rule, <span class="math inline">\(A\)</span> must have been deduced as well. I hesitate to call this a deduction rule, actually… This can only be done top-level, you can’t do it on <span class="math inline">\(A\wedge \neg A\)</span>… A rule of inference cannot always be interpreted as an axiom! This inference rule is sort-of like Kripke… I think you have to be very careful here to avoid <span class="math inline">\(A\to \square A, \neg A \to \square \neg A, \square A \wedge \square \neg A\)</span>.)</p>
<p>Additional axioms.</p>
<ul>
<li>K (Kripke): no more.</li>
<li>K4: <span class="math inline">\(\square A \to \square\square A\)</span>. This is true and provable in PA.</li>
<li>T: <span class="math inline">\(\square A \to A\)</span>. This is NOT provable in PA. What happens if we add it?</li>
<li>S4: K4+T.</li>
<li>B: T+<span class="math inline">\(\square \diamond A\)</span>.</li>
<li>S5: T+<span class="math inline">\(\diamond A \to \square \diamond A\)</span>.</li>
<li>GL: Lob’s Theorem <span class="math inline">\(\square (\square A \to A) \to \square A\)</span>.</li>
</ul>
<p>Inclusions: Note K4<span class="math inline">\(\subeq\)</span>GL, B<span class="math inline">\(\subeq\)</span>S5.</p>
<p>Question: Why doesn’t K4 imply GL?</p>
<h2 id="questions">Questions</h2>
<p>https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems</p>
<p>What is true arithmetic?</p>
<p>Undecidable: <span class="math inline">\(\neg \square X \wedge \neg \square \neg X\)</span>.</p>
<ul>
<li>Let <span class="math inline">\(S\)</span> be the sentence such that <span class="math inline">\(S=\neg \square S\)</span>.
<ul>
<li>Is it true that <span class="math inline">\(\neg \square S \wedge \neg \square \neg S\)</span>?</li>
<li>Is $ (S S)$?</li>
<li>Is $ ((S S))$?</li>
</ul></li>
<li>What happens if we add <span class="math inline">\(\neg \square \perp\)</span> as an axiom?</li>
<li>I don’t understand <span class="math inline">\(\om\)</span>-consistency. First incompleteness: if arithmetic is <span class="math inline">\(\om\)</span>-consistent, then arithmetic is incomplete. Is this weaker or stronger than consistency? 1-consistency is stronger, because <span class="math inline">\(\square \cdots \square \perp\)</span> are not theorems. (?) <span class="math inline">\(\om\)</span>-consistency implies 1-consistency, so <span class="math inline">\(\om\)</span>-consistency is strongest.</li>
</ul>
<p>GLS: GL+<span class="math inline">\(\square A\to A\)</span>, only MP.</p>
<p>GL, GLS have decision procedures. (They only capture part of PA.)</p>
<p>Formalizing existence?</p>
<p>If <span class="math inline">\(X\)</span> is true (in all consistent extensions) but not provable, then is <span class="math inline">\(\square X\)</span> decidable? What about <span class="math inline">\(\square \square X\)</span>? Etc.</p>
<p>Suppose <span class="math inline">\(X=\neg \square X\)</span>. Why doesn’t <span class="math inline">\(\neg X \to \square X \to X\)</span> so <span class="math inline">\(\neg X\to \perp\)</span> constitute a proof of <span class="math inline">\(X\)</span>?</p>
<ul>
<li>Because you can’t use the law of the excluded middle. (Is it true that <span class="math inline">\(\square (X\to Y), \square (\neg X\to Y)\)</span> imply <span class="math inline">\(\square Y\)</span>? I think there shouldn’t be any problem.)</li>
<li>Because you can’t use <span class="math inline">\(\square X \to X\)</span>. You don’t have the axiom <span class="math inline">\(\square X\to X\)</span>, and by Lob, <span class="math inline">\(\square (\square X\to X)\)</span> only when <span class="math inline">\(\square X\)</span>.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[CYHB13] Definability of Truth in Probabilistic Logic</title>
    <link href="http://holdenlee.github.io/notebook/posts/cs/ai/control/truth_in_prob_logic.html" />
    <id>http://holdenlee.github.io/notebook/posts/cs/ai/control/truth_in_prob_logic.html</id>
    <published>2016-08-19T00:00:00Z</published>
    <updated>2016-08-19T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[CYHB13] Definability of Truth in Probabilistic Logic</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-19 
          , Modified: 2016-08-26 
	</p>
      
       <p>Tags: <a href="/tags/ai%20safety.html">ai safety</a>, <a href="/tags/logic.html">logic</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#intro">Intro</a></li>
 <li><a href="#reflection">Reflection</a></li>
 </ul> </div>

  <div class="blog-main">
    <!--Can't have True predicate,
$$\forall \ph: True(\ce{\ph}) \iff \ph.$$-->
<!--don't need to be computable, just definable. already impossible-->
<h2 id="intro">Intro</h2>
<p>Undefinability of truth: no expressive and consistent language can contain its own truth predicate, satisfying <span class="math display">\[\forall \ph: True(\ce{\ph}) \iff \ph.\]</span></p>
<p>Responses:</p>
<ol type="1">
<li>Work with meta-languages.</li>
<li>Accept some sentences are neither true nor false.</li>
</ol>
<p>“Then we may continue to define True by the strong strong reflection property that the truth values of <span class="math inline">\(\ph\)</span> and <span class="math inline">\(True(\ce{\ph})\)</span> are the same for every <span class="math inline">\(\ph\)</span>. (?)</p>
<p>Unsatisfying.</p>
<ul>
<li>No way test if <span class="math inline">\(\ph\)</span> is undefined</li>
<li>No bound on number of undefined sentences.</li>
</ul>
<p>Possible over prob. logic!</p>
<!-- can... -->
<p>Fix a theory <span class="math inline">\(T\)</span> over a language <span class="math inline">\(L\)</span>. (We will let also let <span class="math inline">\(L\)</span> denote the set of sentences.)</p>
<p><strong>Definition</strong>: A probability function <span class="math inline">\(\Pj:L\to [0,1]\)</span> is <strong>coherent</strong> if there is a probability measure <span class="math inline">\(\mu\)</span> over models of <span class="math inline">\(L\)</span> such that <span class="math display">\[\Pj(\ph) = \mu(\set{\mathcal M}{\mathcal M \vDash \ph}).\]</span></p>
<p><strong>Theorem</strong>: <span class="math inline">\(\Pj\)</span> is coherent iff all these hold.</p>
<ol type="1">
<li>For each <span class="math inline">\(\ph,\psi\)</span>, <span class="math inline">\(\Pj(\ph)=\Pj(\ph\wedge \psi)+\Pj(\ph\wedge \neg \psi)\)</span>.</li>
<li>For each tautology (something that is true in all models) <span class="math inline">\(\Pj(\ph)=1\)</span>.</li>
<li>For each contradiction <span class="math inline">\(\ph\)</span>, <span class="math inline">\(\Pj(\ph)=0\)</span>.</li>
</ol>
<p><em>Proof</em>. Define <span class="math inline">\(T_{i+1}\)</span> iteratively by choosing the first statement independent of <span class="math inline">\(T_i\)</span>, and including it with probability <span class="math inline">\(\Pj(\ph_j|T_i)\)</span> and its negation otherwise. Let <span class="math inline">\(T=\bigcup_i T_i\)</span>. (Consistent by compactness.)</p>
<p>Axiom 1 implies <span class="math inline">\(\Pj(\ph|T_i)\)</span> is a martingale. It eventually stabilizes at 0 or 1; the martingale property implies <span class="math inline">\(T\vdash \ph\)</span> with probability <span class="math inline">\(\Pj(\ph|T_0)\)</span>.</p>
<h2 id="reflection">Reflection</h2>
<p>This reflection principle doesn’t work: <span class="math display">\[\forall \ph\in L', \forall a,b\in \Q: a&lt;\Pj(\ph)&lt;b\iff \Pj(a&lt;\Pj(\ce{\ph})&lt;b)=1\]</span> because we can construct <span class="math inline">\(G\iff \Pj(\ce{G})&lt;1\)</span>, <span class="math display">\[\Pj(G)&lt;1\iff \Pj(\Pj(\ce{G})&lt;1)=1 \iff \Pj(G)=1.\]</span></p>
Instead imagine <span class="math inline">\(\Pj\)</span> having arbitrarily precise information about <span class="math inline">\(\Pj\)</span>. Say <span class="math inline">\(\Pj\)</span> is <strong>reflectively consistent</strong> if (these are equivalent)
\begin{align}
\forall \ph\in L',\forall a,b\in \Q: (a&lt;\Pj(\ph)&lt;b) &amp;\implies \Pj(a&lt;\Pj(\ce{\phi})&lt;b) &amp;= 1\\
\forall \ph\in L',\forall a,b\in \Q:  (a\le \Pj(\ph)\le b) &amp;\Leftarrow \Pj(a\le \Pj(\ce{\phi})\le b) &amp;= 1
\end{align}
<p><strong>Theorem</strong> (Consistency of reflection): Let <span class="math inline">\(T\)</span> be a consistent theory over <span class="math inline">\(L\)</span> where <span class="math inline">\((\Q,+)\)</span> can be embedded, and <span class="math inline">\(L'\)</span> be the extension of <span class="math inline">\(L\)</span> by <span class="math inline">\(\Pj\)</span>. There is a coherent probability function <span class="math inline">\(\Pj\)</span> over <span class="math inline">\(L'\)</span> assigning probability 1 to <span class="math inline">\(T\)</span> and satisfying the reflection principle.</p>
<p><em>Proof</em>.</p>
<p><strong>Theorem (Kakutani fixed point)</strong>: <a href="https://en.wikipedia.org/wiki/Kakutani_fixed-point_theorem">wikipedia</a></p>
<ol type="1">
<li>Let <span class="math inline">\(S\subeq \R^n\)</span> be non-empty compact convex, <span class="math inline">\(\ph:S\to 2^S\)</span> such that <span class="math inline">\(\ph(x)\)</span> is non-empty and convex for all <span class="math inline">\(x\in S\)</span>, and <span class="math inline">\(\ph\)</span> has closed graph. Then <span class="math inline">\(\ph\)</span> has a fixed point (<span class="math inline">\(x\in \ph(x)\)</span>).</li>
<li><span class="math inline">\(\ph:X\to 2^Y\)</span>is upper hemicontinuous if for every open <span class="math inline">\(W\subeq Y\)</span>, <span class="math inline">\(\set{x}{\ph(x)\subeq W}\)</span> is open in <span class="math inline">\(X\)</span>. If <span class="math inline">\(X,Y\)</span> are topological vector spaces, <span class="math inline">\(\ph:X\to 2^Y\)</span>, <span class="math inline">\(Y\)</span> convex, then <span class="math inline">\(\ph\)</span> is Kakutani if it is upper hemicontinuous and <span class="math inline">\(\ph(x)\)</span> is non-empty, compact and convex. If <span class="math inline">\(S\)</span> is non-empty, compact, convex subset of Hausdorff locally convex topological vector space, and <span class="math inline">\(\ph:S\to 2^S\)</span> is Kakutani, then it has a fixed point.</li>
</ol>
<ul>
<li>Let <span class="math inline">\(\cA\sub [0,1]^{L'}\)</span> be the set of coherent probability distributions which assign probability 1 to <span class="math inline">\(T\)</span>. <span class="math inline">\(\cA\)</span> is nonempty convex and closed (in a compact set), so compact. (?? Nonempty because can set <span class="math inline">\(\Pj(\ce{\ph})\)</span> arbitrarily and it still satisfies these.</li>
<li>Given <span class="math inline">\(\Pj\in \cA\)</span>, let <span class="math inline">\(R_{\Pj}\)</span> be axioms of the form <span class="math inline">\(a&lt;\Pj(\ce{\ph})&lt;b\)</span> where <span class="math inline">\(a&lt;\Pj(\ph)&lt;b\)</span>. Let <span class="math display">\[ f(\Pj') = \set{\Pj\in \cA}{\Pj(R_{\Pj'}) = 1}.\]</span></li>
<li>Show that <span class="math inline">\(f\)</span> has a closed graph and use Kakutani fixed point.</li>
</ul>
<p>(Why do we care that <span class="math inline">\(R_{\Pj'}\)</span> is countable?)</p>
<blockquote>
<p>“we don’t want P to assert that P is reflectively consistent, we want it to actually <em>be</em> reflectively consistent.”</p>
</blockquote>
<blockquote>
<p>our work shows that the obstructions presented by the liar’s paradox can be overcome by tolerating an infinitesimal error, and that Tarski’s result on the undefinability of truth is in some sense an artifact of the infinite precision demanded by reasoning about complete certainty.</p>
</blockquote>
<p>(Don’t understand the last page: revisit.)</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Technical agenda</title>
    <link href="http://holdenlee.github.io/notebook/posts/cs/ai/control/technical_agenda.html" />
    <id>http://holdenlee.github.io/notebook/posts/cs/ai/control/technical_agenda.html</id>
    <published>2016-08-19T00:00:00Z</published>
    <updated>2016-08-19T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Technical agenda</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-19 
          , Modified: 2016-08-19 
	</p>
      
       <p>Tags: <a href="/tags/ai%20safety.html">ai safety</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#intro">Intro</a></li>
 <li><a href="#highly-reliable-agent-designs">Highly reliable agent designs</a><ul>
 <li><a href="#realistic-world-models">Realistic world models</a></li>
 <li><a href="#decision-theory-w-game-theory">Decision theory (w/ game theory)</a></li>
 <li><a href="#logical-uncertainty">Logical uncertainty</a></li>
 <li><a href="#vingean-reflection">Vingean reflection</a></li>
 </ul></li>
 <li><a href="#error-tolerant-agent-designs">Error-tolerant agent designs</a></li>
 <li><a href="#value-specification">Value specification</a></li>
 <li><a href="#discussion">Discussion</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>[SF14] Agent Foundations for Aligning Machine Intelligence with Human Interests: A Technical Research Agenda</p>
<h2 id="intro">Intro</h2>
<ol type="1">
<li>How can we create an agent that will reliably pursue the goals it is given? (Find an agent architecture to reliably and autonomously pursue a set of objectives.)</li>
<li>How can we formally specify beneficial goals?<br />
</li>
<li>And how can we ensure that this agent will assist and cooperate with its programmers as they improve its design, given that mistakes in early AI systems are inevitable?</li>
</ol>
<p>Make tolerant of human error.</p>
<p>Value learning: how to construct agents that learn what to value?</p>
<p>There are theoretical prerequisits for designing aligned AI above what is require to design misaligned AI.</p>
<p>Topics that are tractable, uncrowded, focused, unable to be outsourced (to machines).</p>
<h2 id="highly-reliable-agent-designs">Highly reliable agent designs</h2>
<p>It’s not sufficient to verify the algorithm behaves well in test settings.</p>
<p>Ex. Bird and Layzell (2002) describe a genetic algorithm which, tasked with making an oscillator, re-purposed the printed circuit board tracks on the motherboard as a makeshift radio to amplify oscillating signals from nearby computers.</p>
<p>Ex. Chess: Trees and backtracking do not immediately yield a solution but they are the conceptual tools; you need them before you can make heuristics work.</p>
<p>In safety-critical applications, it is crucial that theoretical insights come first.</p>
<h3 id="realistic-world-models">Realistic world models</h3>
<p>What is the set of/distribution over possible realities? How are agents scored?</p>
<p>Solomonoff induction (agent predicts, does not affect)</p>
<ol type="1">
<li>Computable environment.</li>
<li>Evaluated by simplicity</li>
<li>Scored by ability to predict next observation.</li>
</ol>
<p>Legg and Hutter define universal measure of intelligence, but high-scoring agents may manipulate the reward function.</p>
<p><strong>Naturalized induction</strong>: But what about an agent learning about environment when embedded within it?</p>
<p>Hutter, <strong>interaction problem</strong>: agent learns and acts upon environment. Maximize reward function.</p>
<p>Human goals are specified in terms of high-level notions.</p>
<p><strong>Ontology identification</strong> (?)</p>
<h3 id="decision-theory-w-game-theory">Decision theory (w/ game theory)</h3>
<p>What does it mean for a decision to be good?</p>
<p>How to construct the <strong>counterfactual</strong> environment?</p>
<p>Ex. different agents logically constrained to behave identically.</p>
<p>Standard causal reasoning identifies “defection” as the best strategy.</p>
<p><strong>Logical counterfactuals</strong> problem.</p>
<p>Updateless decision theory, Wei Dai.</p>
<p>Check out: Model with Haskell implementation of multi-agent games where agents have access to each others’ source code and base decisions on what they can prove. Possible to achieve robust cooperation while remaining unexploitable.</p>
<h3 id="logical-uncertainty">Logical uncertainty</h3>
<p>Knowing the machine/system exactly but not knowing the action/result. ex. reasoning about environment.</p>
<p>What are logically <strong>impossible possibilities</strong>? (???)</p>
<p>How can probabilities be assigned to sentences? Preserving all logical implications requires the system to be deductively omnipotent.</p>
<p>A coherent assignment of probabilities corresponds to a probability distribution over complete, consistent logical theories.</p>
<p>Start with rough approximation of prior and refine.</p>
<p><strong>Logical priors</strong>: What is a satisfactory set of priors over logical sentences that a bounded reasoner can approximate?</p>
<h3 id="vingean-reflection">Vingean reflection</h3>
<p>Humans may specify generally intelligent systems that then create superintelligence.</p>
<p>A self-modifying agent (or any that constructs new agents more intelligent than itself) must reason about the behavior of a system that is more intelligent than the reasoner.</p>
<p>Vingean principle: a parent agent cannot simply check what its successor agent would do in all scenarios, for if it could, then it would already know what actions its successor will take, and the successor would not in any way be smarter. (So it must reason abstractly.)</p>
<p>Study first in the domain of formal logic.</p>
<p><strong>Lobian obstacle</strong>: How can agents gain very high confidence in agents that use similar reasoning system avoiding self-reference paradoxes?</p>
<p>(Carry over into probabilistic logics.)</p>
<h2 id="error-tolerant-agent-designs">Error-tolerant agent designs</h2>
<p>Intelligent systems must be amenable to correction.</p>
<p>Design agents which do not have incentives to escape, manipulate, or deceive. <strong>corrigibility</strong></p>
<p>Ex. moral uncertainty frameworks still incentivize agents not to change the moral framework if it’s flawed.</p>
<p>Toy problem: Shut-down problem: don’t incentivize the agent to cause or prevent pressing the button.</p>
<p><strong>Utility indifference</strong>. Make agents switch preferences on demand without having incentives to cause/prevent switching.</p>
<p><strong>Domesticity</strong>. Incentivize low impact.</p>
<h2 id="value-specification">Value specification</h2>
<p>Not sufficient to construct systems smart enough to figure out intended goals.</p>
<p><strong>Multi-level world-models</strong></p>
<p>Complexities of value: ex. happiness is not smiling humans.</p>
<p><strong>Ambiguity identification</strong>: Identify dimensions neglected by training data.</p>
<p><strong>Operator modeling</strong></p>
<p><strong>Normative uncertainty</strong>: What to do when uncertain about what one should do?</p>
<h2 id="discussion">Discussion</h2>
<p>Why progress can be made today?</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[SF15] Questions of reasoning under logical uncertainty</title>
    <link href="http://holdenlee.github.io/notebook/posts/cs/ai/control/logical_uncertainty.html" />
    <id>http://holdenlee.github.io/notebook/posts/cs/ai/control/logical_uncertainty.html</id>
    <published>2016-08-19T00:00:00Z</published>
    <updated>2016-08-19T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[SF15] Questions of reasoning under logical uncertainty</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-19 
          , Modified: 2016-08-19 
	</p>
      
       <p>Tags: <a href="/tags/ai%20safety.html">ai safety</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#introduction">Introduction</a></li>
 <li><a href="#impossible-possibilities">Impossible possibilities</a></li>
 <li><a href="#logical-priors">Logical priors</a></li>
 <li><a href="#beyond-logical-systems">Beyond logical systems</a></li>
 <li><a href="#discussion">Discussion</a></li>
 <li><a href="#logical-induction-andrew-critch-820">Logical induction, Andrew Critch (8/20)</a><ul>
 <li><a href="#technical">Technical</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="introduction">Introduction</h2>
<p>Knowing how the system behaves but not the result, because of lack of computational resources.</p>
<p>Standard probability theory assumes logical omniscience.</p>
<p>What are logically impossible possibilities? (Things that are logically incorrect, but kept as a possibility because you don’t have the computational power to see they’re incorrect.) Reasonable prior?</p>
<h2 id="impossible-possibilities">Impossible possibilities</h2>
<p>Consider truth values of sentences of logic.</p>
<p>Must preserve some structure: Ex. if probability 1 for <span class="math inline">\(\phi,\phi\implies \psi\)</span>, then probability 1 for <span class="math inline">\(\psi\)</span>.</p>
<p>2 types of logical uncertainty</p>
<ol type="1">
<li>Logical theory</li>
<li>Limited deductive capabilities</li>
</ol>
<p>Complete logical theory: for every sentence <span class="math inline">\(\phi\)</span>, either <span class="math inline">\(\phi\in T\)</span> or <span class="math inline">\(\neg \phi\in T\)</span>.</p>
<p>Incomplete theories can be completed. (Use Zorn.)</p>
<p>(In what sense is there a “true arithmetic”? In the sense that you believe there is a true answer of whether a Turing machine halts!)<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<p>For a deductively unlimited reasoner, an impossible possibiity is any complete theory of logic.</p>
<p>Deductively limited reasoners entertain inconsistent impossible possibilities.</p>
<p>An “impossible possibility,” then, could be any assignment of truth values to logical sentences which does not allow a short proof of inconsistency.</p>
<p>No precise statements about its performance have yet been proven.</p>
<h2 id="logical-priors">Logical priors</h2>
<p>How to construct a weak (e.g. maximum entropy) prior over complete theories? (It’s easy to place 0 probability on complete theories.)</p>
<p><strong>Hutter prior</strong>: For each sentence, select a model in which that sentence is true, and in which certain desirable properties hold (the “Gaifman condition” and the “Cournot condition” (Hutter et al. 2013)). Add the complete theory of that model to the distribution with measure in proportion to the probability of the sentence.</p>
<p>Demski’s prior over complete theories extending <span class="math inline">\(B\)</span>.</p>
<ul>
<li>Take initial set <span class="math inline">\(B\)</span> of known sentences.</li>
<li>Construct complete theory <span class="math inline">\(T\)</span> by starting with <span class="math inline">\(B\)</span> and selecting <span class="math inline">\(\phi\in \Phi\)</span> randomly. (Ex. <span class="math inline">\(\Phi\)</span> is simplicity prior.)</li>
<li>Add <span class="math inline">\(\phi\)</span> to <span class="math inline">\(T\)</span> if consistent, else <span class="math inline">\(\neg \phi\)</span>.</li>
</ul>
<p>Resulting prior probability is uncomputable but approximable.</p>
<p>Christiano uses standard ML techniques.</p>
<p>Undesirable: If <span class="math inline">\(B=\phi\)</span>, then 0 probabilities on complete theories where PA holds—any theory not finitely axiomatizable.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<p>Two ways to update Demski’s prior:</p>
<ol type="1">
<li>completely regenerate from <span class="math inline">\(B\cup \{\phi\}\)</span></li>
<li>condition on <span class="math inline">\(\phi\)</span>.</li>
</ol>
<p>Why these are different: Consider <span class="math inline">\(\psi,\neg \psi\)</span> consistent with <span class="math inline">\(\phi\)</span>.</p>
<ol type="1">
<li>Regenerate: <span class="math inline">\(\ge 2^{-|\psi|}\)</span> probability on <span class="math inline">\(\psi\)</span>.</li>
<li>Conditioned on <span class="math inline">\(\phi\)</span>: may be arbitrarily low. Ex.
<ul>
<li><span class="math inline">\(\neg \psi \to \phi\)</span></li>
<li>almost all theories with <span class="math inline">\(\psi\)</span> also contain <span class="math inline">\(\neg \phi\)</span>.</li>
</ul></li>
</ol>
<p>Intuition</p>
<ol type="1">
<li>Regenerating: doesn’t alter lower bound. (It learns differently from facts it always knew.)</li>
<li>Conditioning: favor explanations</li>
</ol>
<p>Desiderata</p>
<ol type="1">
<li>Coherence: probability distribution over complete theories.</li>
<li>Computable approximation</li>
<li>Occam property: length-based lower bound on probability of consistent sentence.</li>
<li>Inductive: <span class="math inline">\(\Pj(\forall n.\psi(n))\to 1\)</span> as get more confirmations</li>
<li>PA-weakness: nonzero probability on set of complete extensions of PA.</li>
<li>Bounded regret: Regret at most constant worse than other distributions over complete theories</li>
<li>Practicality</li>
<li>Reflectivity: There is <span class="math inline">\(P\)</span> in the language which can be interpreted as representation of <span class="math inline">\(\Pj\)</span>.</li>
</ol>
<p>Note approximations must be incoherent. Reflectivity is possible up to infinitesimal error but difficult.</p>
<ul>
<li>Hutter: coherent (1), inductive (4), PA-weak (5) as long as prob. dist generated from has Occam property. Not computably approximable, practical</li>
<li>Demski: coherent, computably approximable (2), Occam if <span class="math inline">\(\Phi\)</span> (3).</li>
</ul>
<p>Inductivity is the Gaifman condition: If <span class="math inline">\(\Pj\)</span> is logical prior and <span class="math inline">\(\phi\)</span> is true <span class="math inline">\(\Pi_1\)</span> sentence <span class="math inline">\(\forall n:\psi(n)\)</span>, then <span class="math inline">\(\Pj(\cdot |\text{true for }1,...,N)\to 1\)</span>. But must assign probability 0 to some true <span class="math inline">\(\Pi_2\)</span> sentences. These priors are not weak enough!</p>
<p><span class="math inline">\(\Pi_1\)</span>: <span class="math inline">\(\psi\)</span> decidable by primitive recursive function. <span class="math inline">\(\Pi_2\)</span>: <span class="math inline">\(\forall\exists\)</span>.</p>
<p>(Why do we care about statements that don’t follow from PA? Toy problem for reasoning about uncertainty… is this the right problem?)</p>
<p>Desirable property:</p>
<ul>
<li>If prior conditioned on <span class="math inline">\(\phi\)</span> true for <span class="math inline">\(p\)</span> of first <span class="math inline">\(L\)</span> numbers (<span class="math inline">\(L\)</span> large), then <span class="math inline">\(\phi(0)\)</span> has probability <span class="math inline">\(p\)</span>.</li>
</ul>
<h2 id="beyond-logical-systems">Beyond logical systems</h2>
<p>Logical sentences are not the right tool for reasoning about the behavior of objects in the real world.</p>
<p>Realistic reasoning shortcuts. (cf. regret?)</p>
<p>It is not clear that sentences of first order logic are the “correct” underlying structure for logically uncertain reasoning about the world.</p>
<p>Ex. billiards player (?)</p>
<h2 id="discussion">Discussion</h2>
<h2 id="logical-induction-andrew-critch-820">Logical induction, Andrew Critch (8/20)</h2>
<table style="width:25%;">
<colgroup>
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Domain</th>
<th style="text-align: left;">Agent</th>
<th style="text-align: left;">Minimal sufficient conditions</th>
<th style="text-align: left;">Desirability arguments</th>
<th style="text-align: left;">Feasibility</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Rational choice</td>
<td style="text-align: left;">VNM utility maximizer</td>
<td style="text-align: left;">VNM axioms</td>
<td style="text-align: left;">Dutch book arguments, compelling axioms</td>
<td style="text-align: left;">AIXI, POMDP solvers</td>
</tr>
<tr class="even">
<td style="text-align: left;">Probability</td>
<td style="text-align: left;">Bayesian updater</td>
<td style="text-align: left;">axioms of probability theory</td>
<td style="text-align: left;">Dutch book arguments, compelling axioms</td>
<td style="text-align: left;">Solomonoff induction</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Logical uncertainty</td>
<td style="text-align: left;">Garrabrant inductor</td>
<td style="text-align: left;">???</td>
<td style="text-align: left;">Dutch book arguments, historical desiderata</td>
<td style="text-align: left;">LIA2016</td>
</tr>
</tbody>
</table>
<ol type="1">
<li>Improve logical uncertainty theory (min conditions, more conseq)</li>
<li>Using Garrabrant inductors</li>
<li></li>
</ol>
<p>Replace logical omniscience with logical uncertain.</p>
<p>Logical uncertainty was a roadblock to decision theory, etc.</p>
<p>(What would you do tomorrow if you found Critch changed <span class="math inline">\(\pi\)</span> to 4? Would you still go to work? Will pigs fly?)</p>
<p>Suppose you’re an algorithm. What if I do x?</p>
<p>Algorithm: what if I did this thing I didn’t do?</p>
<!-- Gameable but works.
Bayesian updaters don't exist.
Use AI to research AI safety.
when ask question about themselves, as q about math objects. Theory about understanding place in world.
converge to paper \iy
-->
<h3 id="technical">Technical</h3>
<ul>
<li><span class="math inline">\(\Ga\)</span> language for encoding and proving statements about variables and computer programs (PA)</li>
<li><span class="math inline">\(\La\)</span> all sentences expresible in <span class="math inline">\(\Ga\)</span>.</li>
<li>Belief state: <span class="math inline">\(\La\to [0,1]\)</span> constant outside some finite subset (of things you’ve thought of)</li>
<li>Reasoning process: computable sequence of belief states <span class="math inline">\(\set{P_n}{L\to [0,1]}\)</span>.</li>
</ul>
<p>Put criteria for reasoning in a good language.</p>
<p>A good reasoning process <span class="math inline">\(P\)</span> should satisfy</p>
<ol type="1">
<li>computability of <span class="math inline">\(P_n(\phi)\)</span> for any <span class="math inline">\((n,\phi)\)</span>.</li>
<li>convergence: <span class="math inline">\(P_\iy(phi) = \lim_{n\to \iy} P_n(\phi)\)</span> exists</li>
<li>coherent limit: <span class="math inline">\(P_{\iy}\)</span> should be coherent probability distribution, <span class="math inline">\(P_\iy(A\wedge B) +P_iy( A \vee B) = P(A) + P(B)\)</span>.</li>
<li>non-dogmatism: If <span class="math inline">\(\Ga \not \vdash \phi\)</span> then <span class="math inline">\(P_\iy(\phi)&lt;1\)</span> and if <span class="math inline">\(\Ga \not \vdash \neg\phi\)</span> then <span class="math inline">\(P_\iy(\phi)&gt;0\)</span>.</li>
</ol>
<p>There’s a secret property (master criterion) that implies all 4 of these.</p>
<p>Conservatism (not too extreme)</p>
<ul>
<li>Uniform non-dogmatism: for any recursively enumerable sequence of sentences <span class="math inline">\(\{\phi_n\}\)</span> such that <span class="math inline">\(\Ga \cup \{\phi_n\}\)</span> is consistent, there is <span class="math inline">\(\ep&gt;0\)</span> such that for all <span class="math inline">\(n\)</span>, <span class="math inline">\(P_\iy(\phi_n) \ge \ep\)</span>.</li>
<li>Occam bonds: <span class="math inline">\(\ka\)</span> Kolmogorov complexity, <span class="math inline">\(P_\iy(phi) \ge 2^{-\ka(\phi)}\)</span>, <span class="math inline">\(P_\iy(\phi)\le 1-C2^{-\ka(\phi)}\)</span>. (Occam: Simple things shouldn’t be too extreme.)</li>
</ul>
<p>Self-reflection</p>
<p>Tension between Godel (can’t prove soundness/consistency), vs. can notice certain things about own sanity.</p>
<ul>
<li>Belief in consistency: <span class="math inline">\(con(t)=\)</span> “There is no proof of <span class="math inline">\(\perp\)</span> from <span class="math inline">\(\Ga\)</span> using <span class="math inline">\(t\)</span> or fewer symbols.” Then <span class="math inline">\(\lim_{n\to \iy} P_n(con(n))=1\)</span>.</li>
<li>Belief in future consistency: For any encoding <span class="math inline">\(\ol f\)</span> of computable <span class="math inline">\(f:\N\to \N\)</span>, <span class="math display">\[
\lim_{n\to \iy} P_n(con(\ol f(n)))=1.
\]</span> Ex. <span class="math inline">\(f(n) = Ack(n,n)\)</span></li>
</ul>
<p>Sequence of statements <span class="math inline">\(\phi\)</span> is polytime generable if <span class="math inline">\(M(n)=\phi_n\)</span> polytime.</p>
<p>Sequence of T/F questions relatively easy to generate, but can be arbitrarily difficult to answer deductively as <span class="math inline">\(n\)</span> grows. p.g. statements are easy to state, hard to verify.</p>
<!--2nd problem hard -->
<p>Ex. <span class="math inline">\(G(\ol n)\iff\)</span> There is no proof of <span class="math inline">\(\ol G(\ol n)\)</span> in <span class="math inline">\(\le \ol f (\ol n)\)</span> characters.</p>
<p>True, but can’t be proven in fewer characters.</p>
<ul>
<li>Provability induction: Any p.g. sequence of provable theorems <span class="math inline">\(\phi_n\)</span> will eventually be believed by <span class="math inline">\(P_n\)</span> as soon as they are generated, <span class="math inline">\(\lim_{n\to \iy} P_n(\phi_n)=1\)</span>. <span class="math inline">\(P\)</span> outpaces deduction. <!-- p.g. sufficiently expressive for writing down. Failing to have makes exploitable. Can make larger class.--></li>
</ul>
<p>Analogy: Ramanujan vs. Hardy.</p>
Timely manner:
\begin{align}
x_n \simeq_n y_n \iff \lim_{n\to \iy} x_n-y_n=0\\
x_n \gtrsim_n y_n\iff \lim\inf &gt;0\\
...
\end{align}
<ul>
<li>Timely adoption of limits. <span class="math inline">\(p\)</span> sequence of rational probabilities. If <span class="math inline">\(P_\iy(\phi_n) \simeq_n p_n\)</span> then <span class="math inline">\(P_n(\phi_n) \simeq_n p_n\)</span>. (Keep up with todo list.)</li>
<li>Introspection: ask about themselves. Knows what its own beliefs are at time it has them. For any pg <span class="math inline">\(\phi_n\)</span>, any <span class="math inline">\((a,b)\)</span> (or sequence of intervals), any <span class="math inline">\(\ep&gt;0\)</span>, large enough <span class="math inline">\(n\)</span>,
\begin{align}
P_n(\phi_n)&amp;\in (a+\ep,b-\ep) &amp;\implies P_n'P_n(\phi_n)\in (a,b)' &gt; 1-\ep\\
P_n(\phi_n)&amp;\nin (a-\ep,b+\ep) &amp;\Leftarrow P_n('P_n(\phi_n)\in (a,b)') &lt;\ep
\end{align}
(Spit out at once at time <span class="math inline">\(n\)</span>. It’s one static table with introspective property.)</li>
<li>Liar’s paradox resistance: Liar sentences <span class="math inline">\(L_n\)</span>, <span class="math inline">\(\Ga\vdash L_n \iff 'P_n(L_n)\le p'\)</span>. Then <span class="math inline">\(\lim_{n\to \iy} P_n(L_n)=p\)</span>. (Oscillations…)</li>
</ul>
<p>(How sure of being sure…)</p>
<p>Self-trust</p>
<ul>
<li>Trust in future beliefs: For any computable <span class="math inline">\(f(n)&gt;n\)</span>, <span class="math display">\[''P(\phi_n | ' P_{f(n)}(\phi_n) \ge p_n') \gtrsim_n p_n''\]</span> Precise statement <!--$$ \E_n([\phi_n \Ind_{\de_n} (' P_{f(n)}(\phi_n) \ge p_n') \gtr$$--></li>
</ul>
<p>Learning statistical patterns.</p>
<p>Learning provable relationships:</p>
<ul>
<li>case breakdowns: Let <span class="math inline">\(\phi^1,\ldots, \phi^k\)</span> be <span class="math inline">\(k\)</span> p.g. sequences of sentences such that for each <span class="math inline">\(n\)</span>, <span class="math inline">\(\Ga\)</span> proves that <span class="math inline">\(\phi_n^1,\ldots, \phi_n^k\)</span> are exclusive and exhaustive. Then <span class="math display">\[ \lim_{n\to \iy} \sum P (\phi_i)= 1.\]</span></li>
<li>affine relations</li>
</ul>
<p>No study of convergence rates. Point is that it verifies desiderata (first 9) are not contradictory.</p>
<p><strong>Open question</strong>: Continuum hyp, bounded by Kolm complexity. depend on how parametrized. Different versions of Garrabrant inductors. Probabilities dominate each other? All universal semimeasures dominate each other. Garrabrant dominate universal semimeasures. Are all the different good ways comparable?</p>
<p>(Ex. <span class="math inline">\(\phi_n\)</span>: Turing machine has not halted by time <span class="math inline">\(n\)</span>.)</p>
<p>Try not to lose a lot of money on bets. (On agents simpler than you.)</p>
<p>A tad under doubly exponential time.</p>
<p>Mentally simulate all polytime traders. Run Wall Street traders in head output algebraic expressions. Open, algorithms respond. Algorithms respond to beliefs. This is what beliefs are, this is what I wish instead. Find fixpoint. Computably approximate.</p>
<p>Property: can’t lost too much money to polytime traders.</p>
<p>Pascal’s mugging. Where’s your <span class="math inline">\(\iy\)</span> dollars. Bring the briefcase with the cash!</p>
<p>Start with finite budget, inversely proportional to complexity. By being complicated, smarter. Smart ones gain more money, larger bets, affect market prices more. Smart ones get rick and fill the market.</p>
<p>(Maybe that’s what neurons are! jk. SoM: Competing processes.)</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Try to define like this. If there is no proof for “Turing machine halts” then say it is false. (This is a tie-breaker.) Now derive all logical consequences. Take some unknown statement <span class="math inline">\(X\)</span>. Consider machines trying to find proofs of <span class="math inline">\(X,\neg X\)</span>. Neither will halt. But this doesn’t say anything about <span class="math inline">\(X\)</span>! Why is there a true arithmetic?<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Induction schema consists of infinitely many axioms. What if we tried to do this with higher-order logic? What difficulties arise?<a href="#fnref2">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Lob's Theorem</title>
    <link href="http://holdenlee.github.io/notebook/posts/cs/ai/control/lobs_theorem.html" />
    <id>http://holdenlee.github.io/notebook/posts/cs/ai/control/lobs_theorem.html</id>
    <published>2016-08-19T00:00:00Z</published>
    <updated>2016-08-19T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Lob's Theorem</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-19 
          , Modified: 2016-08-19 
	</p>
      
       <p>Tags: <a href="/tags/ai%20safety.html">ai safety</a>, <a href="/tags/logic.html">logic</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#proof">Proof</a></li>
 <li><a href="#questions">Questions:</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>Does PA claim to be sound? (More standard notation: <span class="math inline">\(\square X\)</span> for <span class="math inline">\(PA[X]\)</span>.) <span class="math display">\[PA[PA[X]\implies X]\]</span> No. Then PA would be inconsistent. (Why?) <span class="math display">\[PA[PA[X]\implies X]\implies PA[X].\]</span> (Converse is obviously true.)</p>
<p>Alas, this means we can’t prove PA sound with respect to any important class of statements. (?)</p>
<p>Lob’s sentence: L says “If a proof of L exists, then C.” <span class="math display">\[L = \square L \implies C\]</span> (?If we assume <span class="math inline">\(\square L \implies L\)</span>, then <span class="math inline">\(\square L \implies C\)</span>. Assuming <span class="math inline">\(\neg \square L\)</span> doesn’t give anything.)</p>
<p>cf. “If this sentence is true, then Santa Claus exists.”<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<p>For a sentence to refer to itself, it must contain a self-replicating recipe - when the recipe is executed, it produces a copy of the complete sentence, including the recipe itself</p>
<p>Generalities about Lob.</p>
<ol type="1">
<li>The difference between truth and provability. If we defined <span class="math inline">\(\square\)</span> correctly, <span class="math inline">\(\square S \to S\)</span> looks to always be true. However, it’s not provable, unless <span class="math inline">\(S\)</span> is provable!</li>
<li><span class="math inline">\(\square\)</span> seems like negation in some way. (Cf. proof by contradiction)</li>
<li>We would want PA on occassion to assert its own soundness. But no, it only asserts its soundness on statements that are actually provable.</li>
<li><span class="math inline">\(\square (\square S \to S)\)</span> only when <span class="math inline">\(S\)</span> is provable.</li>
<li>It seems bizarre!</li>
</ol>
<h2 id="proof">Proof</h2>
<p>Note:</p>
<ul>
<li><span class="math inline">\(\implies\)</span> means a deduction rule. From <span class="math inline">\(A\)</span> you can deduce <span class="math inline">\(B\)</span>. <span class="math inline">\(A\implies B\)</span> is not a statement within PA, it is within the meta-logic you’re defining everything in (which can be PA if you want!). (But you can consider <span class="math inline">\(A\to B\)</span>, which is a statement in PA.)</li>
<li><span class="math inline">\(\to\)</span> is a statement in PA. <span class="math inline">\(\to\)</span> is turned into <span class="math inline">\(\implies\)</span> by MP.</li>
</ul>
<p>I’m not distinguishing between <span class="math inline">\(PA \vdash X\)</span> and <span class="math inline">\(\square X\)</span>. They are equivalent (in the meta-logic).</p>
<p>Use</p>
<ol type="1">
<li>(A1) <span class="math inline">\(\square X \implies \square \square X\)</span>. This takes work to prove. Show that given a proof of <span class="math inline">\(X\)</span>, you can turn it into a proof that a proof of <span class="math inline">\(X\)</span> exist! Mind-bending.</li>
<li>(A2) <span class="math inline">\(\square (\square X \to \square \square X)\)</span>. (A1) is provable.</li>
<li>(A3) <span class="math inline">\(\square (\square (A \to B) \to (\square A \to \square B))\)</span>. This is straightforward: if you have a proof of <span class="math inline">\(A\)</span>, and a proof of <span class="math inline">\(A\to B\)</span>, you can make a proof of <span class="math inline">\(B\)</span> by joining the proofs together and adding a modus ponens at the end. You can furthermore prove this within PA.</li>
<li>(MP) <span class="math inline">\(\square A, \square (A\to B)\implies \square B\)</span>. The non-meta version of the above.</li>
<li>(B1) <span class="math inline">\(\square (A\to B), \square (B\to C), \implies \square (A\to C)\)</span>.</li>
<li>(B2) <span class="math inline">\(\square (A\to B), \square (A\to (B\to C)), \implies \square (A\to C)\)</span>.</li>
</ol>
<p>Let <span class="math inline">\(L=\square L \to C\)</span>. If <span class="math inline">\(L\)</span> is provable, then <span class="math inline">\(C\)</span>.</p>
<ol type="1">
<li><span class="math inline">\(\square (\square L \lra \square (\square L \to C))\)</span>. Unpack the box.</li>
<li><span class="math inline">\(\square (\square C\to C)\)</span>. Lob’s hypothesis. <!--We aim to show that if $L$ is provable, then we can prove $C$ is provable, and hence $C$ is provable.--></li>
<li><span class="math inline">\(\square (\square (\square L \to C) \to (\square(\square L)\to \square C))\)</span> by A3.</li>
<li><span class="math inline">\(\square( \square L \to (\square (\square L)\to \square C))\)</span>. I.e., we distributed the <span class="math inline">\(\square\)</span> in (1). If <span class="math inline">\(\square L\)</span> and <span class="math inline">\(\square \square L\)</span>, then <span class="math inline">\(C\)</span>. But this means we can get <span class="math inline">\(C\)</span> from just <span class="math inline">\(\square L\)</span>, because of A2!</li>
<li>Use A2.</li>
<li><span class="math inline">\(\square (\square L \to \square C)\)</span>.</li>
<li><span class="math inline">\(\square (\square L \to C)\)</span>.</li>
<li><span class="math inline">\(\square\square (\square L \to C)\)</span>.</li>
<li><span class="math inline">\(\square\square L\)</span>. KEY! To prove <span class="math inline">\(L\)</span>, you just have to prove <span class="math inline">\(\square L \to C\)</span>, by definition of <span class="math inline">\(L\)</span>! So you get <span class="math inline">\(\square\square L\)</span> for free! (Not <span class="math inline">\(\square L\)</span>.) How did we get <span class="math inline">\(\square\square L\)</span>? We used Lob’s hypothesis and A2.</li>
<li><span class="math inline">\(\square C\)</span>.</li>
</ol>
<h2 id="questions">Questions:</h2>
<ul>
<li>What happens if we add the axiom <span class="math inline">\(\square X \implies X\)</span>? What might break? Is Proofs can now include the statement “<span class="math inline">\(\square X \implies X\)</span>”?</li>
<li>“if PA asserted its own soundness (in general), PA would become inconsistent.” Does this just mean we can’t add this axiom, or that it being provable leads to a contradiction? (Can you get this from “This theorem is unprovable” type things?)</li>
<li>Undefinability of truth: no predicate True such that <span class="math inline">\(True(X)\text{ xor } True(\neg X)\)</span> and <span class="math inline">\(True(X)\implies X\)</span>. Or: if you extend with primitive “True” you get a contradiction.</li>
</ul>
<p>Intuition: If a program doesn’t have native support for meta-programming, then you can’t program in meta-programming. Here, I mean: take a data structure and splice it into the program itself. (But you could always write a compiler in the language, and apply the compiler to the string…)</p>
<p>This setting is different. To prove something, you have to write down the entire proof. You aren’t allowed to make a program that writes down the proof. Ex. you want to show <span class="math inline">\(\forall x, P(x)\)</span>. You are not allowed to write down a program that given <span class="math inline">\(x\)</span>, spits out a proof of <span class="math inline">\(x\)</span>.</p>
<p>If you try the recursive fix, it’s not clear that the language is still consistent. Perhaps now <span class="math inline">\(\square \perp\)</span>, and hence <span class="math inline">\(\perp\)</span>!</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Confused ramblings (wrong?) This sentence is true—even if Santa Claus doesn’t exist! The problem with provability logic - why you can’t just add in <span class="math inline">\(\square X \implies X\)</span> as an axiom—is that you can now prove both <span class="math inline">\(X, \neg X\)</span> by constructing Lob’s sentence! Reflection doesn’t work—it’s not that simple! You have to design a logic to make reflection not inconsistent!<a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Distribution of critical points</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/distribution_of_critical_points.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/distribution_of_critical_points.html</id>
    <published>2016-08-13T00:00:00Z</published>
    <updated>2016-08-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Distribution of critical points</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-13 
          , Modified: 2016-08-13 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <p><a href="http://mathoverflow.net/questions/247469/expected-number-of-local-minima-of-random-polynomial-in-high-dimensions">MO question</a></p>
<p>Consider the inner product space <span class="math inline">\(P_k\)</span> of polynomials of degree <span class="math inline">\(\le k\)</span> on the unit sphere <span class="math inline">\(\mathbb S^{n-1}\subset \mathbb R^n\)</span>. Let <span class="math inline">\(p\sim N(0,\sigma^2 I)\)</span> be a randomly chosen polynomial in <span class="math inline">\(P_k\)</span>, according to the standard Gaussian distribution. What is the average number of local minima of <span class="math inline">\(p\)</span>?</p>
<p><a href="http://arxiv.org/pdf/1101.5990.pdf">Nicolaescu</a> used the Kac-Rice formula to address the case where <span class="math inline">\(n\)</span> is kept fixed and <span class="math inline">\(k\to \infty\)</span>. In fact, his result is more general: the manifold is fixed and the space of eigenfunctions of the Laplacian with eigenvalue <span class="math inline">\(\le L\)</span> is considered.</p>
<p>However, I’m interested in the asymptotics when <span class="math inline">\(k\)</span> is fixed (ex. take <span class="math inline">\(k=3\)</span> or 4) and <span class="math inline">\(n\to \infty\)</span>, in order to help understand the tractability of random optimization problems as <span class="math inline">\(n\to \infty\)</span>. I’m also interested in any other asymptotics that are known, e.g. distribution of values at local minima, distribution of other critical points, etc.</p>
<!--  (modulo $\sum_{i=1}^{n} x_i^2-1$)-->

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2016-08-13</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-08-13.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-08-13.html</id>
    <published>2016-08-13T00:00:00Z</published>
    <updated>2016-08-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2016-08-13</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-13 
          , Modified: 2016-08-13 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p>Goals:</p>
<ul>
<li>Run basic PMI experiments
<ul>
<li><a href="/posts/tcs/machine_learning/neural_nets/pmi_images.html">PMI images</a>, <a href="/posts/tcs/machine_learning/nlp/pmi.html">PMI idea</a></li>
<li>Get dictionary learning/weighted SVD code from Yingyu.</li>
<li>Write up findings.</li>
</ul></li>
<li>Learning
<ul>
<li>Go through half of ATAP.</li>
<li>Ch. 4 of HDP.</li>
<li>Understand conjugate gradients using Chebyshev polys. Look at AGD from this point of view.</li>
<li>Summarize bandit paper.</li>
</ul></li>
<li>Representation learning
<ul>
<li>Clarify idea.</li>
</ul></li>
<li>Tensorflow
<ul>
<li>Get basic LSTM running.</li>
<li>Play with parameters in MNIST.</li>
</ul></li>
</ul>
<p>Summary:</p>
<ul>
<li>Got PMI code running.</li>
<li><a href="/posts/math/analysis/numerical/ATAP.html">Approximation theory and approximation practice</a>: Notes
<ul>
<li>Understood conjugate gradient bound using Chebyshev polynomials (<span class="citation" data-cites="Cyril">@Cyril</span>). <a href="/posts/math/analysis/numerical/chebyshev.html">Notes</a></li>
</ul></li>
<li>Talk: Yuanzhi on Bandit convex optimization (FOLLOW UP)</li>
<li>Representation learning <a href="/posts/tcs/machine_learning/representation.html">summary</a>. Talked to Yingyu and kernel DL.</li>
<li>Read <a href="/posts/tcs/machine_learning/matrices/BH16.html">BH16</a>.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
