<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2016-06-29T00:00:00Z</updated>
    <entry>
    <title>Stanford quals</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/stanford_quals.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/stanford_quals.html</id>
    <published>2016-06-29T00:00:00Z</published>
    <updated>2016-06-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Stanford quals</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-06-29 
          , Modified: 2016-06-29 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#supervised-learning-1">Supervised learning [1]</a></li>
 <li><a href="#unsupervised-learning-1">Unsupervised learning [1]</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>(from Jacob’s notes on Stanford quals)</p>
<h2 id="supervised-learning-1">Supervised learning [1]</h2>
<p><a href="http://cs229.stanford.edu/materials.html">CS229</a></p>
<ul>
<li>Perceptron, logistic regression, SVMs</li>
<li>Kernel methods, Gaussian processes</li>
<li>Boosting (AdaBoost)
<ul>
<li>What is the convergence rate?
<ul>
<li><span class="math inline">\(\exp(-\sum_{t=1}^T \gamma_t^2)\)</span></li>
<li>Key property: <em>adaptive</em></li>
</ul></li>
<li>What problems can it be applied to?
<ul>
<li>binary classification on fixed dataset</li>
</ul></li>
<li><a href="http://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf">notes</a></li>
</ul></li>
<li><a href="cart.html">Decision trees, random forests</a>
<ul>
<li>Bagging (given subset of size <span class="math inline">\(N\)</span>, create many versions of the dataset by subsampling <span class="math inline">\(N\)</span> things with replacement repeatedly)</li>
<li>For each of these versions, also subsample <span class="math inline">\(\sqrt{d})\)</span> of the features to use for the decision tree</li>
</ul></li>
<li>Neural networks</li>
<li>Linear regression</li>
<li>Regularization: L1, L2 and their properties</li>
</ul>
<h2 id="unsupervised-learning-1">Unsupervised learning [1]</h2>
<p><a href="http://cs229.stanford.edu/materials.html">CS229</a></p>
<ul>
<li><a href="matrices/k-means.html">K-means</a></li>
<li><a href="matrices/dimensionality_reduction.html">Linear dimension reduction</a> PCA, CCA, factor analysis, ICA
<ul>
<li>What is PCA / what is it used for?</li>
<li>Given input dataset, assuming it’s elliptical, finds the principle axes of the ellipse
<ul>
<li>In more statistical language, this finds a low-dimensional representation that explains as much of the variance as possible</li>
</ul></li>
<li>Can be computed by just taking SVD of covariance matrix</li>
<li>Typically we mean-center first</li>
<li>Sometimes want to do other scalings but no clear consensus on the best one</li>
<li><a href="matrices/cca.html">CCA</a> What is CCA / what is it used for?</li>
<li>Same intuition as PCA, but wants to find cross-correlations between two sets of variables (X and Y)</li>
<li>Obtained by taking singular vectors of <span class="math inline">\(\Cov(X,X)^{\rc 2}\Cov(X,Y)Cov(Y,Y)^{-\rc2}\)</span>.</li>
<li>?Isn’t this used for semi-supervised learning?
<ul>
<li>?E.g. given two sets of features, use CCA as a regularizer.</li>
</ul></li>
<li><a href="matrices/factor-analysis.html">Factor analysis</a> What is factor analysis / what is it used for?</li>
<li>Basically, this is just matrix factorization</li>
<li>Often allows more domain knowledge to be incorporated</li>
<li><a href="matrices/ica.html">ICA</a> What is ICA / what is it used for?</li>
<li>Blind source separation</li>
<li>Tries to break into independent signals</li>
<li>Often done by maximizing non-gaussianity of signals</li>
</ul></li>
<li>EM</li>
<li>What theoretical property does EM satisfy?
<ul>
<li>Maximizes lower bound <span class="math inline">\(\log p(x) - KL(q(z|x) || p(z|x))\)</span></li>
</ul></li>
<li>What are the general updates?
<ul>
<li>Compute expectation of log-likelihood under current model</li>
<li>Minimize expectation</li>
<li>Sort of like iteratively approximating setting the gradient to zero</li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>CART and random forests</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/cart.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/cart.html</id>
    <published>2016-06-29T00:00:00Z</published>
    <updated>2016-06-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>CART and random forests</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-06-29 
          , Modified: 2016-06-29 
	</p>
      
       <p>Tags: <a href="/tags/CART.html">CART</a>, <a href="/tags/adaptive%20basis%20functions.html">adaptive basis functions</a>, <a href="/tags/random%20forests.html">random forests</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#cart-classification-and-regression-trees">CART (Classification and regression trees)</a></li>
 <li><a href="#random-forests">Random forests</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>See Ch. 16 of Murphy. <a href="http://math.bu.edu/people/mkon/MA751/L18RandomForests.pdf">Presentation</a></p>
<p>An adaptive basis-function model (ABM) is a model of the form <span class="math display">\[ f(x) = w_0+\sumo mM w_m \phi_m(x).\]</span> Typically <span class="math inline">\(\phi_m(x) = \phi(x;v_m)\)</span></p>
<h2 id="cart-classification-and-regression-trees">CART (Classification and regression trees)</h2>
<p>Decision trees recursively partition the input space and define a local model on each region.</p>
<p>For example, if the model is constant on each region, <span class="math inline">\(f(x) = \sumo mM w_m (x\in R_m)\)</span>.</p>
<p>At each node, consider these kinds of splits:</p>
<ul>
<li>Thresholds: <span class="math inline">\(x_i&lt;t\)</span>, <span class="math inline">\(x_i\ge t\)</span>. (Quantitative feature)</li>
<li><span class="math inline">\(x_i=c,x_i\ne c\)</span> (Categorical feature)</li>
</ul>
<p>The cost can be regression or classification cost. Sum the costs for each leaf. Cost:</p>
<ul>
<li>Regression: cost of fitting model on the leaf.</li>
<li>Classification</li>
<li>Misclassification rate of leaf. (Use the most probable class label.)</li>
<li>Entropy: <span class="math inline">\(-\sumo cC \wh\pi_c\lg \wh\pi_c\)</span>. (Recommended)</li>
<li>Gini index <span class="math inline">\(\sumo cC \wh \pi_c(1-\wh \pi_c) = 1-\sumo cC \wh\pi_c^2\)</span>.</li>
</ul>
<p>Algorithm:</p>
<ol type="1">
<li>Start at the root node of a single-node tree, and put all data points at that node.</li>
<li>Find the split at the current node (attribute) that minimizes the cost (maximizes information gain). (If it is deemed not worth splitting, e.g. it doesn’t decrease the cost by much<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>, if it’s reached a specified depth, etc., then move on instead.) Make the split. (The data points are now distributed among the two children.)</li>
<li>Add both child nodes. to the queue.</li>
<li>Continue the algorithm (at 2) with the next node in the queue.</li>
</ol>
<p>Advantages:</p>
<ul>
<li>Easy to interpret</li>
<li>Handle mixed inputs</li>
<li>Insensitive to monotone transformations, robust to outliers.</li>
<li>Automatic variable selection</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>Features are very restricted.</li>
<li>Unstable: small changes in input data can have large changes because of the hierarchical nature of the tree-growing process. (They are high variance estimators.)</li>
</ul>
<h2 id="random-forests">Random forests</h2>
<p><strong>Bagging</strong> (bootstrap aggregating): Train <span class="math inline">\(M\)</span> different trees on independently selected subsets of the data, and compute <span class="math inline">\(f(x)=\sumo mM \rc M f_m(x)\)</span>.</p>
<p>(OR: use boosting instead of taking majority vote.)</p>
<p>BUT this can result in highly correlated predictors.</p>
<p><strong>Random forest</strong>: Decorrelate base learners by learning rees based on a randomly chosen subset of input variables and data points.</p>
<p>(What are the right parameters? <span class="math inline">\(\sqrt d\)</span> features?)</p>
<p><strong>Bayesian adaptive regression trees (BART)</strong>.</p>
<p>? Hierarchical mixtures of experts.</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>This is usually too myopic. Instead use pruning. Grow the full tree, evaluate the cross-validated error on subtrees, and pick a minimal tree whose CV error is within 1 se of the min.<a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>k-means clustering</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/k-means.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/k-means.html</id>
    <published>2016-06-28T00:00:00Z</published>
    <updated>2016-06-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>k-means clustering</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-06-28 
          , Modified: 2016-06-28 
	</p>
      
       <p>Tags: <a href="/tags/clustering.html">clustering</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#questions">Questions</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="http://cs229.stanford.edu/notes/cs229-notes7a.pdf">Reference</a></p>
<p>The <span class="math inline">\(k\)</span>-means clustering is the following. (Lloyd’s algorithm) Let data points be <span class="math inline">\(x^{(i)}\)</span>.</p>
<ol type="1">
<li>Initialize <span class="math inline">\(\mu_1,\ldots, \mu_k\)</span>.</li>
<li>Repeat until convergence.
<ol type="1">
<li>Let <span class="math inline">\(c(i) = \amin_j |x^{(i)}-\mu_j|\)</span>.</li>
<li>Let <span class="math inline">\(\mu_j = \EE_{c(i)=j} x^{(i)}\)</span>.</li>
</ol></li>
</ol>
<p>This is alternating minimization on the distortion function <span class="math display">\[J(c,\mu) = \sumo im \ve{x^{(i)}-\mu_{c(i)}}^2\]</span> with respect to <span class="math inline">\(c\)</span> and <span class="math inline">\(\mu\)</span>. This value decreases and so converges (possible to local optimum); in practice, the <span class="math inline">\(\mu\)</span>’s converge.</p>
<h2 id="questions">Questions</h2>
<p>Are there provable guarantees on <span class="math inline">\(k\)</span>-means clustering—if so, what? And/or is there a (worst-case) hardness result?</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>ICA (Independent components analysis)</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/ica.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/ica.html</id>
    <published>2016-06-28T00:00:00Z</published>
    <updated>2016-06-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>ICA (Independent components analysis)</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-06-28 
          , Modified: 2016-06-28 
	</p>
      
       <p>Tags: <a href="/tags/ICA.html">ICA</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#algorithm">Algorithm</a></li>
 <li><a href="#tensor-algorithm">Tensor algorithm</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="http://cs229.stanford.edu/notes/cs229-notes11.pdf">Reference</a></p>
<p>Given that the components of <span class="math inline">\(s\in \R^n\)</span> are independent, and we observe <span class="math inline">\(x=As\)</span>, we want to find <span class="math inline">\(W=A^{-1}\)</span> and recover <span class="math inline">\(s\)</span>. Let <span class="math inline">\(w_i^T\)</span> be the rows of <span class="math inline">\(W\)</span>.</p>
<p>When the <span class="math inline">\(s_i\)</span> are non-gaussian, the solution is unique up to permutation and scaling. (Otherwise, there is rotational invariance.)</p>
<h2 id="algorithm">Algorithm</h2>
<p>Suppose the cdf of the components is logistic. (This is a reasonable default. Mean-center first.) Let <span class="math inline">\(g(s) = \rc{1+e^{-s}}\)</span>.</p>
<p>Change of coordinates gives <span class="math inline">\(p_x(x) = p_s(Wx)\det(W)\)</span>. The log-likelihood is <span class="math display">\[\ell(W) = \sumo im \pa{\sumo jn \ln g'(w_j^T x^{(i)}) + \ln |W|}.\]</span> Use stochastic gradient ascent.</p>
<p>Note: for problems where successive training examples are correlated, when implementing stochastic gradient ascent, it also sometimes helps accelerate convergence if we visit training examples in a randomly permuted order.</p>
<h2 id="tensor-algorithm">Tensor algorithm</h2>
<p>See “new_thread.pdf” page 30.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Factor analysis</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/factor-analysis.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/factor-analysis.html</id>
    <published>2016-06-28T00:00:00Z</published>
    <updated>2016-06-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Factor analysis</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-06-28 
          , Modified: 2016-06-28 
	</p>
      
       <p>Tags: <a href="/tags/factor%20analysis.html">factor analysis</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#solution-1-assume-independence">Solution 1: Assume independence</a></li>
 <li><a href="#solution-2-factor-analysis">Solution 2: Factor analysis</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="http://cs229.stanford.edu/notes/cs229-notes9.pdf">Reference</a></p>
<p>If <span class="math inline">\(n\gg m\)</span>, and we have data points <span class="math inline">\(x^{(i)}\in \R^n\)</span>, <span class="math inline">\(1\le i\le m\)</span>, how can we find Gaussian structure? We don’t have enough data points to even fit a single Gaussian.</p>
<h2 id="solution-1-assume-independence">Solution 1: Assume independence</h2>
<p>If the covariance matrix <span class="math inline">\(\Si\)</span> is diagonal, minimize the negative log likelihood <span class="math display">\[\sum\pa{\pf{\pa{x_j^{(i)}-\mu_j}^2}{2\si_j^2} + \ln \si_j}\]</span> to get <span class="math inline">\(\Si_{jj} = \EE_{i=1}^m (x_j^{(i)}-\mu_j)^2\)</span>. If <span class="math inline">\(\Si=\si I\)</span>, then <span class="math inline">\(\si^2 = \EE_{i,j}(x_j^{(i)}- \mu_j)^2\)</span>.</p>
<h2 id="solution-2-factor-analysis">Solution 2: Factor analysis</h2>
Break the coordinates into 2 parts <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span> and assume
\begin{align}
z&amp;\sim N(0,I)\\
\ep &amp;\sim N(0,\Psi)\\
x &amp;= \mu+ \La z + \ep.
\end{align}
<p>Calculate <span class="math display">\[\coltwo zx \sim N\pa{\coltwo 0\mu, \matt{I}{\La^T}{\La}{\La\La^T+\Psi}}.\]</span> Now do EM on the log likelihood with respect to <span class="math inline">\(z\)</span> and <span class="math inline">\(\La\)</span>. (details…)</p>
<ul>
<li>“This is just matrix factorization.”</li>
<li>“Often allows more domain knowledge to be incorporated.”</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Linear dimensionality reduction ([CG15])</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/dimensionality_reduction.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/dimensionality_reduction.html</id>
    <published>2016-06-28T00:00:00Z</published>
    <updated>2016-06-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Linear dimensionality reduction ([CG15])</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-06-28 
          , Modified: 2016-06-28 
	</p>
      
       <p>Tags: <a href="/tags/dimension%20reduction.html">dimension reduction</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#types-of-reductions">Types of reductions</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>A unified framework for linear dimensionality reduction: given <span class="math inline">\(n\)</span> points <span class="math inline">\([x_1,\ldots, x_n]\in \R^{d\times n}\)</span>, optimize <span class="math inline">\(f_X(\cdot)\)</span> to produce a linear transformation <span class="math inline">\(P\in \R^{r\times d}\)</span> and let <span class="math inline">\(Y=PX\in \R^{r\times n}\)</span> be the low-dimensional transformed data.</p>
<p><img src="/images/dim_red_chart.png"></p>
<h2 id="types-of-reductions">Types of reductions</h2>
<ul>
<li>PCA</li>
<li><a href="ica.html">ICA</a></li>
<li><a href="cca.html">CCA</a></li>
<li><a href="factor-analysis.html">Factor analysis</a></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>CCA (Canonical correlation analysis)</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/cca.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/cca.html</id>
    <published>2016-06-28T00:00:00Z</published>
    <updated>2016-06-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>CCA (Canonical correlation analysis)</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-06-28 
          , Modified: 2016-06-28 
	</p>
      
       <p>Tags: <a href="/tags/CCA.html">CCA</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p><a href="https://en.wikipedia.org/wiki/Canonical_correlation">Wikipedia</a></p>
<p>Goal: Find the linear combination of <span class="math inline">\((X_i)\)</span> and <span class="math inline">\((Y_j)\)</span> with maximum correlation.</p>
<p>Let <span class="math inline">\(\Si_{XY} = \Cov(X,Y)\)</span> (i.e., <span class="math inline">\(XY^T\)</span>).</p>
<p>We want to maximize (let <span class="math inline">\(\ve{v}_M=v^TMv\)</span>) <span class="math display">\[\max_{a,b} \fc{a^T\Si_{XY}b}{\ve{a}_{\Si_{XX}}\ve{b}_{\Si_{YY}}}.\]</span> Let <span class="math inline">\(c=\Si_{XX}^{\rc 2}a\)</span> and <span class="math inline">\(d=\Si_{YY}^{\rc 2}b\)</span>. Then this is <span class="math display">\[\fc{c^T \Si_{XX}^{-\rc2} \Si_{XY} \Si_{YY}^{-\rc2}d}{\ve{c}_2\ve{d}_2}.\]</span> Thus, find the SVD of <span class="math display">\[\Si_{XX}^{-\rc2} \Si_{XY} \Si_{YY}^{-1} \Si_{YX} \Si_{XX}^{-\rc 2}.\]</span> Change coordinates back to find <span class="math inline">\(a,b\)</span>.</p>
<p>More generally, to find the top <span class="math inline">\(k\)</span> dimensions, we want <span class="math display">\[\max_{M_X \in \mathcal{O}^{d_a\times r}, M_Y\in \mathcal{O}^{d_b\times r}} \Tr(M_X^T \Si_{XX}^{-\rc2} \Si_{XY} \Si_{YY}^{-\rc2}M_Y).\]</span> Find the rank <span class="math inline">\(k\)</span> SVD, the matrices consist of the top <span class="math inline">\(k\)</span> SV’s.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[JSA15] Beating the Perils of Non-Convexity - Guaranteed Training of Neural Networks using Tensor Methods</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/JSA15.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/JSA15.html</id>
    <published>2016-06-27T00:00:00Z</published>
    <updated>2016-06-27T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[JSA15] Beating the Perils of Non-Convexity - Guaranteed Training of Neural Networks using Tensor Methods</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-06-27 
          , Modified: 2016-06-27 
	</p>
      
       <p>Tags: <a href="/tags/neural%20net.html">neural net</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#notes">Notes</a></li>
 <li><a href="#algorithm">Algorithm</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>Gives the first provable risk bounds for 2-layer neural nets via an efficient algorithm. Assumes that the generative distribution for the inputs <span class="math inline">\(x\)</span>’s is known or estimable (the algorithm uses the 3rd order score function). Algorithm is based on tensor decomposition.</p>
<p>Remarks:</p>
<ul>
<li>The distribution <span class="math inline">\(p(y|x)\)</span> defined by the NN seems funny. <span class="math inline">\(y\in \{0,1\}\)</span> with <span class="math display">\[f(x) = \E[\wt y|x] = \an{\si_2,\si(A_1^T x + b_1)}+b_2.\]</span> But this means that we must have <span class="math inline">\(f(x)\in \{0,1\}\)</span>! Usually this is put through another sigmoid function…</li>
</ul>
<h2 id="notes">Notes</h2>
<p>See [JSA14].</p>
<p><strong>Lemma (Stein’s identity)</strong>: Under some regularity conditions, <span class="math display">\[\EE_{x\sim p} [G(x) \ot \nb_x\ln p(x)] = \EE_{x\sim p} \nb_x G(x).\]</span> (For example, for <span class="math inline">\(p(x)\)</span> Gaussian, <span class="math inline">\(\nb_x \ln p(x) = -x\)</span>.)</p>
<p><em>Proof</em>. By IbP, <span class="math display">\[\int G(x)_i \pdd{x_j} \ln p(x) p(x) \dx = \int G(x)_i \pdd{x_j}p(x) = -\int \pdd{x_j} G(x)_i p(x)\dx.\]</span></p>
<h2 id="algorithm">Algorithm</h2>
<ol type="1">
<li>Tensor decomposition</li>
<li>Fourier algorithm</li>
<li>Ridge (linear) regression</li>
</ol>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[BGKP16] Non-negative matrix factorization under heavy noise</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/BGKP16.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/BGKP16.html</id>
    <published>2016-06-27T00:00:00Z</published>
    <updated>2016-06-27T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[BGKP16] Non-negative matrix factorization under heavy noise</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-06-27 
          , Modified: 2016-06-27 
	</p>
      
       <p>Tags: <a href="/tags/NMF.html">NMF</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#summary">Summary</a></li>
 <li><a href="#assumptions">Assumptions</a></li>
 <li><a href="#algorithm">Algorithm</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="summary">Summary</h2>
<p>A provable algorithm for NMF <span class="math inline">\(A=BC\)</span> without assuming that the noise of every column of <span class="math inline">\(A\)</span> has small noise.</p>
<p>Under <strong>heavy noise</strong> <span class="math display">\[\forall T\subeq [n], |T|\ge \ep n\implies \rc{|T|}\ve{\sum_{j\in T}N_{\bullet,j}}_1\le \ep\]</span> and in the dominant NMF model (dominant features, dominant basis vectors, nearly pure records), the TSVDNMF algorithm finds <span class="math display">\[\ve{B_{\bullet,l}-\wt B_{\bullet, l}}_1\le \ep_0.\]</span></p>
<p>Under dominant NMF assumptions D1, D3, <span class="math inline">\(B\)</span> is identifiable.</p>
<p>Remarks:</p>
<ul>
<li>Dominant features is a relaxation of anchor words.</li>
<li>[AGKM12] (the original algorithm for NMF) requires separability, and does poorly under noise (because under noise the vertices of the simplex may no longer be the vertices of the simplex). Under error <span class="math inline">\(\ve{M-AW}_F^2\le \ep \ve{M}_F^2\)</span>, the algorithm takes time <span class="math inline">\(\exp\pf{r^2}{\ep^2}\)</span>.</li>
<li>Almost pure documents is an assumption not in AGKM12.</li>
<li>It only achieves constant error. (Can we do better than this?)</li>
<li>Heavy noise subsumes many noise models. Note that heavy noise is a bound on <span class="math inline">\(\ve{\sum_{j\in T}N_{\bullet, j}}_1\)</span>, not <span class="math inline">\(\sum_{j\in T}\ve{N_{\bullet, j}}_1\)</span>.</li>
</ul>
<h2 id="assumptions">Assumptions</h2>
<ul>
<li>Heavy noise was defined above. If the covariance of noise in each column is large enough, <span class="math inline">\(\ve{\Si_j}_2=O\pf{\sqrt d}{\ep^2}\)</span>, then whp the heavy noise assumption holds.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>
<ul>
<li>For example, if it is the sum of <span class="math inline">\(w\)</span> random vectors each with covariance matrix <span class="math inline">\(O(1)\)</span> in norm, then we need <span class="math inline">\(w=\Om\pf{d}{\ep^4}\)</span>. Ex. multinomial noise.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></li>
</ul></li>
</ul>
<ul>
<li>Dominant NMF: See picture. Left: Dominant features (D1). (Note: sets have to be disjoint, not necessarily a partition.) Right: Dominant basis vectors (D2) and nearly pure records (D3).</li>
</ul>
<p><img src="/images/bgkp16-dnmf.png" width="640"></p>
<h2 id="algorithm">Algorithm</h2>
<ol type="1">
<li>Apply thresholding to get <span class="math inline">\(D\)</span>.
<ul>
<li>Initialize <span class="math inline">\(R=[d]\)</span>.</li>
<li>For each row <span class="math inline">\(i\)</span>, calculate a cutoff <span class="math inline">\(\ze_i\)</span>. Set <span class="math inline">\(D_{ij}=(A_{ij}\ge \ze_i) \sqrt{\ze_i}\)</span>.</li>
<li>Sort rows in ascending order and prune rows as follows. (Why? We want to prune the non-catchwords. They may be associated with significantly more documents than the catchwords.) <img src="/images/bgkp16-alg1.png" width="640"></li>
</ul></li>
<li><p>Take rank-<span class="math inline">\(k\)</span> SVD <span class="math inline">\(D^{(k)}\)</span> of <span class="math inline">\(D\)</span>. (We hope that this is close to a block matrix with nonzeros in <span class="math inline">\(S_l\times T_l\)</span>.)</p>
<img src="/images/bgkp16-alg2.png" width="640"></li>
<li>Identify dominant basis vectors.
<ul>
<li><span class="math inline">\(k\)</span>-means clustering of columns of <span class="math inline">\(D^{(k)}\)</span>.</li>
<li>Apply Lloyd’s algorithm to <span class="math inline">\(D\)</span> with this initialization.</li>
<li>Let <span class="math inline">\((R_i)\)</span> be the <span class="math inline">\(k\)</span>-partition of <span class="math inline">\([n]\)</span>.</li>
</ul></li>
<li>Identify dominant features <span class="math inline">\(J_l\)</span> for each basis vector by: for each <span class="math inline">\(l\)</span>, take the features <span class="math inline">\(i\)</span> (words) with largest <span class="math inline">\(A_{il}\)</span>.</li>
<li><p>Find the basis vectors by averaging the “purest” documents in each <span class="math inline">\(J_l\)</span>. <img src="/images/bgkp16-alg3.png" width="640"></p></li>
</ol>
<!-- Suppose $\ve{M-AW}_F^2\le \ep \ve{M}_F^2$. Devise a better algorithm for separable approximate NMF. -->
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>The paper isn’t clear on the <span class="math inline">\(\ep\)</span> dependence… see supplement. In any case this is true in the case that noise is a sum of many <span class="math inline">\(O(1)\)</span> noises.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>The covariance matrix is <span class="math inline">\((-1_{i\ne j} p_ip_j)_{i,j}\)</span>. Max eigenvalue is at most <span class="math inline">\(\max p_i(1-p_i)\)</span> in absolute value.<a href="#fnref2">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Relevant coordinates: Low-rank</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/relevant_coordinates_svd.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/relevant_coordinates_svd.html</id>
    <published>2016-06-07T00:00:00Z</published>
    <updated>2016-06-07T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Relevant coordinates: Low-rank</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-06-07 
          , Modified: 2016-06-07 
	</p>
      
       <p>Tags: <a href="/tags/linear%20algebra%2B%2B.html">linear algebra++</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#low-rank-submatrix">Low-rank submatrix</a><ul>
 <li><a href="#assumptions">Assumptions</a></li>
 <li><a href="#algorithm">Algorithm</a></li>
 <li><a href="#proof">Proof</a></li>
 <li><a href="#extensions">Extensions</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <p>See also <a href="relevant_coordinates.html">Relevant coordinates</a>.</p>
<h2 id="low-rank-submatrix">Low-rank submatrix</h2>
<p>Suppose <span class="math inline">\(I\subeq [m]\)</span> is a subset of indices. Draw each column of <span class="math inline">\(A\)</span> as follows. <span class="math inline">\(v_I\)</span> is drawn from some distribution close to being supported on a <span class="math inline">\(k\)</span>-dimensional subspace. (For example, first draw <span class="math inline">\(w\in \R^I\)</span> from a distribution supported on a <span class="math inline">\(k\)</span>-dimensional subspace. Now let <span class="math inline">\(v_I=w+\ep\)</span> where <span class="math inline">\(\ep\)</span> is error, independent on different indices. We can relax this in various ways.) For each <span class="math inline">\(i\nin I\)</span>, suppose there is a distribution <span class="math inline">\(D_i\)</span>. Draw <span class="math inline">\(v_i\)</span> from <span class="math inline">\(D_i\)</span> (independently).</p>
<!-- Let $B_{I\times [n]}$ be a rank-$k$ matrix, and let $A_{I\times [n]}=B_{I\times[n]}+E$ where $E$ is noise. For $i\nin I$, let $A_{ij},1\le j\le m$ be independent draws from some distribution, and suppose that $A_{ij}$ for $i\nin I$ are independent. Recover $I$. -->
<h3 id="assumptions">Assumptions</h3>
<ul>
<li>Suppose for simplicity that for each <span class="math inline">\(i\)</span>, <span class="math inline">\(\E v_i=0\)</span>, <span class="math inline">\(\E v_i^2=\Var(v_i)=1\)</span>. We can always normalize so this is the case (paying a little bit error for estimation for <span class="math inline">\(\E v_i\)</span>, <span class="math inline">\(\Var(v_i)\)</span>).</li>
<li>Suppose the distribution on <span class="math inline">\(\R^I\)</span> satisfies the following. For every <span class="math inline">\(i\in I\)</span>, there exists <span class="math inline">\(j\in I, j\ne i\)</span> such that <span class="math inline">\(\E |v_iv_j|\ge \ep\)</span> (for example, <span class="math inline">\(\ep=\rc k\)</span>). (Is this a reasonable assumption? Can we replace it by something more standard?) (This is basically saying that you can’t delete a coordinate and make the distribution on <span class="math inline">\(\R^{I\bs \{i\}}\)</span> almost <span class="math inline">\((k-1)\)</span>-dimensional.)
<ul>
<li><p>Actually, we don’t need this, because:</p>
<p>Lemma: if <span class="math inline">\(B_{ii}=1\)</span>, <span class="math inline">\(B\)</span> is symmetric, has entries in <span class="math inline">\([-1,1]\)</span>, and <span class="math inline">\(B_i\)</span> is a linear combination of <span class="math inline">\(k\)</span> other rows, then there is some <span class="math inline">\(j\ne i\)</span> such that <span class="math inline">\(|B_{ij}|\ge \rc k\)</span>.</p>
<p>Proof. Suppose the <span class="math inline">\(k\)</span> rows are <span class="math inline">\([1,k]\)</span>, and the <span class="math inline">\((k+1)\)</span>th row is a linear combination of these with coefficients <span class="math inline">\(w\)</span>, i.e., letting <span class="math inline">\(C=B_{[1,n]\times [1,n]}\)</span>, <span class="math inline">\(w^TC = [B_{k+1,1},\ldots, B_{k+1,k}]\)</span>. Looking at this linear combination on the <span class="math inline">\(k+1\)</span>th column, <span class="math inline">\(w^T[B_{k+1,1},\ldots, B_{k+1,k}]=1\)</span>. Putting these together, we get <span class="math inline">\(w^TCw=1\)</span>. Because the entries of <span class="math inline">\(C\)</span> are at most 1 in absolute value, this means <span class="math display">\[(|w_1|+\cdots +|w_{k}|)^2\ge 1\implies \exists i, |w_i|\ge \rc k.\]</span></p>
Apply this to the covariance matrix.</li>
</ul></li>
<li>Suppose <span class="math inline">\(\ep\)</span>, the noise, is such that each entry is in <span class="math inline">\([-C,C]\)</span>, zero-centered, and independent.</li>
</ul>
<h3 id="algorithm">Algorithm</h3>
<p>Idea: on average, there is greater correlation between 2 random vectors in a <span class="math inline">\(k\)</span>-dimensional subspace than 2 random vectors in <span class="math inline">\(n\)</span> dimensions.</p>
<ul>
<li>Take <span class="math inline">\(n=\Om(\prc{\ep^2}(\ln m))\)</span> samples. Let them be the columns of <span class="math inline">\(A\)</span>.</li>
<li>Calculate <span class="math inline">\(\rc n AA^T\)</span> (as a guess for the covariance matrix for <span class="math inline">\(v\)</span>).</li>
<li>Declare <span class="math inline">\(i\in I\)</span> iff there is <span class="math inline">\(j\ne i\)</span> such that <span class="math inline">\((\rc{n}AA^T)_{ij}&gt;\fc{\ep}2\)</span>. <!-- * In a graph, connect up $i,j\in[n]$ by an edge iff $|A_{ij}|>\fc{\ep}{2}$.
* W.h.p., all non-isolated vertices are in a connected component. This is $I$. (I.e., declare $i\in I$ iff there is $j\ne i$ such that $(\rc{n}AA^T)_{ij}>\fc{\ep}2$.)--></li>
</ul>
<h3 id="proof">Proof</h3>
<ul>
<li>Let <span class="math inline">\(\wt \E\)</span> be the empirical average, i.e., <span class="math inline">\(\wt \E v_iv_j = \rc n(AA^T)_{ij}\)</span>. We have <span class="math inline">\(|v_iv_j|\le 1\)</span>, so by Chernoff, <span class="math display">\[\Pj(\ab{\wt \E v_iv_j - \E v_iv_j} \ge t) \le 2e^{-nt^2}\]</span> for <span class="math inline">\(i,j\nin I\)</span>. We get a similar bound if <span class="math inline">\(i\in I\)</span> or <span class="math inline">\(j\in I\)</span>, where the randomness is over the noise in <span class="math inline">\(\ep\)</span>. Union-bounding, <span class="math display">\[\Pj(\text{for some $i\ne j$, }\ab{\wt \E v_iv_j - \E v_iv_j} \ge \fc{\ep}{2})\le m^2 e^{-cn\ep^2}.\]</span> Take <span class="math inline">\(n=\Om(\prc{\ep^2}\ln m)\)</span> to get this to be <span class="math inline">\(o(1)\)</span>.</li>
<li>When either <span class="math inline">\(i\nin I\)</span> or <span class="math inline">\(j\nin I\)</span>, we have <span class="math inline">\(\E v_iv_j=0\)</span> so the above event makes <span class="math inline">\(|\wt\E v_iv_j|&lt;\fc{\ep}{2}\)</span>; when <span class="math inline">\(i\in I\)</span>, there is <span class="math inline">\(j\in I\)</span> such that <span class="math inline">\(\E |v_iv_j|&gt;\fc{\ep}{2}\)</span> by assumption so the above event makes <span class="math inline">\(|\wt \E v_iv_j|&gt;\fc{\ep}{2}\)</span>.</li>
</ul>
<h3 id="extensions">Extensions</h3>
<ul>
<li>It’s not really necessary for <span class="math inline">\(v_I\)</span> to be independent. It’s enough that <span class="math inline">\(A_{I\times [n]} = B_{I\times [n]}+E\)</span> where <span class="math inline">\(B_{I\times [n]}\)</span> is a rank <span class="math inline">\(k\)</span>-matrix such that for every <span class="math inline">\(i\in I\)</span>, there is <span class="math inline">\(j\in I\)</span> such that <span class="math inline">\(\an{B_i,B_j} &gt;\ep \ve{B_i}\ve{B_j}\)</span>, and <span class="math inline">\(E\)</span> is random-enough error.</li>
<li>Can we get a result using fewer generative assumptions?
<ul>
<li>Connect <span class="math inline">\(i,j\)</span> if <span class="math inline">\(|\wt \E v_iv_j|&gt;\fc{\ep}{2}\)</span>. We probably reduce to a graph problem, where we want to find a subset where the graph is more dense.</li>
<li>How much can we relax independence? Esp. independence of coordinates <span class="math inline">\(i\nin I\)</span>.</li>
<li>Is worst-case hard (cf. max clique, planted clique)? “Find the subset of size <span class="math inline">\(cn\)</span> that is <span class="math inline">\(\ep\)</span>-close to a rank <span class="math inline">\(k\)</span> matrix.” What about a gap-problem, i.e., there’s a guarantee that there is no subset of size <span class="math inline">\(c'n\)</span> that is <span class="math inline">\(\ep'\)</span> close to rank <span class="math inline">\(c''k\)</span>?</li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
