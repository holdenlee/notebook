<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2016-04-22T00:00:00Z</updated>
    <entry>
    <title>Second-order methods</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/second-order.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/second-order.html</id>
    <published>2016-04-22T00:00:00Z</published>
    <updated>2016-04-22T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Second-order methods</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-22 
          , Modified: 2016-04-22 
	</p>
      
       <p>Tags: <a href="/tags/Newton.html">Newton</a>, <a href="/tags/second-order.html">second-order</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#main-points">Main points</a></li>
 <li><a href="#proofs">Proofs</a></li>
 <li><a href="#scraps">Scraps</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>See <a href="GD.html">gradient descent</a>.</p>
<ul>
<li>What is the general framework?
<ol type="1">
<li>Pick a descent direction <span class="math inline">\(\De x\)</span>.</li>
<li>Choose a step size <span class="math inline">\(\tau&gt;0\)</span>: <span class="math display">\[x^{(t+1)} \leftarrow x+\tau \De x.\]</span></li>
<li>Continue until stop criterion.</li>
</ol></li>
</ul>
<h2 id="main-points">Main points</h2>
<ul>
<li>What’s the shortcoming of gradient descent that we want to fix?
<ul>
<li>It is not invariant under linear transformation.</li>
<li>When the condition number of the Hessian is large, it has bad convergence.</li>
</ul></li>
<li>Steepest descent
<ul>
<li>For a norm <span class="math inline">\(\ved\)</span>, the steepest descent direction is (sd = steepest descent, nsd = normalized steepest descent)
\begin{align}
\De x_{sd} &amp;= \min_{\ve{y}} \nb f(x)^T y.
\end{align}</li>
</ul></li>
<li>Newton method
<ul>
<li>Let
\begin{align}
\De x_{sd} &amp;= -H^{-1}\nb f\\
\De x_{nsd} &amp;=\fc{H^{-1} \nb f}{\ve{H^{-\rc 2} \nb f}} = \fc{H^{-1} \nb f}{\la(x)^2}\\
\la(x) &amp;= \ve{H^{-\rc 2}\nb f}^{\rc 2} = (\nb f^T H^{-1} \nb f)^{\rc 2}.
\end{align}
Here <span class="math inline">\(\la(x)\)</span> is the Newton decrement.</li>
</ul></li>
<li>What are the drawbacks of Newton’s method and how to fix them?
<ul>
<li>Naively it requires computing</li>
</ul></li>
</ul>
<h2 id="proofs">Proofs</h2>
<ul>
<li>Newton method is steepest descent for <span class="math inline">\(H\)</span>:
<ul>
<li>Let <span class="math inline">\(A\)</span> be a symmetric positive definite matrix. Defining <span class="math inline">\(\ved_A\)</span> as follows, we note that the dual norm is the norm corresponding to the inverse.
\begin{align}
\ve{A} :&amp;= x^TAx = \ve{\sqrt A}_2\\
\ve{x}_{A}^* &amp; = \ve{x}_{A^{-1}}.
\end{align}
<ul>
<li><em>Proof.</em> <span class="math display">\[\ve{x}_A^* = \max_{\ve{y}_A=1} x^Ty \stackrel{z = \sqrt{A}y}{=} \max_{\ve{z}=1} x^T A^{-\rc 2} z = \ve{A^{-\rc 2}x^T}  = \ve{x}_{A^{-1}}.\]</span></li>
<li>This calculation also shows that <span class="math display">\[\amin_{\ve{y}_A=1} v^Ty = \fc{A^{-1} x}{\ve{A^{-\rc 2}x}}.\]</span></li>
<li>Thus,
\begin{align}
\De x_{nsd} &amp;= \fc{H^{-1} \nb f(x)}{\ve{H^{-\rc 2} \nb f}}.
\end{align}</li>
</ul></li>
<li>Why is <span class="math inline">\(\De x_{nsd}\)</span> the right normalization? <span class="math display">\[ f\pa{x - t\fc{H^{-1} \nb f(x)}{\ve{H^{-\rc 2} \nb f}}} \approx f(x) + (-t+\fc{t^2}2) \fc{H^{-1} \nb f(x)}{\ve{H^{-\rc 2} \nb f}}\]</span> and <span class="math inline">\(t=1\)</span> minimizes this.</li>
</ul></li>
<li>Why do we use <span class="math inline">\(\ved_H\)</span>?
<ul>
<li>It appears in the second-order Taylor approximation. <span class="math inline">\(f(x) + \nb f^T v + \rc 2 v^T \nb^2 f v\)</span> has a minimum at <span class="math inline">\(\fc{H^{-1}f}{\ve{H^{-\rc 2}f}}\)</span>, not in the gradient direction!</li>
<li>It is invariant to linear transformation: <span class="math inline">\(\De x_{nsd} (f\circ A) = \De x_{nsd} f\)</span>.</li>
</ul></li>
<li>Give an example where gradient descent performs poorly compared to Newton. <span class="math inline">\(f = x_1^2 + \ep x_2^\)</span>, <span class="math inline">\(-\nb f = (-2x_1,-2\ep x_2)\)</span>.</li>
</ul>
<h2 id="scraps">Scraps</h2>
<p>estimating Hessian?</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Non-negative matrix factorization</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/nmf_algorithm.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/nmf_algorithm.html</id>
    <published>2016-04-22T00:00:00Z</published>
    <updated>2016-04-22T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Non-negative matrix factorization</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-22 
          , Modified: 2016-04-22 
	</p>
      
       <p>Tags: <a href="/tags/NMF.html">NMF</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#setup">Setup</a></li>
 <li><a href="#preliminary-calculations">Preliminary calculations</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="setup">Setup</h2>
<p>The setup for alternating minimization is as follows.</p>
<ol type="1">
<li>(Decoding) Let <span class="math inline">\(x \leftarrow \text{decode}_{A^{(t)}} y\)</span>.</li>
<li>(Gradient step) Let <span class="math display">\[A^{(t+1)} = A^{(t)} - \eta \pi(\nb L_x(A))\]</span> where the loss function is the KL-divergence (negative log-likelihood)
\begin{align}
L_x(A) &amp;= \sum y_i \ln \pf{y_i}{Ax_i}\\
\nb L_x(A) &amp;= (-y\odot \rc{Ax}) x^T
\end{align}
and <span class="math inline">\(\pi\)</span> is projection to the probability simplex.</li>
</ol>
<p>The plan is to show this satisfies the conditions needed for approximate gradient descent: each step is correlated with the right direction. We want to find <span class="math inline">\(\al,\be,\ep\)</span> so that <span class="math display">\[ \an{g,A-A^*} \ge \al \ve{A-A^*}^2 + \be \ve{g}^2 - \ep\]</span> (for some norm).</p>
<p>We have to be careful about 2 things.</p>
<ul>
<li>Uniqueness doesn’t hold in general—ex. we can expand the simplex, so there would be a manifold of solutions—but it does hold under separability. We have to use this somehow.</li>
<li>The more complicated the decoding map is, the harder this is to analyze. The MLE estimator is difficult to work with because there is no explicit form. The inverse is probably easier to work with. It gives a biased estimate, but this may be OK if we only want to get close enough (ex. from <span class="math inline">\(\rc{\log n}\)</span> to <span class="math inline">\(\rc{n}\)</span>).</li>
</ul>
<p>We proceed in 2 lemmas.</p>
<ul>
<li>When <span class="math inline">\(A\approx A^*\)</span>, the decodings satisfy <span class="math inline">\(x\approx x^*\)</span>.</li>
<li>When <span class="math inline">\(x\approx x^*\)</span>, the descent direction for <span class="math inline">\(L_x(A)\)</span> is correlated with <span class="math inline">\(A-A^*\)</span>.</li>
</ul>
<p>As a first step, show that <span class="math display">\[\an{\EE_y \pa{-y\odot \pa{\rc{Ax} -\rc{A^*x}}x^T} , A-A^*}&gt;0.\]</span></p>
<h2 id="preliminary-calculations">Preliminary calculations</h2>
First suppose <span class="math inline">\(y=A^*x\)</span> (no noise), <span class="math inline">\(x=x^*\)</span> (perfect decoding). Then letting <span class="math inline">\(D= \diag\prc{(Ax)_i}\)</span>,
\begin{align}
\an{\EE_y \pa{-y\odot \pa{\rc{Ax} -\rc{A^*x}}x^T} , A-A^*}
&amp;= x^T\ub{(A-A^*)^T D (A-A^*)}{=:M^2}x\\
&amp;= \ve{x}_{M}^2.
\end{align}
Now relax <span class="math inline">\(y\approx A^*x\)</span>, We get
\begin{align}
\an{\EE_y \pa{-y\odot \pa{\rc{Ax} -\rc{A^*x}}x^T} , A-A^*}
&amp;= \EE_{y} \ve{x}_M^2 + \cancelto0{(x^TA^{*T} - y^T) D (A-A^*)x}
\end{align}
<p>using <span class="math inline">\(\E y = A^* x\)</span>.</p>
Now relax <span class="math inline">\(x\approx x^*\)</span>. (Warning: <span class="math inline">\(x^*\)</span> simply being close may not imply convergence to <span class="math inline">\(A^*\)</span> without additional assumptions on <span class="math inline">\(A^*\)</span>, because of nonuniqueness.)
\begin{align}
\EE_{x,y} \an{-y^T D x^T, A-A^*}
&amp;= \EE_{x,y} [-y^T D(A-A^*) x].
\end{align}
<p>The dependencies of the random variables are <span class="math inline">\(x^* \to y \to x\)</span> (<span class="math inline">\(\E_y = A^*x^*\)</span>). We can’t simply replace <span class="math inline">\(\E y = A^*x^*\)</span> because we have to average over <span class="math inline">\(x\)</span> first, which depends on the decoding. (If we could replace <span class="math inline">\(y\)</span> like that, we get <span class="math inline">\(\an{x,x-x^*}_M\)</span>.)</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Convex geometry</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/analysis/metric/convex_geometry.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/analysis/metric/convex_geometry.html</id>
    <published>2016-04-20T00:00:00Z</published>
    <updated>2016-04-20T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Convex geometry</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-20 
          , Modified: 2016-04-20 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <p>Notes from Vershynin’s course <a href="http://www-personal.umich.edu/~romanv/papers/GFA-book/GFA-book.pdf">Geometric functional analysis</a>.</p>
<h2 id="functional-analysis-and-convex-geometry">Functional analysis and convex geometry</h2>
<p><strong>Theorem (Approximate Caratheodory)</strong>: Let <span class="math inline">\(x\in \conv A\)</span>. There exist <span class="math inline">\(N\)</span> points <span class="math inline">\(x_i\in A\)</span> such that <span class="math display">\[\ve{x-\rc N\sumo iN x_i}\le \fc{r(A)}{N}.\]</span> (Given <span class="math inline">\(x\)</span> as a linear combination of elements of <span class="math inline">\(A\)</span>, there is a probabilistic algorithm to find this combination.)</p>
<p>(Here, <span class="math inline">\(r(A)\)</span> is the radius of <span class="math inline">\(A\)</span>.)</p>
<p><em>Proof</em>. If <span class="math inline">\(x=\sum a_i z_i\)</span>, sample by <span class="math inline">\(a_i=\Pj(x=z_i)\)</span>. By Chebyshev, <span class="math display">\[\E\ve{x-\rc N \sumo jN z_j}\le \fc{r(A)^2}{N}.\]</span></p>
<p><em>Remark</em>: This can be <a href="../../../tcs/coding/ldc.html">adapted to <span class="math inline">\(L_p\)</span></a>.</p>
<h2 id="banach-mazur-distance">Banach-Mazur distance</h2>
<p>(Skipped.)</p>
<h2 id="concentration-of-measure-and-euclidean-sections-of-convex-bodies">Concentration of measure and Euclidean sections of convex bodies</h2>
<p>Observation: Convex bodies like spheres tend to have a lot of mass concentrated in the “middle”. This is a very powerful observation: concentration of measure for a set implies concentration for Lipschitz functions on the set.</p>
<p>Johnson-Lindenstrauss says that a random projection approximately preserves distances. A sophisticated way to look at this is the following. We can view this as a concentration result: the norm of the projection of <span class="math inline">\(x\)</span> is a Lipschitz function; it must be concentrated around its mean.</p>
<p>A powerful application of concentration of measure is Dvoretzky’s Theorem (big generalization of JL?), which find large Euclidean-like subspaces in general Banach spaces. (What’s the relationship between the <span class="math inline">\(\log\)</span> here and in Dvoretzky?)</p>
<h3 id="concentration-of-measure">Concentration of measure</h3>
<p>For a set <span class="math inline">\(A\)</span> let <span class="math inline">\(A_\ep\)</span> denote the <span class="math inline">\(\ep\)</span>-neighborhood.</p>
<h4 id="sphere">Sphere</h4>
<ol type="1">
<li>(Isoperimetric inequality) Among all sets with given measure, spherical caps minimize <span class="math inline">\(\si(A_\ep)\)</span>. (Proof?)</li>
<li>Let <span class="math inline">\(\si\)</span> be the measure on the sphere. For any measurable <span class="math inline">\(A\subeq \bS^{n-1}\)</span> with <span class="math inline">\(\si(A)\ge \rc 2\)</span>, <span class="math inline">\(\si(A_\ep)\ge 1-e^{-\fc{n\ep^2}2}\)</span>. Proof:
<ul>
<li>For the equator <span class="math inline">\(E\)</span>, <span class="math inline">\(\si(E_\ep)\ge 1-2e^{-\fc{n\ep^2}{2}}\)</span>. Use isoperimetric inequality.</li>
</ul></li>
<li>Corollary: For <span class="math inline">\(f:\bS^{n-1}\to \R\)</span> 1-Lipschitz, letting <span class="math inline">\(M\)</span> be the median, <span class="math display">\[\si(|f-M|\le \ep) \ge 1-2e^{-\fc{n\ep^2}{2}}.\]</span> This remains true if <span class="math inline">\(M\)</span> is replaced by the mean.</li>
</ol>
<h4 id="gaussians">Gaussians</h4>
<p>Consider <span class="math inline">\(\R^n\)</span> with measure <span class="math inline">\(\ga\)</span> given by <span class="math inline">\(N(0,I)\)</span>.</p>
<ol type="1">
<li>(Isoperimetric inequality) Among all sets with given measure, halfspaces minimize <span class="math inline">\(\si(A_\ep)\)</span>.</li>
<li>If <span class="math inline">\(\ga(A)\ge \rc2\)</span>, then <span class="math inline">\(\ga(A_t)\ge 1-e^{-\fc{t^2}{2}}\)</span>.</li>
<li>Corollary: For <span class="math inline">\(f:\R^{n-1}\to \R\)</span> 1-Lipschitz, letting <span class="math inline">\(M\)</span> be the median, <span class="math display">\[\ge(|f-M|\le \ep) \ge 1-2e^{-\fc{n\ep^2}{2}}.\]</span> This remains true if <span class="math inline">\(M\)</span> is replaced by <span class="math inline">\((\E|X|^p)^{\rc p}\)</span> for any <span class="math inline">\(p\ge 1\)</span>.</li>
</ol>
<h3 id="johnson-lindenstrauss">Johnson-Lindenstrauss</h3>
<p>See <a href="../../../tcs/algorithms/jl.html">JL</a>.</p>
<p><strong>Theorem (Johnson-Lindenstrauss)</strong>: Let <span class="math inline">\(|X|=n\)</span>, <span class="math inline">\(X\)</span> a subset of Hilbert space. For any <span class="math inline">\(\ep&gt;0\)</span>, there exists a <span class="math inline">\((1+\ep)\)</span>-embedding of <span class="math inline">\(X\)</span> into <span class="math inline">\(\ell_2^k\)</span>, <span class="math inline">\(k\ge \fc{C}{\ep^2}\ln n\)</span>.</p>
<p><em>Proof.</em> Project randomly using gaussians. We can directly bound the concentration, but let’s be more sophisticated.</p>
<ol type="1">
<li>Project randomly using gaussians. Let <span class="math inline">\(f:\R^{k\times n}\to \R\)</span> be defined by <span class="math inline">\(G\mapsto \ve{Gx}_2\)</span>. This is 1-Lipschitz. Use concentration of measure for Gaussian space, and then union bound.</li>
<li>Take a uniformly random projection (ise the Grassmannian, or equivalently, take a uniform rotation followed by a fixed projection). Let <span class="math inline">\(f:\bS^{n-1}\to \R\)</span> be defined by <span class="math inline">\(x\mapsto \ve{Tx}_2\)</span>. Use concentration of measure for the sphere.</li>
</ol>
<h3 id="dvoretzkys-theorem">Dvoretzky’s Theorem</h3>
<h4 id="statements">Statements</h4>
<p>Dvoretzky finds large Euclidean subspaces.</p>
<ol type="1">
<li><strong>Theorem (General Dvoretzky)</strong>: Let <span class="math display">\[M(K) = \int_{\bS^{n-1}}\ve{x}\,d\si(x).\]</span> There exists a subspace <span class="math inline">\(E\)</span>, <span class="math inline">\(\dim E=c(\ep) nM^2\)</span> such that <span class="math display">\[ \ve{x}\in [1-\ep,1+\ep] M \ve{x}_2.\]</span> We can take <span class="math inline">\(c(\ep) = C\fc{\ep^2}{\ln \rc{\ep}}\)</span>.</li>
<li><strong>Theorem (Dvoretzky)</strong>: Let <span class="math inline">\(X\)</span> be <span class="math inline">\(n\)</span>-dimensional Banach. Given <span class="math inline">\(\ep&gt;0\)</span> there exists a subspace <span class="math inline">\(E\)</span> of <span class="math inline">\(X\)</span> of dimension <span class="math inline">\(k=k(n,\ep)\to \iy\)</span> as <span class="math inline">\(n\to \iy\)</span>, such that <span class="math inline">\(d(E,\ell_2^k)\le 1+\ep\)</span>.</li>
<li><strong>Theorem (Geometric Dvoretzky)</strong>: Let <span class="math inline">\(K\)</span> be a symmetric convex body in <span class="math inline">\(\R^n\)</span>. Given any <span class="math inline">\(\ep &gt; 0\)</span>, there exists a section <span class="math inline">\(K \cap E\)</span> of <span class="math inline">\(K\)</span> by a subspace <span class="math inline">\(E\)</span> of <span class="math inline">\(\R^n\)</span> of dimension <span class="math inline">\(k = k(n, \ep)\to \iy\)</span> as <span class="math inline">\(n \to \iy\)</span> such that <span class="math inline">\(E \subeq K \subeq (1 + \ep)\mathcal E\)</span> for some ellipsoid <span class="math inline">\(\mathcal E\)</span>.</li>
</ol>
<p>There is an alternative formulation for gaussian space, which is often computationally easier.</p>
<p>Define <span class="math inline">\(\ell_X:=\pa{\int_{\R^n} \ve{x}^2\,d\ga_n(x)}^{\rc 2} = (\E\ve{g}^2)^{\rc 2}\)</span>. This is off from <span class="math inline">\(M\)</span> by a factor of <span class="math inline">\(\sqrt n\)</span>: <span class="math display">\[ \ell_X\sim \sqrt n M_X.\]</span> Thus we can replace <span class="math inline">\(M_X\)</span> by <span class="math inline">\(\fc{\ell_X}{\sqrt n}\)</span> in the bound.</p>
<h4 id="proofs">Proofs</h4>
<ol type="1">
<li>Show it suffices to bound on <span class="math inline">\(\ep\)</span>-nets, and bound the size of the smallest <span class="math inline">\(\ep\)</span>-net.
<ol type="1">
<li>Let <span class="math inline">\(\mathcal{N}_\de\)</span> be a <span class="math inline">\(\de\)</span>-net of <span class="math inline">\(S_X\)</span>. Then
\begin{align}
\ve{T}&amp;\le \rc{1-\de} \sup_{x\in \mathcal{N}_\de}\ve{Tx}\\
\inf_{y\in S_X} &amp;\ge \inf_{x\in N} - \de\ve{T}.
\end{align}
Applying this to the identity map from <span class="math inline">\(\ved_2\)</span> to <span class="math inline">\(\ved\)</span>, obtain: if <span class="math inline">\(\ve{x}\in [1-\ep,1+\ep]M\)</span> for all <span class="math inline">\(x\in \cal N\)</span>, then for all <span class="math inline">\(x\in \bS^{n-1}\)</span>, <span class="math display">\[\ve{x} \in \ba{1-\ep-2\de, \pf{1+\ep}{1-\de}M}.\]</span></li>
<li>There is an <span class="math inline">\(\ep\)</span>-net of size <span class="math inline">\(\pa{1+\fc 2\ep}^n\)</span>.</li>
</ol></li>
<li>General Dvoretzky: Apply concentration of measure to a fixed vector <span class="math inline">\(x\)</span> of the function <span class="math inline">\(\ved\)</span>. Union bound over a <span class="math inline">\(\de\)</span>-net and approximate the sphere by the <span class="math inline">\(\de\)</span>-net using 1.</li>
<li>(Aside) We can calculate <span class="math inline">\(\ell_X\)</span> for many spaces. Standard concentration bounds give
\begin{align}
1\le p\le 2\implies \ell_{\ell_p^n} &amp;= c(\ep) n&amp;\implies k(\ell_p^n) &amp;\ge c(\ep)n\\
q\ge 2\implies \ell_{\ell_q^n} &amp;= c(\ep) q n^{\fc 2q}&amp;\implies k(\ell_q^n) &amp; \ge c(\ep)q n^{\fc 2q}\\
\ell_{\ell_{\iy}^n} &amp; c\sqrt{\ln n} &amp;\implies k(\ell_\iy^n) &amp; \ge c(\ep)\ln n.
\end{align}</li>
<li>We show <span class="math inline">\(k(\ell_\iy^n) \asymp\ln n\)</span>. Spherical caps!</li>
</ol>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Perfect LCCs</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/coding/perfect_lcc.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/coding/perfect_lcc.html</id>
    <published>2016-04-13T00:00:00Z</published>
    <updated>2016-04-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Perfect LCCs</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-13 
          , Modified: 2016-04-13 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#proof-1">Proof 1</a></li>
 <li><a href="#proof-2">Proof 2</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>There are many proofs showing we must have <span class="math inline">\(n=2^k\)</span> for perfect 2 LCCs. Can we extend these methods to give</p>
<ul>
<li>exponential lower bounds for LCCs?</li>
<li>lower bounds for perfect <span class="math inline">\(q\)</span>-LCCs, <span class="math inline">\(q&gt;2\)</span>?</li>
</ul>
<p>A perfect <span class="math inline">\(q\)</span>-LCC of input length <span class="math inline">\(k\)</span> in <span class="math inline">\(n\)</span> dimensions is a set of <span class="math inline">\(2^k\)</span> points in <span class="math inline">\(\{\pm 1\}^n\)</span>, together with <span class="math inline">\(n\)</span> unions of perfect <span class="math inline">\(q\)</span>-matchings <span class="math inline">\(M_i\)</span> (or just matchings) and a sign <span class="math inline">\(s_m\)</span> for each <span class="math inline">\(m\in M_i\)</span>, such that on codewords, the decoding process defined by taking any <span class="math inline">\(m\in M_i\)</span> and taking <span class="math inline">\(s_m \prod_{j\in m}x_i\)</span> recovers <span class="math inline">\(x_i\)</span> with probability 1. In other words, <span class="math display">\[\EE_{m\in M_i} s_m\prod_{j\in m} x_j = 1.\]</span></p>
<h2 id="proof-1">Proof 1</h2>
<p>For <span class="math inline">\(q=2\)</span>, these are quadratic forms <span class="math inline">\(Q_i\)</span>, with associated matrices <span class="math inline">\(A_i\)</span>. The fact that they are matchings means that <span class="math inline">\(A_i = \rc nS_i\)</span> where <span class="math inline">\(S_i\)</span> is (doubly) stochastic. (In fact, we can deal more generally with perfectly smooth LCCs that recover perfectly on codewords.) The codewords are those with <span class="math display">\[Q_i(x)=x \iff \an{x,A_ix}=x_i.\]</span> Now <span class="math inline">\(S\)</span>, being stochastic, satisfies <span class="math inline">\(\ve{S}_{\iy\to \iy} \le 1\)</span>. Now <span class="math inline">\(\ve{x}_{\iy}\le 1\)</span> so <span class="math inline">\(\ve{A_i x}_{\iy}\le \rc{n}\)</span>. We have <span class="math display">\[n=\ve{x}_1 =\sum_i |\an{x,A_ix}|\le \sum_i \ve{A_ix}_{\iy}\ve{x}_1=n,\]</span> so equality holds and <span class="math inline">\(A_ix = x_ix\)</span>.</p>
<p>This means the <span class="math inline">\(x\in C\)</span> are simultaneous eigenvalues for the <span class="math inline">\(A_i\)</span>. The sequences of eigenvectors are different, so <span class="math inline">\(|C|\le n\)</span>, i.e., <span class="math inline">\(2^k\le n\)</span>. Equality is acheived for the Hadamard code.</p>
<p>To extend this: some notion of “approximate eigenvector,” “well-conditioned linear dependency”?</p>
<h2 id="proof-2">Proof 2</h2>
<p>For LDCs whose matching correspond to a group action, every <span class="math inline">\(i\)</span> corresponds to a matching <span class="math inline">\(M_i=\{(y, y+x_i)\}\)</span>. Now for any <span class="math inline">\(\ep\in \{-1,1\}^k\)</span>, there must exist a set <span class="math inline">\(S\)</span>, the support of the codeword <span class="math inline">\(x=C(\ep)\)</span>, for which <span class="math inline">\(M_i\)</span> only has edges in <span class="math inline">\(S\)</span> if <span class="math inline">\(\ep_i=1\)</span>, and <span class="math inline">\(M_i\)</span> only has edges between <span class="math inline">\(S,S^c\)</span> if <span class="math inline">\(\ep_i=-1\)</span>. We must have <span class="math inline">\(|S|=\fc n2\)</span>.</p>
<p>Now if <span class="math inline">\(x_k=\sum_{i=1}^{k-1} a_ix_i\)</span>, consider the set <span class="math inline">\(S\)</span> where <span class="math inline">\(\ep_i=1\)</span> iff <span class="math inline">\(a_i=1\)</span>, but <span class="math inline">\(\ep_{k}=-1\)</span>. Then <span class="math inline">\(S = S+x_i\)</span> for all <span class="math inline">\(i\)</span>, and <span class="math inline">\(S\ne S+x_k\)</span>, contradiction.</p>
<p>This is very much related to the first proof. There, one can use a linear dependency argument to show there can’t be more than <span class="math inline">\(n\)</span> eigenvectors with distinct sequences of eigenvalues. (Make this relationship more explicit?)</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Notes index</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/notes_index.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/notes_index.html</id>
    <published>2016-04-08T00:00:00Z</published>
    <updated>2016-04-08T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Notes index</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-08 
          , Modified: 2016-04-08 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Martingales</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/probability/martingales.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/probability/martingales.html</id>
    <published>2016-04-08T00:00:00Z</published>
    <updated>2016-04-08T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Martingales</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-08 
          , Modified: 2016-04-08 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#conditional-expectation">Conditional expectation</a></li>
 <li><a href="#martingales-almost-sure-convergence">Martingales, Almost Sure Convergence</a></li>
 <li><a href="#examples">Examples</a></li>
 <li><a href="#doobs-inequality-lp-convergence">Doob’s inequality, <span class="math inline"><em>L</em><sup><em>p</em></sup></span> convergence</a><ul>
 <li><a href="#exercises">Exercises</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <p>Durrett Ch. 5</p>
<h2 id="conditional-expectation">Conditional expectation</h2>
<h2 id="martingales-almost-sure-convergence">Martingales, Almost Sure Convergence</h2>
<h2 id="examples">Examples</h2>
<h2 id="doobs-inequality-lp-convergence">Doob’s inequality, <span class="math inline">\(L^p\)</span> convergence</h2>
<ol type="1">
<li>If <span class="math inline">\(X_n\)</span> is a submartingale and <span class="math inline">\(N\)</span> is a stopping time with <span class="math inline">\(\Pj(N\le k)=1\)</span>, then <span class="math display">\[ \E X_0\le \E X_N \le \E X_k.\]</span> (Note the first inequality is not true in general if <span class="math inline">\(N\)</span> is unbounded.) <em>Proof.</em>
<ol type="1">
<li>Show that <span class="math inline">\(X_n - X_{N\wedge n}\)</span> is a submartingale. To see this, wrie it as a dot product with a predictable sequence, <span class="math inline">\(X_n-X_{N\wedge n} = (1_{\{n-1 \ge N\}}\cdot X\)</span>.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></li>
<li>(Alternative) Condition on <span class="math inline">\(N\)</span> and use the submartingale property. See exercise 1.</li>
</ol></li>
<li><p><strong>Doob’s inequality</strong>: Let <span class="math inline">\(X_m\)</span> be a submartingale, <span class="math inline">\(\ol X_n = \max_{0\le m\le n} X_m^+\)</span>, <span class="math inline">\(\la&gt;0\)</span>. Then <span class="math display">\[ \la \Pj(A) \le \E X_n 1_A \le \E X_n^+.\]</span></p>
<p><em>Proof.</em> Interpret the event <span class="math inline">\(A\)</span> as early stopping by defining the stopping time <span class="math inline">\(N=\inf \set{m}{X_m\ge \la \text{ or }m=n}\)</span>. Now apply 1.</p>
<em>Corollary</em>: Kolmogorov’s inequality</li>
<li><strong><span class="math inline">\(L^p\)</span> maximum inequality</strong>: If <span class="math inline">\(X_n\)</span> is a submartingale then for <span class="math inline">\(p&gt;1\)</span>, <span class="math display">\[ \E (\ol X_n^p) \le \pf{p}{p-1}^p \E(X_n^+)^p.\]</span> <em>Proof.</em> Our “only tool” is to use Doob’s inequality to bound <span class="math inline">\(\Pj(\ol X_n\ge \la)\)</span> (actually, <span class="math inline">\(\ol X_n\wedge M\)</span>). To make this term appear, bound <span class="math inline">\(\E[(\ol X_n\wedge M)^p]\)</span> by <span class="math inline">\(\int_0^{\iy} p\la^{p-1}\Pj(\ol X_n\wedge M \ge \la)\)</span>. We get one less power of <span class="math inline">\(\ol X_n \wedge M\)</span> on the RHS. Use Holder and bootstrap.</li>
<li>(<span class="math inline">\(L^1\)</span> version)</li>
<li><strong><span class="math inline">\(L^p\)</span> convergence theorem</strong>: If <span class="math inline">\(X_n\)</span> is a matringale with <span class="math inline">\(\sup \E|X_n|^p&lt;\iy\)</span> when <span class="math inline">\(p&gt;1\)</span>, then <span class="math inline">\(X_n\to X\)</span> a.s. and in <span class="math inline">\(L^p\)</span>. <em>Proof.</em> The hypothesis implies <span class="math inline">\((\E X_n^+)\)</span> is bounded so a.s. follows from martingale convergence. To show <span class="math inline">\(X_n\to X\)</span> in <span class="math inline">\(L^p\)</span>, it suffices to show that <span class="math inline">\(|X_n-X|^p\le Y\)</span> with <span class="math inline">\(\E|Y|&lt;\iy\)</span> and <span class="math inline">\(X_n\to X\)</span> a.s. (Dominated convergence theorem.) Bound <span class="math inline">\(|X_n-X|^p\le (2\sup |X_n|^p)\)</span> and use <span class="math inline">\(L^p\)</span> maximal inequality to bound the RHS (use MCT on <span class="math inline">\(\sup_{0\le m\le n}X_n\)</span>).</li>
<li><strong>Orthogonality of martingale increments</strong>: Let <span class="math inline">\(X_n\)</span> be a martingale with <span class="math inline">\(\E X_n^2&lt;\iy\)</span> for all <span class="math inline">\(n\)</span>. If <span class="math inline">\(m\le n, Y\in \cal F_m, \E Y^2&lt;\iy\)</span>, then <span class="math display">\[ \E[(X_n - X_m)Y]=0. \]</span> <em>Proof.</em> Cauchy Schwarz gives this is <span class="math inline">\(L^1\)</span>. Now just use the martingale property.</li>
<li><strong>Conditional variance formula</strong>: If <span class="math inline">\(X_n\)</span> is a <span class="math inline">\(L^2\)</span> martingale, <span class="math inline">\(\E[(X_n-X_m)^2|\cal F_m]=\E [X_n^2|\cal F_m]-X_m^2\)</span>.</li>
<li><p><strong>Branching processes</strong>: Letting <span class="math inline">\(X_n=Z_n/\mu^2\)</span>, use 7 to get an expression for <span class="math inline">\(\E[X_n^2|\cal F_{n-1}]\)</span> and <span class="math inline">\(\E X_n^2\)</span>. This shows <span class="math inline">\(\sup \E X_n^2&lt;\iy\)</span>, so <span class="math inline">\(X_n\to X\)</span> in <span class="math inline">\(L^2\)</span>.</p></li>
</ol>
<h3 id="exercises">Exercises</h3>
<ol type="1">
<li>Use submartingale property with the fact that <span class="math inline">\(N=j\)</span> is in <span class="math inline">\(\cal F_j\)</span>. Now sum over <span class="math inline">\(N\)</span>.</li>
<li></li>
<li></li>
<li>?</li>
<li></li>
</ol>
<p>A good counterexample is the 1-D simple random walk starting at 1 with absorbing barrier at 0.</p>
<ol type="1">
<li>We have <span class="math inline">\(\E X_0=1&gt;\E S_N\)</span> where <span class="math inline">\(N=\inf\set{n}{S_n=0}\)</span>.</li>
<li>We have <span class="math inline">\(\E(\max_m X_m) = \iy\)</span>, so no <span class="math inline">\(L^1\)</span> maximal inequality holds.</li>
</ol>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Recall this means dot product with the differences.<a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Matrix factorizations</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/matrix-factorization.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/matrix-factorization.html</id>
    <published>2016-04-04T00:00:00Z</published>
    <updated>2016-04-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Matrix factorizations</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-04 
          , Modified: 2016-04-04 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#flavors">Flavors</a></li>
 <li><a href="#previous-work">Previous work</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="flavors">Flavors</h2>
<ul>
<li><strong>Sparse coding/Dictionary learning</strong>: Write <span class="math inline">\(M=AX\)</span> where the columns of <span class="math inline">\(X\)</span> are sparse. Assume some properties on <span class="math inline">\(A\)</span> (e.g., incoherence) and on the distribution of <span class="math inline">\(X\)</span>.</li>
<li><strong>Nonnegative matrix factorization</strong>: Sparse coding with the caveat that <span class="math inline">\(A,X\)</span> are positive. Write <span class="math inline">\(M=AW\)</span> where <span class="math inline">\(A,W\)</span> have nonnegative entries. <span class="math inline">\(A\)</span> is fixed (assume some properties on <span class="math inline">\(A\)</span>) and <span class="math inline">\(W\)</span> consists of random samples. I.e., we are given many samples <span class="math inline">\(y=Ax\)</span>.</li>
<li><strong>Topic modeling</strong>: Given <span class="math inline">\(x\)</span>, we see samples from the probability vector <span class="math inline">\(Ax\)</span> (rather than <span class="math inline">\(Ax\)</span>). Learn <span class="math inline">\(A\)</span> and infer the <span class="math inline">\(x\)</span>’s. ??</li>
</ul>
<p>NMF is the least understood.</p>
<h2 id="previous-work">Previous work</h2>
<ul>
<li>NMF
<ul>
<li>Hardness: If there exists a <span class="math inline">\(n^{o(r)}\)</span> time algorithm, then there exists a <span class="math inline">\(2^{o(n)}\)</span> algorithm for 3SAT.</li>
<li>[AGKM12, M14] Given that the rank equals the nonnegative rank, there is a <span class="math inline">\(n^{O(r)}\)</span> time algorithm for NMF. (Relies on solving polynomial inequalities.)</li>
<li>[AGKM12] Under separability, NMF can be solved in polynomial time in <span class="math inline">\(n\)</span>. (Use the geometry.)</li>
<li>Inference: <a href="arora-topic-models.html">A16</a> Given <span class="math inline">\(A\)</span>, recover <span class="math inline">\(x\)</span> from <span class="math inline">\(Ax\)</span> if <span class="math inline">\(x\)</span> is sparse and <span class="math inline">\(A\)</span> has small <span class="math inline">\(\ell_\iy\to \ell_1\)</span> condition number. (“Parameter learning” is learning <span class="math inline">\(A\)</span>, “inference” is finding <span class="math inline">\(x\)</span>.)</li>
</ul></li>
<li>Topic modeling
<ul>
<li>… see p. 23 in new_thread.pdf.</li>
<li>[AGHMMSWZ12] A Practical Algorithm for Topic Modeling with Provable Guarantees:</li>
</ul></li>
<li>Sparse coding/ Dictionary learning
<ul>
<li><a href="SW08.html">SW08</a>: Algorithm for full-rank matrices (no noise).
<ul>
<li>Apply a <span class="math inline">\(\ved_0\to \ved_1\)</span> relaxation.</li>
</ul></li>
<li><a href="AGM14.html">AGM14</a>: Fixed-parameter tractable algorithm for overcomplete (incoherent) dictionaries, up to sparsity <span class="math inline">\(n^{\rc 2-\ep}\)</span>.
<ul>
<li>Use the fact that high dot product between two vectors indicates intersection of supports to reduce to a <strong>overlapping community detection</strong> problem.</li>
<li>Run <strong>SVD</strong> within communities.</li>
</ul></li>
<li><a href="AGMM15.html">AGMM15</a>: Efficient polytime algorithm for overcomplete (incoherent) dictionaries, that works up to sparsity <span class="math inline">\(\fc{\sqrt{n}}{\mu\poly\log n}\)</span>.
<ul>
<li>Initialize with <strong>SVD</strong>, noting that with good probability the <strong>intersection of supports</strong> of two vectors <span class="math inline">\(x,x'\)</span> will have one index in common.</li>
<li>Apply <strong>alternating minimization</strong> and analyze using <strong>approximate gradient descent</strong> (correlation with the right direction).</li>
<li>To get a sparse decoding, apply <strong>thresholding</strong> in the decoding step of AM.</li>
</ul></li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[A16] Provable algorithms for inference in topic models</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/arora-topic-models.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/arora-topic-models.html</id>
    <published>2016-04-04T00:00:00Z</published>
    <updated>2016-04-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[A16] Provable algorithms for inference in topic models</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-04 
          , Modified: 2016-04-04 
	</p>
      
       <p>Tags: <a href="/tags/topic%20models.html">topic models</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#model">Model</a><ul>
 <li><a href="#definitions">Definitions</a></li>
 <li><a href="#model-1">Model</a></li>
 </ul></li>
 <li><a href="#algorithm">Algorithm</a></li>
 <li><a href="#analysis">Analysis</a><ul>
 <li><a href="#thresholded-linear-inverse-algorithm">Thresholded linear inverse algorithm</a></li>
 <li><a href="#mle-estimate">MLE estimate</a></li>
 <li><a href="#sample-complexity-lower-bounds">Sample complexity lower bounds</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="model">Model</h2>
<h3 id="definitions">Definitions</h3>
<p>Define</p>
<ul>
<li>the <span class="math inline">\(\ell_1\)</span> condition number <span class="math display">\[\ka(A) = \min \set{\ka}{\forall x\in \R^k, \ve{Ax}_1\ge \rc{\ka}\ve{x}_1}\]</span></li>
<li>the <span class="math inline">\(\ell_\iy\to \ell_1\)</span> condition number <span class="math display">\[\la(A) = \min \set{\la}{\forall x\in \R^k, \ve{Ax}_1\ge \rc{\la}\ve{x}_\iy}\]</span></li>
<li>the <span class="math inline">\(\de\)</span>-biased <span class="math inline">\(\ell_\iy\to \ell_1\)</span> condition number
\begin{align}
\la_\de&amp;=\min_{|BA-I_k|\le \de, B\in \R^{k\times D}} |B|_{\iy}\\
&amp;=\max_{\ve{Ax}_1\le 1} \ve{x}_{\iy} - \de\ve{x}_1.
\end{align}
(The equality follows from a duality calculation.)</li>
<li>the restricted <span class="math inline">\(\ell_1\to \ell_1\)</span> condition number is <span class="math display">\[\ol{\ka}_r(A) = \min\set{\ol{\ka}}{\forall \ve{v}_0, \ve{Av}_1\ge \rc{\ol \ka}\ve{v}}.\]</span></li>
</ul>
<p>Note <span class="math display">\[\la_\de\le \la_0=\la \le \ka.\]</span> (To see this, take <span class="math inline">\(x=By\)</span>.)</p>
<h3 id="model-1">Model</h3>
<ul>
<li><span class="math inline">\(A\)</span> is a fixed <span class="math inline">\(n\times k\)</span> matrix with <span class="math inline">\(\de\)</span>-biased condition number <span class="math inline">\(\la_\de(A)\)</span>. (We want this to be small.)</li>
<li><span class="math inline">\(x\in \De_k\)</span> is <span class="math inline">\(r\)</span>-sparse.</li>
<li><span class="math inline">\(y\sim Ax\)</span> (<span class="math inline">\(Ax\)</span> is treated as a probability vector).</li>
</ul>
<h2 id="algorithm">Algorithm</h2>
<p>Given <span class="math inline">\(y\sim Ax\)</span> and <span class="math inline">\(A\)</span>,</p>
<ul>
<li>Thresholded linear inverse
<ul>
<li>Let <span class="math inline">\(B=\amin_{|BA-I_k|\le \de, B\in \R^{k\times D}} |B|_{\iy}\)</span> be a <span class="math inline">\(\de\)</span>-biased minimum variance inverse.</li>
<li>Compute <span class="math inline">\(\wh x = \rc n By\)</span>.</li>
<li>Let <span class="math display">\[x_i = \wh x_i (\wh x_i \ge \ub{2\la_\de(A) \sfc{\ln k}{n} + \de}{\tau}).\]</span></li>
</ul></li>
<li>TLI finds the support of <span class="math inline">\(x^*\)</span> with high probability. Now find the MLE <span class="math inline">\(x^*\)</span> given the support. (This is a convex problem.)</li>
</ul>
<h2 id="analysis">Analysis</h2>
<h3 id="thresholded-linear-inverse-algorithm">Thresholded linear inverse algorithm</h3>
<p>We have <span class="math display">\[ \E \wh x_i  =x_j^* + \sumo jk ((BA)_{ij} - \de_{i,j}) x_j^*.\]</span> <em>This is why it’s natural to consider the <span class="math inline">\(\de\)</span>-biased inverse</em>: we don’t need <span class="math inline">\(B=A^+\)</span> exactly, we can relax this to each <span class="math inline">\((BA)_{ij} - \de_{i,j}\)</span> being small. Now use Bernstein’s inequality to get concentration on the order of <span class="math inline">\(\tau\)</span>. Finally use union bound. <!--check this--></p>
<h3 id="mle-estimate">MLE estimate</h3>
<p>If</p>
<ul>
<li><span class="math inline">\(x^*\in \De_k\)</span> is <span class="math inline">\(r\)</span>-sparse</li>
<li><span class="math inline">\(x_i^*\ge \fc{\tau}r\)</span> for any <span class="math inline">\(i\in R\)</span>,</li>
<li><span class="math inline">\(A\)</span> has <span class="math inline">\(\le \ol{\ka}\)</span> restricted <span class="math inline">\(\ell_1\to \ell_1\)</span> condition number (a bound on <span class="math inline">\(\ka\)</span> is sufficient as <span class="math inline">\(\ol\ka\le \ka\)</span>)</li>
<li><span class="math inline">\(n\ge c\ol{\ka}^2 r^2 \fc{\log k}{\tau^2}\)</span></li>
</ul>
then with high probability
\begin{align}
\ve{Ax_{MLE}-Ax^*}_1 &amp;\le \wt O\pa{\sfc rn}\\
\ve{x_{MLE}-x^*}_1 &amp;\le \wt O\pa{\ol \ka \sfc rn}
\end{align}
<p>The proof is like the <a href="../../math/statistics/fisher-info.html">proof of asymptotic normality of MLE</a>, but with matrix concentration to get a finite sample bound.</p>
<h3 id="sample-complexity-lower-bounds">Sample complexity lower bounds</h3>
<p>These exist!</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 4-9-16</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-04-09.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-04-09.html</id>
    <published>2016-04-04T00:00:00Z</published>
    <updated>2016-04-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 4-9-16</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-04 
          , Modified: 2016-04-04 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Maximum entropy distributions</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/statistics/max-entropy.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/statistics/max-entropy.html</id>
    <published>2016-04-04T00:00:00Z</published>
    <updated>2016-04-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Maximum entropy distributions</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-04-04 
          , Modified: 2016-04-04 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#references">References</a><ul>
 <li><a href="#constraint-distribution-entropy-base-e">Constraint | Distribution | Entropy (base e)</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="references">References</h2>
<ul>
<li><a href="http://www.math.uconn.edu/~kconrad/blurbs/analysis/entropypost.pdf">Keith Conrad’s notes</a></li>
<li><a href="http://www.ski.org/Rehab/Coughlan_lab/General/TutorialsandReference/MaxEnt.pdf">ML vs. ME</a></li>
<li><a href="https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution">Wikipedia</a></li>
</ul>
<p>Many naturally occurring distributions are the maximal entropy distribution under some constraint. Here is a table.</p>
<h3 id="constraint-distribution-entropy-base-e">Constraint | Distribution | Entropy (base e)</h3>
<p>Mean <span class="math inline">\(\mu\)</span>, variance <span class="math inline">\(\si^2\)</span> | Normal<span class="math inline">\((\mu,\si^2)\)</span> | <span class="math inline">\(\rc2 (1+\ln (2\pi \si^2))\)</span> Support <span class="math inline">\([0,\iy)\)</span>, mean <span class="math inline">\(la\)</span> | Exponential <span class="math inline">\(\rc{\la} e^{-\fc x\la}\)</span> | <span class="math inline">\(1+\ln \la\)</span> <span class="math inline">\(\E X= \mu\)</span>, <span class="math inline">\(\E |X - \E X| = \la\)</span> | Laplace<span class="math inline">\((\mu,2\la^2)\)</span> | <span class="math inline">\(1+\ln(2\la)\)</span> Energy <span class="math inline">\(\sum p_iE_i = \ol E\)</span> | Boltzmann <span class="math inline">\(\Pj(i) = \fc{e^{-\be E_i}}{Z}\)</span>, <span class="math inline">\(Z=\sum_i e^{-\be E_i}\)</span> | <span class="math inline">\(\E(-\be E) - \ln Z\)</span></p>
<p>Note that in the continuous case, the Boltzmann formula encompasses everything! For example, for the normal distribution, energy is <span class="math inline">\((x-\mu)^2\)</span>.</p>
<p>A systematic way to show this is Lagrange multipliers.</p>
<p>A more elegant way is to do the following:</p>
<ul>
<li>Note that by nonnegativity of KL divergence, <span class="math display">\[\int_{\Om} p \ln p \ge \int_{\Om} p \ln q.\]</span></li>
<li>For <span class="math inline">\(F(p)\)</span> the property of the distribution you’re interested in and <span class="math inline">\(q\)</span> equal to the maximizing distribution, find that <span class="math display">\[-\int_{\Om} p \ln q = g(F(p))\]</span> for some function <span class="math inline">\(g\)</span>.</li>
<li>Conclude that if <span class="math inline">\(F(p)=F(q)\)</span> then <span class="math inline">\(-\int_{\Om} p\ln q = -\int_{\Om} q\ln q\)</span>. Hence <span class="math display">\[H(p) = -\int p\ln p \ge -\int p\ln q = -\int q\ln q = H(q).\]</span></li>
</ul>
<p>Here is an example. For <span class="math inline">\(q=\rc{\sqrt{2\pi}}e^{-\fc{x^2}{2\si^2}}\)</span>, <span class="math display">\[-\int_{\R} p \ln q \dx = \rc2 \ln(2\pi \si^2) + \int_{\R}p\cdot \rc2 \pf{x}{\si^2}^2\dx = \rc2 \ln(2\pi \si^2) + \rc{2}\fc{\Var(q)}{\si^2}.\]</span></p>
<p>Question: why is the maximum entropy distribution the best choice in statistical problems?</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
