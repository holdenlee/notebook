<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2016-08-01T00:00:00Z</updated>
    <entry>
    <title>High-dimensional probability</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/probability/random_matrices/hi_dim_prob.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/probability/random_matrices/hi_dim_prob.html</id>
    <published>2016-08-01T00:00:00Z</published>
    <updated>2016-08-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>High-dimensional probability</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-01 
          , Modified: 2016-08-01 
	</p>
      
       <p>Tags: <a href="/tags/random%20matrix.html">random matrix</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#introduction">Introduction</a></li>
 <li><a href="#variance-bounds-and-poincare-inequalities">Variance bounds and Poincare inequalities</a><ul>
 <li><a href="#section">2.1</a><ul>
 <li><a href="#problems">Problems</a></li>
 </ul></li>
 <li><a href="#markov-semigroups">Markov semigroups</a></li>
 <li><a href="#variance-identities-and-exponential-ergodicity">2.4 Variance identities and exponential ergodicity</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <p>Based on Ramon von Handel’s ORF570 notes.</p>
<h2 id="introduction">Introduction</h2>
<p>Themes:</p>
<ul>
<li>concentration: if <span class="math inline">\(X_{1:n}\)</span> are independent or weakly dependent random variables, and <span class="math inline">\(f\)</span> is not too <em>sensitive</em> to any coordinate, then <span class="math inline">\(f(X_{1:n})\)</span> is <em>close</em> to its mean.</li>
<li>suprema</li>
<li>universality</li>
</ul>
<h2 id="variance-bounds-and-poincare-inequalities">Variance bounds and Poincare inequalities</h2>
<h3 id="section">2.1</h3>
<p>Trivial bound: <span class="math display">\[ \Var[f(X)]\le \rc4 (\sup f - \inf f)^2 \qquad \Var[f(x)] \le \E[(f(X)-\inf f)^2].\]</span></p>
<p>Tensorization gives a bound for functions of independent random variables from bounds for functions of each individual random variable.</p>
<p><strong>Theorem</strong> (Tensorization of variance): <span class="math display">\[\Var[f(X_1,\ldots, X_n)]\le \E\ba{\sumo in \Var_f f(X_1,\ldots, X_n)}\]</span> whenever <span class="math inline">\(X_{1:n}\)</span> are independent.</p>
<p>This is sharp for linear functions.</p>
<em>Proof</em>. Write <span class="math display">\[ f(X_{1:n}) - \E f(X_{1:n}) = \sumo kn \ub{\E[f(X_{1:n}|X_{1:k})] - \E[f(X_{1:n})|X_{1:k-1}]}{\De_k}. \]</span> The <span class="math inline">\(\De_k\)</span> form a martingale. By independence of martingale increments,
\begin{align}
\Var(f) &amp;= \sumo kn \E[\De_k^2] \\
\E[\De_k^2] &amp;= \E[\E[\wt \De_k |X_{1:k}]^2]\\
&amp; \le  \E[(\ub{f - \E[f|X_{1:k-1,k+1:n}]}{\wt \De_k})^2] &amp; \text{Jensen}\\
&amp;= \E\ba{\sumo in \Var_f f(X_1,\ldots, X_n)}.
\end{align}
Define
\begin{align}
D_i f(x) &amp;= (\sup_z-\inf_z)(f(x_{1:i-1},z,x_{i+1:n}))\\
D_i^- f(x) &amp;= f(x) - \inf_z(f(x_{1:i-1},z,x_{i+1:n}))\\
\end{align}
<p><strong>Corollary</strong> (Bounded difference inequality): Tensorization + trivial inequality.</p>
<p><strong>Example</strong>: Consider Bernoulli symmetric matrices. What is the variance of <span class="math inline">\(\la_{\max}(M) = \an{v_{\max}(M), Mv_{\max}(M)}\)</span>? Fix <span class="math inline">\(i,j\)</span>. Let <span class="math display">\[M^- = \amin_{\text{only }M_{ij} \text{ varies}} \la_{\max}(M).\]</span> Then <span class="math display">\[D_{ij}^-\la_{\max}(M) \le \an{v_{\max}(M), (M-M^-) v_{\max}(M)}\le 4|v_{\max}(M)_i||v_{\max}(M)_j|.\]</span> Use the corollary to get <span class="math inline">\(\le 16\)</span>.</p>
<p>This is not sharp. (<span class="math inline">\(\sim n^{-\rc 3}\)</span> is correct.)</p>
<p>Drawbacks to this method:</p>
<ul>
<li>bounds using bounded di↵erence inequalities are typically restricted to situations where the random variables <span class="math inline">\(X_i\)</span> and/or the function <span class="math inline">\(f\)</span> are bounded.</li>
<li>Bounded difference inequalities do not capture any information on the distribution of <span class="math inline">\(X_i\)</span>. In the other direction, the tensorization inequality is too distribution-dependent.</li>
<li>Tensorization depends on independence.</li>
</ul>
<p>Inequalities in this section are roughly of the following form (Poincare inequalities): <span class="math display">\[\Var(f) \le \E[\ve{\nb f}^2].\]</span> “The validity of a Poincar´e inequality for a given distribution is intimately connected the convergence rate of a Markov process that admits that distribution as a stationary measure.”</p>
<h4 id="problems">Problems</h4>
<ol type="1">
<li>Use <span class="math inline">\(\Var\pa{\ve{\rc n \sumo kn X_k}_B} = \sup_{y\in B^*} \an{\rc n\sumo kn X_k, y}\)</span>. Use the corollary, get <span class="math inline">\(D_k^-\le 2 \sup_{y\in B^*}\an{\rc X_k,y} \le \fc{2C}{n}\)</span>. Now square and sum.</li>
<li>.</li>
<li>.</li>
<li>.</li>
<li>We have <span class="math inline">\((f(x) - f(..., a, ...))^2\le \ve{(b-a)\nb f}^2\)</span>. Now take expectations and sum over different coordinates.</li>
</ol>
<h3 id="markov-semigroups">Markov semigroups</h3>
<p>A <strong>Markov process</strong> satisfies: For every bounded measurable <span class="math inline">\(f\)</span> and <span class="math inline">\(s,t\in \R_+\)</span>, here is abounded measurable <span class="math inline">\(P_sf\)</span> such that <span class="math display">\[\E[f(X_{t+s})|\{X_r\}_{r\le t}] = (P_s f)(X_t).\]</span> <span class="math inline">\(\mu\)</span> is <strong>stationary</strong> if <span class="math inline">\(\mu(P_tf)=\mu(f)\)</span> for all <span class="math inline">\(t\in \R_+\)</span>, bounded measurable <span class="math inline">\(f\)</span>.</p>
<p><strong>Lemma 2.7</strong>. <span class="math inline">\(\{P_t\}_{t\in \R_+}\)</span> defines a semigroup of linear operators on <span class="math inline">\(L^p(\mu)\)</span>. It is contractive and conservative (<span class="math inline">\(P_t1=1\)</span> <span class="math inline">\(\mu\)</span>-a.s.).</p>
<p><em>Proof</em>. Jensen.</p>
<p>The semigroup in fact acts on any <span class="math inline">\(f\in L^1(\mu)\)</span>.</p>
<p><strong>Lemma 2.9</strong>. If <span class="math inline">\(\mu\)</span> is stationary, for every <span class="math inline">\(f\)</span>, <span class="math inline">\(\Var_\mu(P_tf)\)</span> is decreasing.</p>
<p><em>Proof</em>. <span class="math inline">\(L^2\)</span> contractivity and semigroup property.</p>
<p>The <strong>generator</strong> is <span class="math display">\[\cal L f = \lim_{t\searrow 0} \fc{P_tf-f}t.\]</span> The set of <span class="math inline">\(f\)</span> where this is defined is the domain; <span class="math inline">\(\cal L:\text{Dom}(\cal L) \to L^2(\mu)\)</span>.</p>
<p>Warning: for Markov processes with continuous sample paths, such as Brownian motion, <span class="math inline">\(Dom(\cL)\sub L^2(\mu)\)</span>. Functional analysis is required for rigor, but results usually extend.</p>
<p><span class="math inline">\(P_t\)</span> is the solution of the Kolmogorov equation <span class="math display">\[ \ddd{t} P_t f = P_t \cL f, \quad P_0f=f.\]</span> The generator and semigroup commute.</p>
<p>A finite-state Markov process with <span class="math display">\[ \Pj[X_{t+\de}=j|X_t=i] = \la_{ij} \de + o(\de), \quad i\ne j\]</span> has generator equal to the transition matrix <span class="math inline">\(\La\)</span>.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<p>Then <span class="math inline">\(P_t=e^{t\La}\)</span>. (In the non-finite case, this makes sense as a power series.)</p>
<p><span class="math inline">\(P_t\)</span> is <strong>reversible</strong> if <span class="math inline">\(P_t\)</span> are self-adjoint on <span class="math inline">\(L^2(\mu)\)</span>: <span class="math display">\[\an{f,P_tg}_\mu = \an{P_tf,g}_\mu.\]</span> Reversibility implies <span class="math display">\[P_tf(x) =\E[f(X_t)|X_0=x] = \E[f(X_0)|X_t=x];\]</span> i.e., the Markov process viewed backwards has the same law. <!-- delta functions? --></p>
<p>For finite state space, this is equivalent to <span class="math display">\[\mu_i \La_{ij} = \mu_j \La_{ji},\]</span> <strong>detailed balance</strong>.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<p><strong>Definition</strong>. A Markov semigroup is <strong>ergodic</strong> if <span class="math inline">\(P_tf \to \mu f\)</span> in <span class="math inline">\(L^2(\mu)\)</span> as <span class="math inline">\(t\to \iy\)</span> for every <span class="math inline">\(f\in L^2(\mu)\)</span>.</p>
<blockquote>
<p>A measure <span class="math inline">\(\mu\)</span> satisfies a Poincare inequality for a certain notion of “gradient” if and only if an ergodic Markov semigroup associated to this “gradient” converges exponentially fast to <span class="math inline">\(\mu\)</span>.</p>
</blockquote>
<p>The <strong>Dirichlet form</strong> is <span class="math display">\[\cal E(f,g) = -\an{f,\cL g}_\mu.\]</span> Note: for complex-valued functions, we take the real part.</p>
<p><strong>Theorem</strong> (Poincare inequality). Let <span class="math inline">\(P_t\)</span> be reversible ergodic Markov semigroup with stationary measure <span class="math inline">\(\mu\)</span>. For <span class="math inline">\(c\ge 0\)</span>, TFAE:</p>
<ol type="1">
<li>(Poincare inequality) <span class="math inline">\(\Var_\mu(f) \le c\cal E(f,f)\)</span></li>
<li><span class="math inline">\(\forall f, t, \ve{P_t f- \mu f}_{L^2(\mu)} \le e^{-\fc tc}\ve{f-\mu f}_{L^2(\mu)}\)</span></li>
<li><span class="math inline">\(\forall f, t, \cal E(P_t f, P_t f) \le e^{-2t/c} \cal E(f,f)\)</span></li>
<li><span class="math inline">\(\forall f \exists \ka(f), \ve{P_t f-\mu f}_{L^2(\mu)} \le \ka(f) e^{-\fc tc}\)</span>.</li>
<li><span class="math inline">\(\forall f \exists \ka(f), \cal E(P_tf,P_tf)\le \ka(f)e^{-2t/c}\)</span>.</li>
</ol>
<p>Note <span class="math inline">\(5\Leftarrow 3\implies 1\Leftrightarrow 2\Rightarrow 4\)</span> doesn’t require reversibility.</p>
<p>Example: Gaussian distribution</p>
<ol type="1">
<li>Define the <strong>Ornstein-Uhlenbeck process</strong> by <span class="math display">\[X_t = e^{-t} X_0 + e^{-t} W_{e^{2t}-1}, \quad X_0\perp W\]</span> where <span class="math inline">\(W_t\)</span> is standard Brownian motion. Note <span class="math inline">\(N(0,1)\)</span> is stationary.</li>
<li>Using Gaussian integration by parts <span class="math inline">\(\E_{N(0,1)} [\xi f(\xi)] = \E_{N(0,1)} [f'(\xi)]\)</span>, show that
<ul>
<li><span class="math inline">\(X_t\)</span> is a Markov process with semigroup <span class="math inline">\(\E[f(e^{-t} x + \sqrt{1-e^{-2t}}\xi)]\)</span>, <span class="math inline">\(\xi\in N(0,1)\)</span>.</li>
<li>The generator is <span class="math inline">\(\cL f(x) = -xf'+f''\)</span>.</li>
<li><span class="math inline">\(\cE (f,g) = \an{f',g'}_\mu\)</span>.</li>
<li>In particular, <span class="math inline">\(\cE(f,f) = \ve{f'}^2_{L^2(\mu)} = \E[f'(\xi)^2]\)</span> is exctly the expected square gradient.</li>
</ul></li>
<li>From the expression for <span class="math inline">\(P_t\)</span> obtain <span class="math inline">\(\ddd{x} P_t f(x) = e^{-t} P_t f'(x)\)</span>. Then <span class="math inline">\(\cE (P_tf,P_tf) \le e^{-2t} \cE(f,f)\)</span>. Hence for <span class="math inline">\(\mu=N(0,1)\)</span>, <span class="math display">\[\Var_\mu(f) \le \ve{f'}_{L^2(\mu)}.\]</span></li>
<li>By tensorization, <span class="math display">\[\Var_\mu(f) \le \E[\ve{\nb f(X_1,\ldots, X_n)}^2].\]</span></li>
</ol>
<p>Note: The O-U process is the solution of the stochastic differential equation <span class="math display">\[dX_t = -X_t \,dt + \sqrt2 \, dB_t.\]</span> Revisit this after I learn stochastic calculus.</p>
<p>Tensorization using Poincare inequality:</p>
<ol type="1">
<li>Construct a random process <span class="math inline">\(X_t=(X_t^1,\ldots, X_t^n)\)</span> by having coordinates re-randomize according to independent Poisson processes.</li>
<li>Then <span class="math display">\[P_tf(x) = \sum_{I\subeq [n]} (1-e^{-t})^{|I|} e^{-t(n-|I|)} \int f(x_1,\ldots, x_n) \prod_{i\in I}\mu_i(dx_i)+o(t).\]</span> (Note the integral is only over the indices in <span class="math inline">\(I\)</span>.) Only the <span class="math inline">\(|I|=1\)</span> terms matter in the limit (makes sense, we’re taking the derivative!),
\begin{align}
\cL f &amp;= -\sumo in \de_i f\\
\de_if(x)&amp;:= f(x) - \int f(x_1,\ldots, x_{i-1}, z, x_{i+1},\ldots, x_n)\,\mu_i(dz).
\end{align}</li>
<li><span class="math inline">\(\int h\de_i g\,d\mu=0\)</span> if <span class="math inline">\(h\)</span> does not depend on <span class="math inline">\(x_i\)</span>. Thus <span class="math inline">\(\cE(f,g) = \sumo in \int \de_i f\de_i g\,d\mu\)</span>. This is symmetric, so the process is reversible.</li>
<li>We have <span class="math inline">\(\cE(f,f) = \sumo in \int \Var_if \,d\mu\)</span>, so the tensorization inequality is exactly <span class="math inline">\(\Var_\mu(f) \le \cE(f,f)\)</span>.</li>
<li>Conclude ergodicity. Conversely, we can prove the tensorization inequality from ergodicity: Note <span class="math display">\[\de_i P_t f=e^{-t} \sum_{I\nin i} (1-e^{-t})^{|I|} e^{-t(n-1-|I|)} \int \de_i f\prod_{i\in I}\mu_i(dx_i)\]</span> so <span class="math inline">\(\cE(P_tf,P_tf) \le \ka(f) e^{-2t}\)</span>.</li>
</ol>
<h3 id="variance-identities-and-exponential-ergodicity">2.4 Variance identities and exponential ergodicity</h3>
<p>We prove the Poincare inequality.</p>
<ol type="1">
<li><strong>Lemma</strong>. <span class="math display">\[\ddd t \Var_\mu(P_t f) = -2\cal E(P_tf,P_tf)\]</span>. <em>Proof</em>. Use <span class="math inline">\(\mu(P_tf)=\mu(f)\)</span>. Both are equal to <span class="math inline">\(\mu(2P_t f\ddd tP_tf)\)</span>.</li>
<li><strong>Corollary</strong>. <span class="math inline">\(\cE(f,f)\ge 0\)</span>.</li>
<li>Integral representation of variance: If the Markov semigroup is ergodic, integrating gives <span class="math inline">\(\Var_\mu (f) = 2\iiy \cE(P_tf,P_tf)\,dt\)</span>.</li>
<li>(<span class="math inline">\(3\implies1\)</span>) Use the integral representation.</li>
<li>(<span class="math inline">\(1\implies2\)</span>) Using <span class="math inline">\(\cal E\propto -\ddd t \Var\)</span>, get a differential inequality that gives exponential decay.</li>
<li>(<span class="math inline">\(2\implies1\)</span>) Write <span class="math inline">\(\cE\)</span> as a limit and apply the inequality to <span class="math inline">\(\Var\)</span>.</li>
<li>If <span class="math inline">\(P_t\)</span> is reversible, then <span class="math inline">\(t\mapsto \log\ve{P_t f}_{L^2(\mu)}^2\)</span>, <span class="math inline">\(\log \cE(P_tf,P_tf)\)</span> are convex. Proof. First derivative is <span class="math inline">\(-\fc{2\an{\cL P_tf, f}}{\ve{P_tf}^2}\)</span>. Differentiate again, use CS.</li>
<li>(<span class="math inline">\(2\implies3\)</span>) The first derivative is increasing. Rearrange to get <span class="math display">\[\fc{\cE(P_tf,P_tf)}{\cE(f,f)}\le \fc{\ve{P_tf}_{L^2(\mu)}^2}{\ve{f}_{L^2(\mu)}^2}\]</span>.</li>
<li>(<span class="math inline">\(4\implies2\)</span>, <span class="math inline">\(5\implies3\)</span>) Use the lemma: if <span class="math inline">\(g\)</span> is convex and <span class="math inline">\(g(t)\le K-\al t\)</span> for <span class="math inline">\(t\ge 0\)</span> then <span class="math inline">\(g(t)\le g(0)-\al t\)</span> for <span class="math inline">\(t\ge 0\)</span>.</li>
</ol>
Intuition: If reversibility holds,
\begin{align}
\cE(f,g) &amp;=\rc2 \sum \mu_i \La_{ij} (f_i-f_j)(g_i-g_j)\\
\cE(f,f) &amp;=\rc2 \sum \mu_i \La_{ij} (f_i-f_j)^2.
\end{align}
<p><a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>In finite dimensions, if <span class="math inline">\(\mu f=0\)</span>, <span class="math display">\[
\cE(f,f) \ge (\ub{\la_1}0-\la_2) \Var_\mu(f).
\]</span> The best constant in the Poincare inequality is the spectral gap. The spectral gap controls the exponential convergence rate. Note it’s essential that <span class="math inline">\(\La\)</span> admits a real spectral decomposition.</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>The kernel is the same as <span class="math inline">\(\La\)</span> except it also records the probbability of staying. <span class="math inline">\(K-I = \La\)</span>.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p><span class="math inline">\(K^*(x,y) = \fc{K(y,x)}{\pi(x)}\pi(y)\)</span>.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>(cf. Laplacian)<a href="#fnref3">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Types and programming languages, Benjamin Pierce</title>
    <link href="http://holdenlee.github.io/notebook/posts/cs/type_theory/types_and_pl.html" />
    <id>http://holdenlee.github.io/notebook/posts/cs/type_theory/types_and_pl.html</id>
    <published>2016-07-31T00:00:00Z</published>
    <updated>2016-07-31T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Types and programming languages, Benjamin Pierce</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-07-31 
          , Modified: 2016-07-31 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#recursive-types">20 Recursive types</a><ul>
 <li><a href="#formalities">20.2 Formalities</a></li>
 <li><a href="#subtyping">20.3 Subtyping</a></li>
 </ul></li>
 <li><a href="#metatheory-of-recursive-types">21 Metatheory of recursive types</a><ul>
 <li><a href="#subtyping-1">21.3 Subtyping</a></li>
 <li><a href="#regular-trees">21.7 Regular trees</a></li>
 <li><a href="#mu-types">21.8 Mu-types</a></li>
 </ul></li>
 <li><a href="#type-reconstruction">22 Type reconstruction</a><ul>
 <li><a href="#let-polymorphism">22.7 Let-polymorphism</a></li>
 </ul></li>
 <li><a href="#universal-types">Universal types</a><ul>
 <li><a href="#system-f">23.3 System F</a></li>
 <li><a href="#section">23.10</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="recursive-types">20 Recursive types</h2>
<p><span class="math inline">\(\mu\)</span> is a recursion operator for types. A definition <span class="math inline">\(T = \mu X. Y\)</span> means: let <span class="math inline">\(T\)</span> be the infinite type satisfying <span class="math inline">\(X=Y\)</span>.</p>
<pre><code>Hungry = \mu A. Nat -&gt; A
Stream = \mu A. Unit -&gt; {Nat, A}
Process = \mu A. Nat -&gt; {Nat, A}
Counter = \mu C. {get: Nat, inc: Unit -&gt; C}</code></pre>
<p>Note: you can’t define Hungry in Haskell because (Then how does printf work? Something with type classes?)</p>
<p>Recursive types well-types the fixed-point combinator. <span class="math display">\[
fix_T = \la f:T\to T.(\la x:(\mu A. A\to T). f (x x)) (\la x:(\mu A. A\to T). f (x x))
\]</span></p>
<p>Every type is inhabited (<span class="math inline">\(\la\_:(). fix_T (\la x:T.x)\)</span>), so systems with recursive types are useless as logics.</p>
<p>[Embed untyped lambda calculus]</p>
<h3 id="formalities">20.2 Formalities</h3>
<p>There are 2 basic approaches to recursive types. What is the relationship between the type and its one-step unfolding?</p>
<ol type="1">
<li>Equi-recursive: They are definitionally equal.</li>
<li>Iso-recursive: They are different but isomorphic. There are functions <code>unfold</code> and <code>fold</code> going both ways. (Ex. Haskell)</li>
</ol>
<p>Note equi-recursive places more demands on the typechecker.</p>
<h3 id="subtyping">20.3 Subtyping</h3>
<h2 id="metatheory-of-recursive-types">21 Metatheory of recursive types</h2>
<p><strong>Theorem</strong> (Knaster-Tarski): Let <span class="math inline">\(X\)</span> be a poset, <span class="math inline">\(f:X\to X\)</span> be order-preserving. Then there exists a fixed point, <span class="math inline">\(\sup\set{x\in X}{x\le f(x)}\)</span>.</p>
<p>Let <span class="math inline">\(\cal U\)</span> be the universal set. Consider <span class="math inline">\((\cal P(\cal U), \subeq)\)</span>. Say <span class="math inline">\(X\)</span> is <span class="math inline">\(F\)</span>-closed/consistent if <span class="math inline">\(F(X)\subeq/\supeq X\)</span>.</p>
<p><em>Corollary</em>. The intersection/union of all <span class="math inline">\(F\)</span>-closed/consistent is the least/greatest fixed point of <span class="math inline">\(F\)</span>, denoted by <span class="math inline">\(\mu F, \nu F\)</span>.</p>
<p>(Principle of induction/coinduction) If <span class="math inline">\(X\)</span> is <span class="math inline">\(F\)</span>-closed, <span class="math inline">\(\mu F\subeq X\)</span>; if <span class="math inline">\(X\)</span> is <span class="math inline">\(F\)</span>-consistent, <span class="math inline">\(X\subeq \nu F\)</span>.</p>
<p>Finite tree types are given by</p>
<pre><code>T = Top | (T, T) | T -&gt; T </code></pre>
<p>Infinite tree types are like this but the tree can be infinite.</p>
<h3 id="subtyping-1">21.3 Subtyping</h3>
<p>Say <span class="math inline">\(T&lt;:Top\)</span>, <span class="math inline">\(S_1&lt;:T_1, S_2&lt;:T_2 \implies (S_1\times S_2)&lt;:(T_1,T_2)\)</span> and similarly for <span class="math inline">\(\to\)</span>. Take the transitive closure to get the subtyping relation.</p>
<h3 id="regular-trees">21.7 Regular trees</h3>
<p>A tree type is regular if subtrees(T) is finite.</p>
<h3 id="mu-types">21.8 Mu-types</h3>
<pre><code>T = X
	| Top
	| T x T
	| T -&gt; T
	| \mu X. T</code></pre>
<p>“Keep substituting” <span class="math inline">\(\mu X. T\)</span> to get the tree type corresponding to the <span class="math inline">\(\mu\)</span>-type, treeof<span class="math inline">\(([X\mapsto \mu X. T]T)(\pi)\)</span>.</p>
<h2 id="type-reconstruction">22 Type reconstruction</h2>
<p>2 questions:</p>
<ol type="1">
<li>Are all substitution instances of t well typed? <span class="math display">\[\forall \si, (\si \Ga \vdash \si t:T)\]</span> Type variables should be held abstract. This leads to <strong>parametric polymorphism</strong>.</li>
<li>Is some substitution instance of <span class="math inline">\(t\)</span> well typed? <span class="math display">\[\exists \si, (\si \Ga \vdash \si t:T)\]</span> Can <span class="math inline">\(t\)</span> be instantiated to a well-typed term by choosing appropriate values? This leads to type reconstruction/inference.</li>
</ol>
<p>Constraint typing: <span class="math inline">\(\Ga \vdash t:T|_{\cal X} C\)</span> means “term <span class="math inline">\(t\)</span> has type <span class="math inline">\(T\)</span> under assumptions <span class="math inline">\(\Ga\)</span> whenever constraints <span class="math inline">\(C\)</span> are satisfied.” <span class="math inline">\(\cal X\)</span> tracks type variables introduced in each subderivation.</p>
<p>(This is a hybrid between the normal deductive system, and the bottom-up constraint generation system.)</p>
<h3 id="let-polymorphism">22.7 Let-polymorphism</h3>
<p>Not allowed: doubleFun:<span class="math inline">\(\forall a . (\forall f : a\to a) \to a \to a\)</span> defined by</p>
<pre class="hs"><code>let doubleFun = \f x -&gt; f (f x)</code></pre>
<p>Reason: a polytype cannot appear inside <code>-&gt;</code>.</p>
<p>T-LetPoly: <span class="math display">\[
\frac{\Ga \vdash [x\mapsto t_1]t_2:T_2 \quad \Ga \vdash t_1:T_1}{\Ga \vdash \text{let }x=t_1\text{ in }t_2:T_2}.
\]</span> Instead of calculating a type for <span class="math inline">\(t_1\)</span>, it substitutes <span class="math inline">\(t_1\)</span> in the body. I.e., perform a step of evaluation before calculating types.</p>
<p>Problem: If the body contains many occurrences, we have to check once for each occurrence. This can take exponential time. See p. 333-4 for solution. Worst-case is still exponential, but in practice it is essentially linear.</p>
<h2 id="universal-types">Universal types</h2>
<p>We need to abstract out a type from a term and instantiate the abstract term with concrete type annotations.</p>
<ul>
<li>Parametric polymorphism: a single piece of code can be typed generically using variables in place of types, and then instantiated. They behave uniformly.
<ul>
<li>Impredicative/first-class</li>
<li>Let-polymorphism (restricted to top-level let-bindings). Functions cannot take polymorphic values as arguments.</li>
</ul></li>
<li>Ad-hoc polymorphism: Exhibit different behaviors when viewed at different types. Overloading: associate single function symbol with many implementations.</li>
<li>Multi-method dispatch</li>
<li>Intensional polymorphism: restricted computation over types at run time.</li>
<li>Subtype polymorphism</li>
</ul>
<h3 id="system-f">23.3 System F</h3>
<p>Equivalent to polymorphic lambda-calculus a.k.a. 2nd-order lambda calculus because it corresponds to 2nd-order intuitionistic logic, which allows quantification over predicates (types) not just terms.</p>
<p>New terms are</p>
<ul>
<li><span class="math inline">\(\la X.t\)</span> (type abstraction)</li>
<li><span class="math inline">\(t [T]\)</span> (type application)</li>
</ul>
<h3 id="section">23.10</h3>
<p>Impredicative: definition involves thing being defined. <span class="math inline">\(T=\forall X.X\to X\)</span> ranges over all types, including <span class="math inline">\(T\)</span> itself.</p>
<p>Predicative/stratified: range is restricted to monotypes.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Type theory</title>
    <link href="http://holdenlee.github.io/notebook/posts/cs/type_theory/type_theory.html" />
    <id>http://holdenlee.github.io/notebook/posts/cs/type_theory/type_theory.html</id>
    <published>2016-07-31T00:00:00Z</published>
    <updated>2016-07-31T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Type theory</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-07-31 
          , Modified: 2016-07-31 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#hindley-milner-type-system">Hindley-Milner type system</a><ul>
 <li><a href="#ingredients">Ingredients</a></li>
 </ul></li>
 <li><a href="#axioms">Axioms</a></li>
 <li><a href="#algorithm-w">Algorithm W</a></li>
 <li><a href="#bottom-up-algorithm-w">Bottom-up Algorithm W</a></li>
 <li><a href="#lambda-cube">Lambda cube</a></li>
 <li><a href="#scratch">Scratch</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="hindley-milner-type-system">Hindley-Milner type system</h2>
<p>The Hindley-Milner type system is a very nice point in the space of possible type systems because there is a reasonable algorithm to deduce the most general type of a program without type annotations (it is sound and complete).</p>
<p>To understand it, we need to understand</p>
<ul>
<li>the ingredients: what are syntactically valid expressions?</li>
<li>the axioms: the rules that allow you to say what types more complicated expressions are, given the types of the building blocks.</li>
<li>the algorithm: an efficient way to find the most general type (ex. <code>Int -&gt; a -&gt; List a</code>) of an expression, given the types of the building blocks. This algorithm can be proved to capture all possible types for the expression.</li>
</ul>
<p>What rules make sense? <span class="math display">\[ x:a,\quad f:a\to b\vdash f x:b\]</span> And we need some kind of specialization <span class="math display">\[ x : \forall a, F(a) \vdash x : F(a').\]</span> We need lambda expressions too.</p>
<h3 id="ingredients">Ingredients</h3>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">newtype</span> <span class="dt">Var</span> <span class="fu">=</span> <span class="dt">Var</span> <span class="dt">String</span>
<span class="kw">newtype</span> <span class="dt">TVar</span> <span class="fu">=</span> <span class="dt">TVar</span> <span class="dt">String</span>

<span class="kw">data</span> <span class="dt">Expr</span> <span class="fu">=</span> <span class="dt">ExprV</span> <span class="dt">Var</span> <span class="co">-- x</span>
	<span class="fu">|</span> <span class="dt">App</span> <span class="dt">Expr</span> <span class="dt">Expr</span> <span class="co">-- e_1 e_2</span>
	<span class="fu">|</span> <span class="dt">Lambda</span> <span class="dt">Var</span> <span class="dt">Expr</span> <span class="co">-- \lambda x. e</span>
	<span class="fu">|</span> <span class="dt">Let</span> <span class="dt">Var</span> <span class="dt">Expr</span> <span class="dt">Expr</span> <span class="co">-- let x = e_1 in e_2</span>

<span class="kw">data</span> <span class="dt">Mono</span> <span class="fu">=</span> <span class="dt">MonoV</span> <span class="dt">TVar</span> <span class="co">-- a</span>
	<span class="fu">|</span> <span class="dt">TApp</span> <span class="dt">Type</span> [<span class="dt">Mono</span>] <span class="co">-- D t_1 ..., here D is a parametric type with some number of arguments</span>

<span class="kw">data</span> <span class="dt">Poly</span> <span class="fu">=</span> <span class="dt">PolyM</span> <span class="dt">Mono</span> <span class="co">-- t</span>
	<span class="fu">|</span> <span class="dt">Forall</span> <span class="dt">TypeVar</span> <span class="dt">Poly</span> <span class="co">-- \forall a . s</span></code></pre></div>
<p>(Note, “-&gt;” is a special case of TApp: TApp fun [a, b].)</p>
<p>Monotypes can only be one type (ex. <code>Int -&gt; [Int]</code>) while polytypes can be many different types (ex. <code>a -&gt; [a]</code>, forall is implicit here).</p>
<p>We need to make a distinction between monotypes and polytypes because <strong>only monotypes can go in the forall</strong>.</p>
<p>We also need the notion of <strong>free variable</strong>. These are variables that have not been captured by a <span class="math inline">\(\forall\)</span>.</p>
<pre><code>import Data.Set as S

freeM :: Mono -&gt; S.Set TVar
freeM = \case
	MonoV t -&gt; S.singleton t
	TApp _ ts -&gt; S.unions (map freeM ts)

freeP :: Poly -&gt; S.Set TVar
freeP = \case
	PolyM m s -&gt; S.delete m (freeP si)</code></pre>
<p>(Warning: in Haskell all type variables are implicitly bound, so free variables do not appear. See Ex. 1 in wikipedia.)</p>
<p>Next we need the notion of a context, which says what expressions are of what type. For example, it can say what types the variables are; in the inside of <code>let</code> we need to know what the context is to do typing.</p>
<pre><code>data Bindings = Bind Var Poly -- x:s

type Context = S.Set Bindings

freeC :: Context -&gt; S.Set Var
freeC ga = S.unions (map (\case Bind v s -&gt; freeP s) (S.elems ga))
	</code></pre>
<!--Is set a monad?-->
<p>The polymorphic types form a partial order <span class="math inline">\(\si\sqsubseteq \si'\)</span>, <span class="math inline">\(\si\)</span> is more special. Ex. <code>Map Int Int</code><span class="math inline">\(\sqsubseteq\)</span><code>Map Int v</code><span class="math inline">\(\sqsubseteq\)</span><code>Map k v</code>.</p>
<!--this requires a bit more work to code...-->
<!--note: need to add deriving...-->
<h2 id="axioms">Axioms</h2>
\begin{align}
\frac{x:\si\in \Ga}{\Ga\vdash x:\si}&amp;&amp; \text{[Var]}\\
\frac{\Ga\vdash e_0:\tau \to \tau'\quad
\Ga\vdash e_1:\tau}{\Ga\vdash e_0 \,e_1:\tau'}&amp;&amp; \text{[App]}\\
\frac{\Ga\cup \{ x:\tau\} \vdash e:\tau'}{\Ga \vdash \lambda x.e:\tau \to \tau'} &amp;&amp;\text{[Abs]}\\
\frac{\Ga \vdash  e_0:\si\quad \Ga\cup \{x:\si\}\vdash e_1:\tau}{\Ga \vdash \text{let }x=e_0\text{ in }e_1:\tau}&amp;&amp; \text{[Let]}\\
\frac{\Ga \vdash e:\si'\quad \si'\sqsubseteq \si}{\Ga \vdash e:\si}&amp;&amp;\text{[Inst]}\\
\frac{\Ga \vdash e:\si\quad \al\nin \text{free}(\Ga)}{\Ga \vdash e:\forall \al.\si}&amp;&amp;\text{[Gen]}.
\end{align}
<p>Abs is abstraction. Inst is instantiation. Note we add to the context when we go inside a lambda or a let. Gen then Inst together help specialize given information in context.</p>
<p>Subtlety: in <code>let</code>, variables enter in polymorphic form and can be specialized. Contrast <span class="math display">\[
\la f. (f \,\text{true}, f \,0)
\]</span> with <span class="math display">\[
\text{let } f = \la x. x\text{ in } (f\text{ true}, f \, 0).
\]</span> This is why <code>let</code> is NOT just syntactic sugar for <span class="math inline">\((\la x.e_2)\,e_1\)</span>; it genuinely adds expressivity.</p>
<h2 id="algorithm-w">Algorithm W</h2>
<p>Algorithm is simple, but there’s a lot of things you have to define first (ex. substitution, instantiation).</p>
<p>First, define a unification algorithm. It takes expressions (AST’s) <span class="math inline">\(\si,\tau\)</span> and returns a substitution (map) <span class="math inline">\(U\)</span>, such that for any substitution <span class="math inline">\(R\)</span> unifying <span class="math inline">\(\si\)</span> and <span class="math inline">\(\tau\)</span>, <span class="math inline">\(R=SU\)</span>. I.e., it gives the most general unification. (Unify by making more specific.)</p>
<p>Algorithm W: Given a context/type environment <span class="math inline">\(\ol p\)</span> (map from strings to polytypes/schemes), and an expression <span class="math inline">\(e\)</span>, return a substitution and a typing for <span class="math inline">\(e\)</span> and all subexpressions. (We will denote such a typing by <span class="math inline">\(\ol{e}_\si\)</span> where <span class="math inline">\(\si\)</span> is the type for <span class="math inline">\(e\)</span>, and <span class="math inline">\(\ol{\bullet}\)</span> means that all subexpressions have been annotated.) If <span class="math inline">\(e\)</span> is…</p>
<ul>
<li>variable <span class="math inline">\(x\)</span>: Lookup <span class="math inline">\(x\)</span> in the type environment. If it’s not there, ERROR. Let <span class="math inline">\(\tau\)</span> be the type. Substitute generic (bound) variables in <span class="math inline">\(\tau\)</span> by new (free) variables. I.e., <code>({}, instantiate(tau))</code>.</li>
<li>application <span class="math inline">\(d\,e\)</span>:
<ul>
<li>Run <span class="math inline">\(W\)</span> on the function: <span class="math inline">\((R,\ol d_\rh) = W(\ol p, d)\)</span>.</li>
<li>Run <span class="math inline">\(W\)</span> on the argument, where we apply the substitution output by the function, <span class="math inline">\((S, \ol e_\si) = W(R\ol p, e)\)</span>.</li>
<li>We’ve now calculated a type <span class="math inline">\(\rh\)</span> for the function, and a type <span class="math inline">\(\si\)</span> for the argument. Now we need to unify these. (Ex. the function is <span class="math inline">\(a\to a\)</span> and the type is <code>Int</code>.) Let <span class="math inline">\(\be\)</span> be a new variable. Unify <span class="math inline">\(S\rh\)</span> and <span class="math inline">\(\si\to \be\)</span>, <span class="math inline">\(U=U(S\rh, \si\to \be)\)</span>.</li>
<li>Return <span class="math inline">\((USR, U(((S\ol d)\ol e)_\be))\)</span>. (Compose the substitutions in the order that we calculated them.) Explanation:
<ul>
<li>We had a typing for <span class="math inline">\(\ol d\)</span>. We update that by <span class="math inline">\(S\)</span>.</li>
<li>The type for <span class="math inline">\(d\, e\)</span> is <span class="math inline">\(\be\)</span> (found in the previous step).</li>
<li>Apply <span class="math inline">\(U\)</span> to get the type for the whole expression.</li>
</ul></li>
<li>Note: if <span class="math inline">\(x\)</span> came from <span class="math inline">\(\la x:\be\)</span>, then <span class="math inline">\(\be\)</span> is a monotype (possibly with free variables), and no substitution is done. If <span class="math inline">\(x\)</span> came from <code>let</code> then <span class="math inline">\(x\)</span> may have bound variables, so we instantiate new variables.</li>
</ul></li>
<li>abstraction <span class="math inline">\(\la x. \,d\)</span>:
<ul>
<li>Let <span class="math inline">\(\be\)</span> be a new type variable.</li>
<li>Add <span class="math inline">\(x:\be\)</span> to the context, <span class="math inline">\(\ol p \cup \{x:\be\}\)</span>.</li>
<li>Run <span class="math inline">\(W\)</span> with this new context, <span class="math inline">\(W(\ol p \cup \{x:\be\}, d)\)</span>.</li>
<li>Return <span class="math inline">\((R, (\la x_{R\be}.\ol{d}_\rh)_{R\be \to \rh})\)</span>.</li>
</ul></li>
<li><code>let x=d in e</code>. This different similar to <span class="math inline">\(\la\)</span> with application (<span class="math inline">\((\la x . d) \, e\)</span>) because there we would apply the substitution to the function <span class="math inline">\(d\)</span> (<span class="math inline">\(S\rh\)</span>) and attempt to unify, but here we keep the bound variables in <span class="math inline">\(d\)</span>.
<ul>
<li>Run <span class="math inline">\(W\)</span> on <span class="math inline">\(d\)</span>: Let <span class="math inline">\((R,\ol d_\rh)=W(\ol p, d)\)</span>.</li>
<li>Run <span class="math inline">\(W\)</span> on <span class="math inline">\(e\)</span> with <span class="math inline">\(\{x:\rh\}\)</span> added: Let <span class="math inline">\((s,\ol e_\si) = W(R\ol p\cup \{x:\rh\}, e)\)</span>.</li>
<li>Return <span class="math inline">\((SR, (\text{let }x_{S\rh} = S\ol d\text{ in }\ol e)_{\si})\)</span>.</li>
<li>!! Should generalize here: abstract (<span class="math inline">\(\forall\)</span>) over all variables free in <span class="math inline">\(d\)</span> but not free in the environment. Ex. <code>let foo = \y -&gt; x</code> in context <code>x:a</code>. <code>\y -&gt; x : b -&gt; a</code> is not yet generalized. Make it <span class="math inline">\(\forall b: b\to a\)</span>.</li>
</ul></li>
</ul>
<p>Note we don’t really need to keep track of the intermediate typings, just the substitutions.</p>
<p>Subtle point I’m still trying to get clear (ex. 1):</p>
<pre><code>let bar [forall a. forall b. a -&gt; (b -&gt; a)] = \x -&gt;
	let foo [forall b. b -&gt; a] = \y -&gt; x
	in foo
in bar</code></pre>
<p>is the same as</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell">\x <span class="ot">-&gt;</span> (\y <span class="ot">-&gt;</span> x)</code></pre></div>
<p>right?</p>
<h2 id="bottom-up-algorithm-w">Bottom-up Algorithm W</h2>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">data</span> <span class="dt">Constraint</span> <span class="fu">=</span> <span class="dt">EqC</span> <span class="dt">Poly</span> <span class="dt">Poly</span>
	<span class="fu">|</span> <span class="dt">InstM</span> <span class="dt">Poly</span> (<span class="dt">S.Set</span> <span class="dt">Mono</span>) <span class="dt">Poly</span>
	<span class="fu">|</span> <span class="dt">GenericInst</span> <span class="dt">Poly</span> <span class="dt">Poly</span></code></pre></div>
<p>Generate the constraint set as follows. For an expression <span class="math inline">\(e\)</span>, if <span class="math inline">\(e\)</span> is</p>
<ul>
<li>variable <span class="math inline">\(x\)</span>: Get fresh <span class="math inline">\(\be\)</span>, note <span class="math inline">\(x:\be\)</span>.</li>
<li>application <span class="math inline">\(e_1\,e_2\)</span>: Recurse on <span class="math inline">\(e_1\)</span> and <span class="math inline">\(e_2\)</span> (take union of constraints and typings). Let <span class="math inline">\(e_1:\tau_1\)</span>, <span class="math inline">\(e_2:\tau_2\)</span>. Note <span class="math inline">\(e_1\, e_2:\be\)</span>, add <span class="math inline">\(\tau_1\equiv \tau_2\to \be\)</span> to the constraint set.</li>
<li>abstraction <span class="math inline">\(\la x. e\)</span>: Recurse on <span class="math inline">\(e\)</span>, suppose <span class="math inline">\(e:\tau\)</span>. Take all typings of the form <span class="math inline">\(x:\tau'\)</span> and make constraints <span class="math inline">\(\tau'\equiv \be\)</span>. Generate fresh <span class="math inline">\(\be\)</span>. Type <span class="math inline">\(\la x.e : (\be \to \tau)\)</span>.</li>
<li><code>let x=e_1 in e_2</code>: Recurse on <span class="math inline">\(e_1:\tau_1\)</span>, <span class="math inline">\(e_2:\tau_2\)</span>, and type as <span class="math inline">\(\tau_2\)</span>. For all typings of the form <span class="math inline">\(x:\tau'\)</span> generated by <span class="math inline">\(e_2\)</span>, add <span class="math inline">\(\tau'\le_M \tau_1\)</span> to the constraint set.</li>
</ul>
<p>Note that for the <span class="math inline">\(\le_M\)</span> constraint, we need to keep a list of monomorphic variables <span class="math inline">\(M\)</span> (corresponding to free—introduced in lambdas) as we recurse down the tree. (Things in lambdas DO NOT generalize, in <span class="math inline">\(\la x. e\)</span>, <span class="math inline">\(x\)</span> can’t have two different types/interpretations in <span class="math inline">\(e\)</span>. Thus within the lambda expression, <span class="math inline">\(x\)</span> is in the monomorphic set—you can’t do <span class="math inline">\(\forall x\)</span>.)</p>
<p>The bottom-up inference rules are different from the usual inference rules:</p>
<ul>
<li>Usual rules keep the context the same; these change the context.</li>
<li>They translate more directly into an algorithm.</li>
<li>They involve the constraints, not the context.</li>
</ul>
<p>See p. 10 for the algorithm.</p>
<h2 id="lambda-cube">Lambda cube</h2>
<p>https://en.wikipedia.org/wiki/Lambda_cube</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/1/19/Lambda_cube.png"></p>
<p>3 dimensions:</p>
<ul>
<li>Polymorphism (bottom/top)</li>
<li>Type operators/types depending on types (front/back)</li>
<li>Types depending on terms, dependent types (left/right)</li>
</ul>
<p>Front:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">*</th>
<th style="text-align: left;">None</th>
<th style="text-align: left;">Dependent types</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Polymorphism</td>
<td style="text-align: left;">F, <span class="math inline">\(\la2\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\la P2\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">None</td>
<td style="text-align: left;"><span class="math inline">\(\la_{\to}\)</span></td>
<td style="text-align: left;">LF, <span class="math inline">\(\la P\)</span></td>
</tr>
</tbody>
</table>
<p>Back: (types depending on types)</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">*</th>
<th style="text-align: left;">None</th>
<th style="text-align: left;">Dependent types</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Polymorphism</td>
<td style="text-align: left;"><span class="math inline">\(F_\om\)</span>, <span class="math inline">\(\la \om\)</span></td>
<td style="text-align: left;">CIC, <span class="math inline">\(\la P\om\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">None</td>
<td style="text-align: left;"><span class="math inline">\(\la_\om\)</span>, <span class="math inline">\(\la\ul{\om}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\la P \ul{\om}\)</span></td>
</tr>
</tbody>
</table>
<p>Hindley-Milner is a subset of System F (in between <span class="math inline">\(\la_{\to}\)</span> and <span class="math inline">\(F=\la 2\)</span>). Haskell contains system F.</p>
<p>References:</p>
<ul>
<li><span class="math inline">\(F_{&lt;:}\)</span> (F with subtyping): Ch. 26, 28</li>
<li><span class="math inline">\(\la_\om\)</span> (types depending on types): Ch. 29</li>
<li><span class="math inline">\(F_\om\)</span> (F with types depending on types): Ch. 30</li>
<li><span class="math inline">\(F_{&lt;:}^\om\)</span>: CH. 31</li>
</ul>
<h2 id="scratch">Scratch</h2>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>LLVM tutorial</title>
    <link href="http://holdenlee.github.io/notebook/posts/cs/PL/llvm.html" />
    <id>http://holdenlee.github.io/notebook/posts/cs/PL/llvm.html</id>
    <published>2016-07-31T00:00:00Z</published>
    <updated>2016-07-31T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>LLVM tutorial</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-07-31 
          , Modified: 2016-07-31 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p><a href="http://www.stephendiehl.com/llvm/">Tutorial</a></p>
<ul>
<li>Source</li>
<li>lexer</li>
<li>parser</li>
<li>checking</li>
<li>codegen</li>
<li>Output</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>PMI and feature vectors</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/pmi.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/pmi.html</id>
    <published>2016-07-29T00:00:00Z</published>
    <updated>2016-07-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>PMI and feature vectors</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-07-29 
          , Modified: 2016-07-29 
	</p>
      
       <p>Tags: <a href="/tags/nlp.html">nlp</a>, <a href="/tags/word%20embeddings.html">word embeddings</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <ul>
<li><a href="randwalk.html">ALLMR16 Randwalk</a></li>
<li><a href="polysemy.html">ALLMR16 Polysemy</a></li>
</ul>
<p>Reduce the dimensionality of words by finding a low-dimensional vector for each word, such that the inner products approximate the log of the co-occurrence matrix (perhaps in a weighted sense).</p>
<p>Why does this work? I.e., why are the low-dimensional vectors useful for NLP tasks? One “task” is analogies. Word embeddings are useful for analogies if addition naturally corresponds to composing their meanings.</p>
<p>What could be a low-dimensional representation of a word? Its PMI with all possible contexts. (Firth: a word’s sense is captured by the distribution of other words around it.) Assume all these PMI vectors live in a low-dimensional space; why does the log of co-occurrence find these vectors?</p>
<p>Here’s a simplified model. Consider words drawn as follows: pick a random context vector, and then take words <span class="math inline">\(w, w'\)</span> with probability <span class="math inline">\(\rc{Z^2} e^{-\an{v_w,c} - \an{v_w',c}}\)</span>. Integrating this gives <span class="math inline">\(\rc{Z^2} \exp\pf{|v_w+v_w'|^2}{2}\)</span>. Then (I’m not being careful with the factor of <span class="math inline">\(d\)</span>) <span class="math display">\[PMI(w,w') = \lg \fc{\Pj(w,w')}{\Pj(w)\Pj(w')} \approx \rc d \an{v_w,v_w'}+o(1).\]</span></p>
<p>If the context vector drifts slowly enough, this analysis still works.</p>
<p>(?) The right optimization problem is <span class="math display">\[\min\sum_{w_1,w_2} \Pj(w_1,w_2) (PMI(w_1,w_2)-\an{v_{w_1},v_{w_2}}.\]</span></p>
<p>Actually, better is <span class="math display">\[
\min_{\{v_w\}, C} \sum_{w,w'} X_{w,w'}(\ln (X_{w,w'} - \ve{v_w+v_{w'}}_2^2-C))^2.
\]</span></p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Representation learning</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/representation.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/representation.html</id>
    <published>2016-07-28T00:00:00Z</published>
    <updated>2016-07-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Representation learning</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-07-28 
          , Modified: 2016-07-28 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#low-dimensional-structure-in-high-dimensional-space">Low-dimensional structure in high-dimensional space</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>Representation learning means you have to first find a good representation—some hidden structure—for the data in order to learn it.</p>
<h2 id="low-dimensional-structure-in-high-dimensional-space">Low-dimensional structure in high-dimensional space</h2>
<p>Consider a distribution on a low-dimensional space. Now suppose this low-dimensional space is embedded in a high-dimensional space, and noise in the complementary (may be orthogonal) subspace is added. Recover the low-dimensional space and the structure on this space.</p>
<ul>
<li>Suppose complementary noise is Gaussian. [TV16] show how to recover given that the fourth moments of the distribution on the low-dim space are bounded away from that of a Gaussian. (Ex. clusters; a product distribution is limited by the dimension closest to Gaussian, etc.)</li>
<li>Can we weaken the assumption on the complementary noise? (Arora, Ge, Ma) Put an assumption on “unimodality” in other directions.</li>
<li>Suppose the subspace is a coordinate subspace, and the structure is a low-rank subspace within that. See <a href="matrices/relevant_coordinates.html">relevant coordinates</a>.</li>
</ul>
<p>Consider a dictionary-learning setting. There are many variations here. What we want to say is the following: given there’s a transformation in a certain class—ex. a neural net—that makes the data structured, can you learn the structure/parameters?</p>
<ul>
<li>Suppose the samples are sparse linear combinations of <span class="math inline">\(a_i\)</span>. This is the setting of dictionary learning. See <a href="matrices/AGM14.html">AGM14</a>, <a href="matrices/AGMM15.html">AGMM15</a>
<ul>
<li>Can we generalize from independent <span class="math inline">\(p\de_0+(1-p)D\)</span> distributions to non-Gaussian distributions?</li>
<li>Suppose that noise is added. The algorithm above works if the dot product between 2 noise vectors is <span class="math inline">\(o(1)\)</span>. Ex. each coordinate is <span class="math inline">\(o\prc{n^{\rc 4}}\)</span>. Then the dot product is summing <span class="math inline">\(n\)</span> numbers <span class="math inline">\(o\prc{n^{\rc 2}}\)</span> which is <span class="math inline">\(o(1)\)</span>. Can we do noise up to <span class="math inline">\(o(\sqrt n)\)</span>? The overlapping communities problem seems hard now. Before we could threshold at <span class="math inline">\(\rc 2\)</span>, but now, noise is up to <span class="math inline">\(\sqrt n\)</span>. cf. in SBM, you can deal with <span class="math inline">\(\rc{\sqrt n}\)</span> difference in probabilities; in SVD, you can add a random matrix of entries <span class="math inline">\(o(\sqrt n)\)</span> because the eigenvalues are much smaller.</li>
<li>In the undercomplete case, if the noise is orthogonal to the subspace spanned by <span class="math inline">\(a_i\)</span>, we’re in the setting of [TV16]. Products of non-Gaussians are OK.</li>
<li>Changing the formulation instead to that <span class="math inline">\((\an{x,a_i})_i\)</span> is sparse is equivalent to learning the dictionary <span class="math inline">\((A^+)^T\)</span>. (Warning: this looks different in the over/undercomplete case.)</li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Complexity of neural networks</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/complexity_of_neural_nets.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/complexity_of_neural_nets.html</id>
    <published>2016-07-25T00:00:00Z</published>
    <updated>2016-07-25T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Complexity of neural networks</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-07-25 
          , Modified: 2016-07-25 
	</p>
      
       <p>Tags: <a href="/tags/neural%20net.html">neural net</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#cs14-simnets---a-generalization-of-convolutional-networks">[CS14] SimNets - A Generalization of Convolutional Networks</a></li>
 <li><a href="#cs16-convolutional-rectifier-networks-as-generalized-tensor-decompositions">[CS16] Convolutional Rectifier Networks as Generalized Tensor Decompositions</a></li>
 <li><a href="#css16-on-the-expressive-power-of-deep-learning---a-tensor-analysis">[CSS16] On the Expressive Power of Deep Learning - A Tensor Analysis</a></li>
 <li><a href="#misc">Misc</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="http://www.cs.huji.ac.il/~cohennadav/index.html">Nadav Cohen’s webpage</a></p>
<p>Papers:</p>
<ul>
<li>[CS14] SimNets - A Generalization of Convolutional Networks</li>
<li>[CS16] Convolutional Rectifier Networks as Generalized Tensor Decompositions</li>
<li>[CS16] Inductive Bias of Deep Convolutional Networks through Pooling Geometry</li>
<li>[CSS16] Deep SimNets</li>
<li>[CSS16] On the Expressive Power of Deep Learning - A Tensor Analysis</li>
</ul>
<h2 id="cs14-simnets---a-generalization-of-convolutional-networks">[CS14] SimNets - A Generalization of Convolutional Networks</h2>
<p>A layer now looks like <span class="math display">\[ h(x) = MEX_\xi [u_l^T \phi(x,z_l) + b_l]_{l=1,\ldots, n} \]</span> where MEX is the exponential mean <span class="math display">\[
\rc{\xi} \ln\pa{\rc n \sumo in \exp(\xi c_i)}
\]</span> and <span class="math inline">\(\phi\)</span> is a kernel function, like <span class="math inline">\((x_iz_i)_i\)</span> or <span class="math inline">\((-|x_i-z_i|^p)_i\)</span>. These give rise to linear and generalized Gaussian kernels when composed with MEX.</p>
<p>MEX interpolates between minimum, average, and maximum pooling.</p>
<!--(This looks to be a generalization of $1\times 1$ convolution? What about larger?)-->
<p>There’s a natural unsupervised initialization scheme for SimNets based on statistical estimation. Assume the data is drawn from a mixture of generalized Gaussians and find max-likelihood parameters.</p>
<p>(cf. initialize the convolution kernels by looking at statistical properties of patches)</p>
<h2 id="cs16-convolutional-rectifier-networks-as-generalized-tensor-decompositions">[CS16] Convolutional Rectifier Networks as Generalized Tensor Decompositions</h2>
<p>ICML16.</p>
<blockquote>
<p>First, we show that convolutional rectifier networks are universal with max pooling but not with average pooling. Second, and more importantly, we show that depth efficiency is weaker with convolutional rectifier networks than it is with convolutional arithmetic circuits. This leads us to believe that developing effective methods for training convolutional arithmetic circuits, thereby fulfilling their expressive potential, may give rise to a deep learning architecture that is provably superior to convolutional rectifier networks but has so far been overlooked by practitioners.</p>
</blockquote>
<p>Generalized tensor decompositions</p>
<p><span class="math display">\[(A\ot_gB)_{d_1,\ldots, d_{P+Q}} = g(A_{d_1,\ldots, d_P}, B_{d_{P+1},\ldots, d_{P+Q}}). \]</span></p>
<p>For arbitrary commutative/associative pooling function, replace <span class="math inline">\(\ot\)</span> with <span class="math inline">\(\ot_g\)</span>.</p>
<h2 id="css16-on-the-expressive-power-of-deep-learning---a-tensor-analysis">[CSS16] On the Expressive Power of Deep Learning - A Tensor Analysis</h2>
<p>COLT2016.</p>
<p><img src="/images/deepnets_tensor_1.png"></p>
<p>Consider a neural network defined on input <span class="math inline">\((x_1,\ldots, x_N)\in (\R^s)^N\)</span>, as follows.</p>
\begin{align}
rep(i,d) &amp;= f_{\te_d}(x_i)\\
conv(i,z) &amp;= \an{a^{z,i}, rep(i,:)} = \sum_d a_d^{z,i} f_{\te_d}(x_i)\\
pool(z) &amp;= \prod_{i=1}^N conv(i,z) = \prod_{i=1}^N \sum_d a_d^{z,i} f_{\te_d}(x_i)\\
out(y) &amp;= \an{a_y, pool(:)} = \sum_{z=1}^Z a_{y,z} \prod_{i=1}^N\sum_d a_d^{z,i} f_{\te_d}(x_i)
\end{align}
<p>Note pool(z) is a 1-D tensor <span class="math display">\[ (a_d^{z,i\ot N *})_{i_1,\ldots, i_N, d} \ot (f_{\te_d}(x_i))^{i\ot N}_{d,i_1,\ldots, i_N} \]</span> so this is a rank <span class="math inline">\(Z\)</span> tensor.</p>
<p>Think of conv as <span class="math inline">\(1\times 1\)</span> convolution. Weight sharing makes the tensor decomposition symmetric.</p>
<p>Deep nets:</p>
<p><img src="/images/deepnets_tensor_2.png"></p>
<p>(<strong>Warning: The indexing of <span class="math inline">\(a\)</span> has been switched around here.</strong></p>
<p>We have a hierarchical decomposition. Ex. first layer:</p>
\begin{align}
conv_1(j,\ga) &amp;= \an{a^{1,j,\ga}, pool_0(j,:)}\\
&amp;=\an{a^{1,j,\ga}, \pa{\prod_{j'\in \{2j-1,2j\}} conv_0(j',k)}_k}\\
&amp;=\an{a^{1,j,\ga}, (\conv_0(2j-1,k)\conv_0(2j,k))_k}\\
&amp;=\sum_k a^{1,j,k} \conv_0(2j-1, k) \conv_0(2j,k).
\end{align}
<p>Iterating this gives the <strong>hierarchical Tucker decomposition</strong></p>
<p><img src="/images/deepnets_ht.png"></p>
<p><strong>Theorem 1</strong>: Consider the space of hierarchical tensors of order <span class="math inline">\(N\)</span> and dimension <span class="math inline">\(M\)</span> in each mode, parametrized by <span class="math inline">\(\{a^{l,j,\ga\}\)</span>. In this space, the tensor has CP-rank <span class="math inline">\(\ge r^{\fc N2}\)</span> a.e.</p>
<p><em>Proof</em>: Because the space of tensors with CP-rank forms an algebraic variety, it suffices to show that there exists a tensor of this rank. Do this by induction on layers and work with the matricization of the tensors (<span class="math inline">\(\ot\)</span> corresponds to Kronecker product). The CP-rank equals the rank of the matricization.</p>
<p>Question: what about convolution with larger windows?</p>
<h2 id="misc">Misc</h2>
<p>What is the motivation behind pooling? How much does it actually help?</p>
<p>Do you want universality?</p>
<p>Arithmetic circuits look much more like complexity-theoretic circuits! Training is harder. Product is AND, like soft version of min on <span class="math inline">\(\{0,1\}^n\)</span>?</p>
<p>(Replacing sigmoids/relus by 0-1?)</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Tensorflow setup</title>
    <link href="http://holdenlee.github.io/notebook/posts/cs/neural_nets/tensorflow.html" />
    <id>http://holdenlee.github.io/notebook/posts/cs/neural_nets/tensorflow.html</id>
    <published>2016-07-22T00:00:00Z</published>
    <updated>2016-07-22T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Tensorflow setup</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-07-22 
          , Modified: 2016-07-22 
	</p>
      
       <p>Tags: <a href="/tags/programming.html">programming</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <h2 id="setup-on-tiger">Setup on Tiger</h2>
<p>Load tensorflow on startup, e.g. put in <code>.bashrc</code>.</p>
<pre><code>module load python cudatoolkit/7.5 cudann
pip install --user /tigress/plazonic/public_html/tensorflow/rhel6/tensorflow_pkg_gpu/tensorflow-0.8.0-py2-none-any.whl</code></pre>
<p>Sample script</p>
<pre><code>#!/bin/bash

#SBATCH -t 10:00:00
#SBATCH -N 1
#SBATCH --ntasks-per-node=4
#SBATCH --ntasks-per-socket=2
#SBATCH --gres=gpu:2
#SBATCH --mail-type=begin  
#SBATCH --mail-type=end  
#SBATCH --mail-user=holdenl@princeton.edu  

module load python
module load cudatoolkit/7.5
module load cudann
THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python cifar10_multi_gpu_train.py --num_gpus=2 --train_dir='/tigress/holdenl/tmp/cifar10_train1'</code></pre>
<p>Run by <code>sbatch script.cmd</code>.</p>
<h2 id="cifar-setup">CIFAR setup</h2>
<ul>
<li>Train by calling <code>cifar10_multi_gpu_train.py</code> or <code>cifar10_train.py</code>.
<ul>
<li>This calls <code>cifar10.py</code> to build the graph.</li>
<li>It calls <code>cifar10_input.py</code> to download or load the data.
<ul>
<li><code>data_dir</code> defined in <code>cifar10.py</code>. (Changed to <code>/tigress/knv/cifar10_data</code>.)</li>
</ul></li>
</ul></li>
</ul>
<p>Settings: Override flags as above.</p>
<ul>
<li><code>num_gpus=2</code> seems to work best.</li>
<li>Specify training directory, ex. <code>train_dir='/tigress/holdenl/tmp/cifar10_train1'</code>.</li>
</ul>
<h2 id="train">Train</h2>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">train(fs, step_f, output_steps<span class="op">=</span><span class="dv">10</span>, summary_steps<span class="op">=</span><span class="dv">100</span>, save_steps<span class="op">=</span><span class="dv">1000</span>, eval_steps <span class="op">=</span> <span class="dv">1000</span>, max_steps<span class="op">=</span><span class="dv">1000000</span>, train_dir<span class="op">=</span><span class="st">&quot;/&quot;</span>, log_device_placement<span class="op">=</span><span class="va">False</span>, batch_size<span class="op">=</span><span class="dv">128</span>,train_data<span class="op">=</span><span class="va">None</span>,validation_data<span class="op">=</span><span class="va">None</span>, test_data<span class="op">=</span><span class="va">None</span>, train_feed<span class="op">=</span>{}, eval_feed<span class="op">=</span>{}, x_pl<span class="op">=</span><span class="st">&quot;x&quot;</span>, y_pl<span class="op">=</span><span class="st">&quot;y_&quot;</span>, batch_feeder_args<span class="op">=</span>[])</code></pre></div>
<ul>
<li><code>fs</code> is a dictionary containing: inference, loss functions</li>
<li><p><code>step_f</code> is function to execute at each training step, taking arguments <code>fs</code> and <code>global_step</code>. Example</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">lambda</span> fs, global_step: (
  train_step(fs[<span class="st">&quot;loss&quot;</span>], fs[<span class="st">&quot;losses&quot;</span>], global_step, 
             <span class="kw">lambda</span> gs: tf.train.AdamOptimizer(<span class="fl">1e-4</span>)))</code></pre></div></li>
<li></li>
</ul>
<h2 id="misc-notes">Misc notes</h2>
<p>Constants</p>
<ul>
<li><code>NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN=50000</code></li>
<li><code>NUM_EXAMPLES_PER_EPOCH_FOR_EVAL=10000</code></li>
<li><code>NUM_EPOCHS_PER_DECAY</code></li>
<li><code>INITIAL_LEARNING_RATE</code></li>
<li><code>LEARNING_RATE_DECAY_FACTOR</code></li>
<li><code>NUM_CLASSES=10</code></li>
<li><code>MOVING_AVERAGE_DECAY = 0.9999</code></li>
<li><code>NUM_EPOCHS_PER_DECAY = 350.0</code></li>
<li><code>LEARNING_RATE_DECAY_FACTOR = 0.1</code></li>
<li><code>INITIAL_LEARNING_RATE = 0.1</code></li>
</ul>
<h2 id="todo">Todo</h2>
<ul>
<li>Find tiger’s policies on storing files.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Stanford quals</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/stanford_quals.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/stanford_quals.html</id>
    <published>2016-06-29T00:00:00Z</published>
    <updated>2016-06-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Stanford quals</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-06-29 
          , Modified: 2016-06-29 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#supervised-learning-1">Supervised learning [1]</a></li>
 <li><a href="#unsupervised-learning-1">Unsupervised learning [1]</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>(from Jacob’s notes on Stanford quals)</p>
<h2 id="supervised-learning-1">Supervised learning [1]</h2>
<p><a href="http://cs229.stanford.edu/materials.html">CS229</a></p>
<ul>
<li>Perceptron, logistic regression, SVMs</li>
<li>Kernel methods, Gaussian processes</li>
<li>Boosting (AdaBoost)
<ul>
<li>What is the convergence rate?
<ul>
<li><span class="math inline">\(\exp(-\sum_{t=1}^T \gamma_t^2)\)</span></li>
<li>Key property: <em>adaptive</em></li>
</ul></li>
<li>What problems can it be applied to?
<ul>
<li>binary classification on fixed dataset</li>
</ul></li>
<li><a href="http://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf">notes</a></li>
</ul></li>
<li><a href="cart.html">Decision trees, random forests</a>
<ul>
<li>Bagging (given subset of size <span class="math inline">\(N\)</span>, create many versions of the dataset by subsampling <span class="math inline">\(N\)</span> things with replacement repeatedly)</li>
<li>For each of these versions, also subsample <span class="math inline">\(\sqrt{d})\)</span> of the features to use for the decision tree</li>
</ul></li>
<li>Neural networks</li>
<li>Linear regression</li>
<li>Regularization: L1, L2 and their properties</li>
</ul>
<h2 id="unsupervised-learning-1">Unsupervised learning [1]</h2>
<p><a href="http://cs229.stanford.edu/materials.html">CS229</a></p>
<ul>
<li><a href="matrices/k-means.html">K-means</a></li>
<li><a href="matrices/dimensionality_reduction.html">Linear dimension reduction</a> PCA, CCA, factor analysis, ICA
<ul>
<li>What is PCA / what is it used for?</li>
<li>Given input dataset, assuming it’s elliptical, finds the principle axes of the ellipse
<ul>
<li>In more statistical language, this finds a low-dimensional representation that explains as much of the variance as possible</li>
</ul></li>
<li>Can be computed by just taking SVD of covariance matrix</li>
<li>Typically we mean-center first</li>
<li>Sometimes want to do other scalings but no clear consensus on the best one</li>
<li><a href="matrices/cca.html">CCA</a> What is CCA / what is it used for?</li>
<li>Same intuition as PCA, but wants to find cross-correlations between two sets of variables (X and Y)</li>
<li>Obtained by taking singular vectors of <span class="math inline">\(\Cov(X,X)^{\rc 2}\Cov(X,Y)Cov(Y,Y)^{-\rc2}\)</span>.</li>
<li>?Isn’t this used for semi-supervised learning?
<ul>
<li>?E.g. given two sets of features, use CCA as a regularizer.</li>
</ul></li>
<li><a href="matrices/factor-analysis.html">Factor analysis</a> What is factor analysis / what is it used for?</li>
<li>Basically, this is just matrix factorization</li>
<li>Often allows more domain knowledge to be incorporated</li>
<li><a href="matrices/ica.html">ICA</a> What is ICA / what is it used for?</li>
<li>Blind source separation</li>
<li>Tries to break into independent signals</li>
<li>Often done by maximizing non-gaussianity of signals</li>
</ul></li>
<li>EM</li>
<li>What theoretical property does EM satisfy?
<ul>
<li>Maximizes lower bound <span class="math inline">\(\log p(x) - KL(q(z|x) || p(z|x))\)</span></li>
</ul></li>
<li>What are the general updates?
<ul>
<li>Compute expectation of log-likelihood under current model</li>
<li>Minimize expectation</li>
<li>Sort of like iteratively approximating setting the gradient to zero</li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>CART and random forests</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/cart.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/cart.html</id>
    <published>2016-06-29T00:00:00Z</published>
    <updated>2016-06-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>CART and random forests</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-06-29 
          , Modified: 2016-06-29 
	</p>
      
       <p>Tags: <a href="/tags/CART.html">CART</a>, <a href="/tags/adaptive%20basis%20functions.html">adaptive basis functions</a>, <a href="/tags/random%20forests.html">random forests</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#cart-classification-and-regression-trees">CART (Classification and regression trees)</a></li>
 <li><a href="#random-forests">Random forests</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>See Ch. 16 of Murphy. <a href="http://math.bu.edu/people/mkon/MA751/L18RandomForests.pdf">Presentation</a></p>
<p>An adaptive basis-function model (ABM) is a model of the form <span class="math display">\[ f(x) = w_0+\sumo mM w_m \phi_m(x).\]</span> Typically <span class="math inline">\(\phi_m(x) = \phi(x;v_m)\)</span></p>
<h2 id="cart-classification-and-regression-trees">CART (Classification and regression trees)</h2>
<p>Decision trees recursively partition the input space and define a local model on each region.</p>
<p>For example, if the model is constant on each region, <span class="math inline">\(f(x) = \sumo mM w_m (x\in R_m)\)</span>.</p>
<p>At each node, consider these kinds of splits:</p>
<ul>
<li>Thresholds: <span class="math inline">\(x_i&lt;t\)</span>, <span class="math inline">\(x_i\ge t\)</span>. (Quantitative feature)</li>
<li><span class="math inline">\(x_i=c,x_i\ne c\)</span> (Categorical feature)</li>
</ul>
<p>The cost can be regression or classification cost. Sum the costs for each leaf. Cost:</p>
<ul>
<li>Regression: cost of fitting model on the leaf.</li>
<li>Classification</li>
<li>Misclassification rate of leaf. (Use the most probable class label.)</li>
<li>Entropy: <span class="math inline">\(-\sumo cC \wh\pi_c\lg \wh\pi_c\)</span>. (Recommended)</li>
<li>Gini index <span class="math inline">\(\sumo cC \wh \pi_c(1-\wh \pi_c) = 1-\sumo cC \wh\pi_c^2\)</span>.</li>
</ul>
<p>Algorithm:</p>
<ol type="1">
<li>Start at the root node of a single-node tree, and put all data points at that node.</li>
<li>Find the split at the current node (attribute) that minimizes the cost (maximizes information gain). (If it is deemed not worth splitting, e.g. it doesn’t decrease the cost by much<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>, if it’s reached a specified depth, etc., then move on instead.) Make the split. (The data points are now distributed among the two children.)</li>
<li>Add both child nodes. to the queue.</li>
<li>Continue the algorithm (at 2) with the next node in the queue.</li>
</ol>
<p>Advantages:</p>
<ul>
<li>Easy to interpret</li>
<li>Handle mixed inputs</li>
<li>Insensitive to monotone transformations, robust to outliers.</li>
<li>Automatic variable selection</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>Features are very restricted.</li>
<li>Unstable: small changes in input data can have large changes because of the hierarchical nature of the tree-growing process. (They are high variance estimators.)</li>
</ul>
<h2 id="random-forests">Random forests</h2>
<p><strong>Bagging</strong> (bootstrap aggregating): Train <span class="math inline">\(M\)</span> different trees on independently selected subsets of the data, and compute <span class="math inline">\(f(x)=\sumo mM \rc M f_m(x)\)</span>.</p>
<p>(OR: use boosting instead of taking majority vote.)</p>
<p>BUT this can result in highly correlated predictors.</p>
<p><strong>Random forest</strong>: Decorrelate base learners by learning rees based on a randomly chosen subset of input variables and data points.</p>
<p>(What are the right parameters? <span class="math inline">\(\sqrt d\)</span> features?)</p>
<p><strong>Bayesian adaptive regression trees (BART)</strong>.</p>
<p>? Hierarchical mixtures of experts.</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>This is usually too myopic. Instead use pruning. Grow the full tree, evaluate the cross-validated error on subtrees, and pick a minimal tree whose CV error is within 1 se of the min.<a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
