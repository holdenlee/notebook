<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2016-03-05T00:00:00Z</updated>
    <entry>
    <title>Mirror descent</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/mirror-descent.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/mirror-descent.html</id>
    <published>2016-03-05T00:00:00Z</published>
    <updated>2016-03-05T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Mirror descent</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-05 
          , Modified: 2016-03-05 
	</p>
      
       <p>Tags: <a href="/tags/gradient.html">gradient</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <h2 id="summary">Summary</h2>
<p>Mirror descent lemma <span class="math display">\[
\al\an{\nb f(z_k), z_k-u}\le \fc{\al^2}{2}\ve{\nb f(z_k)}^2 + \rc2\ve{z_k-u}^2 - \rc2 \ve{z_{k+1}-u}^2.
\]</span></p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Accelerated Gradient descent</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/AGD.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/AGD.html</id>
    <published>2016-03-05T00:00:00Z</published>
    <updated>2016-03-05T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Accelerated Gradient descent</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-05 
          , Modified: 2016-03-05 
	</p>
      
       <p>Tags: <a href="/tags/gradient.html">gradient</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <h2 id="questions">Questions</h2>
<ul>
<li>Lanczos method</li>
<li>Chebyshev approach</li>
<li>Lower bound</li>
<li>Dependence on dimension?</li>
<li>Ellipsoid method</li>
</ul>
<h2 id="references">References</h2>
<p>Keywords: accelerated gradient descent, Chebyshev polynomials, mirror descent</p>
<ul>
<li>http://blog.mrtz.org/2013/09/07/the-zen-of-gradient-descent.html</li>
<li>https://en.wikipedia.org/wiki/Chebyshev_polynomials</li>
<li>http://people.csail.mit.edu/zeyuan/publications.htm</li>
<li>https://blogs.princeton.edu/imabandit/2013/04/01/acceleratedgradientdescent/</li>
<li>https://blogs.princeton.edu/imabandit/2015/06/30/revisiting-nesterovs-acceleration/</li>
<li>https://www.cs.cmu.edu/~ggordon/10725-F12/slides/09-acceleration.pdf</li>
<li>http://statweb.stanford.edu/~candes/math301/Lectures/acc_nesterov.pdf</li>
<li>http://www.asc.tuwien.ac.at/~winfried/teaching/106.079/SS2011/downloads/script-p-046-060.pdf</li>
<li>http://msekce.karlin.mff.cuni.cz/~strakos/download/2012_GerStr.pdf</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Analysis of neural networks</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/analysis.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/analysis.html</id>
    <published>2016-03-05T00:00:00Z</published>
    <updated>2016-03-05T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Analysis of neural networks</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-05 
          , Modified: 2016-03-05 
	</p>
      
       <p>Tags: <a href="/tags/neural%20net.html">neural net</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <h2 id="research-brainstorm">Research brainstorm</h2>
<ul>
<li>Complexity theorety point of view: What can be calculated by depth <span class="math inline">\(n\)</span> threshold circuits? Each gate is a function <span class="math inline">\(\sgn(\sum \al_ix_i)\)</span>.</li>
<li>It doesn’t make sense to assume the ground truth is a neural net. How can we model the ground truth?
<ul>
<li>As a set on the boolean cube <span class="math inline">\(\{0,1\}^n\)</span> or <span class="math inline">\([0,1]^n\)</span>. Model 1: Ising model on the Boolean cube.
<ul>
<li>Understand such a distribution qualitatively. Ex. what is the average noise sensitivity? Fourier spectra? (What’s the boundary look like?) Are these questions answered for random k-SAT? Might other models (ex. DNF’s, CNF’s) be better?</li>
<li>Another layer: There is a probability distribution over inputs as well (you don’t sample uniformly from the whole boolean cube. Makes sense because of next point).</li>
<li>Note that the set of pictures that make sense is probably very small.</li>
<li>What if you want the learned function to be noise stable? What is the noise stability of learned functions? Are threshold circuits noise stable?</li>
</ul></li>
<li>How does this compare to known algorithms for learning boolean functions with low weight? What additional structure do we need to assume? (Low degree algorithm. Lower bounds (min sample complexity, by info theory) for learning b/c of number of functions.)</li>
</ul></li>
<li>“It doesn’t make sense to assume the ground truth is a neural net,” so the approach above is to consider a class of functions and show the class can be learned by neural nets. Is there reason to think of ground truth as a neural net?</li>
<li>Explain me:
<ul>
<li>adversarial phenomena</li>
<li>the distribution of local minima follows laws from statistical physics. Getting to global optima can actually hurt generalization.</li>
</ul></li>
</ul>
<h2 id="people-to-talk-to">People to talk to</h2>
<ul>
<li>ML: Tengyu, Elad, Arora</li>
<li>Zeev: threshold circuits, what’s known. (See Kane’s paper?)</li>
<li>Aizenmann, Abbe: intuition for Ising model over <span class="math inline">\(B^n\)</span>, or other models that make sense (ex. 3SAT).</li>
<li>Fefferman, analysis/geometry students: approximation theory</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>SDP duality</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/sdp-duality.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/sdp-duality.html</id>
    <published>2016-03-04T00:00:00Z</published>
    <updated>2016-03-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>SDP duality</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-04 
          , Modified: 2016-03-04 
	</p>
      
       <p>Tags: <a href="/tags/SDP.html">SDP</a>, <a href="/tags/duality.html">duality</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <p><a href="duality.html">Duality</a></p>
<h2 id="summary">Summary</h2>
<p>The following SDP’s are dual: <span class="math display">\[
\max_{X\succeq 0,\an{A_i,X}=b_i}\an{C,X} \lra \min_{\nu, \sum \nu_iA_i\succeq C} \nu^Tb.
\]</span> (When are they equal? When there is a strictly feasible point.)</p>
<h2 id="derivation">Derivation</h2>
<ol type="1">
<li>First write as a convex program. Replace the condition <span class="math inline">\(X\succeq 0\)</span> by <span class="math display">\[
 \la_{\min}(X)=\min_{\Tr(A)=1, A\succeq 0}\an{A,X}\ge 0.
 \]</span> (Use (2) in <a href="duality.html">duality</a>.)</li>
<li>Simplify, using minimax as a key step.
\begin{align}
 g(\la,\nu) &amp;= \max_X(\an{C,X} + \la \min_{\Tr(A)=1, A\succeq 0}\an{A,X} - \sum \nu_i (\an{A_i,X}-b_i)\\
 &amp;=\max_X\min_{\Tr(A)=1}\an{\ub{C-\sum \nu_iA_i}{Z}+\la A,X} + \nu^Tb\\
 &amp;=\max_X\min_{\Tr(A)=\la}\an{Z + A,X} + \nu^Tb\\
 &amp;= \min_{\Tr(A)=\la}\max_X \an{Z+A,X}\\
 &amp;=\pa{\begin{cases}
 0,&amp;\Tr(Z)=-\la, Z\preceq 0\\
 \iy,&amp;\text{else.}
 \end{cases}}-\nu^Tb
 \end{align}
using minimax, then noting the inside <span class="math inline">\(\max\)</span> forces the condition <span class="math inline">\(Z=-A\)</span>.</li>
<li>Taking <span class="math inline">\(\min\)</span> of this, <span class="math inline">\(Z\preceq 0\)</span> turns into a constraint.</li>
</ol>
<p>See <a href="http://www.eecs.berkeley.edu/~wainwrig/ee227a/SDP_Duality.pdf">notes</a>.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Duality</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/duality.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/duality.html</id>
    <published>2016-03-04T00:00:00Z</published>
    <updated>2016-03-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Duality</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-04 
          , Modified: 2016-03-04 
	</p>
      
       <p>Tags: <a href="/tags/duality.html">duality</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#summary">Summary</a><ul>
 <li><a href="#duality">Duality</a></li>
 <li><a href="#kkt-conditions">KKT conditions</a></li>
 <li><a href="#examples">Examples</a><ul>
 <li><a href="#dual-functions">Dual functions</a></li>
 </ul></li>
 </ul></li>
 <li><a href="#proofs-and-intuition">Proofs and intuition</a><ul>
 <li><a href="#weak-duality">Weak duality</a></li>
 <li><a href="#strong-duality">Strong duality</a></li>
 <li><a href="#intuition">Intuition</a></li>
 </ul></li>
 <li><a href="#questions">Questions</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="summary">Summary</h2>
<h3 id="duality">Duality</h3>
These problems are dual.
\begin{align}
\min_{f_i\le 0, Ax=b} f &amp; \lra \max_{\la \ge 0,\nu} \ub{\min_x \ub{f+\la^T\vec{f} + \nu^T (Ax-b)}{\cal L(x,\la,\nu)}}{g(\la,\nu)}\\
\max_{f_i\ge 0, Ax=b} f &amp; \lra \min_{\la \ge 0,\nu} \ub{\max_x \ub{f+\la^T\vec{f} - \nu^T (Ax-b)}{\cal L(x,\la,\nu)}}{g(\la,\nu)}
\end{align}
<p>In (1) the <span class="math inline">\(f_i\)</span> are convex; in (2) they are concave. In (1), we have dual<span class="math inline">\(\le\)</span>primal: <span class="math display">\[
\min_{f_i\le 0, Ax=b} f \ge \max_{\la \ge 0,\nu} \ub{\min_x \ub{f+\la^T\vec{f} + \nu^T (Ax-b)}{\cal L(x,\la,\nu)}}{g(\la,\nu)}
\]</span> Note: <span class="math inline">\(f^*(y)=\sup y^T - f(x)\)</span> is the conjugate or <strong>Legendre transform</strong>. It is convex, and for convex functions, the double conjugate is the original function.</p>
<p><strong>Slater’s constraints</strong>: Equality holds if the problem is strictly feasible: there exists <span class="math inline">\(x\)</span> such that <span class="math inline">\(f_i(x)&lt;0, Ax=b\)</span>. Linear inequalities are allowed to be non-strict, <span class="math inline">\(f_i(x)\le 0\)</span>.</p>
<h3 id="kkt-conditions">KKT conditions</h3>
<p>The KKT conditions are (here <span class="math inline">\(h=Ax-b\)</span>)</p>
<ol type="1">
<li>(derivative 0) <span class="math inline">\(\nb f+\sum \la_i \nb f_i + \nu^TA=0\)</span>.</li>
<li>(constraints satisfied) <span class="math inline">\(\la \ge 0, f_i\le 0, h_i=0\)</span>.</li>
<li>(complementary slackness) <span class="math inline">\(\la f_i=0\)</span>.</li>
</ol>
<p>Interpretation:</p>
<ul>
<li>If <span class="math inline">\(x,\la,\nu\)</span> satisfy the conditions, then <span class="math inline">\(x\)</span> and <span class="math inline">\((\la,\nu)\)</span> are primal and dual optimal and the optimal values are equal.</li>
<li>if Slater’s condition is satisfied, <span class="math inline">\(x\)</span> is optimal if and only if there exist, <span class="math inline">\(\la,\nu\)</span> that satisfy KKT conditions</li>
</ul>
<h3 id="examples">Examples</h3>
<ol type="1">
<li>Linear programming (equality when there exists a feasible solution)
\begin{align}
\min_{Ax=b,x\ge 0} c^Tx &amp;= \max_{\la\ge 0, A^T\nu\ge -c} -b^T\nu\\
\min_{Ax\le b} c^Tx &amp;= \max_{\la\ge 0, A^T\nu=-c} -b^T\nu.
\end{align}</li>
</ol>
<h4 id="dual-functions">Dual functions</h4>
<p>(Check this.)</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Function</th>
<th style="text-align: left;">Dual</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(u\ln u\)</span></td>
<td style="text-align: left;"><span class="math inline">\(e^{v-1}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\ve{x}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\begin{cases}0,&amp;\ve{y}_*\le 1\\ \iy,&amp;\text{else}\end{cases}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\ln \det(X^{-1})\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\ln \det(-Y)^{-1}-n\)</span></td>
</tr>
</tbody>
</table>
<h2 id="proofs-and-intuition">Proofs and intuition</h2>
<h3 id="weak-duality">Weak duality</h3>
<p>Given a dual feasible <span class="math inline">\(\la,\nu\)</span>, choose <span class="math inline">\(x=x^*\)</span>. Then <span class="math display">\[
g(\la,\nu) = f(x^*) + \sum \la_i \ub{f_i(x^*)}_{\le 0} + \nu^T \ub{(Ax^*-b)}{=0}\le f(x^*).
\]</span></p>
<h3 id="strong-duality">Strong duality</h3>
<h3 id="intuition">Intuition</h3>
<p>Think of the dual as (1) relaxing the strict conditions that <span class="math inline">\(f_i\le 0\)</span> and (2) being a game. If you fail to meet the constraint <span class="math inline">\(f_i\le 0\)</span>, you are penalized by cost <span class="math inline">\(\la_if_i\)</span> (and you gain money from constraints that are met). The adversary first sets “shadow prices” <span class="math inline">\(\la_i,\nu_i\)</span>, the cost for violating the constraints, and then you choose <span class="math inline">\(x^*\)</span>. Duality says you can’t do better in this game. (You can do just as well by choosing your original <span class="math inline">\(x^*\)</span>; this is weak duality.</p>
Let <span class="math inline">\(p^*(u,v)\)</span> be the optima under constraints <span class="math inline">\(f_i\le u_i,h_i=v_i\)</span>. The sensitivity to the constraints are given by the dual optima. We have the following (the derivative equations hold if <span class="math inline">\(p^*\)</span> is differentiable.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>
\begin{align}
p^*(u,v) &amp;\ge p^*(0,0)-\la^{*T} u - \nu^{*T} v\\
\pd{p^*}{v_i} &amp;= -\nu_i\\
\pd{p^*}{u_i} &amp;= -\la_i.
\end{align}
<h2 id="questions">Questions</h2>
<p>If <span class="math inline">\(x\)</span> is a primal optimal, is there necessarily a <span class="math inline">\((\la,\nu)\)</span> that satisfies the equations? I think if <span class="math inline">\(x\)</span> is primal optimal and <span class="math inline">\((\la,\nu)\)</span> is dual optimal, <span class="math inline">\((x,\la,\nu)\)</span> does not necessarily satisfy. Check this.</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Otherwise maybe an inequality? Multiple dual optima are possible.<a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Convex optimization</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/convex_optimization.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/convex_optimization.html</id>
    <published>2016-03-04T00:00:00Z</published>
    <updated>2016-03-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Convex optimization</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-04 
          , Modified: 2016-03-04 
	</p>
      
       <p>Tags: <a href="/tags/convex%20optimization.html">convex optimization</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p>Notes:</p>
<ul>
<li><a href="duality.html">Duality</a></li>
<li></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Gradient descent</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/GD.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/GD.html</id>
    <published>2016-03-04T00:00:00Z</published>
    <updated>2016-03-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Gradient descent</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-04 
          , Modified: 2016-03-04 
	</p>
      
       <p>Tags: <a href="/tags/gradient.html">gradient</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <p>(See 10/15 notebook for detailed notes.)</p>
<h2 id="summary">Summary</h2>
<table style="width:25%;">
<colgroup>
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Algorithm</th>
<th style="text-align: left;">General</th>
<th style="text-align: left;"><span class="math inline">\(\al\)</span>-strongly convex</th>
<th style="text-align: left;"><span class="math inline">\(\be\)</span>-smooth</th>
<th style="text-align: left;"><span class="math inline">\(\ga\)</span>-convex</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Gradient descent</td>
<td style="text-align: left;"><span class="math inline">\(\rc{\sqrt{T}}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\rc{\al T}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\fc{\be}T\)</span></td>
<td style="text-align: left;"><span class="math inline">\(e^{-\ga T}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Accelerated gradient descent</td>
<td style="text-align: left;"><span class="math inline">\(\fc{d}{T}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\rc{\al T^2}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\fc{\be}{T^2}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(e^{-\sqrt{\ga} T}\)</span></td>
</tr>
</tbody>
</table>
<h3 id="gradient-descent-main-points">Gradient descent main points</h3>
<ul>
<li>What is the general framework?
<ol type="1">
<li>Pick a descent direction <span class="math inline">\(\De x\)</span>.</li>
<li>Choose a step size <span class="math inline">\(\tau&gt;0\)</span>: <span class="math display">\[x^{(t+1)} \leftarrow x+\tau \De x.\]</span></li>
<li>Continue until stop criterion.</li>
</ol></li>
<li>What is vanilla (one shot) gradient descent?
<ul>
<li>Gradient descent lemma: Suppose <span class="math inline">\(f\)</span> is convex and <span class="math inline">\(L\)</span>-smooth, <span class="math inline">\(\ve{\nb f(x)-\nb f(y)}\le L\ve{x-y}\)</span>.
\begin{align}
 x':&amp;=x-\rc L \nb f(x)\\
 \implies f(x')&amp;\le f(x) - \rc{2L}\ve{\nb f(x)}^2.
 \end{align}</li>
<li>There’s no guarantee on smoothness unless we assume <span class="math inline">\(f\)</span> is <span class="math inline">\(l\)</span>-strongly convex, <span class="math inline">\(\ve{\nb f(x)-\nb f(y)}\le l\ve{x-y}\)</span>. Let <span class="math inline">\(\ka=\fc{L}{l}, t = \fc{2}{L+l}\)</span>.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> Then linear convergence holds:
\begin{align}
 \ve{x_{k+1}-x^*} \le \pf{\ka-1}{\ka+1} \ve{x_k-x^*}
 \end{align}</li>
</ul></li>
<li>What is gradient descent with backtracking?
<ul>
<li>Parameters <span class="math inline">\(\al\in (0,0.5)\)</span>, step size, <span class="math inline">\(\be\in (0,1)\)</span> scaling factor.</li>
<li>Choose <span class="math inline">\(\De x=\nb f(x)\)</span>.</li>
<li>Choose <span class="math inline">\(\tau\)</span> by backtracking: Set <span class="math inline">\(t=1\)</span>. While <span class="math inline">\(f(x+t\De x)&gt;f(x)+\al \nb f^T\De x\)</span>, <span class="math display">\[\De x\leftarrow \al \De x.\]</span></li>
<li>Lemma: suppose <span class="math inline">\(mI \preceq \nb^2 f \preceq MI\)</span>. Then <span class="math display">\[\fc{f(x_t)-f(p^*)}{f(x_{t-1})-f(p^*)}\le 1-\min\bc{\fc{2\al \be m}{M}, 2m\al}.\]</span></li>
</ul></li>
<li></li>
</ul>
<h2 id="proofs">Proofs</h2>
<p>Gradient descent lemma: Let <span class="math inline">\(D=\nb f(x)\)</span>. Move to origin. Upper bound is <span class="math display">\[f(x) \le \fc{L}2 x^2 + Dx.\]</span> The minimum is at <span class="math inline">\(-\fc{D}{L}\)</span> and is <span class="math inline">\(\fc{-b^2}{4a} = -\fc{D^2}{2L}.\)</span></p>
<p>For strongly convex: Choose <span class="math inline">\(s\)</span> to maximize the minimum progress in terms of <span class="math inline">\(x\)</span>. <span class="math display">\[\fc{s-\rc{l}}{\rc{l}} = \fc{\rc L-s}{\rc L} \implies s = \fc{2}{L+l}.\]</span></p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Think of this as the harmonic average of how much to move to get to the minima of the upper and lower-bounding quadratics.<a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[GRSY15] How Hard is Inference for Structured Prediction?</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/community/GRSY15.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/community/GRSY15.html</id>
    <published>2016-03-03T00:00:00Z</published>
    <updated>2016-03-03T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[GRSY15] How Hard is Inference for Structured Prediction?</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-03 
          , Modified: 2016-03-03 
	</p>
      
       <p>Tags: <a href="/tags/paper.html">paper</a>, <a href="/tags/CBM.html">CBM</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <p>Model: In a <span class="math inline">\(n\times n\)</span> grid, observe <span class="math inline">\(p\)</span>-noisy edges <span class="math inline">\(Y_uY_v\)</span> and <span class="math inline">\(q\)</span>-noisy nodes <span class="math inline">\(Y_u\)</span>. Attempto to recover the original labeling. What is the maximum correlation you can achieve on average?</p>
<p>Answer: Error <span class="math inline">\(p^2n\)</span> up to a constant.</p>
<ul>
<li>Achievable: find the partition that maximizes the cut value <span class="math inline">\(\sum X_{uv}Y_uY_v\)</span>. Now negate all labelings if it would increase the node agreement.
<ul>
<li>Claim: this is polynomial time. How? This is max cut with negative weights. (https://en.wikipedia.org/wiki/Maximum_cut, http://cstheory.stackexchange.com/questions/2312/max-cut-with-negative-weight-edges, http://cstheory.stackexchange.com/questions/9323/hardness-of-max-cut-on-sparse-graphs)</li>
<li>They mention an LP relaxation, but don’t actually give the formula…</li>
</ul></li>
<li>Tight: consider the checkerboard graph.</li>
</ul>
<p>Applications: Image segmentation. (How would you have both node and edge measurements?)</p>
<p>More directions:</p>
<ul>
<li>Optimal constant?</li>
<li>If you can pay to get better observations, how would you spend a budget?</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>LDCs</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/coding/ldc.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/coding/ldc.html</id>
    <published>2016-03-03T00:00:00Z</published>
    <updated>2016-03-03T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>LDCs</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-03 
          , Modified: 2016-03-03 
	</p>
      
       <p>Tags: <a href="/tags/LDC.html">LDC</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#talk-with-zeev">Talk with Zeev</a></li>
 <li><a href="#low-weight-ldcs-to-ldcs">Low-weight LDCs to LDCs</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="talk-with-zeev">Talk with Zeev</h2>
<p>Consider a partition of the complete bipartite graph into matchings. What is the worst partition, in the sense that you have to take the union of many (<span class="math inline">\(\Om(\ln n)\)</span>) matchings before you get an expander in the sense of the Expander Mixing Lemma?</p>
<p>Now consider if the bipartite graph represents a Cayley graph (<span class="math inline">\(x\)</span> on the left is connected to <span class="math inline">\(gx\)</span> on the right). “Abelian groups are the worst expanders.”</p>
<p>If you need <span class="math inline">\(\ge d\)</span> matchings, you get a LDC from <span class="math inline">\(d\)</span> to <span class="math inline">\(n\)</span>.</p>
<p>Consider tripartite hypergraphs, with <span class="math inline">\(n^3\)</span> matchings partitioned into <span class="math inline">\(n^2\)</span>. Is the worst partition take <span class="math inline">\((\ln n)^2\)</span>?</p>
<h2 id="low-weight-ldcs-to-ldcs">Low-weight LDCs to LDCs</h2>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Perron-Frobenius Theorem</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/algebra/linear/perron-frobenius.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/algebra/linear/perron-frobenius.html</id>
    <published>2016-03-03T00:00:00Z</published>
    <updated>2016-03-03T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Perron-Frobenius Theorem</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-03 
          , Modified: 2016-03-03 
	</p>
      
       <p>Tags: <a href="/tags/eigenvalue.html">eigenvalue</a>, <a href="/tags/theorem.html">theorem</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <h2 id="statement">Statement</h2>
<p>Let <span class="math inline">\(A\)</span> be a matrix with (strictly) positive entries. Then the eigenvalue with maximal absolute value is positive and simple. The corresponding eigenvector is (strictly) positive.</p>
<p><a href="https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem">Wikipedia</a></p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
