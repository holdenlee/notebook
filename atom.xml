<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2016-10-25T00:00:00Z</updated>
    <entry>
    <title>RL references</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/rl_refs.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/rl_refs.html</id>
    <published>2016-10-25T00:00:00Z</published>
    <updated>2016-10-25T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>RL references</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-25 
          , Modified: 2016-10-25 
	</p>
      
       <p>Tags: <a href="/tags/reinforcement%20learning.html">reinforcement learning</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#online">Online</a></li>
 <li><a href="#books">Books</a></li>
 <li><a href="#papers">Papers</a><ul>
 <li><a href="#theoretical-frameworks-and-results">Theoretical frameworks and results</a></li>
 <li><a href="#mdps">MDPs</a><ul>
 <li><a href="#convergence-of-classic-algorithms">Convergence of classic algorithms</a></li>
 <li><a href="#theory-algorithms">Theory algorithms</a></li>
 </ul></li>
 <li><a href="#factored-mdps-mdps-with-exponential-state-space">Factored MDPs, MDPs with exponential state space</a></li>
 <li><a href="#pomdps">POMDPs</a></li>
 <li><a href="#open-questions">Open questions</a></li>
 <li><a href="#surveys">Surveys</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="online">Online</h2>
<ul>
<li><a href="https://github.com/andrewliao11/Deep-Reinforcement-Learning-Survey/blob/master/Reinforcement-Learning-Papers.md">Deep RL</a></li>
<li>ICML presentation (David Silver)</li>
<li><a href="http://castlelab.princeton.edu/">CASTLE Labs</a>
<ul>
<li><a href="http://optimallearning.princeton.edu/">Optimal learning</a></li>
<li><a href="http://adp.princeton.edu/">Approximate dynamic programming</a>
<ul>
<li><a href="http://adp.princeton.edu/adpIntros.htm">Intros</a></li>
</ul></li>
<li><a href="http://castlelab.princeton.edu/jungle.htm#unifiedframework">Unified framework</a></li>
</ul></li>
</ul>
<h2 id="books">Books</h2>
<p><a href="https://www.quora.com/What-are-the-best-books-about-reinforcement-learning">Quora recommendations</a></p>
<ul>
<li>(*) Sutto Barton. <a href="rl.html"><strong>Notes</strong></a></li>
<li><a href="https://books.google.com/books?id=VvBjBAAAQBAJ&amp;printsec=frontcover&amp;dq=continuous+markov+decision+processes&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjo3OLywOnPAhVHWD4KHXzgDWUQ6AEIKTAC#v=onepage&amp;q=continuous%20markov%20decision%20processes&amp;f=false">Puterman14</a></li>
<li><a href="http://site.ebrary.com/lib/princeton/reader.action?docID=10501323">Approximate DP, Powell</a></li>
<li><a href="http://site.ebrary.com/lib/princeton/detail.action?docID=10560566">Optimal learning, Powell</a></li>
<li><a href="http://www.crcnetbase.com/isbn/9781439821091">Function approximators, Lucian Busoniu, Robert Babuska, Bart De Schutter, and Damien Ernst</a></li>
<li><a href="http://web.mit.edu/dimitrib/www/dpchapter.pdf">ADP chapter, Bertsekas</a></li>
<li><a href="https://books.google.com/books?id=-6RiQgAACAAJ&amp;dq=Dynamic+Programming:+Deterministic+and+Stochastic+Models&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjc0pfAyefPAhUGFz4KHaVIDecQ6AEIHjAA">Bertsekas87</a></li>
</ul>
<h2 id="papers">Papers</h2>
<h3 id="theoretical-frameworks-and-results">Theoretical frameworks and results</h3>
<ul>
<li>(*) [KMN02] A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes <a href="http://download.springer.com/static/pdf/530/art%253A10.1023%252FA%253A1017932429737.pdf?originUrl=http%3A%2F%2Flink.springer.com%2Farticle%2F10.1023%2FA%3A1017932429737&amp;token2=exp=1477079019~acl=%2Fstatic%2Fpdf%2F530%2Fart%25253A10.1023%25252FA%25253A1017932429737.pdf%3ForiginUrl%3Dhttp%253A%252F%252Flink.springer.com%252Farticle%252F10.1023%252FA%253A1017932429737*~hmac=6c901205464aff209a8d3ca5ba481b36b72959a0d61fc762dfc512f12c01a38c">paper</a>
<ul>
<li>PAC formulation</li>
</ul></li>
<li>[AAKMR02] A Note on the Representational Incompatibility of Function Approximation and Factored Dynamics.pdf
<ul>
<li>Barrier to solving factored MDP’s is not just computational ([PT87]), it is representational (there is no succinct policy)</li>
<li>DBN-MDP (factored MDP): transition law <span class="math inline">\(\de\)</span> is dynamic Bayes net. The first layer are the variables (and action) at time <span class="math inline">\(t\)</span>, the second layer are the variables at time <span class="math inline">\(t+1\)</span>, the graph is directed, the indegree of each second-layer node is at most constant.</li>
<li>Rewards are linear.</li>
<li>Connection with AM-games: V’s state corresponds to state, P implements policy.</li>
<li>If PSPACE is not contained in P/POLY, then there is a family of DBN-MDPs, such that for any two polynomials <span class="math inline">\(s,a\)</span>, there exist infinitely many <span class="math inline">\(n\)</span> such that no circuit <span class="math inline">\(C\)</span> of size <span class="math inline">\(s(n)\)</span> can compute a policy having expected reward greater than <span class="math inline">\(\rc{a(n)}\)</span> times the optimum.</li>
<li>(This is the policy optimization part. Can you learn Bayes nets? <span class="citation" data-cites="Andrej">@Andrej</span>)</li>
<li>(Note that the “drifting context vector (RANDWALK)” model can be represented by a model with <span class="math inline">\(1\to 1', 2\to 2',\ldots\)</span>.)</li>
<li>What if you only compared to the best policy in a class of policies? (cf. EXP4)</li>
</ul></li>
</ul>
<h3 id="mdps">MDPs</h3>
<h4 id="convergence-of-classic-algorithms">Convergence of classic algorithms</h4>
<ul>
<li>[PB79] On the convergence of policy iteration in stationary dynamic programming.pdf
<ul>
<li>Equivalent to Newton-Kantorovich iteration procedure applied to functional equation of dynamic programming.</li>
<li>Sufficient conditions for superlinear or quadratic convergence. See [Howard].</li>
<li>Note: Does NOT apply to finite state MDPs! (Problem being that “best action” is not continuous in parameters?)</li>
</ul></li>
<li>[SR04] Convergence properties of policy iteration.pdf
<ul>
<li>Compare to method of successive approximations. SA is bad when <span class="math inline">\(\ga\approx 1\)</span>.</li>
</ul></li>
<li>[TVR96] An Analysis of Temporal-Difference Learning with Function Approximation <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.92&amp;rep=rep1&amp;type=pdf">paper</a>
<ul>
<li><span class="math inline">\(TD(\la)\)</span> convergence</li>
<li>What is rate??</li>
</ul></li>
<li>[B95] Residual Algorithms: Reinforcement learning with function approximation <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=akijBQAAQBAJ&amp;oi=fnd&amp;pg=PA30&amp;dq=baird+residual+algorithms+function+approximation&amp;ots=MJ_Hs5vMBs&amp;sig=RTG7KgQgM4GWWIwSvg1wOYRmJDc#v=onepage&amp;q=baird%20residual%20algorithms%20function%20approximation&amp;f=false">paper</a>
<ul>
<li>Q-learning instability</li>
</ul></li>
<li>[WD92] Q-learning
<ul>
<li>Given bounded rewards, learning rates <span class="math inline">\(0\le \al_n&lt;1\)</span>, and <span class="math inline">\(\sumo i{\iy} \al_{n^i(x,a)}=\iy\)</span> (<span class="math inline">\(n^i(x,a)\)</span> is the <span class="math inline">\(i\)</span>th time <span class="math inline">\(a\)</span> is tried in state <span class="math inline">\(x\)</span>) then <span class="math inline">\(Q_n\to Q^*\)</span> wp 1.</li>
<li>Doesn’t address: under what <span class="math inline">\(Q\)</span>? What if updating policy at same time? What’s regret?</li>
</ul></li>
</ul>
<h4 id="theory-algorithms">Theory algorithms</h4>
<ul>
<li>(*) [AO06] UCRL
<ul>
<li>Maintain confidence bounds on rewards and transition probabilities.</li>
<li>Only apply to unichain MDP’s (where fixing an action, any other state is reachable - I think this is unrealistic. This is unreasonable - NO, WRONG DEFINITION).</li>
<li>Right definition in KS02: the stationary distribution of any policy does not depend on the start state (this is to make things easier, can do without)</li>
<li>Other work: adversarial reward, index policies (choose action with max return in confidence region)</li>
</ul></li>
<li>[LH12] PAC bounds for discounted MDPs <a href="https://arxiv.org/pdf/1202.3890.pdf">paper</a>
<ul>
<li>UCRL, under assumption of 2 possible next-states for each state/action pair, PAC bound of <span class="math inline">\(\wt O \pa{\fc{|S\times A|}{\ep^2(1-\ga)^3}\ln \prc{\de}}\)</span>.</li>
</ul></li>
<li>[KS02] Near-optimal reinforcement learning in polynomial time</li>
<li>(*) [JOA10] Near-optimal regret bounds for reinforcement learning <a href="http://www.jmlr.org/papers/volume11/jaksch10a/jaksch10a.pdf">paper</a>
<ul>
<li>Improved UCRL (UCRL2)</li>
</ul></li>
<li>[KN09] Near-Bayesian Exploration in Polynomial Time <a href="http://www.zicokolter.com/wp-content/uploads/2015/10/kolter-icml09a-full.pdf">paper</a></li>
<li>(*) [KAL16] Contextual-MDP, which is contextual bandits + RL.
<ul>
<li>Regret wrt policy class.</li>
<li>Poly in parameters, log in number of policies, independent of size of observation space. <!--what does no dependence on numspace can represent exact-best solution, state transition dynamics are deterministic.--></li>
<li>Unlike POMDP, optimal policy is memoryless.</li>
<li>Warning: inefficient b/c requires enumeration of policy class. (? does this contradict the poly/log time above)</li>
</ul></li>
<li>[DPWR15] Bayesian Nonparametric Methods for Partially-Observable Reinforcement Learning <a href="http://dspace.mit.edu/handle/1721.1/97034">paper</a>
<ul>
<li>Bayes!</li>
</ul></li>
</ul>
<h3 id="factored-mdps-mdps-with-exponential-state-space">Factored MDPs, MDPs with exponential state space</h3>
<ul>
<li>[HSMM15] Off-policy Model-based Learning under Unknown Factored Dynamics.pdf
<ul>
<li>Under 3 assumptions, using a greedy approach to finding parents, estimate the transition function (parameters to Bayes net) (compre with prob models literature?)</li>
<li>This is for off-policy evaluation; it doesn’t tell us how to find the optimal policy.</li>
<li>(Is the model learning and policy evaluation coupled or not?)</li>
<li>(It seems to be learning the Bayes net rather than evaluating <span class="math inline">\(\pi\)</span>. Ah, once you learn the Bayes net then you can evaluate just by sampling.)</li>
<li>The difference from simpling learning a Bayes net is that the samples aren’t independent—they were from following a certain policy. Assumptions will ensure that you can still learn the model even if you only have samples from that policy.</li>
</ul></li>
</ul>
<h3 id="pomdps">POMDPs</h3>
<ul>
<li>(*) [ALA16] Reinforcement Learning of POMDPs using Spectral Methods
<ul>
<li>Spectral parameter estimation for POMDP’s</li>
<li>Combine with UCRL (exploration-exploitation framework) to get regret bounds (compared to memoryless policies) optimal in dependence on <span class="math inline">\(N\)</span> (<span class="math inline">\(O(\sqrt N)\)</span>)</li>
<li>Challenges
<ul>
<li>Unlike HMM, consecutive observations not conditionally independent</li>
<li>Technical: Concentration inequalities for dependent rv’s. Extend to marix value functions.</li>
</ul></li>
<li>Previous/other work
<ul>
<li>UCRL</li>
<li>model-free algorithms (<span class="math inline">\(Q\)</span>-learning)</li>
<li>policy search methods</li>
<li>separate exploration and exploitation collect examples, then estimate parameters [Guo16]. PAC in RL POMDP?</li>
</ul></li>
<li>Open: analyze UCRL for finite horizon.</li>
<li>Stochastic policies are near-optimal in many domains (?). NP-hard to optimize but under some conditions can approximate</li>
</ul></li>
<li>[ALA16] Open Problem - Approximate Planning of POMDPs in the class of Memoryless Policies (COLT2016)
<ul>
<li>Find exact or approximate optimal stochastic memoryless policy for POMDP.</li>
<li>What [ALA16] don’t address in other paper: planning. (Complexity considerations? i.e. is this tractable? Kaelbling98)</li>
<li>In their paper they assume access to an optimization oracle that gives best memoryless planning policy at end of each episode. - No algorithm for this right now! <!--SoS? First check if you can reduce from Nash equilibrium, etc.--></li>
</ul></li>
<li>[GDB16] A PAC RL algorithm for episodic POMDPs <a href="http://www.jmlr.org/proceedings/papers/v51/guo16b.pdf">paper</a>
<ul>
<li>PAC: whp, selects near-optial action on all but a number of steps poly in problem paramters (what is the definition?)</li>
<li>PAC learns in time <span class="math inline">\(T(\ep)\)</span> means: achieves an expected episodic reward of <span class="math inline">\(V\ge V^*-\ep\)</span> on all but <span class="math inline">\(T(\ep)\)</span> episodes.</li>
<li>First PAC POMDP RL algorithm for episodic domains</li>
<li>EEPORL
<ul>
<li>Algorithm 1:
<ul>
<li>In each episode, take first four steps randomly (in correlated fashion) to explore. Need to assume that have probability of being anywhere in 2 steps.</li>
<li>Take chosen policy for the rest of the steps.</li>
</ul></li>
<li>Algorithm 2: Update estimates for POMDP parameters.</li>
<li>Algorithm 3: Find best policy for current estimates of parameters.</li>
</ul></li>
</ul></li>
</ul>
<h3 id="open-questions">Open questions</h3>
<ul>
<li>[S99] Open Theoretical Questions in Reinforcement Learning (not sure how open these are anymore!)
<ul>
<li>Control with function approximation
<ul>
<li>TD(<span class="math inline">\(\la\)</span>) understood [TsVR97]</li>
<li>Q-learning unstable [Baird95]</li>
<li>Sarsa ??? (tends to oscillate close to best)</li>
</ul></li>
<li>MC ES convergence (see update in BS?)</li>
<li>Bootstrapping more efficient than MC?</li>
<li>VC dimension over RL
<ul>
<li>Difficulty: different actions lead to different parts of space, so we don’t have a natural “test set” that can be reused to evaluate different policies (Test set seems like it would be drawn from different policies?)</li>
<li>Proposal: trajectory trees: tree of all sample transitions</li>
<li>Extend PAC to this setting.</li>
</ul></li>
</ul></li>
</ul>
<h3 id="surveys">Surveys</h3>
<ul>
<li>[G] Reinforcement learning - a Tutorial Survey and Recent Advances.pdf</li>
<li>[KLM96] Reinforcement Learning - A Survey.pdf</li>
<li>(*) [P14] Clearing the Jungle of Stochastic Optimization
<ul>
<li>4 classes of policies</li>
<li>Dynamic vs. stochastic programs</li>
</ul></li>
<li>[P14] Energy and Uncertainty - models and algorithms for complex energy systems.pdf</li>
<li>(*) [P16] A Unified Framework for Optimization under Uncertainty.pdf</li>
<li><a href="http://people.csail.mit.edu/agf/Files/13FTML-RLTutorial.pdf">lin function approximators</a></li>
<li><a href="https://hal.archives-ouvertes.fr/hal-00747575v5/document">optimistic principle</a></li>
<li><a href="http://www.morganclaypool.com/doi/abs/10.2200/S00268ED1V01Y201005AIM009">Algorithms for RL</a></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Reinforcement learning convergence</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/rl_convergence.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/rl_convergence.html</id>
    <published>2016-10-24T00:00:00Z</published>
    <updated>2016-10-24T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Reinforcement learning convergence</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-24 
          , Modified: 2016-10-24 
	</p>
      
       <p>Tags: <a href="/tags/reinforcement%20learning.html">reinforcement learning</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#policy-estimation">Policy estimation</a></li>
 <li><a href="#policyvalue-iteration">Policy/value iteration</a><ul>
 <li><a href="#changing-ga">Changing <span class="math inline">$\ga$</span></a></li>
 <li><a href="#alternating-policy-improvementevaluation">Alternating policy improvement/evaluation</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <p>Here we’re interested in convergence guarantees for algorithms/methods used in practice. (Rather than, e.g., coming up with provable polytime theoretical algorithms that work but are too slow to be used in practice.)</p>
<p>Recall: Mixing time <span class="math inline">\(\tau_\ep = \min\set{t}{\max_{P_0} \ve{P_t - P_{\text{eq}}}\le \ep} \le \fc{\ln \pf{N}{\ep}}{\de}\)</span>, <span class="math inline">\(\de=1-\max_{k\ge 2}|\la_k|\)</span>.</p>
<h2 id="policy-estimation">Policy estimation</h2>
Policy satisfies
\begin{align}
v_\pi  &amp;= d(P_\pi^T R_\pi) + \ga P_\pi v\\
\iff 
v_\pi &amp;= (I-\ga P_\pi)^{-1} d(P_\pi^T R_\pi).
\end{align}
Then
\begin{align}
v_{k+1} &amp;= d(P_\pi^T R_\pi) + \ga P_\pi v_k\\
v_{k+1} - v_\pi &amp;= \ga P_\pi (v_k-v_\pi).
\end{align}
<p>So convergence happens at rate of <span class="math inline">\(\ga \ve{P_\pi}_2\)</span> to <span class="math inline">\(v_\pi\)</span>.</p>
<p>For nondiscounted case, we get <span class="math display">\[
v_t = \rc t[d + P_\pi d+ \cdots + P_\pi^{t-1} d].
\]</span> We find (are stochastic matrices diagonalizable?) <span class="math inline">\(v\)</span> is the projection of <span class="math inline">\(d\)</span> onto space of 1-eigenvectors.</p>
<p>(Notation is easier if the reward only depends on <span class="math inline">\(s,a\)</span>; then we just get <span class="math inline">\(v_\pi = r_\pi + \ga P_\pi v_\pi\)</span>.)</p>
<h2 id="policyvalue-iteration">Policy/value iteration</h2>
<p>Why does it improve? Let</p>
<ul>
<li><span class="math inline">\(P_{\pi, s',s} = \Pj(s'|s,\pi(s))\)</span></li>
<li><span class="math inline">\(R_{\pi, s',s} = r(s,\pi(s),s')\)</span></li>
<li><span class="math inline">\(q_{\pi,\pi'} = q_\pi(s,\pi'(s))\)</span>.</li>
</ul>
Write the Bellman equation as (<span class="math inline">\(d(A)\)</span> is the diagonal of <span class="math inline">\(A\)</span>)
\begin{align}
v_\pi &amp;= d(P_\pi^TR) + \ga P_\pi^T v_\pi\\
q_{\pi,\pi'} &amp;= d(P_{\pi'}^TR_{\pi'}) + \ga P_{\pi'}^T v_\pi\\
v_\pi &amp;=(I-\ga P_\pi^T)^{-1} d(P_{\pi'}^T, R_{\pi'})\\
q_{\pi,\pi'} &amp;= (1-\ga P_{\pi'}^T)^{-1} d(P_{\pi'}^TR_{\pi'})
\end{align}
We have
\begin{align}
q_{\pi,\pi'} &amp; \ge q_{\pi,\pi} = v_\pi\\
\iff 
d(P_{\pi'}^TR_{\pi'}) + \ga P_{\pi'}^T v_\pi &amp; \ge v_\pi \\
\iff
v_{\pi'} = (I-\ga P_{\pi'}^T)^{-1} d(P_{\pi'}^TR) &amp; \ge (I-\ga P_{\pi'}^T)^{-1} (I-\ga P_{\pi'}^T) v_\pi = v_\pi.
\end{align}
<!--\ge d(P_\pi^T R_\pi) + P_\pi^T v_\pi-->
<p>(Note <span class="math inline">\(I-\ga P_{\pi'}^T\)</span> is a geometric series so has positive entries.)</p>
<!-- analysis for $q$-values. How improve when $\ga$ increases to 1?-->
<!--$q$-iteration weirder? $q_a^{t+1} = R_a + \ga P_a^t q_b$. No, don't work with q-->
<ol type="1">
<li><p>Value iteration: <span class="math inline">\(\ve{v^{n+1}-v^n}\le \fc{\ep(1-\la)}{2\la}\implies \ve{v^{n+1}-v_\la^*}&lt;\eph\)</span>. (161)</p>
Proof: Let <span class="math inline">\(v^{n+1}\)</span> be the vale if you follow <span class="math inline">\(\pi^n\)</span> after choosing the best <span class="math inline">\(a\)</span> and <span class="math inline">\(v^{*n+1}\)</span> be the value if you follow <span class="math inline">\(\pi^{n+1}\)</span>. Then
\begin{align}
\ve{v^{*n+1} - v^{n+1}} &amp;\le \ve{Lv^{*n+1} - Lv^{n+1}} + \ve{Lv^{n+1} - v^{n+1}}\\
\implies \ve{v^{*n+1} - v^{n+1}} &amp;\le \fc{\la }{1-\la} \ve{v^{n+1}-v^n}
\end{align}
Geometric series gives <span class="math display">\[\ve{v^{n+1} - v_\la^*} \le \fc{\la}{1-\la} \ve{v^{n+1} - v^n}.\]</span></li>
<li><p>Policy iteration (180)</p></li>
</ol>
<h3 id="changing-ga">Changing <span class="math inline">\(\ga\)</span></h3>
<p>Do a triangle inequality between <span class="math display">\[
v_{*'}^{\ga'} = (I-\ga'P_*')^{-1}r_*, v_{*'}^{\ga}, v_\pi^\ga, v_\pi^{\ga'}.
\]</span> More involved with averaging <span class="math inline">\(\lim_{T\to \iy}\rc T\cdots\)</span>. Choose <span class="math inline">\(\ga\)</span> small enough so that can be approximated with rectangles, etc.</p>
<h3 id="alternating-policy-improvementevaluation">Alternating policy improvement/evaluation</h3>
<p><span class="math display">\[\pi'(s) = \amax_a \sum_{s'} p(s'|s,a) [r(s,a,s') + \ga v_\pi(s')].\]</span></p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2016-10-29</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-10-29.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-10-29.html</id>
    <published>2016-10-24T00:00:00Z</published>
    <updated>2016-10-24T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2016-10-29</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-24 
          , Modified: 2016-10-24 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#threads">Threads</a><ul>
 <li><a href="#priority">Priority</a></li>
 <li><a href="#other">Other</a></li>
 </ul></li>
 <li><a href="#other-papers">Other papers</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="2016-10-22.html">Last week</a>. (See wonderings there.)</p>
<h2 id="threads">Threads</h2>
<h3 id="priority">Priority</h3>
<ul>
<li>Come up with a class of MDPs on exponential space that is interesting and tractable. <a href="/posts/tcs/machine_learning/reinforcement_learning/exponential.html">Thoughts</a>
<ul>
<li>Understand provable guarantees on MDP’s first</li>
</ul></li>
<li>Alexa <a href="https://docs.google.com/document/d/1OtvefjviKSSWH2gzOtYo8T_DVEwPEsI2n0kdrC8WlZI/edit">references</a></li>
</ul>
<h3 id="other">Other</h3>
<ul>
<li>PMI - get some results!
<ul>
<li>Mon. - train CIFAR.</li>
<li>Check MNIST model.</li>
</ul></li>
<li>SoS - chapters 2 and 3</li>
<li>DL experiments <a href="/posts/tcs/machine_learning/matrices/DL_generalization.html">DL generalization</a></li>
<li>Papers</li>
<li>On hold
<ul>
<li>(*) NN learns DL. Can write up weak result, worth doing?</li>
</ul></li>
<li>Papers
<ul>
<li>[HM16] on unsupervised learning (finish)</li>
<li>[HMR16] on dynamical system learning</li>
</ul></li>
</ul>
<p>Analyze Arora and Ge’s NMF algorithm in the presence of noise. Exactly how much noise can it tolerate?</p>
<h2 id="other-papers">Other papers</h2>
<ul>
<li>TODO Read this paper: [CFP16] Assessing significance in a Markov chain without mixing</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Reinforcement learning theory</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/rl_theory.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/rl_theory.html</id>
    <published>2016-10-22T00:00:00Z</published>
    <updated>2016-10-22T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Reinforcement learning theory</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-22 
          , Modified: 2016-10-24 
	</p>
      
       <p>Tags: <a href="/tags/reinforcement%20learning.html">reinforcement learning</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#known-model">Known model</a></li>
 <li><a href="#unknown-model">Unknown model</a></li>
 <li><a href="#parametrized-policy">Parametrized policy</a></li>
 <li><a href="#references">References</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>Figure out what’s provably known about RL!</p>
<ul>
<li><a href="dl_refs.html">References</a></li>
<li><a href="rl_convergence.html">Convergence for basic algorithms</a></li>
</ul>
<h2 id="known-model">Known model</h2>
<ul>
<li>LP solves in poly time.</li>
<li>Policy improvement converges to global optimum. Is it poly time? (cf. simplex method is not poly-time, but is under smoothed analysis)
<ul>
<li>This seems unclear - it’s an open problem as of 04.</li>
</ul></li>
<li>Does alternating policy estimation/improvement converge? In poly time? (cf. alternating minimization)</li>
<li>What about attaining the optimal Cesaro sum?</li>
</ul>
<h2 id="unknown-model">Unknown model</h2>
<ul>
<li>Episodic: does MC converge? What is the convergence rate (regret)?
<ul>
<li>Exploring starts, etc. (ability to choose starts? cf. optimal learning)</li>
</ul></li>
<li>TD learning (non-episodic): Convergence (or non-convergence) rate of
<ul>
<li>SARSA (model estimation)</li>
<li>Q-learning.</li>
</ul></li>
</ul>
<h2 id="parametrized-policy">Parametrized policy</h2>
<p>Suppose payout is convex in policy parameters. But why would this ever be the case???</p>
<p>Or: have to decide between several experts.</p>
<h2 id="references">References</h2>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Learning structured, robust, and multimodal deep models</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/salakhutdinov.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/salakhutdinov.html</id>
    <published>2016-10-21T00:00:00Z</published>
    <updated>2016-10-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Learning structured, robust, and multimodal deep models</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-21 
          , Modified: 2016-10-21 
	</p>
      
       <p>Tags: <a href="/tags/neural%20networks.html">neural networks</a>, <a href="/tags/deep%20learning.html">deep learning</a>, <a href="/tags/multimodal.html">multimodal</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#abstract">Abstract</a></li>
 <li><a href="#learning-deep-generative-models">Learning deep generative models</a></li>
 <li><a href="#multi-modal-learning">Multi-modal learning</a></li>
 <li><a href="#open-problems">Open problems</a></li>
 <li><a href="#questions">Questions</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="abstract">Abstract</h2>
<p>Building intelligent systems that are capable of extracting meaningful representations from high-dimensional data lies at the core of solving many Artificial Intelligence tasks, including visual object recognition, information retrieval, speech perception, and language understanding. In this talk I will first introduce a broad class of deep learning models and show that they can learn useful hierarchical representations from large volumes of high-dimensional data with applications in information retrieval, object recognition, and speech perception. I will next introduce deep models that are capable of extracting a unified representation that fuses together multiple data modalities as well as present the Reverse Annealed Importance Sampling Estimator (RAISE) for evaluating these deep generative models. Finally, I will discuss models that can generate natural language descriptions (captions) of images, as well as generate images from captions using attention mechanism. I will show that on several tasks, including modelling images and text, these models significantly improve upon many of the existing techniques.</p>
<ul>
<li>Develop statistical models to mine for structure: Deep learning models support inferences and discover structure at multiple levels. <!-- drug rec-->
<ul>
<li>Ex. understanding images (Nearest neighbor sentence: people taking pictures of a crazy person)</li>
</ul></li>
</ul>
<h2 id="learning-deep-generative-models">Learning deep generative models</h2>
<ul>
<li>RBM: visible <span class="math inline">\(v\in B^D\)</span>, hidden <span class="math inline">\(h\in B^F\)</span>, bipartite connections. <span class="math inline">\(\Pj(v,h) \propto \exp(v^TWh + a^Th + b^Tv)\)</span>.
<ul>
<li>Ex. alphabets</li>
<li>Derivative of LL. Partition function difficult to compute!</li>
<li>Can change to Gaussians (real-valued variables), etc.</li>
<li>Word counts (undirected version of topic models) (bag of words)</li>
<li>Easy to infer states of hidden variables <span class="math inline">\(\Pj(h|v)\)</span>.</li>
<li>“Product of experts”: after marginalizing over hidden variables (Government, corruption, and oil give high probability of Putin). Better for info retrieval than traditional topic models.</li>
</ul></li>
<li>DBM
<ul>
<li>Compose representations.</li>
<li>MRF with hidden variables and specific structure</li>
<li>Hidden variables dependent even conditioned on input.</li>
<li>Both <span class="math inline">\(\E\)</span> now intractable</li>
<li>Use variational inference for <span class="math inline">\(\E_{P_{data}}[vh^{1T}]\)</span> and stochastic approximation (MCMC) for <span class="math inline">\(\E_{P_\te}[vh^{1T}]\)</span>.</li>
<li>Handwritten data: real data more diverse, crisp.</li>
<li>Pattern completion (3-D object recognition) <!-- true bayesian hedges bets--></li>
<li>Model A vs. B: Take training example at random and show, vs. RBM. Compute <span class="math inline">\(P\)</span> on validation set. Need estimate of <span class="math inline">\(Z\)</span>. RBM better than mixture of Bernoullis by 50 nats.</li>
<li>Simple importance sampling. Given easy-to-sample-from and intractable target distribution, reweight and use MC approximation. Can’t just draw from uniform distribution!</li>
<li>Annealed importance sampling, <span class="math inline">\(p_0,\ldots, p_K\)</span>. Geometric average <span class="math inline">\(p_\be(x) = f_\be/Z_\be = f_0^{1-\be}/f_{target}(x)^\be/Z_\be\)</span>. If initial is uniform, <span class="math inline">\(p_\be = f_t^\be/Z_\be\)</span>, <span class="math inline">\(\be\)</span> inverse temperature. (Annealing by averaging moments)
<ul>
<li>AIS gives unbiased estimator of <span class="math inline">\(Z_t\)</span>.</li>
<li>We are interested in estimating <span class="math inline">\(\ln Z_t\)</span>. Jensen: <span class="math inline">\(\E \ln Z_t\le \ln Z_t\)</span>. Underestimate! We get a stochastic lower bound.</li>
<li>Log-probability on test set, overestimate <span class="math inline">\(\ln p = \ln f - \ln Z_t\)</span>. <!--If sloppy, model looks nice!--></li>
</ul></li>
<li>Gibbs sampling. Pretend it’s equilibrium after 1000 steps.
<ul>
<li>Unrolled RBM as deep generative model. As approximation to RBM.</li>
<li><span class="math inline">\(p_{fwd}(x_{0:K}) = p_0(x_0)\prodo kK T_k(x_k|x_{k-1})\)</span>.</li>
<li>Reverse AIS estimator (RAISE). Start at data and melt distribution. Tends to underestimate log-probs.</li>
</ul></li>
<li>Learning hierarchical representations.</li>
</ul></li>
<li>Model evaluation: Good way of evaluating!</li>
</ul>
<p>Learn feature representations! <!--textons, audio features--></p>
<h2 id="multi-modal-learning">Multi-modal learning</h2>
<ul>
<li>Image, text, audio. Joint representations?</li>
<li>Product recommendations</li>
<li><p>Robotic</p></li>
<li>Challenges
<ul>
<li>Images are real-valued, text is sparse.</li>
<li>Noisy and missing data</li>
</ul></li>
<li>Multimodal DBM, go up and then down the other way. Define joint distribution over images and text.</li>
<li>Given text, sample from images
<ul>
<li>MIR-Flickr dataset</li>
</ul></li>
<li>Solve supervised learning tasks. Can do better if use unlabeled data. Learn better features and representations.</li>
<li>Can pre-train image pathway and text pathways. <!-- Q: how much can you decouple? --></li>
<li>Complete descriptions of images.
<ul>
<li>Encoder: CNN to semantic feature space.</li>
<li>Decoder: neural language model.</li>
<li>Learn joint embedding space of images and text. Natural definition of scoring function.</li>
<li>Ex. Fluffy.</li>
<li>Multimodal linguistic regularities: Addition and subtraction. (Cat - bowl + box)
<ul>
<li>Bird and reflection: Two birds are trying to be seen in the water.</li>
<li>Giraffe is standing next to a fence in a field.</li>
<li>Handlebars are trying to ride a bike rack.</li>
</ul></li>
<li>Caption generation with visual attention.</li>
<li>Generate images from captions. (school bus flying in blue skies)</li>
<li>Helmholtz machines/variational autoencoders. Directed counterparts. Generative process goes down. Approximate inference going up. Hinton95 (Science). Now it works, Kingma2014 (NIPS)
<ul>
<li>A toilet seat sits open in the bathroom, grass field</li>
<li>Ask google. <!--worked on toilet project--></li>
</ul></li>
</ul></li>
</ul>
<h2 id="open-problems">Open problems</h2>
<ul>
<li>Unsupevised learning/transfer learning/one-shot learning</li>
<li>Reasoning, attention, memory</li>
<li>Natural language understanding
<ul>
<li>Sequence-to-sequence learning</li>
<li>Skip-thought model
<ul>
<li>Generate previous and forward sentence</li>
<li>Objective: sum of log-probabilities for previous/next sentence conditioned on current.</li>
<li>How similar are 2 sentence are on the scale 1 to 5. (A person is performing a trick on a motorcycle? A person is tricking a man on a motorcycle.)</li>
<li>We use no semantic features. <!-- AdaSent --></li>
</ul></li>
</ul></li>
<li>Deep reinforcement learning</li>
</ul>
<p>Neural storytelling. Take corpus of books (romantic), generate captions about the image.</p>
<p>Kiros2015 NIPS</p>
<p>One-shot learning: humans vs. machines. How can we learn novel concept from few examples (Lake, S, Tenenbaum 2015, Science)</p>
<h2 id="questions">Questions</h2>
<p>CNN better for supervised. We’re trying to build convolutional DBM.</p>
<p>vs. variational autoencoder. Reparametrization trick, backprop through whole model. Optimization better for VA. Both useful.</p>
<p>Learning representation, not with language?</p>
<!-- evaluation
neural image on google $10^5$
-->
<p>Microsoft dataset: 80000 images, 5 captions each. Not big enough, but captions clean!</p>
<p>Topics vs. coherent model of sentences. What do we need? New architectures, training sets? <!--need rep to corresp with reality. have memory, check for consistency with memory --></p>
<!--AlphaGo is more technological. Fast, evaluating -->
<p>Actor-Mimic model.</p>
<p>Transfer learning: learn new games faster by leveraging knowledge about previous games. Ex. star gunner</p>
<p>Continuous state.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2016-10-22</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-10-22.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-10-22.html</id>
    <published>2016-10-19T00:00:00Z</published>
    <updated>2016-10-19T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2016-10-22</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-19 
          , Modified: 2016-10-19 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#threads">Threads</a></li>
 <li><a href="#other-papers">Other papers</a></li>
 <li><a href="#talk-with-arora-1019-wed">Talk with Arora 10/19 (Wed)</a></li>
 <li><a href="#wonderings">Wonderings</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="threads">Threads</h2>
<ul>
<li>PMI - get some results!</li>
<li>SoS - chapters 2 and 3</li>
<li>DL: do experiments suggested in <a href="/posts/tcs/machine_learning/matrices/DL_generalization.html">DL generalization</a> (Mon, Tue)
<ul>
<li>(*) NN learns DL. (Mon, Tue) - Wrote up progress so far, where I am stuck.</li>
</ul></li>
<li>Papers
<ul>
<li>[HM16] on unsupervised learning (went through 1st half, Tue)</li>
<li>[HMR16] on dynamical system learning (read <a href="http://www.offconvex.org/2016/10/13/gradient-descent-learns-dynamical-systems/">blog post</a> Tue)</li>
</ul></li>
<li>Come up with a class of MDPs on exponential space that is interesting and tractable. <a href="/posts/tcs/machine_learning/reinforcement_learning/exponential.html">Thoughts</a>
<ul>
<li>Understand provable guarantees on MDP’s first</li>
</ul></li>
<li>Alexa <a href="https://docs.google.com/document/d/1OtvefjviKSSWH2gzOtYo8T_DVEwPEsI2n0kdrC8WlZI/edit">references</a></li>
</ul>
<p>Analyze Arora and Ge’s NMF algorithm in the presence of noise. Exactly how much noise can it tolerate?</p>
<h2 id="other-papers">Other papers</h2>
<ul>
<li>TODO Read this paper: [CFP16] Assessing significance in a Markov chain without mixing</li>
</ul>
<h2 id="talk-with-arora-1019-wed">Talk with Arora 10/19 (Wed)</h2>
<p>Dynamical systems + MDP!</p>
<h2 id="wonderings">Wonderings</h2>
<ul>
<li>Can we generalize the random walk of the context vectors? There’s no reason to think that context vectors just drift on the sphere. (p. 139)
<ul>
<li>Make it a RBM. Say with bounded degree. (There are ways to learn - see the factored MDP paper. We don’t care about MDP here, so it’s easier.)</li>
<li>For example, one node (dimension) could simply control output of common words.</li>
<li>Given the observations, whose probs are <span class="math inline">\(\propto e^{w^TAc}\)</span>, learn the RBM. (Note we can replace <span class="math inline">\(w\)</span> by <span class="math inline">\(A^Tw\)</span>… but if <span class="math inline">\(c\)</span> is in larger space, then it’s not obvious how to learn the <span class="math inline">\(A\)</span>! Can we modify the word embeddings to deal with this? Beware of difficulties… HMMs usually assume full column-rank observations, violated here. Look at the proper hard instance for HMM. - the version I saw with noisy parity wasn’t quite a HMM)</li>
<li>Prereq: given <span class="math inline">\((x,h)\)</span> how to learn RBM or Bayes net? (When <span class="math inline">\(W\)</span>’s entries are small enough, can do via MCMC estimation of partition function and optimization of log-likelihood. Otherwise, is hard worst-case.)
<ul>
<li>I’m confused! There seems to be a line of work on factorial MDP’s. However, where are the basic results about learnability of Bayes nets? Learning the model for FMDP’s is strictly harder—why so much work on this (with too much assumptions, or weak results) without results on learning Bayes nets?</li>
<li>Bresler.</li>
</ul></li>
<li>cf. work on continuous HMM’s. Work on factored HMMs? Any bounds when hidden state has larger dimension? Also, adapt HMM learning to vector observations. (Is the natural generalization a factored prob model rather than a dynamical system? Note probabilistic linear dynamical system IS straightforward generalization of HMM, but the factored prob model is not. Weird generalization though, because only having states <span class="math inline">\(\{e_1,\ldots, e_n\}\)</span> seems decoupled - can couple together any way you want.) <span class="citation" data-cites="Andrej">@Andrej</span> on this.
<ul>
<li>When state has larger dimension, need overcomplete tensor factorization.</li>
</ul></li>
<li>Start with: given an HMM with both transitions and observations being RBMs (say of degree at most 2), observations don’t “lose info” (analogue of full column rank), infer RBM. (Z is over words that exist). Breaks symmetries - the various dimensions are important now? <!-- sparse vectors are meaningful --></li>
</ul></li>
<li>Dictionary learning experiment
<ul>
<li>The kernel DL I want is different from in the literature. There they want <span class="math inline">\(\Phi(Y) \approx \Phi(A)X\)</span>, here we want <span class="math inline">\(\Phi(Y) \approx \Phi(AX)\)</span>. I.e. we want to maximize <span class="math inline">\(K(Y,AX)\)</span> where <span class="math inline">\(X\)</span> is restricted to be sparse. Usual algorithms break down here, but can still consider <span class="math inline">\(K(Y,AX) + \ve{X}_1\)</span>. (137)</li>
<li>Use kernel in <a href="/posts/tcs/machine_learning/neural_nets/PMDH16.html">PMDH16</a>.</li>
</ul></li>
<li>RL questions (135)
<ul>
<li>Given a (continuous) space of policies, converge to a local min in the space of policies.</li>
<li>Find some measure of complexity of a class of policies. Branching is important. (Getting limited info from other policies…) Get a bound independent of number of states, involving this complexity.
<ul>
<li>Example to keep in mind: <span class="math inline">\(2^n\)</span> strategies all branching off into different rewards at end of their paths.</li>
<li>Alternatively, complexity of class of models of environment.</li>
</ul></li>
<li>What is the VC dimension bound for contextual (expert) bandits? Also look at the contextual MDP paper, cf. EXP4.</li>
<li>[ALA16] open question</li>
<li>Scraps
<ul>
<li>Right <span class="math inline">\(\la\)</span>, how do well without learning model?</li>
<li>SoS, minimax, etc.</li>
<li>EXP3:Scrible:EXP4::UCB1:?:? (LinUCB? RUCB?)</li>
</ul></li>
<li>Increasing <span class="math inline">\(\ga\)</span> towards 1 (simulated annealing, temperature schedule…)</li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>DL experiments</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/dl_experiments.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/dl_experiments.html</id>
    <published>2016-10-17T00:00:00Z</published>
    <updated>2016-10-17T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>DL experiments</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-17 
          , Modified: 2016-10-17 
	</p>
      
       <p>Tags: <a href="/tags/dictionary%20learning.html">dictionary learning</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#experiments">Experiments</a></li>
 <li><a href="#results">Results</a><ul>
 <li><a href="#first-observations">First observations</a></li>
 </ul></li>
 <li><a href="#evaluation">Evaluation</a></li>
 <li><a href="#code">Code</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>What are convergence guarantees for dictionary learning? Consider the settings</p>
<ul>
<li>AGMM15 (Alternating minimization)</li>
<li>2-layer NN
<ul>
<li>With <span class="math inline">\(b^Ty\)</span></li>
<li>With <span class="math inline">\(\sgn(b^Ty)\)</span></li>
</ul></li>
</ul>
<!--
Sanjeev told me you did some experiments, so I wanted to check with you.

Experimentally, does dictionary learning converge to the right dictionary under random initialization? What if we randomly initialize with samples drawn from $x=Ah$? What about for the neural net (backprop) model you showed me last time - does random initialization (with samples) converge to the dictionary? If you have code for experiments, please send it to me.

I've done a lot of the calculations for neural nets learning dictionaries, and am getting stuck on the following: it appears that the gradient of the entire matrix is correlated with the right direction, but individual rows may not be (so a row may get far away until it no longer decodes correctly). Did you encounter something like this? If you have the bandwidth I'd be interested in working with you on this.

-->
<h2 id="experiments">Experiments</h2>
<p>Code is in <code>dl_convergence.py</code>. Run on ionic.</p>
<h2 id="results">Results</h2>
<!--1218589: -->
<ul>
<li>s = 3</li>
<li>m = 50 # hidden vector</li>
<li>n = 25 # observed vector</li>
<li>q = s/m</li>
<li>alpha = .01</li>
<li>batchsize = 1024</li>
<li>vary sigma (how close initialization is) <!-- * Approximate convergence for sigma = .05, .1; not 0.5--></li>
</ul>
<p>Next,</p>
<ul>
<li>add random initialization - check</li>
<li>vary (s,m,n)</li>
<li>check sparsity of learned vectors (do thresholding too) - check</li>
<li>add initialization from samples - check
<ul>
<li>try overcomplete initialization from samples - check</li>
</ul></li>
</ul>
<h3 id="first-observations">First observations</h3>
<p>See <code>am_dl_3_50_25.txt</code> and <code>slurm-1218768.out</code></p>
<ul>
<li>Converges when close enough (as in AGMM15). For this, even 0.5 is close enough. Note it doesn’t converge to <span class="math inline">\(A\)</span> - it converges to something that has columns <span class="math inline">\(\approx 0.1\)</span> away from <span class="math inline">\(A\)</span>, consistant bias. (This makes sense.)</li>
<li>Random initialization does not converge to global optimum. Initialization with samples seems to do slightly better.</li>
</ul>
<h2 id="evaluation">Evaluation</h2>
<p>How to evaluate?</p>
<ul>
<li>Closeness of columns.</li>
<li>Loss: how much sparsity, and how far away. (Reconstruction error)
<ul>
<li>How does reconstruction error compare to SVD? (Make dimensions comparable.)</li>
</ul></li>
<li>Put in random SVM on top. Can it learn the SVM well?</li>
<li>Check framework in [HM16].</li>
</ul>
<h2 id="code">Code</h2>
<ul>
<li>Displaying images
<ul>
<li><a href="http://stackoverflow.com/questions/902761/saving-a-numpy-array-as-an-image">No PIL</a></li>
<li><a href="http://stackoverflow.com/questions/2659312/how-do-i-convert-a-numpy-array-to-and-display-an-image">PIL</a></li>
<li><a href="https://pillow.readthedocs.io/en/3.4.x/reference/index.html">Pillow</a></li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>MDP's with continuous state space (scratch)</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/continuous_scratch.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/continuous_scratch.html</id>
    <published>2016-10-14T00:00:00Z</published>
    <updated>2016-10-14T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>MDP's with continuous state space (scratch)</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-14 
          , Modified: 2016-10-25 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#starting-points">Starting points</a></li>
 <li><a href="#model">Model</a><ul>
 <li><a href="#first-try">First try</a></li>
 <li><a href="#second-try">Second try</a></li>
 </ul></li>
 <li><a href="#misc">Misc</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="continuous.html">written-up things</a></p>
<p><a href="https://en.wikipedia.org/wiki/Kalman_filter">Kalman filter</a></p>
<p>Come up with a class of MDPs on exponentially large/continuous space that is interesting and tractable. Think of generalizing from contextual bandits * Basically we want a reasonable model of a MDP with a very large (exponential or continuous) state space and be able to do something with it. Wanted to include some dynamics like in Kalman filters but we weren’t sure whether Kalman filters are tractable * Todo: learn about Kalman filters</p>
<h2 id="starting-points">Starting points</h2>
<ol type="1">
<li>HMM’s have discrete state space. What happens with continuous state space? Suppose there are some dynamics as in Kalman filters. Infer the hidden state. References
<ul>
<li>Continuous HMM paper (RKHS)</li>
<li>Kalman filters (see examples)</li>
<li>Grad descent learning dynamical systems.</li>
</ul></li>
<li>Contextual bandits + MDP’s. Don’t assume there’s a hidden state here, just that next state depends, say, linearly on action and noise.</li>
<li>Context vector/random walk model for documents: transition probabilities <span class="math inline">\(\propto \exp(-\an{c_1,c_2})\)</span> and observation probabilities <span class="math inline">\(\propto \exp(-\an{c_1,x})\)</span>.</li>
</ol>
<h2 id="model">Model</h2>
<h3 id="first-try">First try</h3>
<ul>
<li>Stochastic setting.</li>
<li><span class="math inline">\(c_t\)</span> is context at time <span class="math inline">\(t\)</span>.</li>
<li>Set of actions <span class="math inline">\(A\)</span>. (For example, <span class="math inline">\(A=\{e_1,\ldots, e_n\}\)</span>.)</li>
<li>Next context <span class="math inline">\(c_{t+1}=\)</span> (Here <span class="math inline">\(w_t\)</span> is noise.)
<ul>
<li><span class="math inline">\(F_a c_t + w_t\)</span>. (Transformation depends on action.)</li>
<li><span class="math inline">\(F c_t + B a + w_t\)</span>. (Action is a forcing term. This matches Kalman formulation. More reasonable?) (*)</li>
</ul></li>
<li>Payoff depends on context and actions in some way.
<ul>
<li>Model 1: depends only on context <span class="math inline">\(\te^T c_t\)</span>. (*)</li>
<li>Model 2: depends on context and action <span class="math inline">\(\te^T[c_t;a]\)</span>.</li>
<li>? Some probability?</li>
</ul></li>
</ul>
<p>This setting looks like reinforcement learning + control theory. Prior work? How is RL used in continuous systems right now? Basic control theory background?</p>
<p>Need the model to be a generalization of regular MDP.</p>
<p>(*) may be interesting from control theory perspective, but doesn’t generalize discrete MDP. (Seems like best to learn the dynamics, and then do optimal thing from there…)</p>
<h3 id="second-try">Second try</h3>
<ul>
<li>Finite number of actions</li>
<li><span class="math inline">\(c_{t+1} = F_a c_t + w_t\)</span>. (Only probability is noise.)</li>
<li>Payout <span class="math inline">\(\te_^T c_t\)</span>.</li>
</ul>
<p>Captures deterministic MDP, but not probabilistic, by letting <span class="math inline">\(A=\{e_i\}\)</span>.</p>
<h2 id="misc">Misc</h2>
<p>Do as well as best Bayes net? Actions in some class. Finite set of actions, vs. exponential/continuous set of actions. In latter case, will depend on optimizability of that set…</p>
<p>Ex. class is a SVM.</p>
<p>“Do as well as best estimator of <span class="math inline">\(q\)</span> function in a certain class (assume convexity or something?)” (cf. contextual bandits first)</p>
<!--Definitely need something stronger than: there exist something that works! if can encode crypto 

Upper confidence bounds
-->

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>MDP's with continuous state space</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/continuous.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/continuous.html</id>
    <published>2016-10-14T00:00:00Z</published>
    <updated>2016-10-14T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>MDP's with continuous state space</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-14 
          , Modified: 2016-10-14 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#finding-optimal-policy-given-dynamics">Finding optimal policy (given dynamics)</a><ul>
 <li><a href="#simplest-problem">Simplest problem</a></li>
 <li><a href="#different-linear-transformations">Different linear transformations</a><ul>
 <li><a href="#finite-actions">Finite actions</a></li>
 <li><a href="#continuous-actions">Continuous actions</a></li>
 </ul></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <p>Other pages</p>
<ul>
<li><a href="continuous_scratch.html">scratch</a></li>
<li><a href="rl_refs.html">papers</a></li>
</ul>
<h2 id="finding-optimal-policy-given-dynamics">Finding optimal policy (given dynamics)</h2>
<h3 id="simplest-problem">Simplest problem</h3>
<p>State is <span class="math inline">\(s\in \R^n\)</span>.</p>
<p>Suppose</p>
<ul>
<li>reward is given by <span class="math inline">\(\an{r,s}\)</span></li>
<li>discount factor for future reward is <span class="math inline">\(\ga\)</span>,</li>
<li>action set is <span class="math inline">\(A\)</span> (finite or convex),</li>
<li>update is <span class="math inline">\(s_{t+1} = Us_t + a_t\)</span>, <span class="math inline">\(a_t\in A\)</span>.</li>
</ul>
<p>Then we can solve this in closed form (geometric series). The best action is the same at each time step, <span class="math display">\[\max_{a\in A} \an{(I-\ga U)^{T\,-1}r, a}.\]</span></p>
<p>For infinite-horizon, we look at instead the average of rewards over next <span class="math inline">\(T\)</span> time steps as <span class="math inline">\(T\to \iy\)</span>; interesting case is when <span class="math inline">\(U\)</span> has eigenvalues equal to 1. Straightforward.</p>
<h3 id="different-linear-transformations">Different linear transformations</h3>
<h4 id="finite-actions">Finite actions</h4>
<p>Now consider a more general case. (We don’t put in probability yet.)</p>
<ul>
<li>The reward is still <span class="math inline">\(\an{r,s}\)</span></li>
<li>discount factor <span class="math inline">\(\ga\)</span></li>
<li>Finite set <span class="math inline">\(A\)</span>. (Think about <span class="math inline">\(|A|=n\)</span>.)</li>
<li>Given action <span class="math inline">\(a\)</span>, the update is <span class="math inline">\(s_{t+1} = U_a s_t + v_a\)</span>, <span class="math inline">\(a\in A\)</span>.</li>
</ul>
<p>Given fixed discount factor <span class="math inline">\(\ga\)</span>, desired approximation <span class="math inline">\(\ep\)</span>, can we find something that does at most <span class="math inline">\(\ep\)</span> worse in <span class="math inline">\(\poly\prc{\ep}\)</span> time?</p>
<ul>
<li>Dynamic programming: We can achieve <span class="math inline">\(|A|^{\log_\ga\prc{\ep}} = \prc{\ep}^{O(\ln |A|)}\)</span> time by searching over a tree.</li>
<li>Policy iteration: The policy is a function in <span class="math inline">\(n\)</span> dimensions, and it’s not even clear that we can represent it succinctly! If we discretize with a mesh, this takes exponential time/space.</li>
</ul>
<p>Questions</p>
<ol type="1">
<li>Can we find a nice class of functions (SVM, etc.), and find the best <span class="math inline">\(v\)</span> within that class?</li>
<li>Is there a class that can approximate all possible <span class="math inline">\(v\)</span>’s within <span class="math inline">\(\ep\)</span> or constant factor?</li>
<li>Can <span class="math inline">\(v\)</span>’s be complicated? (Ex. break the space into exponentially many regions.)</li>
</ol>
<p>Careful: finding the best approximation to a unitary transformation (take <span class="math inline">\(U_a\)</span> unitary and <span class="math inline">\(v_a=0\)</span>) with a certain set of unitaries is a well-studied problem that can involve number theory—we want to exclude this. Ex. make sure we’re not in this regime - have the discount factor, or add noise.</p>
<h4 id="continuous-actions">Continuous actions</h4>
<p>Can also make <span class="math inline">\(A\)</span> continuous.</p>
<ul>
<li><span class="math inline">\(A\sub \R^m\)</span> convex</li>
<li>Given action <span class="math inline">\(a\)</span>, the update is <span class="math inline">\(U[s_t;a] + Va\)</span> where <span class="math inline">\(U_a\in \R^{n\times (m+n)}\)</span>, <span class="math inline">\(V\in \R^{n\times m}\)</span>.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Finite model theory</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/logic/finite_model_theory.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/logic/finite_model_theory.html</id>
    <published>2016-10-14T00:00:00Z</published>
    <updated>2016-10-14T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Finite model theory</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-14 
          , Modified: 2016-10-14 
	</p>
      
       <p>Tags: <a href="/tags/model%20theory.html">model theory</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#definability-and-undefinability">1 Definability and undefinability</a><ul>
 <li><a href="#expressive-power">Expressive power</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="https://simons.berkeley.edu/talks/finite-and-algorithmic-model-theory">Link</a></p>
<h2 id="definability-and-undefinability">1 Definability and undefinability</h2>
<ul>
<li>Expressive power of logics on class of finite structures.</li>
<li>Problems in computer science (complexity theory, database theory) are naturally questions about expressive power of logics.</li>
</ul>
<p><span class="math display">\[Mod(\ph) = \set{\mathbb A}{ \mathbb A \vDash \ph}.\]</span> Here <span class="math inline">\(\mathbb A\)</span> ranges over finite relational structures.</p>
<p>What classes of structures are definable? (Set of possible <span class="math inline">\(Mod(\ph)\)</span>’s for <span class="math inline">\(\ph\)</span> in logic <span class="math inline">\(\mathcal L\)</span>.)</p>
<ul>
<li>Syntactic restrictions on <span class="math inline">\(\ph\)</span> vs. semantic restrictions on <span class="math inline">\(Mod(\ph)\)</span>.</li>
<li>Computational complexity of <span class="math inline">\(Mod(\ph)\)</span> vs. syntactic complexity of <span class="math inline">\(\ph\)</span>.</li>
</ul>
<h3 id="expressive-power">Expressive power</h3>
<p>Relational vocabulary: finite set <span class="math inline">\(A\)</span> with relations <span class="math inline">\(R_1,\ldots, R_m\)</span> and constants <span class="math inline">\(c_1,\ldots, c_n\)</span>.</p>
<p>A property of finite structures is any isomorphism-closed class of structures. (Morphisms are permutations. Ex. graph ismomorphisms.) Given logic, for which properties <span class="math inline">\(P\)</span> is there a sentence <span class="math inline">\(\ph\)</span> such that <span class="math inline">\(\mathbb A\in P\iff A\vDash \ph\)</span>.</p>
<p>Ex. colored graphs: one binary relation <span class="math inline">\(E^2\)</span> and some unary relations <span class="math inline">\(C_i^1\)</span>. First-order logic formulas involve these relations and <span class="math inline">\(\wedge, \vee, \neg, \exists, \forall\)</span>.</p>
<ul>
<li>Vertex cover of size at most <span class="math inline">\(k\)</span>. (<span class="math inline">\(k\)</span> fixed)</li>
<li>3-colorability when allow quantification over sets of vertices.</li>
</ul>
<p>Compactness, completeness, preservation fail. (What are these exactly?)</p>
<p>Methods:</p>
<ul>
<li>Ehrenfeucht-Fraisse Games and related model-comparison games</li>
<li>Locality Theorems</li>
<li>Automata-based methods</li>
<li>Complexity</li>
<li>Asymptotic Combinatorics</li>
</ul>
<p>Equivalence means satisfying the same statements in the logic. On finite structures, two structures are equivalent iff they are isomorphic.</p>
<p>Quantifier rank:</p>
<ol type="1">
<li>For atoms, <span class="math inline">\(qr(\ph)=0\)</span>.</li>
<li><span class="math inline">\(qr(\neg \psi) = qr(\psi)\)</span>.</li>
<li><span class="math inline">\(qr(\psi_1\wedge/\vee \psi_2) = \max_i(qr(\psi_i))\)</span>.</li>
<li><span class="math inline">\(qr(\exists/\forall x \psi) = qr(\psi)+1\)</span>.</li>
</ol>
<p><span class="math inline">\(\mathbb A\equiv_p \mathbb B\)</span> iff for all <span class="math inline">\(qr(\ph)\le p\)</span>, <span class="math inline">\(A\vDash \ph\iff B\vDash \ph\)</span>.</p>
<p><span class="math inline">\(S\)</span> is definable by first order sentence iff <span class="math inline">\(S\)</span> is closed under <span class="math inline">\(\equiv_p\)</span> for some <span class="math inline">\(p\)</span>. (13) ?? (Is <span class="math inline">\(S\)</span> on finite set? What does “finite relational vocab” mean?) (Is this trivial by taking <span class="math inline">\(p=|A|\)</span>?) (NO: <span class="math inline">\(A\)</span> is of arbitrary finite size!)</p>
<p>Ex. I think connectedness is not first-order!</p>
<p>Define <strong>partial isomorphism</strong>.</p>
<p>Ehrenfeucht-Fraisse game:</p>
<p>For <span class="math inline">\(p\)</span> rounds:</p>
<ul>
<li>Spoiler choose one structure and an element of that structure.</li>
<li>Duplicator responds with element of other structure <span class="math inline">\(a_i\)</span>.</li>
</ul>
<p>After <span class="math inline">\(p\)</span> rounds, Duplicator wins if <span class="math inline">\(a_i\mapsto b_i\)</span> is partial iso. Duplicator has winning strategy iff <span class="math inline">\(\mathbb A\equiv_p \mathbb B\)</span>.</p>
<p>Proof: choose the witnesses of existence, going to the other structure for a <span class="math inline">\(\forall\)</span> because negated <span class="math inline">\(\forall\)</span> gives <span class="math inline">\(\exists\)</span>.</p>
<p>So to show not definable, for every <span class="math inline">\(p\)</span> produce <span class="math inline">\(\mathbb A_p\)</span>, <span class="math inline">\(\mathbb B_p\)</span> such that one is in <span class="math inline">\(S\)</span>, and duplicator wins.</p>
<p>Ex. Not definable: 2-colorability, even cardinality, connectivity.</p>
<p>(Duplicator’s strategy is to ensure that after r moves, the distance between corresponding pairs of pebbles is either equal or <span class="math inline">\(2p^r\)</span>.)</p>
<p>Alternative: stratify by number of free variables (in any sub-formula), <span class="math inline">\(\equiv^k\)</span>. <span class="math inline">\(\equiv^k\implies \equiv_k\)</span>.</p>
<p>Connectivity and 2-colorability are axiomatizable in <span class="math inline">\(L^k\)</span> (define <span class="math inline">\(path_{\le l}\)</span>, <span class="math inline">\(disconnect_l\)</span>); even cardinality is not. (I’m confused… why can we reuse variables?? 21)</p>
<p><span class="math inline">\(\equiv_p, \equiv^k\)</span> have finitely/infinitely many equivalence classes.</p>
<p>How is the pebble game different?? 23</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
