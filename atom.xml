<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2016-10-24T00:00:00Z</updated>
    <entry>
    <title>Weekly summary 2016-10-29</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-10-29.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-10-29.html</id>
    <published>2016-10-24T00:00:00Z</published>
    <updated>2016-10-24T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2016-10-29</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-24 
          , Modified: 2016-10-24 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#threads">Threads</a><ul>
 <li><a href="#priority">Priority</a></li>
 <li><a href="#other">Other</a></li>
 </ul></li>
 <li><a href="#other-papers">Other papers</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="2016-10-22.html">Last week</a>. (See wonderings there.)</p>
<h2 id="threads">Threads</h2>
<h3 id="priority">Priority</h3>
<ul>
<li>Come up with a class of MDPs on exponential space that is interesting and tractable. <a href="/posts/tcs/machine_learning/reinforcement_learning/exponential.html">Thoughts</a>
<ul>
<li>Understand provable guarantees on MDP’s first</li>
</ul></li>
<li>Alexa <a href="https://docs.google.com/document/d/1OtvefjviKSSWH2gzOtYo8T_DVEwPEsI2n0kdrC8WlZI/edit">references</a></li>
</ul>
<h3 id="other">Other</h3>
<ul>
<li>PMI - get some results!
<ul>
<li>Mon. - train CIFAR.</li>
<li>Check MNIST model.</li>
</ul></li>
<li>SoS - chapters 2 and 3</li>
<li>DL experiments <a href="/posts/tcs/machine_learning/matrices/DL_generalization.html">DL generalization</a></li>
<li>Papers</li>
<li>On hold
<ul>
<li>(*) NN learns DL. Can write up weak result, worth doing?</li>
</ul></li>
<li>Papers
<ul>
<li>[HM16] on unsupervised learning (finish)</li>
<li>[HMR16] on dynamical system learning</li>
</ul></li>
</ul>
<p>Analyze Arora and Ge’s NMF algorithm in the presence of noise. Exactly how much noise can it tolerate?</p>
<h2 id="other-papers">Other papers</h2>
<ul>
<li>TODO Read this paper: [CFP16] Assessing significance in a Markov chain without mixing</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Learning structured, robust, and multimodal deep models</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/salakhutdinov.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/salakhutdinov.html</id>
    <published>2016-10-21T00:00:00Z</published>
    <updated>2016-10-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Learning structured, robust, and multimodal deep models</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-21 
          , Modified: 2016-10-21 
	</p>
      
       <p>Tags: <a href="/tags/neural%20networks.html">neural networks</a>, <a href="/tags/deep%20learning.html">deep learning</a>, <a href="/tags/multimodal.html">multimodal</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#abstract">Abstract</a></li>
 <li><a href="#learning-deep-generative-models">Learning deep generative models</a></li>
 <li><a href="#multi-modal-learning">Multi-modal learning</a></li>
 <li><a href="#open-problems">Open problems</a></li>
 <li><a href="#questions">Questions</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="abstract">Abstract</h2>
<p>Building intelligent systems that are capable of extracting meaningful representations from high-dimensional data lies at the core of solving many Artificial Intelligence tasks, including visual object recognition, information retrieval, speech perception, and language understanding. In this talk I will first introduce a broad class of deep learning models and show that they can learn useful hierarchical representations from large volumes of high-dimensional data with applications in information retrieval, object recognition, and speech perception. I will next introduce deep models that are capable of extracting a unified representation that fuses together multiple data modalities as well as present the Reverse Annealed Importance Sampling Estimator (RAISE) for evaluating these deep generative models. Finally, I will discuss models that can generate natural language descriptions (captions) of images, as well as generate images from captions using attention mechanism. I will show that on several tasks, including modelling images and text, these models significantly improve upon many of the existing techniques.</p>
<ul>
<li>Develop statistical models to mine for structure: Deep learning models support inferences and discover structure at multiple levels. <!-- drug rec-->
<ul>
<li>Ex. understanding images (Nearest neighbor sentence: people taking pictures of a crazy person)</li>
</ul></li>
</ul>
<h2 id="learning-deep-generative-models">Learning deep generative models</h2>
<ul>
<li>RBM: visible <span class="math inline">\(v\in B^D\)</span>, hidden <span class="math inline">\(h\in B^F\)</span>, bipartite connections. <span class="math inline">\(\Pj(v,h) \propto \exp(v^TWh + a^Th + b^Tv)\)</span>.
<ul>
<li>Ex. alphabets</li>
<li>Derivative of LL. Partition function difficult to compute!</li>
<li>Can change to Gaussians (real-valued variables), etc.</li>
<li>Word counts (undirected version of topic models) (bag of words)</li>
<li>Easy to infer states of hidden variables <span class="math inline">\(\Pj(h|v)\)</span>.</li>
<li>“Product of experts”: after marginalizing over hidden variables (Government, corruption, and oil give high probability of Putin). Better for info retrieval than traditional topic models.</li>
</ul></li>
<li>DBM
<ul>
<li>Compose representations.</li>
<li>MRF with hidden variables and specific structure</li>
<li>Hidden variables dependent even conditioned on input.</li>
<li>Both <span class="math inline">\(\E\)</span> now intractable</li>
<li>Use variational inference for <span class="math inline">\(\E_{P_{data}}[vh^{1T}]\)</span> and stochastic approximation (MCMC) for <span class="math inline">\(\E_{P_\te}[vh^{1T}]\)</span>.</li>
<li>Handwritten data: real data more diverse, crisp.</li>
<li>Pattern completion (3-D object recognition) <!-- true bayesian hedges bets--></li>
<li>Model A vs. B: Take training example at random and show, vs. RBM. Compute <span class="math inline">\(P\)</span> on validation set. Need estimate of <span class="math inline">\(Z\)</span>. RBM better than mixture of Bernoullis by 50 nats.</li>
<li>Simple importance sampling. Given easy-to-sample-from and intractable target distribution, reweight and use MC approximation. Can’t just draw from uniform distribution!</li>
<li>Annealed importance sampling, <span class="math inline">\(p_0,\ldots, p_K\)</span>. Geometric average <span class="math inline">\(p_\be(x) = f_\be/Z_\be = f_0^{1-\be}/f_{target}(x)^\be/Z_\be\)</span>. If initial is uniform, <span class="math inline">\(p_\be = f_t^\be/Z_\be\)</span>, <span class="math inline">\(\be\)</span> inverse temperature. (Annealing by averaging moments)
<ul>
<li>AIS gives unbiased estimator of <span class="math inline">\(Z_t\)</span>.</li>
<li>We are interested in estimating <span class="math inline">\(\ln Z_t\)</span>. Jensen: <span class="math inline">\(\E \ln Z_t\le \ln Z_t\)</span>. Underestimate! We get a stochastic lower bound.</li>
<li>Log-probability on test set, overestimate <span class="math inline">\(\ln p = \ln f - \ln Z_t\)</span>. <!--If sloppy, model looks nice!--></li>
</ul></li>
<li>Gibbs sampling. Pretend it’s equilibrium after 1000 steps.
<ul>
<li>Unrolled RBM as deep generative model. As approximation to RBM.</li>
<li><span class="math inline">\(p_{fwd}(x_{0:K}) = p_0(x_0)\prodo kK T_k(x_k|x_{k-1})\)</span>.</li>
<li>Reverse AIS estimator (RAISE). Start at data and melt distribution. Tends to underestimate log-probs.</li>
</ul></li>
<li>Learning hierarchical representations.</li>
</ul></li>
<li>Model evaluation: Good way of evaluating!</li>
</ul>
<p>Learn feature representations! <!--textons, audio features--></p>
<h2 id="multi-modal-learning">Multi-modal learning</h2>
<ul>
<li>Image, text, audio. Joint representations?</li>
<li>Product recommendations</li>
<li><p>Robotic</p></li>
<li>Challenges
<ul>
<li>Images are real-valued, text is sparse.</li>
<li>Noisy and missing data</li>
</ul></li>
<li>Multimodal DBM, go up and then down the other way. Define joint distribution over images and text.</li>
<li>Given text, sample from images
<ul>
<li>MIR-Flickr dataset</li>
</ul></li>
<li>Solve supervised learning tasks. Can do better if use unlabeled data. Learn better features and representations.</li>
<li>Can pre-train image pathway and text pathways. <!-- Q: how much can you decouple? --></li>
<li>Complete descriptions of images.
<ul>
<li>Encoder: CNN to semantic feature space.</li>
<li>Decoder: neural language model.</li>
<li>Learn joint embedding space of images and text. Natural definition of scoring function.</li>
<li>Ex. Fluffy.</li>
<li>Multimodal linguistic regularities: Addition and subtraction. (Cat - bowl + box)
<ul>
<li>Bird and reflection: Two birds are trying to be seen in the water.</li>
<li>Giraffe is standing next to a fence in a field.</li>
<li>Handlebars are trying to ride a bike rack.</li>
</ul></li>
<li>Caption generation with visual attention.</li>
<li>Generate images from captions. (school bus flying in blue skies)</li>
<li>Helmholtz machines/variational autoencoders. Directed counterparts. Generative process goes down. Approximate inference going up. Hinton95 (Science). Now it works, Kingma2014 (NIPS)
<ul>
<li>A toilet seat sits open in the bathroom, grass field</li>
<li>Ask google. <!--worked on toilet project--></li>
</ul></li>
</ul></li>
</ul>
<h2 id="open-problems">Open problems</h2>
<ul>
<li>Unsupevised learning/transfer learning/one-shot learning</li>
<li>Reasoning, attention, memory</li>
<li>Natural language understanding
<ul>
<li>Sequence-to-sequence learning</li>
<li>Skip-thought model
<ul>
<li>Generate previous and forward sentence</li>
<li>Objective: sum of log-probabilities for previous/next sentence conditioned on current.</li>
<li>How similar are 2 sentence are on the scale 1 to 5. (A person is performing a trick on a motorcycle? A person is tricking a man on a motorcycle.)</li>
<li>We use no semantic features. <!-- AdaSent --></li>
</ul></li>
</ul></li>
<li>Deep reinforcement learning</li>
</ul>
<p>Neural storytelling. Take corpus of books (romantic), generate captions about the image.</p>
<p>Kiros2015 NIPS</p>
<p>One-shot learning: humans vs. machines. How can we learn novel concept from few examples (Lake, S, Tenenbaum 2015, Science)</p>
<h2 id="questions">Questions</h2>
<p>CNN better for supervised. We’re trying to build convolutional DBM.</p>
<p>vs. variational autoencoder. Reparametrization trick, backprop through whole model. Optimization better for VA. Both useful.</p>
<p>Learning representation, not with language?</p>
<!-- evaluation
neural image on google $10^5$
-->
<p>Microsoft dataset: 80000 images, 5 captions each. Not big enough, but captions clean!</p>
<p>Topics vs. coherent model of sentences. What do we need? New architectures, training sets? <!--need rep to corresp with reality. have memory, check for consistency with memory --></p>
<!--AlphaGo is more technological. Fast, evaluating -->
<p>Actor-Mimic model.</p>
<p>Transfer learning: learn new games faster by leveraging knowledge about previous games. Ex. star gunner</p>
<p>Continuous state.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2016-10-22</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-10-22.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-10-22.html</id>
    <published>2016-10-19T00:00:00Z</published>
    <updated>2016-10-19T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2016-10-22</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-19 
          , Modified: 2016-10-19 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#threads">Threads</a></li>
 <li><a href="#other-papers">Other papers</a></li>
 <li><a href="#talk-with-arora-1019-wed">Talk with Arora 10/19 (Wed)</a></li>
 <li><a href="#wonderings">Wonderings</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="threads">Threads</h2>
<ul>
<li>PMI - get some results!</li>
<li>SoS - chapters 2 and 3</li>
<li>DL: do experiments suggested in <a href="/posts/tcs/machine_learning/matrices/DL_generalization.html">DL generalization</a> (Mon, Tue)
<ul>
<li>(*) NN learns DL. (Mon, Tue) - Wrote up progress so far, where I am stuck.</li>
</ul></li>
<li>Papers
<ul>
<li>[HM16] on unsupervised learning (went through 1st half, Tue)</li>
<li>[HMR16] on dynamical system learning (read <a href="http://www.offconvex.org/2016/10/13/gradient-descent-learns-dynamical-systems/">blog post</a> Tue)</li>
</ul></li>
<li>Come up with a class of MDPs on exponential space that is interesting and tractable. <a href="/posts/tcs/machine_learning/reinforcement_learning/exponential.html">Thoughts</a>
<ul>
<li>Understand provable guarantees on MDP’s first</li>
</ul></li>
<li>Alexa <a href="https://docs.google.com/document/d/1OtvefjviKSSWH2gzOtYo8T_DVEwPEsI2n0kdrC8WlZI/edit">references</a></li>
</ul>
<p>Analyze Arora and Ge’s NMF algorithm in the presence of noise. Exactly how much noise can it tolerate?</p>
<h2 id="other-papers">Other papers</h2>
<ul>
<li>TODO Read this paper: [CFP16] Assessing significance in a Markov chain without mixing</li>
</ul>
<h2 id="talk-with-arora-1019-wed">Talk with Arora 10/19 (Wed)</h2>
<p>Dynamical systems + MDP!</p>
<h2 id="wonderings">Wonderings</h2>
<ul>
<li>Can we generalize the random walk of the context vectors? There’s no reason to think that context vectors just drift on the sphere. (p. 139)
<ul>
<li>Make it a RBM. Say with bounded degree. (There are ways to learn - see the factored MDP paper. We don’t care about MDP here, so it’s easier.)</li>
<li>For example, one node (dimension) could simply control output of common words.</li>
<li>Given the observations, whose probs are <span class="math inline">\(\propto e^{w^TAc}\)</span>, learn the RBM. (Note we can replace <span class="math inline">\(w\)</span> by <span class="math inline">\(A^Tw\)</span>… but if <span class="math inline">\(c\)</span> is in larger space, then it’s not obvious how to learn the <span class="math inline">\(A\)</span>! Can we modify the word embeddings to deal with this? Beware of difficulties… HMMs usually assume full column-rank observations, violated here. Look at the proper hard instance for HMM. - the version I saw with noisy parity wasn’t quite a HMM)</li>
<li>Prereq: given <span class="math inline">\((x,h)\)</span> how to learn RBM or Bayes net? (When <span class="math inline">\(W\)</span>’s entries are small enough, can do via MCMC estimation of partition function and optimization of log-likelihood. Otherwise, is hard worst-case.)
<ul>
<li>I’m confused! There seems to be a line of work on factorial MDP’s. However, where are the basic results about learnability of Bayes nets? Learning the model for FMDP’s is strictly harder—why so much work on this (with too much assumptions, or weak results) without results on learning Bayes nets?</li>
<li>Bresler.</li>
</ul></li>
<li>cf. work on continuous HMM’s. Work on factored HMMs? Any bounds when hidden state has larger dimension? Also, adapt HMM learning to vector observations. (Is the natural generalization a factored prob model rather than a dynamical system? Note probabilistic linear dynamical system IS straightforward generalization of HMM, but the factored prob model is not. Weird generalization though, because only having states <span class="math inline">\(\{e_1,\ldots, e_n\}\)</span> seems decoupled - can couple together any way you want.) <span class="citation" data-cites="Andrej">@Andrej</span> on this.
<ul>
<li>When state has larger dimension, need overcomplete tensor factorization.</li>
</ul></li>
<li>Start with: given an HMM with both transitions and observations being RBMs (say of degree at most 2), observations don’t “lose info” (analogue of full column rank), infer RBM. (Z is over words that exist). Breaks symmetries - the various dimensions are important now? <!-- sparse vectors are meaningful --></li>
</ul></li>
<li>Dictionary learning experiment
<ul>
<li>The kernel DL I want is different from in the literature. There they want <span class="math inline">\(\Phi(Y) \approx \Phi(A)X\)</span>, here we want <span class="math inline">\(\Phi(Y) \approx \Phi(AX)\)</span>. I.e. we want to maximize <span class="math inline">\(K(Y,AX)\)</span> where <span class="math inline">\(X\)</span> is restricted to be sparse. Usual algorithms break down here, but can still consider <span class="math inline">\(K(Y,AX) + \ve{X}_1\)</span>. (137)</li>
<li>Use kernel in <a href="/posts/tcs/machine_learning/neural_nets/PMDH16.html">PMDH16</a>.</li>
</ul></li>
<li>RL questions (135)
<ul>
<li>Given a (continuous) space of policies, converge to a local min in the space of policies.</li>
<li>Find some measure of complexity of a class of policies. Branching is important. (Getting limited info from other policies…) Get a bound independent of number of states, involving this complexity.
<ul>
<li>Example to keep in mind: <span class="math inline">\(2^n\)</span> strategies all branching off into different rewards at end of their paths.</li>
<li>Alternatively, complexity of class of models of environment.</li>
</ul></li>
<li>What is the VC dimension bound for contextual (expert) bandits? Also look at the contextual MDP paper, cf. EXP4.</li>
<li>[ALA16] open question</li>
<li>Scraps
<ul>
<li>Right <span class="math inline">\(\la\)</span>, how do well without learning model?</li>
<li>SoS, minimax, etc.</li>
<li>EXP3:Scrible:EXP4::UCB1:?:? (LinUCB? RUCB?)</li>
</ul></li>
<li>Increasing <span class="math inline">\(\ga\)</span> towards 1 (simulated annealing, temperature schedule…)</li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>DL experiments</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/dl_experiments.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/dl_experiments.html</id>
    <published>2016-10-17T00:00:00Z</published>
    <updated>2016-10-17T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>DL experiments</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-17 
          , Modified: 2016-10-17 
	</p>
      
       <p>Tags: <a href="/tags/dictionary%20learning.html">dictionary learning</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#experiments">Experiments</a></li>
 <li><a href="#results">Results</a><ul>
 <li><a href="#first-observations">First observations</a></li>
 </ul></li>
 <li><a href="#evaluation">Evaluation</a></li>
 <li><a href="#code">Code</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>What are convergence guarantees for dictionary learning? Consider the settings</p>
<ul>
<li>AGMM15 (Alternating minimization)</li>
<li>2-layer NN
<ul>
<li>With <span class="math inline">\(b^Ty\)</span></li>
<li>With <span class="math inline">\(\sgn(b^Ty)\)</span></li>
</ul></li>
</ul>
<!--
Sanjeev told me you did some experiments, so I wanted to check with you.

Experimentally, does dictionary learning converge to the right dictionary under random initialization? What if we randomly initialize with samples drawn from $x=Ah$? What about for the neural net (backprop) model you showed me last time - does random initialization (with samples) converge to the dictionary? If you have code for experiments, please send it to me.

I've done a lot of the calculations for neural nets learning dictionaries, and am getting stuck on the following: it appears that the gradient of the entire matrix is correlated with the right direction, but individual rows may not be (so a row may get far away until it no longer decodes correctly). Did you encounter something like this? If you have the bandwidth I'd be interested in working with you on this.

-->
<h2 id="experiments">Experiments</h2>
<p>Code is in <code>dl_convergence.py</code>. Run on ionic.</p>
<h2 id="results">Results</h2>
<!--1218589: -->
<ul>
<li>s = 3</li>
<li>m = 50 # hidden vector</li>
<li>n = 25 # observed vector</li>
<li>q = s/m</li>
<li>alpha = .01</li>
<li>batchsize = 1024</li>
<li>vary sigma (how close initialization is) <!-- * Approximate convergence for sigma = .05, .1; not 0.5--></li>
</ul>
<p>Next,</p>
<ul>
<li>add random initialization - check</li>
<li>vary (s,m,n)</li>
<li>check sparsity of learned vectors (do thresholding too) - check</li>
<li>add initialization from samples - check
<ul>
<li>try overcomplete initialization from samples - check</li>
</ul></li>
</ul>
<h3 id="first-observations">First observations</h3>
<p>See <code>am_dl_3_50_25.txt</code> and <code>slurm-1218768.out</code></p>
<ul>
<li>Converges when close enough (as in AGMM15). For this, even 0.5 is close enough. Note it doesn’t converge to <span class="math inline">\(A\)</span> - it converges to something that has columns <span class="math inline">\(\approx 0.1\)</span> away from <span class="math inline">\(A\)</span>, consistant bias. (This makes sense.)</li>
<li>Random initialization does not converge to global optimum. Initialization with samples seems to do slightly better.</li>
</ul>
<h2 id="evaluation">Evaluation</h2>
<p>How to evaluate?</p>
<ul>
<li>Closeness of columns.</li>
<li>Loss: how much sparsity, and how far away. (Reconstruction error)
<ul>
<li>How does reconstruction error compare to SVD? (Make dimensions comparable.)</li>
</ul></li>
<li>Put in random SVM on top. Can it learn the SVM well?</li>
<li>Check framework in [HM16].</li>
</ul>
<h2 id="code">Code</h2>
<ul>
<li>Displaying images
<ul>
<li><a href="http://stackoverflow.com/questions/902761/saving-a-numpy-array-as-an-image">No PIL</a></li>
<li><a href="http://stackoverflow.com/questions/2659312/how-do-i-convert-a-numpy-array-to-and-display-an-image">PIL</a></li>
<li><a href="https://pillow.readthedocs.io/en/3.4.x/reference/index.html">Pillow</a></li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>MDP's with continuous state space</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/continuous.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/continuous.html</id>
    <published>2016-10-14T00:00:00Z</published>
    <updated>2016-10-14T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>MDP's with continuous state space</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-14 
          , Modified: 2016-10-14 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#starting-points">Starting points</a></li>
 <li><a href="#model">Model</a><ul>
 <li><a href="#first-try">First try</a></li>
 <li><a href="#second-try">Second try</a></li>
 </ul></li>
 <li><a href="#references">References</a><ul>
 <li><a href="#online">Online</a></li>
 <li><a href="#books">Books</a></li>
 <li><a href="#papers">Papers</a></li>
 </ul></li>
 <li><a href="#misc">Misc</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="https://en.wikipedia.org/wiki/Kalman_filter">Kalman filter</a></p>
<p>Come up with a class of MDPs on exponentially large/continuous space that is interesting and tractable. Think of generalizing from contextual bandits * Basically we want a reasonable model of a MDP with a very large (exponential or continuous) state space and be able to do something with it. Wanted to include some dynamics like in Kalman filters but we weren’t sure whether Kalman filters are tractable * Todo: learn about Kalman filters</p>
<h2 id="starting-points">Starting points</h2>
<ol type="1">
<li>HMM’s have discrete state space. What happens with continuous state space? Suppose there are some dynamics as in Kalman filters. Infer the hidden state. References
<ul>
<li>Continuous HMM paper (RKHS)</li>
<li>Kalman filters (see examples)</li>
<li>Grad descent learning dynamical systems.</li>
</ul></li>
<li>Contextual bandits + MDP’s. Don’t assume there’s a hidden state here, just that next state depends, say, linearly on action and noise.</li>
<li>Context vector/random walk model for documents: transition probabilities <span class="math inline">\(\propto \exp(-\an{c_1,c_2})\)</span> and observation probabilities <span class="math inline">\(\propto \exp(-\an{c_1,x})\)</span>.</li>
</ol>
<h2 id="model">Model</h2>
<h3 id="first-try">First try</h3>
<ul>
<li>Stochastic setting.</li>
<li><span class="math inline">\(c_t\)</span> is context at time <span class="math inline">\(t\)</span>.</li>
<li>Set of actions <span class="math inline">\(A\)</span>. (For example, <span class="math inline">\(A=\{e_1,\ldots, e_n\}\)</span>.)</li>
<li>Next context <span class="math inline">\(c_{t+1}=\)</span> (Here <span class="math inline">\(w_t\)</span> is noise.)
<ul>
<li><span class="math inline">\(F_a c_t + w_t\)</span>. (Transformation depends on action.)</li>
<li><span class="math inline">\(F c_t + B a + w_t\)</span>. (Action is a forcing term. This matches Kalman formulation. More reasonable?) (*)</li>
</ul></li>
<li>Payoff depends on context and actions in some way.
<ul>
<li>Model 1: depends only on context <span class="math inline">\(\te^T c_t\)</span>. (*)</li>
<li>Model 2: depends on context and action <span class="math inline">\(\te^T[c_t;a]\)</span>.</li>
<li>? Some probability?</li>
</ul></li>
</ul>
<p>This setting looks like reinforcement learning + control theory. Prior work? How is RL used in continuous systems right now? Basic control theory background?</p>
<p>Need the model to be a generalization of regular MDP.</p>
<p>(*) may be interesting from control theory perspective, but doesn’t generalize discrete MDP. (Seems like best to learn the dynamics, and then do optimal thing from there…)</p>
<h3 id="second-try">Second try</h3>
<ul>
<li>Finite number of actions</li>
<li><span class="math inline">\(c_{t+1} = F_a c_t + w_t\)</span>. (Only probability is noise.)</li>
<li>Payout <span class="math inline">\(\te_^T c_t\)</span>.</li>
</ul>
<p>Captures deterministic MDP, but not probabilistic, by letting <span class="math inline">\(A=\{e_i\}\)</span>.</p>
<h2 id="references">References</h2>
<h3 id="online">Online</h3>
<ul>
<li><a href="http://castlelab.princeton.edu/">CASTLE Labs</a>
<ul>
<li><a href="http://optimallearning.princeton.edu/">Optimal learning</a></li>
<li><a href="http://adp.princeton.edu/">Approximate dynamic programming</a>
<ul>
<li><a href="http://adp.princeton.edu/adpIntros.htm">Intros</a></li>
</ul></li>
<li><a href="http://castlelab.princeton.edu/jungle.htm#unifiedframework">Unified framework</a></li>
</ul></li>
<li><a href="https://github.com/andrewliao11/Deep-Reinforcement-Learning-Survey/blob/master/Reinforcement-Learning-Papers.md">Deep RL</a></li>
</ul>
<h3 id="books">Books</h3>
<p><a href="https://www.quora.com/What-are-the-best-books-about-reinforcement-learning">Recommendations</a></p>
<ul>
<li><a href="https://books.google.com/books?id=VvBjBAAAQBAJ&amp;printsec=frontcover&amp;dq=continuous+markov+decision+processes&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjo3OLywOnPAhVHWD4KHXzgDWUQ6AEIKTAC#v=onepage&amp;q=continuous%20markov%20decision%20processes&amp;f=false">Puterman14</a></li>
<li><p><a href="https://books.google.com/books?id=-6RiQgAACAAJ&amp;dq=Dynamic+Programming:+Deterministic+and+Stochastic+Models&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjc0pfAyefPAhUGFz4KHaVIDecQ6AEIHjAA">Bertsekas87</a></p></li>
<li><a href="http://site.ebrary.com/lib/princeton/detail.action?docID=10560566">Optimal learning</a></li>
<li><p><a href="http://www.crcnetbase.com/isbn/9781439821091">Function approximators</a></p></li>
</ul>
<h3 id="papers">Papers</h3>
<ul>
<li><a href="http://people.csail.mit.edu/agf/Files/13FTML-RLTutorial.pdf">lin function approximators</a></li>
<li><a href="https://hal.archives-ouvertes.fr/hal-00747575v5/document">optimistic principle</a></li>
<li><a href="http://web.mit.edu/dimitrib/www/dpchapter.pdf">ADP</a></li>
<li><a href="http://site.ebrary.com/lib/princeton/reader.action?docID=10501323">Approximate DP</a></li>
<li><p><a href="http://www.morganclaypool.com/doi/abs/10.2200/S00268ED1V01Y201005AIM009">Algorithms for RL</a></p></li>
<li></li>
</ul>
<h2 id="misc">Misc</h2>
<p>Do as well as best Bayes net? Actions in some class. Finite set of actions, vs. exponential/continuous set of actions. In latter case, will depend on optimizability of that set…</p>
<p>Ex. class is a SVM.</p>
<p>“Do as well as best estimator of <span class="math inline">\(q\)</span> function in a certain class (assume convexity or something?)” (cf. contextual bandits first)</p>
<!--Definitely need something stronger than: there exist something that works! if can encode crypto 

Upper confidence bounds
-->

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Finite model theory</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/logic/finite_model_theory.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/logic/finite_model_theory.html</id>
    <published>2016-10-14T00:00:00Z</published>
    <updated>2016-10-14T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Finite model theory</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-14 
          , Modified: 2016-10-14 
	</p>
      
       <p>Tags: <a href="/tags/model%20theory.html">model theory</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#definability-and-undefinability">1 Definability and undefinability</a><ul>
 <li><a href="#expressive-power">Expressive power</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="https://simons.berkeley.edu/talks/finite-and-algorithmic-model-theory">Link</a></p>
<h2 id="definability-and-undefinability">1 Definability and undefinability</h2>
<ul>
<li>Expressive power of logics on class of finite structures.</li>
<li>Problems in computer science (complexity theory, database theory) are naturally questions about expressive power of logics.</li>
</ul>
<p><span class="math display">\[Mod(\ph) = \set{\mathbb A}{ \mathbb A \vDash \ph}.\]</span> Here <span class="math inline">\(\mathbb A\)</span> ranges over finite relational structures.</p>
<p>What classes of structures are definable? (Set of possible <span class="math inline">\(Mod(\ph)\)</span>’s for <span class="math inline">\(\ph\)</span> in logic <span class="math inline">\(\mathcal L\)</span>.)</p>
<ul>
<li>Syntactic restrictions on <span class="math inline">\(\ph\)</span> vs. semantic restrictions on <span class="math inline">\(Mod(\ph)\)</span>.</li>
<li>Computational complexity of <span class="math inline">\(Mod(\ph)\)</span> vs. syntactic complexity of <span class="math inline">\(\ph\)</span>.</li>
</ul>
<h3 id="expressive-power">Expressive power</h3>
<p>Relational vocabulary: finite set <span class="math inline">\(A\)</span> with relations <span class="math inline">\(R_1,\ldots, R_m\)</span> and constants <span class="math inline">\(c_1,\ldots, c_n\)</span>.</p>
<p>A property of finite structures is any isomorphism-closed class of structures. (Morphisms are permutations. Ex. graph ismomorphisms.) Given logic, for which properties <span class="math inline">\(P\)</span> is there a sentence <span class="math inline">\(\ph\)</span> such that <span class="math inline">\(\mathbb A\in P\iff A\vDash \ph\)</span>.</p>
<p>Ex. colored graphs: one binary relation <span class="math inline">\(E^2\)</span> and some unary relations <span class="math inline">\(C_i^1\)</span>. First-order logic formulas involve these relations and <span class="math inline">\(\wedge, \vee, \neg, \exists, \forall\)</span>.</p>
<ul>
<li>Vertex cover of size at most <span class="math inline">\(k\)</span>. (<span class="math inline">\(k\)</span> fixed)</li>
<li>3-colorability when allow quantification over sets of vertices.</li>
</ul>
<p>Compactness, completeness, preservation fail. (What are these exactly?)</p>
<p>Methods:</p>
<ul>
<li>Ehrenfeucht-Fraisse Games and related model-comparison games</li>
<li>Locality Theorems</li>
<li>Automata-based methods</li>
<li>Complexity</li>
<li>Asymptotic Combinatorics</li>
</ul>
<p>Equivalence means satisfying the same statements in the logic. On finite structures, two structures are equivalent iff they are isomorphic.</p>
<p>Quantifier rank:</p>
<ol type="1">
<li>For atoms, <span class="math inline">\(qr(\ph)=0\)</span>.</li>
<li><span class="math inline">\(qr(\neg \psi) = qr(\psi)\)</span>.</li>
<li><span class="math inline">\(qr(\psi_1\wedge/\vee \psi_2) = \max_i(qr(\psi_i))\)</span>.</li>
<li><span class="math inline">\(qr(\exists/\forall x \psi) = qr(\psi)+1\)</span>.</li>
</ol>
<p><span class="math inline">\(\mathbb A\equiv_p \mathbb B\)</span> iff for all <span class="math inline">\(qr(\ph)\le p\)</span>, <span class="math inline">\(A\vDash \ph\iff B\vDash \ph\)</span>.</p>
<p><span class="math inline">\(S\)</span> is definable by first order sentence iff <span class="math inline">\(S\)</span> is closed under <span class="math inline">\(\equiv_p\)</span> for some <span class="math inline">\(p\)</span>. (13) ?? (Is <span class="math inline">\(S\)</span> on finite set? What does “finite relational vocab” mean?) (Is this trivial by taking <span class="math inline">\(p=|A|\)</span>?) (NO: <span class="math inline">\(A\)</span> is of arbitrary finite size!)</p>
<p>Ex. I think connectedness is not first-order!</p>
<p>Define <strong>partial isomorphism</strong>.</p>
<p>Ehrenfeucht-Fraisse game:</p>
<p>For <span class="math inline">\(p\)</span> rounds:</p>
<ul>
<li>Spoiler choose one structure and an element of that structure.</li>
<li>Duplicator responds with element of other structure <span class="math inline">\(a_i\)</span>.</li>
</ul>
<p>After <span class="math inline">\(p\)</span> rounds, Duplicator wins if <span class="math inline">\(a_i\mapsto b_i\)</span> is partial iso. Duplicator has winning strategy iff <span class="math inline">\(\mathbb A\equiv_p \mathbb B\)</span>.</p>
<p>Proof: choose the witnesses of existence, going to the other structure for a <span class="math inline">\(\forall\)</span> because negated <span class="math inline">\(\forall\)</span> gives <span class="math inline">\(\exists\)</span>.</p>
<p>So to show not definable, for every <span class="math inline">\(p\)</span> produce <span class="math inline">\(\mathbb A_p\)</span>, <span class="math inline">\(\mathbb B_p\)</span> such that one is in <span class="math inline">\(S\)</span>, and duplicator wins.</p>
<p>Ex. Not definable: 2-colorability, even cardinality, connectivity.</p>
<p>(Duplicator’s strategy is to ensure that after r moves, the distance between corresponding pairs of pebbles is either equal or <span class="math inline">\(2p^r\)</span>.)</p>
<p>Alternative: stratify by number of free variables (in any sub-formula), <span class="math inline">\(\equiv^k\)</span>. <span class="math inline">\(\equiv^k\implies \equiv_k\)</span>.</p>
<p>Connectivity and 2-colorability are axiomatizable in <span class="math inline">\(L^k\)</span> (define <span class="math inline">\(path_{\le l}\)</span>, <span class="math inline">\(disconnect_l\)</span>); even cardinality is not. (I’m confused… why can we reuse variables?? 21)</p>
<p><span class="math inline">\(\equiv_p, \equiv^k\)</span> have finitely/infinitely many equivalence classes.</p>
<p>How is the pebble game different?? 23</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Language games</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/language_games.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/language_games.html</id>
    <published>2016-10-13T00:00:00Z</published>
    <updated>2016-10-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Language games</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-13 
          , Modified: 2016-10-13 
	</p>
      
       <p>Tags: <a href="/tags/nlp.html">nlp</a>, <a href="/tags/language.html">language</a>, <a href="/tags/evolution.html">evolution</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#wlm16-learning-language-games-through-interaction">[WLM16] Learning Language Games through Interaction</a></li>
 <li><a href="#fafw16-learning-to-communicate-with-deep-multi-agent-reinforcement-learning">[FAFW16] Learning to Communicate with Deep Multi-Agent Reinforcement Learning</a><ul>
 <li><a href="#task">Task</a></li>
 <li><a href="#model">Model</a></li>
 <li><a href="#conclusions">Conclusions</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <p>How does language develop?</p>
<p>How to learn a language from scratch?</p>
<p>Wittgenstein’s “language games”.</p>
<h2 id="wlm16-learning-language-games-through-interaction">[WLM16] Learning Language Games through Interaction</h2>
<p>See ML seminar notes.</p>
<h2 id="fafw16-learning-to-communicate-with-deep-multi-agent-reinforcement-learning">[FAFW16] Learning to Communicate with Deep Multi-Agent Reinforcement Learning</h2>
<p>Cooperative learning of communication protocols.</p>
<p>see also Kasai [8].</p>
<h3 id="task">Task</h3>
<p>Multiple agents in fully cooperative, partially observable, sequential multi-agent decisiom-making problems. Each gets a private observation of the Markov state.</p>
<ul>
<li>Centralized learning (unrestricted communication)</li>
<li>Decentralized execution (communicate only by discrete limited-bandwidth channel)</li>
</ul>
<p>Actual tasks</p>
<ul>
<li>Switch riddle
<ul>
<li>DIAL &gt; RIAL &gt; Baseline</li>
</ul></li>
<li>MNIST games: see 2 MNIST digits of some color. Reward depends on action, color, and parity. Send 1 bit of info. Agree to send either color or parity (parity better). (DIAL seems to get optimal here. RIAL fails.)</li>
</ul>
<h3 id="model">Model</h3>
<ul>
<li>Reinforced inter-agent learning (RIAL)
<ul>
<li>Deep Q-learning</li>
<li>Independent Q-learning: learn own network parameters, treat other agents as part of environment.</li>
<li>Deep recurrent Q-network. [17]
<ul>
<li><ul>
<li>independent Q-learning = RIAL.</li>
</ul></li>
<li>Disable experience replay (experience obsolete and misleading)</li>
</ul></li>
</ul></li>
<li>Differentiable inter-agent learning (DIAL)
<ul>
<li>Takes advantage of centralized learning.</li>
<li>RIAL is only end-to-end within agent.</li>
<li>Allows real-value messages to pass.</li>
<li>(This is not realistic between agents in terms of evolution. But it can make sense within agents - ex. different brain parts)</li>
<li>During centralized learning, communication replaced with direct connections between output of one agent’s network and input of another’s.</li>
</ul></li>
</ul>
<p>Difficulty: positive rewards are sparse, arising only when sending and interpreting are properly coordinated.</p>
<h3 id="conclusions">Conclusions</h3>
<p>Why is language discrete? Noise forces messages into 2 different modes.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Alexa</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/alexa.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/alexa.html</id>
    <published>2016-10-13T00:00:00Z</published>
    <updated>2016-10-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Alexa</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-13 
          , Modified: 2016-10-13 
	</p>
      
       <p>Tags: <a href="/tags/nlp.html">nlp</a>, <a href="/tags/dialogue.html">dialogue</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#questions">Questions</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="https://docs.google.com/document/d/1OtvefjviKSSWH2gzOtYo8T_DVEwPEsI2n0kdrC8WlZI/edit">References</a></p>
<ul>
<li>Dialogue
<ul>
<li>[LMRG16] Deep reinforcement learning for dialogue generation <a href="https://arxiv.org/pdf/1606.01541.pdf">paper</a></li>
<li>[DGZB16] EVALUATING PREREQUISITE QUALITIES FOR LEARNING END-TO-END DIALOG SYSTEMS <a href="https://arxiv.org/pdf/1511.06931v6.pdf">paper</a></li>
<li>[VL15] A Neural Conversational Model <a href="https://arxiv.org/pdf/1506.05869.pdf">paper</a></li>
</ul></li>
<li>Neural nets
<ul>
<li>[SSWF15] End-To-End Memory Networks
<ul>
<li>Architecture
<ul>
<li>Input <span class="math inline">\(x_i\)</span></li>
<li>Convert input into memory <span class="math inline">\(m_i = A x_i\)</span>, <span class="math inline">\(A\in \R^{d\times V}\)</span></li>
<li>Output vecor <span class="math inline">\(c_i = C x_i\)</span>.</li>
<li>Query <span class="math inline">\(q\)</span></li>
<li>Embedded query <span class="math inline">\(u=Bq\)</span>.</li>
<li>Match between queries and memory <span class="math inline">\(p_i = \text{softmax}(u^Tm_i)\)</span>.</li>
<li>Response is weighted sum <span class="math inline">\(o = \sum_i p_ic_i\)</span>.</li>
<li>Predicted answer <span class="math inline">\(\wh a = \text{softmax}(W(o+u))\)</span>.</li>
</ul></li>
<li>For multiple layers, <span class="math inline">\(u^{k+1}=u^k + o^k\)</span>. Each layer has <span class="math inline">\(A^k,C^k\)</span> to embed inputs.
<ul>
<li>Two types of weight tying
<ul>
<li>output = input above, <span class="math inline">\(A^{k+1}=C^k\)</span>.</li>
<li>RNN: <span class="math inline">\(A^k =A, C^k=C\)</span>, and modify <span class="math inline">\(u^{k+1} = H u^k + o^k\)</span>.</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li>OpenSubtitles
<ul>
<li><a href="http://www.opensubtitles.org/en/search">Main page</a></li>
<li><a href="https://datahub.io/dataset/opus/resource/e5a441a7-73d5-4f8c-a4b5-4bab42a739f2">?</a></li>
</ul></li>
</ul>
<h2 id="questions">Questions</h2>
<p>How does the beam search actually work? Do we just have <span class="math inline">\(\Pj(w_1)\Pj(w_2|w_1)\Pj(w_3|w_2,w_1)\)</span> or is there some more complicated energy-based model? (Don’t normalize?)</p>
<p>How to promote consistency in answers? Hidden state.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Interpretable neural nets</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/interpretability.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/interpretability.html</id>
    <published>2016-10-12T00:00:00Z</published>
    <updated>2016-10-12T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Interpretable neural nets</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-12 
          , Modified: 2016-10-12 
	</p>
      
       <p>Tags: <a href="/tags/neural%20nets.html">neural nets</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p>Enforce sparsity or nonnegativity.</p>
<ul>
<li>[NTPV13] Learning Parts-based Representations with Nonnegative Restricted Boltzmann Machine <a href="http://www.jmlr.org/proceedings/papers/v33/min14.pdf">paper</a>
<ul>
<li>Enforce nonnegativity in RBM by having a regularizer that’s a quadratic barrier function <span class="math inline">\(\min(0,x)^2\)</span>.</li>
</ul></li>
<li>[MNCG14] Interpretable Sparse High-Order Boltzmann Machines <a href="http://jmlr.org/proceedings/papers/v29/Nguyen13.pdf">paper</a></li>
<li>[KV16] Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models <a href="https://arxiv.org/abs/1606.05320">paper</a></li>
</ul>
<p>link to an article on extracting interpretable features out of deep nets? I forget the authors. Maybe Salakhutdinov was one of them.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Hidden Markov Models</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/probabilistic/hmm.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/probabilistic/hmm.html</id>
    <published>2016-10-11T00:00:00Z</published>
    <updated>2016-10-11T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Hidden Markov Models</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-11 
          , Modified: 2016-10-11 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#sch16-unsupervised-part-of-speech-tagging-with-anchor-hidden-markov-models">[SCH16] Unsupervised Part-Of-Speech Tagging with Anchor Hidden Markov Models</a></li>
 <li><a href="#misc">Misc</a></li>
 </ul> </div>

  <div class="blog-main">
    <ul>
<li>[HKZ12] A spectral algorithm for learning Hidden Markov Models</li>
<li>Continuous HMM [LBGS10] [paper](http://repository.cmu.edu/cgi/viewcontent.cgi?article=1057&amp;context=machine_learning)</li>
</ul>
<p><strong>Question</strong>: how robust to noise are these?</p>
<h2 id="sch16-unsupervised-part-of-speech-tagging-with-anchor-hidden-markov-models">[SCH16] Unsupervised Part-Of-Speech Tagging with Anchor Hidden Markov Models</h2>
<p>Reduce anchor-HMM to separable NMF. In anchor-HMM for every hidden state, there is an observed state that can only come from that hidden state. The anchor observations for <span class="math inline">\(h\)</span> are <span class="math display">\[
A(h) = \set{x\in [n]}{o(x|h)&gt;0\wedge o(x|h')=0\forall h'\ne h}.
\]</span> Let <span class="math inline">\(T_{h'h} = t(h'|h)\)</span> denote transition probabilities and <span class="math inline">\(O_{xj}=o(x|h)\)</span> denote observation probabilities.</p>
<p>The key is to define random variables <span class="math inline">\(Y_I\)</span> depending on <span class="math inline">\(H_I\)</span> (nontrivially) so that <span class="math inline">\(\Pj(Y_I|H_I,X_I) = \Pj(Y_I|H_I)\)</span>. We can take <span class="math inline">\(Y_I=X_{I+1}\)</span>! (More accurately, it’s a vector, <span class="math inline">\([Y_I]_{x'} = (X_{I+1}=x')\)</span>.)</p>
<p>Let <span class="math inline">\(\wt O_{xh} = \Pj(h|x)\)</span>, <span class="math inline">\((\Om_X)_{yx} = \Pj(Y_I=y|X_I=x)\)</span>, <span class="math inline">\(\Te_{hy} = \EE(Y_I=y|H_I=h)\)</span>. Then <span class="math display">\[\Om = \wt O \Te\]</span> is a separable NMF.</p>
<p>Brown model is an especially nice A-HMM where the anchors partition the set of all observations.</p>
<h2 id="misc">Misc</h2>
<p>The unsupervised log-linear model described in Berg-Kirkpatrick et al. (2010).</p>
<p>Agnostic HMM? Can spectral methods work?</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
