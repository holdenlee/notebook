<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2018-11-07T00:00:00Z</updated>
    <entry>
    <title>Weekly summary 2018-11-10</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2018-11-07.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2018-11-07.html</id>
    <published>2018-11-07T00:00:00Z</published>
    <updated>2018-11-07T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2018-11-10</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2018-11-07 
          , Modified: 2018-11-07 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#projects">Projects</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="projects">Projects</h2>
<ul>
<li>Multimodal sampling
<ul>
<li>Show mixing for Dirichlet process sampling. (This is a basic distribution and Markov chain, which is often combined with other things, e.g. in Dirichlet process mixture models.)</li>
<li>Show that Gibbs sampling (keep track of cluster membership, sample from mean, and then re-cluster) with merge/split succeeds for mixture of Gaussians.
<ul>
<li>Big problem: how to do merge/split step.
<ul>
<li>In general this is NP-hard. (2-means is NP-hard.)</li>
<li>Use a good enough proposal distribution, like 1 step of Gibbs in each coordinate.</li>
</ul></li>
<li>First do the well-separated case. Instead of canonical paths, use a “Lyapunov function” argument to show that it’s on average getting closer to the right clustering.</li>
<li>Note I keep track of the cluster assignments, not the means. The cluster assignments can “integrate out” the mixing coefficients, not so with the means (?). (My original idea was to do Gibbs with each mean, hoping it’s a tractable multimodal distribution (mixture of gaussians)</li>
</ul></li>
<li>Tensor decomposition
<ul>
<li>Overcomplete tensor decomposition (noiseless), [GM16]</li>
<li><span class="math inline">\(\la u^{\ot 3} + \rc{\sqrt n}W\)</span>, [ADGM16]</li>
<li>Simply flattening out doesn’t seem to work, because it doesn’t increase the “attraction region.”</li>
<li>Can try convolving with gaussian on the sphere to flatten local minima (but will this increase attraction region? do we need that?)</li>
<li>Check previous notebook first (around 2017/9)</li>
</ul></li>
<li>Short-term long-term memory (Koolen &amp; Warmuth’s problem)
<ul>
<li>This is like clustering but with an exponential factor that disincentivizes switches at adjacent times (the graph is a line). Does that make it easier or harder? Seems like it could be easier since you don’t effectively get <span class="math inline">\(2^n\)</span> possible splits. The split step is simple, you can check all possibilities. (Problem: if local is a lot better than the global… but maybe if generative model is true this won’t be true?)</li>
<li>Use log-concavity.</li>
<li>Or: do sequential MC, e.g. with particle filters, estimating Z.
<ul>
<li>In the generative case, each correct mode will have enough mass (?).</li>
</ul></li>
</ul></li>
<li>Other possibilities
<ul>
<li>Dictionary learning, sparse coding (?). Problem: if not incoherent, can get stuck in local min?</li>
<li>Sparse logistic regression. Phase transitions, hardness results - check them out.</li>
<li>Inspire NN training.
<ul>
<li>Split/merge or delete/add.</li>
</ul></li>
</ul></li>
</ul></li>
<li>Online sampling
<ul>
<li>See followups.</li>
<li>Experiments.</li>
<li>For discrete distributions (main thing: need <span class="math inline">\(f\)</span>’s to change slowly, OK with Gibbs sampling coordinate-by-coordinate, not OK with bipartite Gibbs sampling)</li>
<li>Understand RBM training, how can we help there? What’s the right way to take the stochastic gradient there and can we reduce the variance? (may not satisfy above conditions…)</li>
<li>Check out ICML paper on stochastic gradient Gibbs sampling.</li>
</ul></li>
<li>LDS
<ul>
<li>Main obstacle was “projection to random subspace”, use the insight that directions are almost orthogonal</li>
<li>Identification, using Laplace instead of Fourier, <span class="citation" data-cites="Musco">@Musco</span></li>
</ul></li>
<li>Compositional function spaces: <a href="https://dynalist.io/d/80BlcNrzxATvu5wf__C99MZe#z=Hn0y1aMB_FTQLK7GhugDO15S">dynalist</a></li>
<li>Probabilistic model for grammar</li>
<li>Things to clarify
<ul>
<li>The relationship between sampling and optimization (see [RN], Cesa-Bianchi…)</li>
<li>What is a Bayesian relaxation? What are examples of its use? (Check out “sleeping” - is it an example?) Relationship between the finite (ex. k-means) and infinite (ex. CRP) versions.</li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>(CW17) Adversarial Examples Are Not Easily Detected - Bypassing Ten Detection Methods</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/CW17.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/CW17.html</id>
    <published>2018-04-05T00:00:00Z</published>
    <updated>2018-04-05T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>(CW17) Adversarial Examples Are Not Easily Detected - Bypassing Ten Detection Methods</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2018-04-05 
          , Modified: 2018-04-05 
	</p>
      
       <p>Tags: <a href="/tags/neural%20nets.html">neural nets</a>, <a href="/tags/adversarial%20examples.html">adversarial examples</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#takeaways">Takeaways</a><ul>
 <li><a href="#recommendations-for-defenses">Recommendations for defenses</a></li>
 </ul></li>
 <li><a href="#attack-models">Attack models</a><ul>
 <li><a href="#carlini-and-wagners-attack">Carlini and Wagner’s attack</a></li>
 </ul></li>
 <li><a href="#the-10-defenses">The 10 defenses</a></li>
 <li><a href="#more-on-mmd-defense">More on MMD defense</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>Carlini and Wagner defeat 10 proposed methods of detecting adversarial examples for neural networks.</p>
<p>It seems easier produce a detector for adversarial examples (where you don’t have to classify then correctly) than classify them correctly, so this is a strong result.</p>
<h2 id="takeaways">Takeaways</h2>
<ul>
<li>Randomization can increase the distortion required to fool the detector. The most effective defense was one which used dropout.</li>
<li>Defenses which work on MNIST do not necessarily work on CIFAR, etc. MNIST has properties which make it easier to defend.</li>
<li>Detectors based on neural networks were the least effective (possibly because you can easily take gradients through them).</li>
<li>Defenses operating on the raw pixel values are ineffective.</li>
</ul>
<h3 id="recommendations-for-defenses">Recommendations for defenses</h3>
<p>They recommend:</p>
<ul>
<li>Evaluate using a strong attack - do not just use FGSM (which only takes 1 step). For many of the defenses considered, a black-box attack using C&amp;W’s (strong) attack succeeds.</li>
<li>Demonstrate white-box attacks fail.</li>
<li>Report false positive and true positive rates</li>
<li>Evaluate on more than MNIST.</li>
<li>Release source code!</li>
</ul>
<h2 id="attack-models">Attack models</h2>
<p>They consider 3 attack models, in order of increasing power:</p>
<ol type="1">
<li>Zero-knowledge (black box): the attacker has no knowledge of what the detector is. (Attack: take another neural net trained on the same data and make adversarial examples against it.)</li>
<li>Limited-knowledge: the attacker knows the architecture of the detector, but not the weights/the examples that it was trained on. (The attacker can train the same architecture on another sample from the same distribution, and craft adversarial examples against that.)</li>
<li>Perfect-knowledge (white box): the attacker knows everything (all the parameters, so they can take gradients, etc.).</li>
</ol>
<h3 id="carlini-and-wagners-attack">Carlini and Wagner’s attack</h3>
<p>Known as “C&amp;W’s attack”.</p>
<p>In words: given <span class="math inline">\(x\)</span>, and given a target class <span class="math inline">\(t\)</span>, find <span class="math inline">\(x'\)</span> that is</p>
<ul>
<li>close to <span class="math inline">\(x\)</span> (in <span class="math inline">\(L^2\)</span> norm)</li>
<li>and the difference between the <span class="math inline">\(Z(x')_t\)</span> (the predicted logit for class <span class="math inline">\(t\)</span> for <span class="math inline">\(x'\)</span>) and the largest remaining class is as large as possible.</li>
</ul>
<p>There is a tradeoff as you can weight these two objectives differently (do a binary search to find the best tradeoff).</p>
In symbols (needs a parameter <span class="math inline">\(c\)</span> and <span class="math inline">\(\ka\)</span>)
<span class="math display">\[\begin{align}
&amp;\min_{x'}\ba{\ve{x'-x}_2^2 + c\ell(x')}\\
\ell(x') :&amp;= \max
\pa{
\max
\set{Z(x')_i}{i\ne t}
-Z(x')_t,-\kappa
}
\end{align}\]</span>
<h2 id="the-10-defenses">The 10 defenses</h2>
<p>There are 3 defenses that use neural nets, 3 that use PCA, 2 that are statistical.</p>
<p>The table records how the attacks do. (A means attack succeeds, D means defense succeeds.)</p>
<table style="width:39%;">
<colgroup>
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 8%" />
</colgroup>
<thead>
<tr class="header">
<th>Defense (citation number in paper)</th>
<th>Description</th>
<th>Zero-Knowledge</th>
<th>Limited-Knowledge</th>
<th>Perfect-Knowledge</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>[15] (NN)</td>
<td>Train for (N+1)-way classification, additional class for adversarial examples.</td>
<td>D</td>
<td>A</td>
<td>A</td>
</tr>
<tr class="even">
<td>[12] (NN)</td>
<td>Train a NN to classify between real/adversarial. (Reduce to [15].)</td>
<td>D</td>
<td>A</td>
<td>A</td>
</tr>
<tr class="odd">
<td>[18] (NN)</td>
<td>Train a detection network that takes input from intermediate layers.</td>
<td>D (mostly)</td>
<td>A</td>
<td>A</td>
</tr>
<tr class="even">
<td>[19] (PCA)</td>
<td>Authors noted that coefficients of adversarial examples for small principal components were unusually large - but this is an artifact of MNIST: border pixels are often 0 for real examples and nonzero for adversarial examples. Attack: restrict adversary to top k PC’s.</td>
<td>-</td>
<td>-</td>
<td>A</td>
</tr>
<tr class="odd">
<td>[4] (PCA)</td>
<td>Dimensionality reduction: project to top k PC’s, then feed through rest of network. Attack: restrict adversary to top k PC’s.</td>
<td>-</td>
<td>-</td>
<td>A</td>
</tr>
<tr class="even">
<td>[24] (PCA)</td>
<td>Do PCA on inner convolutional layers. Each layer has a detector which is a SVM on the coefficients in the PCA basis. Accept only if ALL detectors accept. Result: when trained against C&amp;W’s attack, SVM fails to separate real and adversarial examples well.</td>
<td>-</td>
<td>-</td>
<td>A</td>
</tr>
<tr class="odd">
<td>[11] (Stat)</td>
<td>Model final layer activations as a gaussian mixture model; reject if probability density is small on the data point. Attack: add clamped “negative log likelihood” term to the objective function of the attacker.</td>
<td>A (CIFAR), D (MNIST)</td>
<td>A</td>
<td>A</td>
</tr>
<tr class="even">
<td>[15] (Stat)</td>
<td>Attempts to detect whether the test distribution is different from the training distribution (ex. the test distribution is adversarial) using the MMD statistical test (see more below). This is defeated by a black-box attack!</td>
<td>A</td>
<td>A</td>
<td>A</td>
</tr>
<tr class="odd">
<td>[11] (Randomization)</td>
<td>Apply dropout. Repeat many times with different randomization. Idea is that for real inputs, different trials will tend to agree; if there is too large a discrepancy between different trials, reject.</td>
<td>D</td>
<td>D/A (need huge distortion to attack)</td>
<td>A (but need +80% distortion)</td>
</tr>
<tr class="even">
<td>[24]</td>
<td>Apply mean blur on 3x3 patches</td>
<td>D</td>
<td>-</td>
<td>A</td>
</tr>
</tbody>
</table>
<h2 id="more-on-mmd-defense">More on MMD defense</h2>
<p>(See [15] (Stat) in table above.)</p>
<p>Note that this defense takes a set of examples and tests whether it’s drawn from the same distribution as the train data - it doesn’t operate on a per-example basis. (So it also doesn’t “produce” a discriminator, it’s really a hypothesis test on distributions.)</p>
<p>The test considered in the paper only operates on the <em>inputs</em>, not hidden layer activations (potentially, considering the hidden layers can help! But it doesn’t seem like this was done.).</p>
<p>The MMD operates as follows. Given a class of functions (e.g. they use a (ball in) Gaussian kernel space), find the function for which <span class="math inline">\(\hat \E f(x_i) - \hat \E f(y_i)\)</span> is largest where <span class="math inline">\(\{x_i\}\)</span> and <span class="math inline">\(\{y_i\}\)</span> are the two samples and <span class="math inline">\(\hat \E\)</span> means average. If this is significantly larger than when <span class="math inline">\(x_i,y_i\)</span> are drawn from the same distribution (the training distribution), then reject the hypothesis that they are drawn from the same distribution; otherwise fail to reject.</p>
<p>The authors found the defense successful against FGSM, JSMA, but Carlini and Wagner broke it with C&amp;W’s attack.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Combinatory Categorial Grammar</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/ccg.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/ccg.html</id>
    <published>2018-01-01T00:00:00Z</published>
    <updated>2018-01-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Combinatory Categorial Grammar</h1>
    </div>
    <div class="info">
       <div class="subtitle"><p>How we all speak in functions</p></div> 
       
        <p>Posted: 2018-01-01 
          , Modified: 2018-01-01 
	</p>
      
       <p>Tags: <a href="/tags/language.html">language</a>, <a href="/tags/grammar.html">grammar</a>, <a href="/tags/CCG.html">CCG</a>, <a href="/tags/NLP.html">NLP</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#todo">Todo</a></li>
 <li><a href="#blog-post">Blog post</a></li>
 <li><a href="#example-math-grammar">Example: Math grammar</a></li>
 <li><a href="#english">English</a><ul>
 <li><a href="#more-on-multiple-arguments-and-prepositions">More on multiple arguments, and prepositions</a><ul>
 <li><a href="#aside-other-languages">Aside: other languages</a></li>
 </ul></li>
 </ul></li>
 <li><a href="#what-else-to-say">What else to say?</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="https://docs.google.com/document/d/1fOOnfYQRWUgvEHg_vI9S9J2Nc2_RJprNwvyhAeClkXM/edit">googledoc</a> <a href="https://github.com/holdenlee/learn-grammar">code</a></p>
<h2 id="todo">Todo</h2>
<ul>
<li>Read papers.</li>
<li>Implement “identity”</li>
<li>Implement prepositional phrases.</li>
<li>Think about higher-level functions, or less exact matches with AST (like lambda x: f(x,c)). cf. nouns as functions/filters.</li>
<li>SHAPES/CLEVR: import data, make AST, try it.</li>
</ul>
<h2 id="blog-post">Blog post</h2>
<p>(Background: it will help to know about functions in programming, and grammar, ex. CFG’s.)</p>
<p><a href="https://dynalist.io/d/80BlcNrzxATvu5wf__C99MZe#z=E-IW9yTDg90oQYxVhl6-PR3k">DL</a></p>
<p>We’re doing functional programming every day in our natural speech without thinking about it.</p>
<p>Combinatory categorial grammar is one view of grammar that looks at parts of speech not just as tags but as “types”, like the kind of types you get in programming languages, including function types.</p>
<p>If you’re not familiar with functional programming, don’t worry. “Function” here means the same as it does in math: something that takes input(s) and gives an output. A given function can’t just take <em>any</em> input - it has to take input of a given “type”.</p>
<p>I will give a very simplified account of CCG’s, and touch on some subtleties and give references at the end.</p>
<h2 id="example-math-grammar">Example: Math grammar</h2>
<p>Math is a good playground for thinking of parsing natural language, because every math statement has a well-defined logical form associated with it. In contrast, it’s not clear what a “logical representation” of a arbitrary sentence such as “Carefully put the bag on the table” is.</p>
<p>So let’s jump right in with some math expressions. Our goal is to formalize a notion of grammar that we can use to program a computer to parse a sentence into a logical form.</p>
<p>With this goal in mind, let’s look at some expressions.</p>
<p><a href="https://github.com/holdenlee/MathGrammar">MathGrammar</a></p>
<blockquote>
<p>even prime number</p>
</blockquote>
<p>What does this mean? It means that we take the set of all numbers (here “number” means “natural number”), <em>filter</em> it for prime numbers, and then <em>filter</em> that for even numbers. “Even” and “prime” are adjectives, but here it makes sense to think of them as <em>functions</em> <code>Number -&gt; Boolean</code>. <code>Even</code> consumes a number as input and returns <code>True</code> if it is even and <code>False</code> otherwise. So this is saying</p>
<pre><code>filter isEven (filter isPrime) [1..]</code></pre>
<p>What about this?</p>
<blockquote>
<p>smallest even number that is greater than 6</p>
</blockquote>
<h2 id="english">English</h2>
<p>Let’s consider sentences built up of nouns, verbs, and adjectives. Instead of “parts of speech”, we’ll talk about “types” (or “categories”). One of the types will be <code>NP</code> for Noun (Phrase), and another will be <code>S</code> for sentence.</p>
<p>How do verbs and adjectives fit in?</p>
<p>(We want the types to <em>carry the information</em> about how the words and phrases combine. In a CFG, the grammar is defined by rules such as <code>S -&gt; NP VP</code>. In a CCG, once we say how individual words and phrases parse, the rest is taken care of - i.e. we only need to specify the terminal rules and the rest is determined.)</p>
<p>Well, an adjective takes a noun (phrase) as input, and the output is still a noun phrase, so we can think of it as a function <code>NP-&gt;NP</code>.</p>
<p>In CCG’s, this is represented as the type <code>NP/NP</code>. This means that it takes a <code>NP</code> argument from the right and returns a <code>NP</code>.</p>
<p>The function application rules in a CCG are:</p>
<ol type="1">
<li><code>A/B:f B:x</code> becomes <code>A:f(x)</code>.</li>
<li><code>B:x A\B:f</code> becomes <code>A:f(x)</code>.</li>
</ol>
<p>Verbs have more variety - they aren’t all the same type. First consider an intransitive verb. If we give it a subject (on the left), then it becomes a complete sentence. So the type is <code>S\NP</code>.</p>
<p><span class="math display">\[
\ub{\text{I}}{\text{NP}}\, \ub{\text{sleep}}{\text{S\NP}}.
\]</span></p>
<p>Now a transitive verb takes an object on the right and a subject to the left.</p>
<p><span class="math display">\[
\ub{\text{I}}{\text{NP}}\,\ub{\text{eat}}{\text{(S\NP)/NP}}\,\ub{\text{artichokes}}{\text{NP}}
\implies
\ub{\text{I}}{\text{NP}}\, \ub{\text{eat artichokes}}{\text{S\NP}}.
\]</span></p>
<p>Some verbs like <code>give</code> take both direct and indirect objects: “I give him flowers.” What would the type be? It has 3 arguments.</p>
<p>(Bonus question: can you think of an English verb which can be thought of as having 4 arguments (without needing any prepositions)? Answer<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>)</p>
<h3 id="more-on-multiple-arguments-and-prepositions">More on multiple arguments, and prepositions</h3>
<p>You may ask: why does “eat” consume the object first, before the subject? (No pun intended.) It seems that there’s a kind of symmetry here: we should allow “eat” to either consume the subject or the object first. Consider the sentences:</p>
<blockquote>
<p>I cook food and eat it.</p>
<p>I cook and Ben eats food.</p>
</blockquote>
<p>In the first sentence we want “cook food” to parse as <code>S\NP</code>; it combines with another <code>S\NP</code> (eat it) to be a single <code>S\NP</code>. In the second sentence we want “I cook” to parse as <code>S/NP</code>; it combines with another such to be a single <code>S/NP</code>, and then consumes <code>food</code>. (Note “and” effectively functions as <code>((*\*)/*)</code> where <code>*</code> is any type, like NP.)</p>
<p>There is another rule called lifting [?] which takes care of this, though I wonder if things could be simplified by introducing “symmetric” arugments.</p>
<p>There are cases when it is NOT symmetric. For example, we can’t flip the two arguments in an intransitive verb.</p>
<blockquote>
<p>I give him flowers.</p>
<p>*I give flowers him.</p>
</blockquote>
<p>However, if we use prepositions we are more free to order the arguments.</p>
<blockquote>
<p>I give flowers to him.</p>
</blockquote>
<p>Many verbs also have optional prepositional phrases to go with them - they’re like named optional arguments. You could imagine a more elaborate grammar which takes into account prepositional phrases. See (ref) for details; we’ll jkeep it simple.</p>
<h4 id="aside-other-languages">Aside: other languages</h4>
<p>Lojban is a constructed language that aims to be logical by thinking of verbs <em>as</em> functions taking a set number of arguments. However, I think that it doesn’t do this correctly. Lojban is missing prepositions - or rather, it has prepositions, but the prepositions just mark the location of the arguments (so you can reorder them), not their purpose (ex. prepositions like “to” are reused between many phrases, but has a certain set of meanings). This means there’s a lot of memorization involved.</p>
<p>On the other hand, Japanese is a very logical language when it comes to prepositions (particles). Pretty much everything is marked by prepositions.</p>
<h2 id="what-else-to-say">What else to say?</h2>
<ul>
<li>The lambda terms.</li>
<li>Building the blocks world language. To make things simple, I’ll let the types match exactly the types in the logical expression. (Although you can put it in the same framework, for example, <code>Act</code> is <code>S</code> here, you can think of <code>Set</code> and <code>Color</code> as subtypes of <code>NP</code>, etc.)
<ul>
<li>Ex. <code>Add : (Act/Set)/Color</code>.</li>
</ul></li>
<li>Math example (take from Percy Liang paper)
<ul>
<li>If you can say math language, you know the basics of functional programming…</li>
</ul></li>
<li>An overview of ZC, and learning CCG’s.</li>
<li>List of papers.</li>
</ul>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>“bet” as in “I bet you five dollars she’ll win.” Here, <code>bet : ((((S\NP)/S)/NP)/NP)</code><a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Control theory</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/control.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/control.html</id>
    <published>2017-12-24T00:00:00Z</published>
    <updated>2017-12-24T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Control theory</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-12-24 
          , Modified: 2016-12-24 
	</p>
      
       <p>Tags: <a href="/tags/control.html">control</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p><a href="https://www.dropbox.com/s/5gpwsa1g03t3mf7/control.md?dl=0">Notes</a>.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Learning grammar</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/learning_grammar.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/learning_grammar.html</id>
    <published>2017-10-04T00:00:00Z</published>
    <updated>2017-10-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Learning grammar</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-10-04 
          , Modified: 2017-10-04 
	</p>
      
       <p>Tags: <a href="/tags/nlp.html">nlp</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#code">Code</a><ul>
 <li><a href="#my-code">My code</a></li>
 </ul></li>
 <li><a href="#previous-work-on-grammar">Previous work on grammar</a></li>
 <li><a href="#conversations">Conversations</a><ul>
 <li><a href="#pcfg">PCFG</a></li>
 <li><a href="#sentence-transformations">Sentence transformations</a></li>
 </ul></li>
 <li><a href="#talk-with-sida-10-4">Talk with Sida (10-4)</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>See also</p>
<ul>
<li>Weekly summary <a href="posts/summaries/2017-09-09.html">2017-09-09</a></li>
<li><a href="percy_liang.html">Percy Liang</a></li>
<li><a href="language_games.html">Language games</a></li>
<li><a href="nlp.html#io-algorithm">IO algorithm</a></li>
</ul>
<h2 id="code">Code</h2>
<p><a href="https://worksheets.codalab.org/worksheets/0xbf8f4f5b42e54eba9921f7654b3c5c5d/">Naturalizing PL</a></p>
<h3 id="my-code">My code</h3>
<ul>
<li><a href="https://github.com/holdenlee/Blocks">Blocks grammar</a></li>
<li><a href="https://github.com/holdenlee/learn-grammar">Learn grammar</a></li>
<li><a href="https://github.com/holdenlee/MathGrammar">Math grammar</a></li>
</ul>
<h2 id="previous-work-on-grammar">Previous work on grammar</h2>
<ul>
<li>[TH] Unsupervised learning of probabilistic context-free grammar using iterative biclustering</li>
<li>[CTC] Automatic Learning of Context-Free Grammar</li>
<li>Chris Manning’s notes <a href="http://www.cs.columbia.edu/~mcollins/io.pdf">Inside-outside</a></li>
<li>Spectral approaches (knowing CFG)</li>
<li><a href="https://www.uio.no/studier/emner/matnat/ifi/INF2820/v12/undervisningsmateriale/unification.pdf">Unification</a> (?)</li>
</ul>
<h2 id="conversations">Conversations</h2>
<h3 id="pcfg">PCFG</h3>
<ul>
<li>it’s not so easy - adding the minimum number of rules often results in the wrong rules</li>
<li>[Greedy] doesn’t really work</li>
<li>for example, suppose you have sentences NV (noun verb) and NVN (noun verb noun, i.e., subject verb object)</li>
<li>it would learn S-&gt;NV from the first and then V-&gt;VN from the second.</li>
<li>but this isn’t right because the second rule can give V-&gt;VN-&gt;VNN-&gt;VNNN…</li>
<li>the right thing would be to have a VP (verb phrase), and S -&gt; N VP, VP-&gt; V N, but this is an extra symbol it would have to come up with</li>
<li>I wonder what would happen if I threw in all possible rules for a PCFG and then just did gradient descent on the probabilities. Ex. if I have 10 symbols then to get all rules A-&gt;BC I would need 1000 parameters. It would be doable (though it wouldn’t scale well) to keep all of them.</li>
<li>having more symbols than required is like overparametrization, which helps avoid local minima when doing gradient descent</li>
<li>rn it seems like it will always be possible to generate ungrammatical things; either that or you will have poor rule diversity</li>
<li>yeah we really want to not have ungrammatical things maybe a lot of rules will have probability close to 0 and we can remove them</li>
<li>also I just realized that HMMs are a special case of PCFGs (hidden) -&gt; (observed) (hidden) so maybe the right thing to do is some kind of EM algorithm?</li>
</ul>
<h3 id="sentence-transformations">Sentence transformations</h3>
<ul>
<li>WH-movement is the way you transform sentences into questions. “You want x.” -&gt; “What do you want []” where x disappears and leaves a hole.</li>
<li>The CFG doesn’t capture very well what’s going on because it’s better thought of as a transformation of the whole sentence</li>
<li>anyway I’m pretty sure you can also model that with a CFG, it would just have a longer description length than our intuitive notion of what’s going on</li>
</ul>
<h2 id="talk-with-sida-10-4">Talk with Sida (10-4)</h2>
<p>Some notes:</p>
<ul>
<li>It would be interesting to formally compare using a CCG parser to using a floating parser on a benchmark task. (The first is really learning a grammar and mapping to logical forms, while the second is doing a search over logical forms and scoring based on features. The first is probably more complex to get working. The second suffers from an exponential explosion of logical forms.)</li>
<li>Building a system that could give grammatical utterances (ex. give commands to the user in SHRDLURN) is a good goal.</li>
<li>Sida is working on a system for data visualization that can take commands in natural language; it learns by demonstration.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2017-09-09</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2017-09-09.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2017-09-09.html</id>
    <published>2017-09-05T00:00:00Z</published>
    <updated>2017-09-05T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2017-09-09</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-09-05 
          , Modified: 2017-09-05 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#current-projects">Current projects</a></li>
 <li><a href="#sidelined">Sidelined</a></li>
 <li><a href="#logic-learning-with-kiran">Logic learning (with Kiran)</a><ul>
 <li><a href="#misc">Misc</a></li>
 </ul></li>
 <li><a href="#neuroscience-reading">Neuroscience reading</a></li>
 <li><a href="#aisfp-preparation">AISFP preparation</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="current-projects">Current projects</h2>
<ul>
<li>Reinforcement learning
<ul>
<li>LQR</li>
<li>Experiments</li>
<li>Kernel</li>
</ul></li>
<li>Next steps with sampling problems, temperature-varying
<ul>
<li>Understand AIS/RAISE</li>
<li>AIS/RAISE estimator - similar criterion?
<ul>
<li>See [BGS14] Accurate and Conservative Estimates of MRF Log-likelihood using Reverse Annealing</li>
<li>“Multiplicative” weights</li>
</ul></li>
<li>Other settings where annealing helps. Analogue on Boolean cube.</li>
<li>Tensor decomposition.
<ul>
<li>Beyond the homotopy method.</li>
<li>[MR16]</li>
</ul></li>
</ul></li>
<li>EGNN</li>
<li>NLP:
<ul>
<li>BoNGs: make recovery work for <span class="math inline">\(n\)</span>-grams, <span class="math inline">\(n\ge 2\)</span>.</li>
<li>Treegrams
<ul>
<li>cf. Sida Wang</li>
<li>hyperdim vectors</li>
</ul></li>
<li>document embedding (axioms?)</li>
</ul></li>
</ul>
<h2 id="sidelined">Sidelined</h2>
<ul>
<li>Long-term memory (COLT open problem) (Tue)
<ul>
<li>For convex optimization</li>
<li>Tue: this seems difficult because of “bottleneck” of probability <span class="math inline">\(\ll \rc{\poly(n)}\)</span>. Next step: familiarize with lower-bound techniques and try to prove lower bound.</li>
</ul></li>
</ul>
<h2 id="logic-learning-with-kiran">Logic learning (with Kiran)</h2>
<ul>
<li>Coming up with the objective
<ul>
<li>Ex. restrict to separable <span class="math inline">\(\sum w_i f_i\)</span>.</li>
</ul></li>
<li>Learn representations with desired properties by human feedback</li>
<li>IRL, CIRL. Learn human references, loss function.</li>
<li>Picking out part of image that matters</li>
<li>Graphs?</li>
<li>Recovery as good property to have?</li>
<li>Learn logic/PCFG over curriculum without supervised data. Language generation.</li>
<li>Readings (Hrishikesh)
<ul>
<li>Probabilistic sentential decision diagrams
<ul>
<li>[KVCD14] PSSD</li>
<li>[D11] SDD</li>
<li>[N86] Probabilistic logic</li>
</ul></li>
<li>Markov random fields</li>
<li>Fuzzy logic</li>
<li>Continuous representations of boolean functions</li>
<li>Neural tensor machines (matrix product)</li>
</ul></li>
<li>Learning language games through interaction
<ul>
<li><a href="https://worksheets.codalab.org/worksheets/0xbf8f4f5b42e54eba9921f7654b3c5c5d/">Naturalizing PL</a></li>
<li><a href="https://github.com/holdenlee/Blocks">Blocks grammar</a></li>
</ul></li>
<li>Learning grammar: see messenger.
<ul>
<li>[TH] Unsupervised learning of probabilistic context-free grammar using iterative biclustering</li>
<li>[CTC] Automatic Learning of Context-Free Grammar</li>
</ul></li>
</ul>
<p>Code</p>
<ul>
<li><a href="https://github.com/holdenlee/learn-grammar">learn-grammar</a></li>
<li><a href="https://github.com/holdenlee/Blocks">Blocks</a></li>
<li><a href="https://github.com/holdenlee/MathGrammar">MathGrammar</a></li>
</ul>
<h3 id="misc">Misc</h3>
<p>Instead of a log-linear model with features from (logical forms, canonical utterances), have something more principled.</p>
<p>Semantic Parsing via Paraphrasing</p>
<p>Agenda-based parsing: reduce from n^3</p>
<p>Unanimous Prediction for 100% Precision with Application to Learning Semantic Mappings: certainty</p>
<p>Learning Executable Semantic Parsers for Natural Language Understanding: score derivations with log-linear (survey)</p>
<p>Bringing machine learning and compositional semantics together (survey)</p>
<p>Simpler Context-Dependent Logical Forms via Model Projections: add context dependence.</p>
<p>Inferring Logical Forms From Denotations: getting consistent logical forms</p>
<p>Building a Semantic Parser Overnight: entire pipeline, with crowdsourcing</p>
<p>“To generate candidate logical forms, we use a simple beam search” - it seems better to parse, keep top from beam search, and then convert?</p>
<p>moral: throw in lots of features</p>
<p>Paraphrasing and transformations</p>
<p>The cat was chased by a dog. The cat was bitten.</p>
<p>If CFG doesn’t have too much “latent”, everything is close to surface, then have hope? Prevents combinatorial blowup of possible hidden states/transitions.</p>
<p><a href="https://www.uio.no/studier/emner/matnat/ifi/INF2820/v12/undervisningsmateriale/unification.pdf">Beyond CFGs</a></p>
<p>Have a superset of the right rules. Now use gradient update to find which best explain it. Also would like to match things that transform, like “You want x.” and “What do you want?”</p>
<h2 id="neuroscience-reading">Neuroscience reading</h2>
<ul>
<li>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5021692/ - the brain optimizing cost functions, ie the generalization of gradient descent, as a hypothesis for how local neuron wiring is learned.</li>
<li>http://www.cell.com/neuron/abstract/S0896-6273(17)30509-3 - this is the one that takes these others and generalizes them to the whole brain, in particular this explains how complicated architectures in deep learning can be used to understand the large scale functional architecture of the brain</li>
<li>http://physrev.physiology.org/content/physrev/95/3/853.full.pdf - overview of the human reward system</li>
<li>http://www.nature.com/neuro/journal/v19/n3/abs/nn.4244.html - comparing supervised deep learning to the (unsupervised probably) vision system</li>
<li>http://rstb.royalsocietypublishing.org/content/371/1705/20160278 - actually physically modeling readings from the brain with the functional statistics from deep learning models trained on the same tasks</li>
</ul>
<h2 id="aisfp-preparation">AISFP preparation</h2>
<ul>
<li>Write up AI safety thoughts (from books, etc.)</li>
<li>Review:
<ul>
<li>Lob’s theorem</li>
<li>Decision theories</li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Theorem proving with modern ML</title>
    <link href="http://holdenlee.github.io/notebook/posts/cs/ai/nn_theorem_proving.html" />
    <id>http://holdenlee.github.io/notebook/posts/cs/ai/nn_theorem_proving.html</id>
    <published>2017-09-05T00:00:00Z</published>
    <updated>2017-09-05T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Theorem proving with modern ML</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-09-05 
          , Modified: 2017-09-05 
	</p>
      
       <p>Tags: <a href="/tags/theorem%20proving.html">theorem proving</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#vlads-references">Vlad’s references</a></li>
 <li><a href="#game-plan">Game plan</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="vlads-references">Vlad’s references</h2>
<ol type="1">
<li>Neural Meta-Induction and Program Synthesis
<ul>
<li><a href="https://arxiv.org/abs/1703.07469">arxiv</a></li>
<li><a href="https://www.microsoft.com/en-us/research/blog/deep-learning-program-synthesis/">microsoft</a></li>
</ul></li>
<li>DeepMath - continuous representations of symbolic expressions <a href="https://arxiv.org/abs/1611.01423">arxiv</a></li>
<li>Terpret- A language for program Induction <a href="https://arxiv.org/abs/1608.04428">arxiv</a></li>
<li>End to End differentiable proving <a href="https://arxiv.org/pdf/1705.11040.pdf">arxiv</a></li>
<li>Alemi et al., “Deepmath - Deep sequence models for premise selection”. NIPS 2016. arxiv, research@google.</li>
<li>Kaliszyk, Chollet, Szegedy., “HolStep: A machine learning dataset for higher-order logic theorem proving”. ICLR 2017. arxiv,research@google</li>
<li>Alemi et al., “Deep variational information bottleneck”. ICLR 2017. arxiv</li>
<li>Loos et al., “Deep network guided proof search”. LPAR 2017. arxiv, research@google.</li>
<li>Learning to Discover Efficient Mathematical Identities <a href="https://arxiv.org/abs/1406.1584">arxiv</a></li>
</ol>
<h2 id="game-plan">Game plan</h2>
<ul>
<li>Skim over.</li>
<li>Understand one theorem prover deeply (ex. HOL), interfacing with it.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Annealed importance sampling</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/probabilistic/ais.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/probabilistic/ais.html</id>
    <published>2017-07-20T00:00:00Z</published>
    <updated>2017-07-20T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Annealed importance sampling</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-07-20 
          , Modified: 2017-07-20 
	</p>
      
       <p>Tags: <a href="/tags/sampling.html">sampling</a>, <a href="/tags/annealing.html">annealing</a>, <a href="/tags/temperature.html">temperature</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#papers">Papers</a></li>
 <li><a href="#log-p">log p</a></li>
 <li><a href="#elbo">ELBO</a></li>
 <li><a href="#ais">AIS</a></li>
 <li><a href="#raise">RAISE</a></li>
 <li><a href="#notes">Notes</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="papers">Papers</h2>
<ul>
<li>[N98] Annealed Importance Sampling</li>
<li>[BGS14] Accurate and Conservative Estimates of MRF Log-likelihood using Reverse Annealing</li>
<li>[WBSG17] ON THE QUANTITATIVE ANALYSIS OF DECODER-BASED GENERATIVE MODELS</li>
</ul>
<h2 id="log-p">log p</h2>
<p>We can get an unbiased estimator for <span class="math inline">\(p(x)\)</span>, say <span class="math inline">\(\wh p\)</span>. But we often want <span class="math inline">\(\ln p(x)\)</span>. We use Jensen’s inequality and Markov’s inequality. So <span class="math inline">\(\ln \wh p\)</span> is a probabilistic lower bound. <span class="math display">\[
\E[\ln \wh p] \le \ln p\implies \quad \Pj(\ln \wh p&gt; \ln p + b) &lt;e^{-b}.
\]</span> (This is true no matter what the variance is. However, this can be a very loose bound. There is no good way of estimating <span class="math inline">\(\E \ln X\)</span> from draws of <span class="math inline">\(X\)</span> (why not?). Oddly, there is a good way of estimating <span class="math inline">\(\E e^X\)</span> from <span class="math inline">\(X\)</span> by power series expansion. (Power series for <span class="math inline">\(\ln\)</span> is terrible over long distances.))</p>
<p>Note this is prone to overestimation with little indication anything is wrong.</p>
<h2 id="elbo">ELBO</h2>
<p>Goal: posterior distribution <span class="math display">\[
p(z|x,\al) = \fc{p(z,x|\al)}{\int_z p(z,x|\al)}.
\]</span> Pick a family of distributions with variational parameters <span class="math inline">\(q(z_{1:m}|\nu)\)</span>. Use <span class="math inline">\(q\)</span> with fitted parameters as proxy.</p>
<p>So want to minimize <span class="math inline">\(KL(q||p)\)</span>.</p>
<p>Why <span class="math inline">\(q||p\)</span>, not <span class="math inline">\(p||q\)</span>?</p>
<ul>
<li>q high, p low is bad. Don’t want to make impossible events happen!</li>
<li>q low, p high is not so bad.</li>
</ul>
<span class="math display">\[\begin{align}
KL(q||p) &amp;=\EE_q\ba{\ln \fc{q(z)}{p(z|x)}}\\
\ln p &amp;=\ln \int \EE_{z\sim q}\ba{\fc{p(x,z)}{q(z)}}\\
&amp; \ge \EE_q \ln p(x,z) - \EE_q [\ln q] :=ELBO\\
KL(q||p) &amp;=-ELBO - \ln p(x)
\end{align}\]</span>
<p><span class="math inline">\(\ln p\)</span> doesn’t depend on <span class="math inline">\(q\)</span>.</p>
<h2 id="ais">AIS</h2>
<p>Given annealed distributions <span class="math inline">\(p_i\propto f_i\)</span>, <span class="math inline">\(p_K=p\)</span> with Markov chains <span class="math inline">\(M_i\)</span> (with transition kernels <span class="math inline">\(T_i\)</span>), to estimate <span class="math inline">\(Z=Z_K\)</span>,</p>
<ul>
<li>Sample from <span class="math inline">\(p_0\)</span>.</li>
<li>Let <span class="math inline">\(w=Z_0\)</span>.</li>
<li>For <span class="math inline">\(k=1:K\)</span>
<ul>
<li><span class="math inline">\(w\leftarrow w \fc{f_k(x_{k-1})}{f_{k-1}(x_{k-1})}\)</span></li>
<li><span class="math inline">\(x_k \sim T_k(\cdot |x_{k-1})\)</span>.</li>
</ul></li>
<li>Estimate is <span class="math inline">\(w\)</span>.</li>
</ul>
<p>For probabilistic neural nets, use Gibbs sampler (alternately sample <span class="math inline">\(h\)</span> and <span class="math inline">\(x\)</span>) as transition.</p>
<p>Think of this as proposing the distribution given by applying the <span class="math inline">\(T_i\)</span> in sequence.</p>
<p>Giving a stochastic lower bound for <span class="math inline">\(Z\)</span> means we overestimate log-likelihood.</p>
<h2 id="raise">RAISE</h2>
<p>Go the other way using samples from the target distribution This gives a probabilistic lower bound on <span class="math inline">\(\fc{Z_0}{Z}\)</span>, so a probabilistic upper bound on <span class="math inline">\(Z\)</span>, so we underestimate log-likelihood.</p>
<p><span class="math inline">\(p(h|v)\)</span> needs to be tractable. ((z|x) in above notation)</p>
<p>For intractable <span class="math inline">\(p(h|v)\)</span> combine the AIS (fixing <span class="math inline">\(v\)</span>) and RAISE steps to get a single estimate. See Algorithm 3 for details.</p>
<h2 id="notes">Notes</h2>
<ul>
<li>Is RAISE a provable (stochastic) upper bound? Even in the intractable case?</li>
<li>I’m surprised AIS/RAISE match so closely. Does this mean partition function calculation for deep belief nets is in practice tractable???</li>
<li>I think AIS can have large variance. It seems better to do the “evolutionary multiplicative update” thing. Does that have provable guarantees under similar conditions as Langevin annealing? Can AIS fail where this works? (Ex. continuously miss the high-prob stuff, stepping into the low-ratios between layers.)
<ul>
<li>Does this give a better bound in practice? I.e. larger estimate for <span class="math inline">\(Z\)</span>? (Warning: not quite unbiased anymore…)</li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>(HKY17) Hyperparameter Optimization - A Spectral Approach</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/HKY17.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/HKY17.html</id>
    <published>2017-07-19T00:00:00Z</published>
    <updated>2017-07-19T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>(HKY17) Hyperparameter Optimization - A Spectral Approach</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-07-19 
          , Modified: 2017-07-19 
	</p>
      
       <p>Tags: <a href="/tags/hyperparameters.html">hyperparameters</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#compressed-sensing-for-orthonormal-polynomials">Compressed sensing for orthonormal polynomials</a></li>
 <li><a href="#harmonica">Harmonica</a></li>
 <li><a href="#questions">Questions</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>The main theorem (Alg. 1, Thm. 6) is a theorem on learning Fourier-concentrated functions with much better sample complexity than [LMN93], using compressed sensing applied to orthonormal polynomials. Apply this theorem to the loss as a function of hyperparameters (A.g. 2, Harmonica). (Note this is heuristic.)</p>
<h2 id="compressed-sensing-for-orthonormal-polynomials">Compressed sensing for orthonormal polynomials</h2>
<p>An orthonormal family with respect to distribution <span class="math inline">\(D\)</span> has <span class="math inline">\(\EE_D[\psi_i (X) \psi_j(X)]=\de_{ij}\)</span>.</p>
<ul>
<li><span class="math inline">\(s\)</span>-sparse: <span class="math inline">\(L_0(f)\le s\)</span></li>
<li><span class="math inline">\((\ep,d)\)</span> concentrated: <span class="math inline">\(\ve{f - \pi_{\{\psi_{i}\}_{i\in S}}(f)}_2\le \ep\)</span>.</li>
<li><span class="math inline">\((\ep,d,s)\)</span>-bounded: additionally, <span class="math inline">\(\ve{f}_1\le s\)</span>.</li>
<li>Note we can approximate <span class="math inline">\(L_1(f)\le s\)</span> to <span class="math inline">\(\ep\)</span> with <span class="math inline">\(L_0(g)\le \fc{s^2}{\ep}\)</span>. (Sampling. Cf. Barron proof)</li>
</ul>
<p>LASSO: With appropriate <span class="math inline">\(\la\)</span>, <span class="math display">\[
\min_{x\in \R^n} [\ve{x}_1 + \la \ve{Ax-y}_2^2]
\]</span></p>
<p>For <span class="math inline">\(z^1,\ldots, z^m\sim D\)</span>, <span class="math inline">\(A_{ij}=\psi_j(z^i)\)</span>, <span class="math inline">\(y=Ax+e\)</span>, <span class="math inline">\(\ve{e}_2\le \eta\sqrt m\)</span>, <span class="math inline">\(x^*\)</span> solving LASSO, <span class="math display">\[
\Pj(\ve{x-x^*}_2\le C \fc{\si_s(x)_1}{\sqrt s}+d\eta) \ge 1-\de
\]</span> where <span class="math inline">\(\si_s(x)_1=\min\set{\ve{x-z}_1}{z\text{ is s-sparse}}\)</span>, <span class="math inline">\(c,d\)</span> constants, with <span class="math inline">\(m\ge CK^2 s \poly\log(K,s,N,\rc{\de})\)</span> samples.</p>
<p>Apply for low-degree recovery: if <span class="math inline">\(f\)</span> is (<span class="math inline">\(\ep,d,s\)</span>)-bounded, then using this finds <span class="math inline">\(g\equiv_\ep f\)</span> in time <span class="math inline">\(O(n^d)\)</span>, with <span class="math inline">\(T=\wt O(K^2s^2 \ln N/\ep)\)</span> samples. (? <span class="math inline">\(\ep\)</span> outside)</p>
<p>(LMN93 needs <span class="math inline">\(\Om\pf{NL_\iy(f)^2}{\ep}\)</span> samples.) (? What is <span class="math inline">\(N\)</span> here? Number of orthonormal polys. Shouldn’t it be <span class="math inline">\(n^d\)</span>?)</p>
<h2 id="harmonica">Harmonica</h2>
<p>Apply in stages, with some degree <span class="math inline">\(d\)</span> and sparsity <span class="math inline">\(s\)</span>. Note this can involve at most <span class="math inline">\(ds\)</span> variables. Suppose the approximation <span class="math inline">\(g\)</span> to <span class="math inline">\(f\)</span> only involves variables in <span class="math inline">\(J\)</span>.</p>
<p>Take the best <span class="math inline">\(t\)</span> solutions <span class="math inline">\(x_i*\)</span> to <span class="math inline">\(g\)</span> on <span class="math inline">\(J\)</span>, and now apply to <span class="math inline">\(\rc t \sumo it f_{J \leftarrow x_i^*}(x)\)</span>.</p>
<h2 id="questions">Questions</h2>
<ul>
<li>Why does multiple stages help?</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>(AR17) Provable benefits of representation learning</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/representation/AR17.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/representation/AR17.html</id>
    <published>2017-07-18T00:00:00Z</published>
    <updated>2017-07-18T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>(AR17) Provable benefits of representation learning</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-07-18 
          , Modified: 2017-07-18 
	</p>
      
       <p>Tags: <a href="/tags/representation%20learning.html">representation learning</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#intro">Intro</a></li>
 <li><a href="#section">2</a><ul>
 <li><a href="#section-1">2.2</a></li>
 </ul></li>
 <li><a href="#section-2">3</a><ul>
 <li><a href="#section-3">3.1</a></li>
 <li><a href="#section-4">3.2</a></li>
 </ul></li>
 <li><a href="#section-5">4</a><ul>
 <li><a href="#section-6">4.1</a></li>
 <li><a href="#section-7">4.2</a></li>
 </ul></li>
 <li><a href="#section-8">5</a><ul>
 <li><a href="#lower-bounds-for-nearest-neighbors">5.1 Lower bounds for nearest neighbors</a></li>
 <li><a href="#section-9">5.3</a></li>
 </ul></li>
 <li><a href="#notes">Notes</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="intro">Intro</h2>
<ul>
<li>Contributions
<ul>
<li>Formalizes representation learning, unifying disparate settings.</li>
<li>Quantifies “utility” from representation learning.</li>
<li>Prove separation results between representation learning and simpler algorithms.</li>
</ul></li>
<li>“Bayes+”</li>
<li>Why representation learning?
<ul>
<li>Allows semi-supervised learning.</li>
<li>Simpler methods need too many samples.</li>
<li>Provably better than manifold learning in some cases.</li>
</ul></li>
<li>The framework
<ul>
<li>Many-to-one map <span class="math inline">\(x\mapsto h\)</span>. <span class="math inline">\(h\)</span> is “high-level” representation.</li>
<li>Generative model <span class="math inline">\(h\to x\)</span>.</li>
<li>Similarity in the latent space (of <span class="math inline">\(h\)</span>) is more informative.</li>
<li>Defintion: A <span class="math inline">\((\ga,\be)\)</span>-valid decoder has <span class="math inline">\(\Pj(\ve{f(x)-h}\le (1-\ga) \ve{h})\ge \be\)</span>. (Think of <span class="math inline">\(\ga,\be\approx 1\)</span>.)</li>
<li>Utility: If <span class="math inline">\(C\)</span> is <span class="math inline">\(\al\)</span>-Lipschitz, <span class="math inline">\(\ve{C(f(x)) - C(h)}_\iy\le (1-\ga)\al \ve{h}\)</span>.</li>
</ul></li>
<li>Examples
<ul>
<li>Clustering</li>
<li>Manifold</li>
<li>Kernel learning</li>
</ul></li>
<li>Non-examples
<ul>
<li>Nearest neighbor (provably weaker in some settings)</li>
<li>LSH - this preserves distance, which is not our goal.</li>
</ul></li>
<li>Contrast [HM16], which is assumption-free and basically lossless compression. (ex. Lempel-Ziv) This notion is different, ex. allows throwing away noise.</li>
<li>Compare to the usual: Maximize log probability (MLE), then <span class="math inline">\(\amax_h p_\te(h|x)\)</span>.
<ul>
<li>Unlike Bayesian which gives a distribution over <span class="math inline">\(h\)</span>, we output single <span class="math inline">\(h\)</span>.</li>
</ul></li>
</ul>
<h2 id="section">2</h2>
<h3 id="section-1">2.2</h3>
<ul>
<li>Encoder exists <span class="math inline">\(\implies\)</span> given <span class="math inline">\(x\)</span>, <span class="math inline">\(h\)</span> is concentrated around <span class="math inline">\(f(x)\)</span>, almost uniquely defined.</li>
<li>Having concentration is stronger than just being able to do inference.</li>
</ul>
<h2 id="section-2">3</h2>
<h3 id="section-3">3.1</h3>
<p>Topic modeling</p>
<ul>
<li><span class="math inline">\(k\)</span> topices</li>
<li>Each distribution on <span class="math inline">\(M\)</span> words. <span class="math inline">\(A_i\in \R^M\)</span>.</li>
<li>Mixture coefficients <span class="math inline">\(h_i\)</span>.</li>
<li>Draw bag of words <span class="math inline">\(x\sim \sum h_i A_i\)</span>, <span class="math inline">\(x\in \Z^N\)</span>.</li>
</ul>
<h3 id="section-4">3.2</h3>
<p>Loglinear model: (continued below)</p>
<p><span class="math display">\[p(x,h) = p(h)p(x|h).\]</span></p>
<h2 id="section-5">4</h2>
<h3 id="section-6">4.1</h3>
<p>Topic modeling: want <span class="math inline">\(\ell_\iy\to \ell_1\)</span> condition number to be small.</p>
<h3 id="section-7">4.2</h3>
<ul>
<li><span class="math inline">\(h\in \R^d\)</span> randomly on unit sphere.</li>
<li><span class="math inline">\(\Pj(x|h)\propto e^{\an{W_x,h}}\)</span>.
<ul>
<li><span class="math inline">\(W_x = Bv\)</span>, <span class="math inline">\(B=O(1)\)</span>, <span class="math inline">\(v\sim N(0,I)\)</span>.</li>
</ul></li>
<li>Take <span class="math inline">\(f(x) = \nv{\sum_i W_{x_i}}\)</span>.</li>
</ul>
<h2 id="section-8">5</h2>
<h3 id="lower-bounds-for-nearest-neighbors">5.1 Lower bounds for nearest neighbors</h3>
<ul>
<li><span class="math inline">\(M\)</span> movies, <span class="math inline">\(k\)</span> genres
<ul>
<li><span class="math inline">\(\ve{h}_0 = s\)</span>, <span class="math inline">\(h\in \{0,1\}^k\)</span></li>
<li>Draw movies <span class="math inline">\(\ve{x}_0=T\)</span>.</li>
</ul></li>
<li>For <span class="math inline">\(T\ll \sqrt m\)</span>, can’t learn using NN because
<ul>
<li>Users will share few movies in common.</li>
<li>Users who share movies won’t share genres. (Construct example where some movies belong to all genres.)</li>
</ul></li>
</ul>
<h3 id="section-9">5.3</h3>
<ul>
<li><span class="math inline">\(k\)</span> genres</li>
<li><span class="math inline">\(T=\Om(\ln M)\)</span> ratings per user</li>
<li><span class="math inline">\(s\)</span> genres per user</li>
<li><span class="math inline">\(\ell(h) = \sgn(\an{w,2h-1})\)</span>.</li>
</ul>
<p>Can do semi-supervised learning by doing representation learn using [AKM16]. (S4.1)</p>
<h2 id="notes">Notes</h2>
<ul>
<li>Check Thm. 4.1 using [AKM16] - review “condition number”.</li>
<li>Check Thm. 5.1. Look up background on NN.</li>
<li>Check Thm. 5.4.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
