<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2017-10-04T00:00:00Z</updated>
    <entry>
    <title>Learning grammar</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/learning_grammar.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/learning_grammar.html</id>
    <published>2017-10-04T00:00:00Z</published>
    <updated>2017-10-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Learning grammar</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-10-04 
          , Modified: 2017-10-04 
	</p>
      
       <p>Tags: <a href="/tags/nlp.html">nlp</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#code">Code</a><ul>
 <li><a href="#my-code">My code</a></li>
 </ul></li>
 <li><a href="#previous-work-on-grammar">Previous work on grammar</a></li>
 <li><a href="#conversations">Conversations</a><ul>
 <li><a href="#pcfg">PCFG</a></li>
 <li><a href="#sentence-transformations">Sentence transformations</a></li>
 </ul></li>
 <li><a href="#talk-with-sida-10-4">Talk with Sida (10-4)</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>See also</p>
<ul>
<li>Weekly summary <a href="posts/summaries/2017-09-09.html">2017-09-09</a></li>
<li><a href="percy_liang.html">Percy Liang</a></li>
<li><a href="language_games.html">Language games</a></li>
<li><a href="nlp.html#io-algorithm">IO algorithm</a></li>
</ul>
<h2 id="code">Code</h2>
<p><a href="https://worksheets.codalab.org/worksheets/0xbf8f4f5b42e54eba9921f7654b3c5c5d/">Naturalizing PL</a></p>
<h3 id="my-code">My code</h3>
<ul>
<li><a href="https://github.com/holdenlee/Blocks">Blocks grammar</a></li>
<li><a href="https://github.com/holdenlee/learn-grammar">Learn grammar</a></li>
<li><a href="https://github.com/holdenlee/MathGrammar">Math grammar</a></li>
</ul>
<h2 id="previous-work-on-grammar">Previous work on grammar</h2>
<ul>
<li>[TH] Unsupervised learning of probabilistic context-free grammar using iterative biclustering</li>
<li>[CTC] Automatic Learning of Context-Free Grammar</li>
<li>Chris Manning’s notes <a href="http://www.cs.columbia.edu/~mcollins/io.pdf">Inside-outside</a></li>
<li>Spectral approaches (knowing CFG)</li>
<li><a href="https://www.uio.no/studier/emner/matnat/ifi/INF2820/v12/undervisningsmateriale/unification.pdf">Unification</a> (?)</li>
</ul>
<h2 id="conversations">Conversations</h2>
<h3 id="pcfg">PCFG</h3>
<ul>
<li>it’s not so easy - adding the minimum number of rules often results in the wrong rules</li>
<li>[Greedy] doesn’t really work</li>
<li>for example, suppose you have sentences NV (noun verb) and NVN (noun verb noun, i.e., subject verb object)</li>
<li>it would learn S-&gt;NV from the first and then V-&gt;VN from the second.</li>
<li>but this isn’t right because the second rule can give V-&gt;VN-&gt;VNN-&gt;VNNN…</li>
<li>the right thing would be to have a VP (verb phrase), and S -&gt; N VP, VP-&gt; V N, but this is an extra symbol it would have to come up with</li>
<li>I wonder what would happen if I threw in all possible rules for a PCFG and then just did gradient descent on the probabilities. Ex. if I have 10 symbols then to get all rules A-&gt;BC I would need 1000 parameters. It would be doable (though it wouldn’t scale well) to keep all of them.</li>
<li>having more symbols than required is like overparametrization, which helps avoid local minima when doing gradient descent</li>
<li>rn it seems like it will always be possible to generate ungrammatical things; either that or you will have poor rule diversity</li>
<li>yeah we really want to not have ungrammatical things maybe a lot of rules will have probability close to 0 and we can remove them</li>
<li>also I just realized that HMMs are a special case of PCFGs (hidden) -&gt; (observed) (hidden) so maybe the right thing to do is some kind of EM algorithm?</li>
</ul>
<h3 id="sentence-transformations">Sentence transformations</h3>
<ul>
<li>WH-movement is the way you transform sentences into questions. “You want x.” -&gt; “What do you want []” where x disappears and leaves a hole.</li>
<li>The CFG doesn’t capture very well what’s going on because it’s better thought of as a transformation of the whole sentence</li>
<li>anyway I’m pretty sure you can also model that with a CFG, it would just have a longer description length than our intuitive notion of what’s going on</li>
</ul>
<h2 id="talk-with-sida-10-4">Talk with Sida (10-4)</h2>
<p>Some notes:</p>
<ul>
<li>It would be interesting to formally compare using a CCG parser to using a floating parser on a benchmark task. (The first is really learning a grammar and mapping to logical forms, while the second is doing a search over logical forms and scoring based on features. The first is probably more complex to get working. The second suffers from an exponential explosion of logical forms.)</li>
<li>Building a system that could give grammatical utterances (ex. give commands to the user in SHRDLURN) is a good goal.</li>
<li>Sida is working on a system for data visualization that can take commands in natural language; it learns by demonstration.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2017-09-09</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2017-09-09.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2017-09-09.html</id>
    <published>2017-09-05T00:00:00Z</published>
    <updated>2017-09-05T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2017-09-09</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-09-05 
          , Modified: 2017-09-05 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#current-projects">Current projects</a></li>
 <li><a href="#sidelined">Sidelined</a></li>
 <li><a href="#logic-learning-with-kiran">Logic learning (with Kiran)</a><ul>
 <li><a href="#misc">Misc</a></li>
 </ul></li>
 <li><a href="#neuroscience-reading">Neuroscience reading</a></li>
 <li><a href="#aisfp-preparation">AISFP preparation</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="current-projects">Current projects</h2>
<ul>
<li>Reinforcement learning
<ul>
<li>LQR</li>
<li>Experiments</li>
<li>Kernel</li>
</ul></li>
<li>Next steps with sampling problems, temperature-varying
<ul>
<li>Understand AIS/RAISE</li>
<li>AIS/RAISE estimator - similar criterion?
<ul>
<li>See [BGS14] Accurate and Conservative Estimates of MRF Log-likelihood using Reverse Annealing</li>
<li>“Multiplicative” weights</li>
</ul></li>
<li>Other settings where annealing helps. Analogue on Boolean cube.</li>
<li>Tensor decomposition.
<ul>
<li>Beyond the homotopy method.</li>
<li>[MR16]</li>
</ul></li>
</ul></li>
<li>EGNN</li>
<li>NLP:
<ul>
<li>BoNGs: make recovery work for <span class="math inline">\(n\)</span>-grams, <span class="math inline">\(n\ge 2\)</span>.</li>
<li>Treegrams
<ul>
<li>cf. Sida Wang</li>
<li>hyperdim vectors</li>
</ul></li>
<li>document embedding (axioms?)</li>
</ul></li>
</ul>
<h2 id="sidelined">Sidelined</h2>
<ul>
<li>Long-term memory (COLT open problem) (Tue)
<ul>
<li>For convex optimization</li>
<li>Tue: this seems difficult because of “bottleneck” of probability <span class="math inline">\(\ll \rc{\poly(n)}\)</span>. Next step: familiarize with lower-bound techniques and try to prove lower bound.</li>
</ul></li>
</ul>
<h2 id="logic-learning-with-kiran">Logic learning (with Kiran)</h2>
<ul>
<li>Coming up with the objective
<ul>
<li>Ex. restrict to separable <span class="math inline">\(\sum w_i f_i\)</span>.</li>
</ul></li>
<li>Learn representations with desired properties by human feedback</li>
<li>IRL, CIRL. Learn human references, loss function.</li>
<li>Picking out part of image that matters</li>
<li>Graphs?</li>
<li>Recovery as good property to have?</li>
<li>Learn logic/PCFG over curriculum without supervised data. Language generation.</li>
<li>Readings (Hrishikesh)
<ul>
<li>Probabilistic sentential decision diagrams
<ul>
<li>[KVCD14] PSSD</li>
<li>[D11] SDD</li>
<li>[N86] Probabilistic logic</li>
</ul></li>
<li>Markov random fields</li>
<li>Fuzzy logic</li>
<li>Continuous representations of boolean functions</li>
<li>Neural tensor machines (matrix product)</li>
</ul></li>
<li>Learning language games through interaction
<ul>
<li><a href="https://worksheets.codalab.org/worksheets/0xbf8f4f5b42e54eba9921f7654b3c5c5d/">Naturalizing PL</a></li>
<li><a href="https://github.com/holdenlee/Blocks">Blocks grammar</a></li>
</ul></li>
<li>Learning grammar: see messenger.
<ul>
<li>[TH] Unsupervised learning of probabilistic context-free grammar using iterative biclustering</li>
<li>[CTC] Automatic Learning of Context-Free Grammar</li>
</ul></li>
</ul>
<p>Code</p>
<ul>
<li><a href="https://github.com/holdenlee/learn-grammar">learn-grammar</a></li>
<li><a href="https://github.com/holdenlee/Blocks">Blocks</a></li>
<li><a href="https://github.com/holdenlee/MathGrammar">MathGrammar</a></li>
</ul>
<h3 id="misc">Misc</h3>
<p>Instead of a log-linear model with features from (logical forms, canonical utterances), have something more principled.</p>
<p>Semantic Parsing via Paraphrasing</p>
<p>Agenda-based parsing: reduce from n^3</p>
<p>Unanimous Prediction for 100% Precision with Application to Learning Semantic Mappings: certainty</p>
<p>Learning Executable Semantic Parsers for Natural Language Understanding: score derivations with log-linear (survey)</p>
<p>Bringing machine learning and compositional semantics together (survey)</p>
<p>Simpler Context-Dependent Logical Forms via Model Projections: add context dependence.</p>
<p>Inferring Logical Forms From Denotations: getting consistent logical forms</p>
<p>Building a Semantic Parser Overnight: entire pipeline, with crowdsourcing</p>
<p>“To generate candidate logical forms, we use a simple beam search” - it seems better to parse, keep top from beam search, and then convert?</p>
<p>moral: throw in lots of features</p>
<p>Paraphrasing and transformations</p>
<p>The cat was chased by a dog. The cat was bitten.</p>
<p>If CFG doesn’t have too much “latent”, everything is close to surface, then have hope? Prevents combinatorial blowup of possible hidden states/transitions.</p>
<p><a href="https://www.uio.no/studier/emner/matnat/ifi/INF2820/v12/undervisningsmateriale/unification.pdf">Beyond CFGs</a></p>
<p>Have a superset of the right rules. Now use gradient update to find which best explain it. Also would like to match things that transform, like “You want x.” and “What do you want?”</p>
<h2 id="neuroscience-reading">Neuroscience reading</h2>
<ul>
<li>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5021692/ - the brain optimizing cost functions, ie the generalization of gradient descent, as a hypothesis for how local neuron wiring is learned.</li>
<li>http://www.cell.com/neuron/abstract/S0896-6273(17)30509-3 - this is the one that takes these others and generalizes them to the whole brain, in particular this explains how complicated architectures in deep learning can be used to understand the large scale functional architecture of the brain</li>
<li>http://physrev.physiology.org/content/physrev/95/3/853.full.pdf - overview of the human reward system</li>
<li>http://www.nature.com/neuro/journal/v19/n3/abs/nn.4244.html - comparing supervised deep learning to the (unsupervised probably) vision system</li>
<li>http://rstb.royalsocietypublishing.org/content/371/1705/20160278 - actually physically modeling readings from the brain with the functional statistics from deep learning models trained on the same tasks</li>
</ul>
<h2 id="aisfp-preparation">AISFP preparation</h2>
<ul>
<li>Write up AI safety thoughts (from books, etc.)</li>
<li>Review:
<ul>
<li>Lob’s theorem</li>
<li>Decision theories</li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Theorem proving with modern ML</title>
    <link href="http://holdenlee.github.io/notebook/posts/cs/ai/nn_theorem_proving.html" />
    <id>http://holdenlee.github.io/notebook/posts/cs/ai/nn_theorem_proving.html</id>
    <published>2017-09-05T00:00:00Z</published>
    <updated>2017-09-05T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Theorem proving with modern ML</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-09-05 
          , Modified: 2017-09-05 
	</p>
      
       <p>Tags: <a href="/tags/theorem%20proving.html">theorem proving</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#vlads-references">Vlad’s references</a></li>
 <li><a href="#game-plan">Game plan</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="vlads-references">Vlad’s references</h2>
<ol type="1">
<li>Neural Meta-Induction and Program Synthesis
<ul>
<li><a href="https://arxiv.org/abs/1703.07469">arxiv</a></li>
<li><a href="https://www.microsoft.com/en-us/research/blog/deep-learning-program-synthesis/">microsoft</a></li>
</ul></li>
<li>DeepMath - continuous representations of symbolic expressions <a href="https://arxiv.org/abs/1611.01423">arxiv</a></li>
<li>Terpret- A language for program Induction <a href="https://arxiv.org/abs/1608.04428">arxiv</a></li>
<li>End to End differentiable proving <a href="https://arxiv.org/pdf/1705.11040.pdf">arxiv</a></li>
<li>Alemi et al., “Deepmath - Deep sequence models for premise selection”. NIPS 2016. arxiv, research@google.</li>
<li>Kaliszyk, Chollet, Szegedy., “HolStep: A machine learning dataset for higher-order logic theorem proving”. ICLR 2017. arxiv,research@google</li>
<li>Alemi et al., “Deep variational information bottleneck”. ICLR 2017. arxiv</li>
<li>Loos et al., “Deep network guided proof search”. LPAR 2017. arxiv, research@google.</li>
<li>Learning to Discover Efficient Mathematical Identities <a href="https://arxiv.org/abs/1406.1584">arxiv</a></li>
</ol>
<h2 id="game-plan">Game plan</h2>
<ul>
<li>Skim over.</li>
<li>Understand one theorem prover deeply (ex. HOL), interfacing with it.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Annealed importance sampling</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/probabilistic/ais.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/probabilistic/ais.html</id>
    <published>2017-07-20T00:00:00Z</published>
    <updated>2017-07-20T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Annealed importance sampling</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-07-20 
          , Modified: 2017-07-20 
	</p>
      
       <p>Tags: <a href="/tags/sampling.html">sampling</a>, <a href="/tags/annealing.html">annealing</a>, <a href="/tags/temperature.html">temperature</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#papers">Papers</a></li>
 <li><a href="#log-p">log p</a></li>
 <li><a href="#elbo">ELBO</a></li>
 <li><a href="#ais">AIS</a></li>
 <li><a href="#raise">RAISE</a></li>
 <li><a href="#notes">Notes</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="papers">Papers</h2>
<ul>
<li>[N98] Annealed Importance Sampling</li>
<li>[BGS14] Accurate and Conservative Estimates of MRF Log-likelihood using Reverse Annealing</li>
<li>[WBSG17] ON THE QUANTITATIVE ANALYSIS OF DECODER-BASED GENERATIVE MODELS</li>
</ul>
<h2 id="log-p">log p</h2>
<p>We can get an unbiased estimator for <span class="math inline">\(p(x)\)</span>, say <span class="math inline">\(\wh p\)</span>. But we often want <span class="math inline">\(\ln p(x)\)</span>. We use Jensen’s inequality and Markov’s inequality. So <span class="math inline">\(\ln \wh p\)</span> is a probabilistic lower bound. <span class="math display">\[
\E[\ln \wh p] \le \ln p\implies \quad \Pj(\ln \wh p&gt; \ln p + b) &lt;e^{-b}.
\]</span> (This is true no matter what the variance is. However, this can be a very loose bound. There is no good way of estimating <span class="math inline">\(\E \ln X\)</span> from draws of <span class="math inline">\(X\)</span> (why not?). Oddly, there is a good way of estimating <span class="math inline">\(\E e^X\)</span> from <span class="math inline">\(X\)</span> by power series expansion. (Power series for <span class="math inline">\(\ln\)</span> is terrible over long distances.))</p>
<p>Note this is prone to overestimation with little indication anything is wrong.</p>
<h2 id="elbo">ELBO</h2>
<p>Goal: posterior distribution <span class="math display">\[
p(z|x,\al) = \fc{p(z,x|\al)}{\int_z p(z,x|\al)}.
\]</span> Pick a family of distributions with variational parameters <span class="math inline">\(q(z_{1:m}|\nu)\)</span>. Use <span class="math inline">\(q\)</span> with fitted parameters as proxy.</p>
<p>So want to minimize <span class="math inline">\(KL(q||p)\)</span>.</p>
<p>Why <span class="math inline">\(q||p\)</span>, not <span class="math inline">\(p||q\)</span>?</p>
<ul>
<li>q high, p low is bad. Don’t want to make impossible events happen!</li>
<li>q low, p high is not so bad.</li>
</ul>
<span class="math display">\[\begin{align}
KL(q||p) &amp;=\EE_q\ba{\ln \fc{q(z)}{p(z|x)}}\\
\ln p &amp;=\ln \int \EE_{z\sim q}\ba{\fc{p(x,z)}{q(z)}}\\
&amp; \ge \EE_q \ln p(x,z) - \EE_q [\ln q] :=ELBO\\
KL(q||p) &amp;=-ELBO - \ln p(x)
\end{align}\]</span>
<p><span class="math inline">\(\ln p\)</span> doesn’t depend on <span class="math inline">\(q\)</span>.</p>
<h2 id="ais">AIS</h2>
<p>Given annealed distributions <span class="math inline">\(p_i\propto f_i\)</span>, <span class="math inline">\(p_K=p\)</span> with Markov chains <span class="math inline">\(M_i\)</span> (with transition kernels <span class="math inline">\(T_i\)</span>), to estimate <span class="math inline">\(Z=Z_K\)</span>,</p>
<ul>
<li>Sample from <span class="math inline">\(p_0\)</span>.</li>
<li>Let <span class="math inline">\(w=Z_0\)</span>.</li>
<li>For <span class="math inline">\(k=1:K\)</span>
<ul>
<li><span class="math inline">\(w\leftarrow w \fc{f_k(x_{k-1})}{f_{k-1}(x_{k-1})}\)</span></li>
<li><span class="math inline">\(x_k \sim T_k(\cdot |x_{k-1})\)</span>.</li>
</ul></li>
<li>Estimate is <span class="math inline">\(w\)</span>.</li>
</ul>
<p>For probabilistic neural nets, use Gibbs sampler (alternately sample <span class="math inline">\(h\)</span> and <span class="math inline">\(x\)</span>) as transition.</p>
<p>Think of this as proposing the distribution given by applying the <span class="math inline">\(T_i\)</span> in sequence.</p>
<p>Giving a stochastic lower bound for <span class="math inline">\(Z\)</span> means we overestimate log-likelihood.</p>
<h2 id="raise">RAISE</h2>
<p>Go the other way using samples from the target distribution This gives a probabilistic lower bound on <span class="math inline">\(\fc{Z_0}{Z}\)</span>, so a probabilistic upper bound on <span class="math inline">\(Z\)</span>, so we underestimate log-likelihood.</p>
<p><span class="math inline">\(p(h|v)\)</span> needs to be tractable. ((z|x) in above notation)</p>
<p>For intractable <span class="math inline">\(p(h|v)\)</span> combine the AIS (fixing <span class="math inline">\(v\)</span>) and RAISE steps to get a single estimate. See Algorithm 3 for details.</p>
<h2 id="notes">Notes</h2>
<ul>
<li>Is RAISE a provable (stochastic) upper bound? Even in the intractable case?</li>
<li>I’m surprised AIS/RAISE match so closely. Does this mean partition function calculation for deep belief nets is in practice tractable???</li>
<li>I think AIS can have large variance. It seems better to do the “evolutionary multiplicative update” thing. Does that have provable guarantees under similar conditions as Langevin annealing? Can AIS fail where this works? (Ex. continuously miss the high-prob stuff, stepping into the low-ratios between layers.)
<ul>
<li>Does this give a better bound in practice? I.e. larger estimate for <span class="math inline">\(Z\)</span>? (Warning: not quite unbiased anymore…)</li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>(HKY17) Hyperparameter Optimization - A Spectral Approach</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/HKY17.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/HKY17.html</id>
    <published>2017-07-19T00:00:00Z</published>
    <updated>2017-07-19T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>(HKY17) Hyperparameter Optimization - A Spectral Approach</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-07-19 
          , Modified: 2017-07-19 
	</p>
      
       <p>Tags: <a href="/tags/hyperparameters.html">hyperparameters</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#compressed-sensing-for-orthonormal-polynomials">Compressed sensing for orthonormal polynomials</a></li>
 <li><a href="#harmonica">Harmonica</a></li>
 <li><a href="#questions">Questions</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>The main theorem (Alg. 1, Thm. 6) is a theorem on learning Fourier-concentrated functions with much better sample complexity than [LMN93], using compressed sensing applied to orthonormal polynomials. Apply this theorem to the loss as a function of hyperparameters (A.g. 2, Harmonica). (Note this is heuristic.)</p>
<h2 id="compressed-sensing-for-orthonormal-polynomials">Compressed sensing for orthonormal polynomials</h2>
<p>An orthonormal family with respect to distribution <span class="math inline">\(D\)</span> has <span class="math inline">\(\EE_D[\psi_i (X) \psi_j(X)]=\de_{ij}\)</span>.</p>
<ul>
<li><span class="math inline">\(s\)</span>-sparse: <span class="math inline">\(L_0(f)\le s\)</span></li>
<li><span class="math inline">\((\ep,d)\)</span> concentrated: <span class="math inline">\(\ve{f - \pi_{\{\psi_{i}\}_{i\in S}}(f)}_2\le \ep\)</span>.</li>
<li><span class="math inline">\((\ep,d,s)\)</span>-bounded: additionally, <span class="math inline">\(\ve{f}_1\le s\)</span>.</li>
<li>Note we can approximate <span class="math inline">\(L_1(f)\le s\)</span> to <span class="math inline">\(\ep\)</span> with <span class="math inline">\(L_0(g)\le \fc{s^2}{\ep}\)</span>. (Sampling. Cf. Barron proof)</li>
</ul>
<p>LASSO: With appropriate <span class="math inline">\(\la\)</span>, <span class="math display">\[
\min_{x\in \R^n} [\ve{x}_1 + \la \ve{Ax-y}_2^2]
\]</span></p>
<p>For <span class="math inline">\(z^1,\ldots, z^m\sim D\)</span>, <span class="math inline">\(A_{ij}=\psi_j(z^i)\)</span>, <span class="math inline">\(y=Ax+e\)</span>, <span class="math inline">\(\ve{e}_2\le \eta\sqrt m\)</span>, <span class="math inline">\(x^*\)</span> solving LASSO, <span class="math display">\[
\Pj(\ve{x-x^*}_2\le C \fc{\si_s(x)_1}{\sqrt s}+d\eta) \ge 1-\de
\]</span> where <span class="math inline">\(\si_s(x)_1=\min\set{\ve{x-z}_1}{z\text{ is s-sparse}}\)</span>, <span class="math inline">\(c,d\)</span> constants, with <span class="math inline">\(m\ge CK^2 s \poly\log(K,s,N,\rc{\de})\)</span> samples.</p>
<p>Apply for low-degree recovery: if <span class="math inline">\(f\)</span> is (<span class="math inline">\(\ep,d,s\)</span>)-bounded, then using this finds <span class="math inline">\(g\equiv_\ep f\)</span> in time <span class="math inline">\(O(n^d)\)</span>, with <span class="math inline">\(T=\wt O(K^2s^2 \ln N/\ep)\)</span> samples. (? <span class="math inline">\(\ep\)</span> outside)</p>
<p>(LMN93 needs <span class="math inline">\(\Om\pf{NL_\iy(f)^2}{\ep}\)</span> samples.) (? What is <span class="math inline">\(N\)</span> here? Number of orthonormal polys. Shouldn’t it be <span class="math inline">\(n^d\)</span>?)</p>
<h2 id="harmonica">Harmonica</h2>
<p>Apply in stages, with some degree <span class="math inline">\(d\)</span> and sparsity <span class="math inline">\(s\)</span>. Note this can involve at most <span class="math inline">\(ds\)</span> variables. Suppose the approximation <span class="math inline">\(g\)</span> to <span class="math inline">\(f\)</span> only involves variables in <span class="math inline">\(J\)</span>.</p>
<p>Take the best <span class="math inline">\(t\)</span> solutions <span class="math inline">\(x_i*\)</span> to <span class="math inline">\(g\)</span> on <span class="math inline">\(J\)</span>, and now apply to <span class="math inline">\(\rc t \sumo it f_{J \leftarrow x_i^*}(x)\)</span>.</p>
<h2 id="questions">Questions</h2>
<ul>
<li>Why does multiple stages help?</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>(AR17) Provable benefits of representation learning</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/representation/AR17.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/representation/AR17.html</id>
    <published>2017-07-18T00:00:00Z</published>
    <updated>2017-07-18T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>(AR17) Provable benefits of representation learning</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-07-18 
          , Modified: 2017-07-18 
	</p>
      
       <p>Tags: <a href="/tags/representation%20learning.html">representation learning</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#intro">Intro</a></li>
 <li><a href="#section">2</a><ul>
 <li><a href="#section-1">2.2</a></li>
 </ul></li>
 <li><a href="#section-2">3</a><ul>
 <li><a href="#section-3">3.1</a></li>
 <li><a href="#section-4">3.2</a></li>
 </ul></li>
 <li><a href="#section-5">4</a><ul>
 <li><a href="#section-6">4.1</a></li>
 <li><a href="#section-7">4.2</a></li>
 </ul></li>
 <li><a href="#section-8">5</a><ul>
 <li><a href="#lower-bounds-for-nearest-neighbors">5.1 Lower bounds for nearest neighbors</a></li>
 <li><a href="#section-9">5.3</a></li>
 </ul></li>
 <li><a href="#notes">Notes</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="intro">Intro</h2>
<ul>
<li>Contributions
<ul>
<li>Formalizes representation learning, unifying disparate settings.</li>
<li>Quantifies “utility” from representation learning.</li>
<li>Prove separation results between representation learning and simpler algorithms.</li>
</ul></li>
<li>“Bayes+”</li>
<li>Why representation learning?
<ul>
<li>Allows semi-supervised learning.</li>
<li>Simpler methods need too many samples.</li>
<li>Provably better than manifold learning in some cases.</li>
</ul></li>
<li>The framework
<ul>
<li>Many-to-one map <span class="math inline">\(x\mapsto h\)</span>. <span class="math inline">\(h\)</span> is “high-level” representation.</li>
<li>Generative model <span class="math inline">\(h\to x\)</span>.</li>
<li>Similarity in the latent space (of <span class="math inline">\(h\)</span>) is more informative.</li>
<li>Defintion: A <span class="math inline">\((\ga,\be)\)</span>-valid decoder has <span class="math inline">\(\Pj(\ve{f(x)-h}\le (1-\ga) \ve{h})\ge \be\)</span>. (Think of <span class="math inline">\(\ga,\be\approx 1\)</span>.)</li>
<li>Utility: If <span class="math inline">\(C\)</span> is <span class="math inline">\(\al\)</span>-Lipschitz, <span class="math inline">\(\ve{C(f(x)) - C(h)}_\iy\le (1-\ga)\al \ve{h}\)</span>.</li>
</ul></li>
<li>Examples
<ul>
<li>Clustering</li>
<li>Manifold</li>
<li>Kernel learning</li>
</ul></li>
<li>Non-examples
<ul>
<li>Nearest neighbor (provably weaker in some settings)</li>
<li>LSH - this preserves distance, which is not our goal.</li>
</ul></li>
<li>Contrast [HM16], which is assumption-free and basically lossless compression. (ex. Lempel-Ziv) This notion is different, ex. allows throwing away noise.</li>
<li>Compare to the usual: Maximize log probability (MLE), then <span class="math inline">\(\amax_h p_\te(h|x)\)</span>.
<ul>
<li>Unlike Bayesian which gives a distribution over <span class="math inline">\(h\)</span>, we output single <span class="math inline">\(h\)</span>.</li>
</ul></li>
</ul>
<h2 id="section">2</h2>
<h3 id="section-1">2.2</h3>
<ul>
<li>Encoder exists <span class="math inline">\(\implies\)</span> given <span class="math inline">\(x\)</span>, <span class="math inline">\(h\)</span> is concentrated around <span class="math inline">\(f(x)\)</span>, almost uniquely defined.</li>
<li>Having concentration is stronger than just being able to do inference.</li>
</ul>
<h2 id="section-2">3</h2>
<h3 id="section-3">3.1</h3>
<p>Topic modeling</p>
<ul>
<li><span class="math inline">\(k\)</span> topices</li>
<li>Each distribution on <span class="math inline">\(M\)</span> words. <span class="math inline">\(A_i\in \R^M\)</span>.</li>
<li>Mixture coefficients <span class="math inline">\(h_i\)</span>.</li>
<li>Draw bag of words <span class="math inline">\(x\sim \sum h_i A_i\)</span>, <span class="math inline">\(x\in \Z^N\)</span>.</li>
</ul>
<h3 id="section-4">3.2</h3>
<p>Loglinear model: (continued below)</p>
<p><span class="math display">\[p(x,h) = p(h)p(x|h).\]</span></p>
<h2 id="section-5">4</h2>
<h3 id="section-6">4.1</h3>
<p>Topic modeling: want <span class="math inline">\(\ell_\iy\to \ell_1\)</span> condition number to be small.</p>
<h3 id="section-7">4.2</h3>
<ul>
<li><span class="math inline">\(h\in \R^d\)</span> randomly on unit sphere.</li>
<li><span class="math inline">\(\Pj(x|h)\propto e^{\an{W_x,h}}\)</span>.
<ul>
<li><span class="math inline">\(W_x = Bv\)</span>, <span class="math inline">\(B=O(1)\)</span>, <span class="math inline">\(v\sim N(0,I)\)</span>.</li>
</ul></li>
<li>Take <span class="math inline">\(f(x) = \nv{\sum_i W_{x_i}}\)</span>.</li>
</ul>
<h2 id="section-8">5</h2>
<h3 id="lower-bounds-for-nearest-neighbors">5.1 Lower bounds for nearest neighbors</h3>
<ul>
<li><span class="math inline">\(M\)</span> movies, <span class="math inline">\(k\)</span> genres
<ul>
<li><span class="math inline">\(\ve{h}_0 = s\)</span>, <span class="math inline">\(h\in \{0,1\}^k\)</span></li>
<li>Draw movies <span class="math inline">\(\ve{x}_0=T\)</span>.</li>
</ul></li>
<li>For <span class="math inline">\(T\ll \sqrt m\)</span>, can’t learn using NN because
<ul>
<li>Users will share few movies in common.</li>
<li>Users who share movies won’t share genres. (Construct example where some movies belong to all genres.)</li>
</ul></li>
</ul>
<h3 id="section-9">5.3</h3>
<ul>
<li><span class="math inline">\(k\)</span> genres</li>
<li><span class="math inline">\(T=\Om(\ln M)\)</span> ratings per user</li>
<li><span class="math inline">\(s\)</span> genres per user</li>
<li><span class="math inline">\(\ell(h) = \sgn(\an{w,2h-1})\)</span>.</li>
</ul>
<p>Can do semi-supervised learning by doing representation learn using [AKM16]. (S4.1)</p>
<h2 id="notes">Notes</h2>
<ul>
<li>Check Thm. 4.1 using [AKM16] - review “condition number”.</li>
<li>Check Thm. 5.1. Look up background on NN.</li>
<li>Check Thm. 5.4.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>(AKV17) A sparse recovery view of sentence embeddings, bag of n-grams, and LSTMs</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/BONGs.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/BONGs.html</id>
    <published>2017-07-18T00:00:00Z</published>
    <updated>2017-07-18T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>(AKV17) A sparse recovery view of sentence embeddings, bag of n-grams, and LSTMs</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-07-18 
          , Modified: 2017-07-18 
	</p>
      
       <p>Tags: <a href="/tags/sparse%20recovery.html">sparse recovery</a>, <a href="/tags/n-grams.html">n-grams</a>, <a href="/tags/lstm.html">lstm</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <ul>
<li>LSTMs can do as well as linear classification over BonGs.</li>
<li>Calderbank: SVM can do as well given compressed <span class="math inline">\(Ax\)</span> instead of <span class="math inline">\(x\)</span>, if <span class="math inline">\(A\)</span> is RIP.
<ul>
<li>But word vectors are not RIP, not even statistically.</li>
</ul></li>
<li>Basis pursuit/LASSO does better on word embeddings than random embeddings, even though word embeddings aren’t incoherent/RIP.
<ul>
<li>What’s a good explanation? Word vectors in sentence are often linearly separable from other vectors - why?</li>
<li>MP/OMP does worse on word embeddings.</li>
</ul></li>
</ul>
<p>(RIP stronger than incoherency? Ex. incoherency can have linear dependencies… Incoherency gives something like statistical RIP?)</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2017-07-22</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2017-07-22.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2017-07-22.html</id>
    <published>2017-07-18T00:00:00Z</published>
    <updated>2017-07-18T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2017-07-22</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-07-18 
          , Modified: 2017-07-18 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#current-projects">Current projects</a></li>
 <li><a href="#reading">Reading</a></li>
 <li><a href="#explorations">Explorations</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="current-projects">Current projects</h2>
<ul>
<li>Langevin (Mon)</li>
<li>EGNN</li>
<li>NLP: (Mon)
<ul>
<li>BoNGs: make recovery work for <span class="math inline">\(n\)</span>-grams, <span class="math inline">\(n\ge 2\)</span>.</li>
<li>Treegrams
<ul>
<li>cf. Sida Wang</li>
<li>hyperdim vectors</li>
</ul></li>
<li>document embedding (axioms?)</li>
<li>GANs for BoNGs</li>
</ul></li>
<li>Long-term memory (COLT open problem) (Tue)
<ul>
<li>For convex optimization</li>
<li>Tue: this seems difficult because of “bottleneck” of probability <span class="math inline">\(\ll \rc{\poly(n)}\)</span>. Next step: familiarize with lower-bound techniques and try to prove lower bound.</li>
</ul></li>
<li>Reinforcement learning (experiments - what to do?)</li>
</ul>
<h2 id="reading">Reading</h2>
<ul>
<li><a href="https://workflowy.com/#/cc7e392e4fff">Do GANs learn?</a></li>
<li><a href="../tcs/machine_learning/representation/AR17.md">Representation learning</a></li>
<li>BONGs</li>
<li>Hyperparameter tuning</li>
</ul>
<h2 id="explorations">Explorations</h2>
<ul>
<li>Further on temperature varying and Langevin
<ul>
<li>AIS/RAISE estimator - similar criterion?
<ul>
<li>See [BGS14] Accurate and Conservative Estimates of MRF Log-likelihood using Reverse Annealing</li>
<li>“Multiplicative” weights</li>
</ul></li>
<li>Other settings where annealing helps. Analogue on Boolean cube.</li>
<li>Tensor decomposition.
<ul>
<li>Beyond the homotopy method.</li>
<li>[MR16]</li>
</ul></li>
</ul></li>
<li>Co-training - predict one part from other. Relating to/extending CCA.</li>
<li>RL - kernel?</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>(TPGB17) The space of transferable adversarial examples</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/TPGB17.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/TPGB17.html</id>
    <published>2017-05-24T00:00:00Z</published>
    <updated>2017-05-24T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>(TPGB17) The space of transferable adversarial examples</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-05-24 
          , Modified: 2017-05-24 
	</p>
      
       <p>Tags: <a href="/tags/neural%20nets.html">neural nets</a>, <a href="/tags/aaml.html">aaml</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#adversarial-subspaces">Adversarial subspaces</a></li>
 <li><a href="#decision-boundaries">Decision boundaries</a><ul>
 <li><a href="#experiments">Experiments</a></li>
 </ul></li>
 <li><a href="#limits-of-transferability">Limits of transferability</a></li>
 </ul> </div>

  <div class="blog-main">
    <!--
See also 

* [intro to problem](adversarial.html).
* [my experiments](adversarial_experiments.html). 
* [confidence](confidence.html).
-->
<h2 id="adversarial-subspaces">Adversarial subspaces</h2>
<ul>
<li>Introduces methods for discovering a subspace of adversarial perturbations.
<ul>
<li>MNIST: 25 dimensions</li>
</ul></li>
<li>Distance traveled before reaching decision boundary is on average larger than distance separating decision boundaries of 2 models in that direction. (This doesn’t seem surprising.)</li>
</ul>
<p>Recall that FGSM is <span class="math display">\[
x^* = x + \ep \nv{\nb_x J(x,y)}.
\]</span></p>
<p>Techniques:</p>
<ul>
<li>Solve optimization problem multiple times, constraining the next direction to be orthogonal to the previous. <strong>GAAS works well, others don’t.</strong>
<ul>
<li>Second-order approximation of loss function <span class="math inline">\(\max_{\ve{r}\le \ep} g^T r + \rc 2 r^T H r\)</span>.</li>
<li>Convex optimization: Write an LP for the region where <span class="math inline">\(f\)</span> is piecewise linear, and throw in orthogonality condition.</li>
<li>GAAS (gradient aligned adversarial subspace): Find orthogonal <span class="math inline">\(r_1,\ldots, r_k\)</span> with <span class="math display">\[\ve{r_i}_2\le \ep, \quad r_i^T \nb_x J(x,y)\ge \ga.\]</span> (Think of a right-angled simplex with vertex at <span class="math inline">\(x\)</span>. We can compute how many there are given desired <span class="math inline">\(\fc{g^Tr_i}{\ve{g}_2}\)</span>: <span class="math inline">\(\min\{\ff{1}{\al^2}, d\}\)</span>.)</li>
<li>JSMA: partition most salient features into <span class="math inline">\(k\)</span> bins; use these <span class="math inline">\(k\)</span> sets to get <span class="math inline">\(k\)</span> orthogonal perturbations.</li>
</ul></li>
</ul>
<p>For DNN, get 44 directions, 25 of which transfer. For CNN, get 15 directions, 2 of which transfer.</p>
<h2 id="decision-boundaries">Decision boundaries</h2>
<p>Adversarial training does not significantly displace decision boundary.</p>
<p>Define unit norm directions <span class="math display">\[
d(f,x) := \fc{x'-x}{\ve{x'-x}}
\]</span> where <span class="math inline">\(x'\)</span> is defined differently in 3 cases:</p>
<ol type="1">
<li>Legitimate direction <span class="math inline">\(d_{leg}\)</span>: <span class="math inline">\(x'\)</span> is closest data point with different class label.</li>
<li>Adversarial example <span class="math inline">\(d_{adv}\)</span>: adversarial example generated from <span class="math inline">\(x\)</span>.</li>
<li><span class="math inline">\(d_{rand}\)</span> random <span class="math inline">\(x'\)</span> in input domain that is classified differently.</li>
</ol>
<p>Define minimum distance <span class="math display">\[
MD_d(f,x) = \amin_{\ep&gt;0} f(x+\ep \cdot d) \ne f(x)
\]</span> and interboundary distance as <span class="math display">\[
ID_d(f_1,f_2,x) = 
|MD_d(f_1,x) - MD_d (f_2,x)|
\]</span></p>
<h3 id="experiments">Experiments</h3>
<p>Transfer from</p>
<ul>
<li>logistic regression (LR)</li>
<li>support vector machine (SVM)</li>
<li>DNN</li>
</ul>
<p>Defenses only prevent white-box attacks by reducing reliability of 1st order approximations (gradient masking).</p>
<h2 id="limits-of-transferability">Limits of transferability</h2>
<p>This hypothesis is false: If 2 models achieve low error while exhibiting low robustness, then adversarial examples transfer between models.</p>
<p>Ex. Adversarial examples on MNIST don’t transfer between linear and quadratic models.</p>
Model-agnostic perturbation: For a fixed feature mapping <span class="math inline">\(\phi\)</span>, define <span class="math inline">\(\de_\phi\)</span> as difference in intra-class means, and the adversarial direction <span class="math inline">\(r_\phi\)</span> for <span class="math inline">\((x,y)\)</span>,
<span class="math display">\[\begin{align}
\de_\phi:&amp;=\rc 2 (\E_{\mu_{+1}} [\phi(x)]
- \E_{\mu_{-1}}[\phi(x)])\\
r_\phi:&amp;= - \ep y \wh \de_\phi.
\end{align}\]</span>
<p>If <span class="math inline">\(f(x) = w^T\phi(x)+b\)</span>, and <span class="math inline">\(\De:=\wh w^T \wh\de_\phi\)</span> is large, and <span class="math inline">\(\phi\)</span> is “pseudo-linear” (<span class="math inline">\(\phi(x+r)-\phi(x)\approx r_\phi\)</span>) then <span class="math inline">\(x+r\)</span> transfers to <span class="math inline">\(f\)</span>.</p>
<p>TLDR: shift points in direction of difference of class means; this transfers well.</p>
<p>Can models with access to same set of input features learn representations that don’t transfer?</p>
<p>There’s a simple (but not very informative…) example where this works: MNIST with XOR artifacts trained on linear and quadratic.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2017-04-29</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2017-04-29.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2017-04-29.html</id>
    <published>2017-04-25T00:00:00Z</published>
    <updated>2017-04-25T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2017-04-29</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-04-25 
          , Modified: 2017-04-25 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#day-by-day">Day by day</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="day-by-day">Day by day</h2>
<ul>
<li><p>Mon. 4-24: If I can bound <span class="math display">\[
\an{\mathcal L g, \an{\nb f, \nb g}} \ll \an{\mathcal L g, \mathcal L g}
\]</span> independent of <span class="math inline">\(f\)</span>, then I can argue that for small <span class="math inline">\(\de\)</span>, eigenvectors for Langevin on <span class="math inline">\((1-\de)f\)</span> are close to eigenvectors for Langevin on <span class="math inline">\(f\)</span>. (One has to be careful with <a href="/posts/math/algebra/linear/matrix_analysis/perturbation.html">which eigenspaces to work with</a>.)</p>
I don’t know how to do this. The best I can do is
<span class="math display">\[\begin{align}
\an{f, \mathcal L_\mu f}_\mu &amp;\le -k \ve{f}_\mu^2\\
\implies \an{f, \mathcal L_{\mu'} f}_{\mu'} &amp;\le -\fc{k}{1+O(\de)}\ve{f}_{\mu'}^2.
\end{align}\]</span>
where <span class="math inline">\(\mu'\)</span> is for <span class="math inline">\((1-\de)f\)</span>, which gives an angle between eigenspaces of <span class="math inline">\(1+O\pf{\la_k}{\la_l}\)</span> where <span class="math inline">\(\la_k,\la_l\)</span> are the threshold values for the eigenspaces. This does NOT go to 1. I need something that goes to 1.</li>
<li>Tue. 4-25:
<ul>
<li>Perceptron writeup.</li>
<li>Experiment with semi-random features.
<ul>
<li>Distribution is normal, square loss. (But maybe square loss is just a proxy?)</li>
<li>Doesn’t seem to work: loss on batch of 100 is on order of 20, even with 50 nodes.</li>
</ul></li>
<li>Can estimate gaussian robustly given samples from something with <span class="math inline">\(L^\iy\)</span> distance (in log space) <span class="math inline">\(h\)</span> with <span class="math inline">\(\wt O (h^2)\)</span> samples. Can we extend this to convex things?</li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
