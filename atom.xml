<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2016-08-13T00:00:00Z</updated>
    <entry>
    <title>Weekly summary 2016-08-13</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-08-13.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-08-13.html</id>
    <published>2016-08-13T00:00:00Z</published>
    <updated>2016-08-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2016-08-13</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-13 
          , Modified: 2016-08-13 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p>Goals:</p>
<ul>
<li>Run basic PMI experiments
<ul>
<li><a href="/posts/tcs/machine_learning/neural_nets/pmi_images.html">PMI images</a>, <a href="/posts/tcs/machine_learning/nlp/pmi.html">PMI idea</a></li>
<li>Get dictionary learning/weighted SVD code from Yingyu.</li>
<li>Write up findings.</li>
</ul></li>
<li>Learning
<ul>
<li>Go through half of ATAP.</li>
<li>Ch. 4 of HDP.</li>
<li>Understand conjugate gradients using Chebyshev polys. Look at AGD from this point of view.</li>
<li>Summarize bandit paper.</li>
</ul></li>
<li>Representation learning
<ul>
<li>Clarify idea.</li>
</ul></li>
<li>Tensorflow
<ul>
<li>Get basic LSTM running.</li>
<li>Play with parameters in MNIST.</li>
</ul></li>
</ul>
<p>Summary:</p>
<ul>
<li><a href="/posts/math/analysis/numerical/ATAP.html">Approximation theory and approximation practice</a>: Notes
<ul>
<li>Understood conjugate gradient bound using Chebyshev polynomials (<span class="citation" data-cites="Cyril">@Cyril</span>)</li>
</ul></li>
<li></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Approximation theory and approximation practice</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/analysis/numerical/ATAP.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/analysis/numerical/ATAP.html</id>
    <published>2016-08-08T00:00:00Z</published>
    <updated>2016-08-08T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Approximation theory and approximation practice</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-08 
          , Modified: 2016-08-08 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#introduction">1 Introduction</a></li>
 <li><a href="#chebyshev-points-and-interpolation">2 Chebyshev points and interpolation</a></li>
 <li><a href="#chebyshev-polynomials-and-series">3 Chebyshev polynomials and series</a></li>
 <li><a href="#interpolation-projection-and-aliasing">4 Interpolation, projection, and aliasing</a></li>
 <li><a href="#barycentric-interpolation-formula">5 Barycentric interpolation formula</a></li>
 <li><a href="#weierstrass-approximation-theorem">6 Weierstrass Approximation Theorem</a></li>
 <li><a href="#convergence-for-differentiable-functions">7 Convergence for differentiable functions</a></li>
 <li><a href="#convergence-for-analytic-functions">8 Convergence for analytic functions</a></li>
 <li><a href="#gibbs-phenomenon">9 Gibbs Phenomenon</a></li>
 <li><a href="#best-approximation">10 Best approximation</a></li>
 <li><a href="#hermite-integral-formula">11 Hermite integral formula</a></li>
 <li><a href="#potential-theory-and-applications">12 Potential theory and applications</a></li>
 <li><a href="#equispaced-points-runge-phenomenon">13 Equispaced points, Runge phenomenon</a></li>
 <li><a href="#discussion-of-high-order-interpolation">14 Discussion of high-order interpolation</a></li>
 <li><a href="#lebesgue-constants">15 Lebesgue constants</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="introduction">1 Introduction</h2>
<h2 id="chebyshev-points-and-interpolation">2 Chebyshev points and interpolation</h2>
Define <strong>Chebyshev points</strong> of parameter <span class="math inline">\(n\)</span> to be <span class="math inline">\(x_j\)</span> where
\begin{align}
\te_j &amp;= \fc{j\pi }n\\
z_j &amp; = e^{\pi j/n}, &amp;0\le j\le n\\
x_j &amp; = \Re (z_j) = \fc{z_j+z_j^{-1}}2 = \cos\te_j.
\end{align}
<p>Write</p>
<div class="sourceCode"><pre class="sourceCode matlab"><code class="sourceCode matlab">xx=chebpts(n+<span class="fl">1</span>)</code></pre></div>
<p>(Note: from now on we will reverse the order of the <span class="math inline">\(x_j\)</span>, to go from left to right.)</p>
<p>The <strong>Chebyshev interpolant</strong> of <span class="math inline">\((f_j)\)</span> is the unique polynomial of degree (<span class="math inline">\(\le\)</span>) <span class="math inline">\(n\)</span> going through points <span class="math inline">\(x_j,f_j\)</span>. To construct the interpolant for a function,</p>
<div class="sourceCode"><pre class="sourceCode matlab"><code class="sourceCode matlab">x = chebfun(<span class="st">'x'</span>);
f = (function of x);
p=chebfun(f,n+<span class="fl">1</span>); hold on, plot(p,<span class="st">'.-'</span>)</code></pre></div>
<p>(Note: <code>x</code> is like a vector here. So you must write <code>.*</code> for multiplication, etc.) <code>.-</code> makes it point out the Chebyshev interpolation points.</p>
<h2 id="chebyshev-polynomials-and-series">3 Chebyshev polynomials and series</h2>
<p>Define <a href="chebyshev.html">Chebyshev polynomials</a>.</p>
The following are Chebyshev, Laurent, and Fourier expansions.
\begin{align}
f(x) &amp;\approx \sumz kn a_kT_k(x), &amp;&amp; x&amp;\in [-1,1]\\
F(z) &amp;= \rc 2 \sumz kn a_k(z^k+z^{-k}) &amp; f(x)&amp;=F\pf{z+z^{-1}}2&amp;|z|&amp;=1\\
\cF (\te) &amp;\approx \rc 2 \sumz kn a_k(e^{ik\te} + e^{-ik\te})&amp; \te&amp;\in [-\pi,\pi]
\end{align}
<p>One can expand a polynomial in Chebyshev basis using the FFT or Fast Cosine Transform.</p>
<p>Chebyshev series: If <span class="math inline">\(f\)</span> is Lipschitz continuous on <span class="math inline">\([-1,1]\)</span>, it has a unique representation <span class="math display">\[f(x)=\sumz k\iy a_k T_k(x),\quad a_k = \fc2\pi \int_{-1}^1 \fc{f(x)T_k(x)}{\sqrt{1-x^2}} \dx. \]</span></p>
<em>Proof</em>.
\begin{align}
a_k &amp;= \rc{2\pi i} \oint_{|z|=1} (z^{-1+k}+z^{-1-k})F(z)\,dz\\
&amp;= \rc{\pi i} \int_{|z|=1} z^{-1}T_k(x) F(z)\,dz\\
&amp;=-\rc{\pi}\fc{f(x)T_k(x)}{\Im z}\,dz &amp; dx&amp;=\rc 2 \pf{z-z^{-1}}{2z}dz = \rc2 i z^{-1}\Im z\\
&amp;=\fc2\pi \int_{-1}^1 \fc{f(x)T_k(x)}{\sqrt{1-x^2}} \dx.
\end{align}
<p>Orthonormality: <span class="math display">\[ \fc 2\pi\int_{-1}^1 \fc{T_l(x)T_k(x)}{\sqrt{1-x^2}} = \de_{kl}.\]</span></p>
<h2 id="interpolation-projection-and-aliasing">4 Interpolation, projection, and aliasing</h2>
<p>There are 2 ways to approximate <span class="math inline">\(f\)</span> by a Chebyshev expansion.</p>
<ol type="1">
<li>Interpolation of Chebyshev points <span class="math inline">\(p_n\)</span></li>
<li>Truncation of Chebyshev expansion <span class="math inline">\(f_n\)</span></li>
</ol>
<p>In computation, <span class="math inline">\(p_n\)</span> are usually almost as good and easier to evaluate.</p>
<p><strong>Theorem</strong> (aliasing). <span class="math inline">\(T_k\)</span> takes the same value on the <span class="math inline">\((n+1)\)</span>-point Chebyshev grid as <span class="math inline">\(T_m\)</span>, <span class="math inline">\(m=|(k+n-1)\bmod{2n} - (n-1)|\)</span>. (i.e., <span class="math inline">\(T_m\)</span> takes the same value as <span class="math inline">\(T_k\)</span> with <span class="math inline">\(k=2jn\pm m\)</span>.)</p>
<p><em>Proof</em>. Look at <span class="math inline">\(2n\)</span>th roots of unity.</p>
<strong>Theorem</strong> (aliasing for Chebyshev coefficients). Let <span class="math inline">\(a_k,c_k\)</span> be Chebyshev coefficients of <span class="math inline">\(f\)</span> and <span class="math inline">\(p_n\)</span>. Then
\begin{align}
c_k &amp;= \sum_{i\in \{2jn\pm k\}\cap \N} a_i.
\end{align}
<p>So <span class="math display">\[f(x)-p_n(x) = \sum_{k=n+1}^{\iy} a_k(T_k(x) - T_{m(k,n)}(x)).\]</span></p>
<p>In Chebfun, to get truncation, interpolation use</p>
<div class="sourceCode"><pre class="sourceCode matlab"><code class="sourceCode matlab">a = chebpoly(f) <span class="co">% truncation/projection</span>
fn = chebfun(f, <span class="st">'trunc'</span>, n+<span class="fl">1</span>) <span class="co">% truncation</span>
pn = chebfun(f, n+<span class="fl">1</span>)
c = chebpoly(pn) <span class="co">% interpolation</span></code></pre></div>
<h2 id="barycentric-interpolation-formula">5 Barycentric interpolation formula</h2>
<p>How to evaluate a Chebyshev interpolant?</p>
<ul>
<li><span class="math inline">\(O(n\ln n)\)</span> for single point evaluation: Compute coefficients and use the series.</li>
<li><span class="math inline">\(O(n)\)</span> work, stable: Barycentric interpolation formula.</li>
</ul>
<p>Lagrange interpolation:</p>
\begin{align}
p(x) &amp;= \sumz jn f_jl_j(x)\\
l_j(x) &amp;=\fc{\prod_{k\ne j}(x-x_k)}{\prod_{k\ne j} (x_j-x_k)}
\end{align}
<p>The computation of <span class="math inline">\(l_j\)</span> is unstable and takes <span class="math inline">\(O(n^2)\)</span> operations.</p>
Modified Lagrange formula (first barycentric formula)
\begin{align}
p(x) &amp;= l(x) \sumz jn \fc{\la_j}{x-x_j} f_j\\
l(x) &amp;= \prod_{k=0}^{n} (x-x_k)\\
\la_j &amp;= \rc{\prod_{k\ne j}(x_j-x_k)} = \rc{l'(x_j)}.
\end{align}
<p>This takes <span class="math inline">\(O(n)\)</span> operations to evaluate. (Computing the weights takes <span class="math inline">\(O(n^2)\)</span> but only needs to be done once.)</p>
<p><strong>Theorem</strong> (Barycentric interpolation formula). <span class="math display">\[p(x) = \left. \sumz jn \fc{\la_j f_j}{x-x_j} \right/ \sumz jn \fc{\la_j}{x-x_j}.\]</span></p>
<p><em>Proof</em>. Note <span class="math inline">\(\sumz jn l_j(n)=1\)</span>. so just replace the <span class="math inline">\(l_j\)</span> by <span class="math inline">\(\fc{\la_j}{x-x_j}\)</span> and then normalize afterwards.</p>
<p>For Chebyshev points <span class="math inline">\(\la_j = (-1)^j \fc{2^{n-1}}{n} (1-\rc 2(j=0\text{ or }n))\)</span>. Obtain <span class="math display">\[p(x) = \left. \sumz jn' \fc{(-1)^jf_j}{x-x_k}\right/\sumz jn' \fc{(-1)^j}{x-x_k}
\]</span> where <span class="math inline">\(\sum'\)</span> means that the first and last terms are multiplied by <span class="math inline">\(\rc2\)</span>.</p>
<em>Proof</em>.
\begin{align}
l(x) &amp;= (x-x_0)\cdots (x-x_n)\\
&amp; = (z^{2n}-1)(z^2-1)(z^{-n-1}-1) \rc{2^{n+1}} &amp; x=\fc{z+z^{-1}}2\\
&amp;= \rc{2^n} \pa{\fc{x^{n+1}+x^{-(n+1)}}2 - \fc{x^{n-1}+x^{-(n-1)}}}\\
\la_j &amp;=\rc{l'(x_j)}  = \fc{2^n}{T_{n+1}'(x_j) - T_{n-1}'(x_j)} = \fc{2^n}{2n(-1)^j(1+(j=0\text{ or }n))}
\end{align}
<p>by using the recurrence relation to compute the derivatives.</p>
<p>This is stable, whereas polynomial interpolation via Vandemonde is exponentially unstable.</p>
<p>Warning:</p>
<ul>
<li>This is unstable for extrapolation.</li>
<li>The type 1 barycentric formula is stable.</li>
<li>The barycentric formulas are forward stable; the 1st barycentric formula is backwards stable.</li>
<li>Use the 1st formula for points far from Chebyshev.</li>
<li>The 1st formula is not scale-invariant.</li>
</ul>
<h2 id="weierstrass-approximation-theorem">6 Weierstrass Approximation Theorem</h2>
<p><strong>Theorem</strong> (Weierstrass approximtion). A continuous function on <span class="math inline">\([-1,1]\)</span> can be arbitrarily well-approximated in <span class="math inline">\(L^{\iy}\)</span> by polynomials.</p>
<p><em>Proof</em>. Extend <span class="math inline">\(f\)</span> to be continuous with compact support. Take <span class="math inline">\(f\)</span> as initial data for <span class="math inline">\(u_t=u_{xx}\)</span>. The solution is <span class="math inline">\(f*e^{-\fc{x^2}{4t}}/\sqrt{2\pi t}\)</span>. For <span class="math inline">\(t&gt;0\)</span> this is analytic and has uniformly convergent Taylor series.</p>
<p>Bernstein’s proof is a discrete analogue replacing diffusion by a random walk.</p>
<p><strong>Theorem</strong> (Mergelyan). If <span class="math inline">\(f\)</span> is continuous on compact simply connected <span class="math inline">\(K\sub \C\)</span> and analytic in <span class="math inline">\(K^{\circ}\)</span>, then <span class="math inline">\(f\)</span> can be approximated on <span class="math inline">\(K\)</span> by polynomials.</p>
<h2 id="convergence-for-differentiable-functions">7 Convergence for differentiable functions</h2>
<p>(Bounded variation means <span class="math inline">\(\ve{f'}_1&lt;\iy\)</span> where <span class="math inline">\(f'\)</span> is interpreted in the distributional sense.)</p>
If <span class="math inline">\(f^{(k)}, k\le v-1\)</span> is absolutely continuous and <span class="math inline">\(f^{(v)}\)</span> has bounded variation, then
\begin{align}
a_k &amp;\le \fc{2V}{\pi k\fp{v+1}} &amp;\le \fc{2V}{\pi (k-v)^{v+1}}\\
\ve{f-f_n}_\iy &amp;\le \sum_{k=n+1}^\iy |a_k| \le \fc{2V}{\pi v(n-v)^v}\\
\ve{f-p_n}_\iy &amp;\le \sum_{k=n+1}^{\iy} 2|a_k|\le \fc{4V}{\pi v(n-v)^v}
\end{align}
<em>Proof</em>.
\begin{align}
a_k &amp;= \rc{\pi i}\int_{|z|} f(\rc 2(z+z^{-1})) z^{k-1}\,dz\\
&amp;=-\rc{\pi i} \int_{|z|=1} f'(\rc 2 (z+z^{-1}))\fc{z^k}{k}\dx\\
|a_k| &amp;\le \fc{2}\pi \int_{-1}^1 |f'\Im\fc{z^k}{k}|\dx.
\end{align}
<p>For <span class="math inline">\(v&gt;0\)</span>, integrate by parts <span class="math inline">\(v+1\)</span> times to get <span class="math display">\[(-1)^{v+1} \rc{\pi i}\int_{|z|=1} f^{(v+1)}(\rc 2 (z+z^{-1})) f_{v, k}(z)\dx\]</span> where <span class="math inline">\(f_{0,k}(z) = \fc{z^k}{k}\)</span> and <span class="math inline">\(f_{n+1,k} = \int f_{n,k} \fc{1-z^{-2}}2\)</span>.</p>
<h2 id="convergence-for-analytic-functions">8 Convergence for analytic functions</h2>
If the analytic continuation of <span class="math inline">\(f\)</span> to the open Bernstein ellipse <span class="math inline">\(\pa{z\mapsto \fc{z+z^{-1}}2}B_\rh\)</span> satisfies <span class="math inline">\(f\le M\)</span>, then
\begin{align}
|a_k| &amp;\le 2M\rh^{-k}\\
\ve{f-f_n}&amp;\le \fc{2M\rh^{-n}}{\rh - 1}\\
\ve{f-p_n}&amp;\le \fc{4M\rh^{-n}}{\rh - 1}.
\end{align}
<p><em>Proof</em>. <span class="math inline">\(a_k = \ab{\rc{\pi i}\int_{|z|=\rh} z^{-1-k} F(z)\,dz}\)</span>.</p>
<p>Converse: If there exist <span class="math inline">\(q_n\)</span>, <span class="math inline">\(\ve{f-q_n}\le C\rh^{-n}\)</span>, then <span class="math inline">\(f\)</span> is continuable to the Bernstein ellipse <span class="math inline">\(E_\rh\)</span>.</p>
<p><em>Proof</em>. Write <span class="math inline">\(f=q_0+(q_1-q_0)+\cdots\)</span>. Use the estimate <span class="math inline">\(\deg p\le d\implies \ve{p}_{L^{\iy}(E_\rh)}\le \rh^d \ve{p}_{L^{\iy}[-1,1]}\)</span>. Proof: Let <span class="math inline">\(q(z) = \fc{p(z)}{(z+\sqrt{z^2-1})^d}\)</span>. This is analytic on <span class="math inline">\(\C\cup \{\iy\}\bs [-1,1]\)</span>. (Why is it well-defined?) (It is defined at <span class="math inline">\(\iy\)</span> because taking <span class="math inline">\(z\leftarrow \rc z\)</span> we get <span class="math inline">\(\fc{p_{\text{rev}}(z)}{(1+\sqrt{1-z^2})^d}\)</span>.) The maximum is attained at the boundary <span class="math inline">\([-1,1]\)</span> where the absolute value is 1.</p>
<h2 id="gibbs-phenomenon">9 Gibbs Phenomenon</h2>
<p>At discontinuities, the overshoot of a Chebyshev approximation does not <span class="math inline">\(\to 0\)</span> as <span class="math inline">\(n\to \iy\)</span>.</p>
For Chebyshev interpolants of <span class="math inline">\(\sgn(x)\)</span> on <span class="math inline">\([-1,1]\)</span>,
\begin{align}
\lim_{n\to \iy,n\text{ odd}} \ve{p_n}_\iy &amp;\approx1.282...\\
\lim_{n\to \iy,n\text{ even}} \ve{p_n}_\iy &amp;\approx1.066...
\end{align}
<p>For Chebyshev projections, <span class="math display">\[\lim_{n\to \iy} \ve{f_n} = \fc 2\pi \int_0^\pi \fc{\sin x}x\dx.\]</span></p>
<p>Intuition: <span class="math inline">\(p(x) = \sum_{j=(n+1)/2}^n l_j(x)\)</span>. This is an alternating series which in the limit has finite nonzero sum.</p>
<p>Notes:</p>
<ul>
<li>Note even though each prtial sum is continuous, a series may converge pointwise to a discontinuous limit.</li>
<li>Gibbs oscillations decay slowly at rate <span class="math inline">\(O(k^{v+1})\)</span> if <span class="math inline">\(f^{(v)}\)</span> has a jump.</li>
<li>This is an algebraic rate of decay, while splins have exponential decay.</li>
<li>We can get exponential decay by splitting up the function and approximating each piece separately.</li>
</ul>
<h2 id="best-approximation">10 Best approximation</h2>
<p><strong>Theorem</strong>. A continuous <span class="math inline">\(f\)</span> on <span class="math inline">\([-1,1]\)</span> has a unique best approximation <span class="math inline">\(p^*\in \cP_n\)</span>. It is uniquely characterized by <span class="math inline">\(f-p\)</span> equioscillating in <span class="math inline">\(\ge n+2\)</span> extreme points.</p>
<p><em>Proof</em>. The minimum is attained because it lies in <span class="math inline">\(\{\ve{f-p}\le \ve{f}\}\)</span>. (<span class="math inline">\(L^\iy\)</span> norms.) If <span class="math inline">\(p,q\)</span> satisfy the condition, and <span class="math inline">\(\ve{f-q}&lt; \ve{f_p}\)</span>, then <span class="math inline">\(p-q\)</span> has <span class="math inline">\(n+1\)</span> zeros. If it doesn’t equioscillate, perturb by <span class="math inline">\((x_1-x)\cdots (x_k-x)\)</span>. (If <span class="math inline">\(\ve{f-q}=\ve{f-p}\)</span>, use the following argument: suppose <span class="math inline">\(p\)</span> has equioscillation extreme points <span class="math inline">\(x_{0:n+1}\)</span>. Induct on: <span class="math inline">\(p-q\)</span> has <span class="math inline">\(\ge j\)</span> roots in <span class="math inline">\([x_0,x_j]\)</span>. Suppose this is first violated at <span class="math inline">\(k\)</span> and obtain a contradiction.)</p>
<h2 id="hermite-integral-formula">11 Hermite integral formula</h2>
<p><strong>Theorem</strong>. Let <span class="math inline">\(f\)</span> be analytic in a region <span class="math inline">\(\Om\)</span> containing distinct points <span class="math inline">\(x_0,\ldots, x_n\)</span> and <span class="math inline">\(\Ga\)</span> be a contour in <span class="math inline">\(\Om\)</span> looping once around them. The polynomial interpolant of degree <span class="math inline">\(\le n\)</span> to <span class="math inline">\(f\)</span> at <span class="math inline">\(\{x_j\}\)</span> is <span class="math display">\[p(x) = \rc{2\pi i} \int_{\Ga} \fc{f(t)(l(t)-l(x))}{l(t)(t-x)}\,dt\]</span> and if <span class="math inline">\(x\)</span> is enclosed by <span class="math inline">\(\Ga\)</span>, the error is <span class="math display">\[f(x) - p(x) = \rc{2\pi i}\int_{\Ga} \fc{l(x)}{l(t)} \fc{f(t)}{t-x}\,dt.\]</span></p>
<p>This is useful because it gives an estimate for the error in terms of “analyticity” of <span class="math inline">\(f\)</span>. If <span class="math inline">\(f\)</span> is analytic on a larger region, we can take <span class="math inline">\(\Ga\)</span> to be bigger, and the ratio <span class="math inline">\(\fc{l(x)}{l(t)} = \prod_{j=0}^n \fc{x-x_j}{t-x_j}\)</span> is smaller. This also tells us a property we desire for the interpolation points: they should make <span class="math inline">\(l(x)\)</span> small. (We shouldn’t have to go far away to make the ratio <span class="math inline">\(\fc{l(x)}{l(t)}\)</span> small.)</p>
<p><em>Proof</em>.</p>
<ol type="1">
<li>Let <span class="math inline">\(\Ga_j\)</span> enclose <span class="math inline">\(x_j\)</span> only, and not <span class="math inline">\(x\)</span> By Cauchy, <span class="math display">\[l_j(x) = \fc{l(x)}{l'(x_j)(x-x_j)} = \Res_{x_j}\pf{l(x)}{l(t)(x-t)}\,dt = \rc{2\pi i} \int_{\Ga_j} \fc{l(x)}{l(t)(x-t)}\,dt.\]</span></li>
<li>Similarly, if <span class="math inline">\(\Ga'\)</span> encloses all <span class="math inline">\(x_j\)</span> but not <span class="math inline">\(x\)</span>, <span class="math display">\[\rc{2\pi i} \int_{\Ga'} \fc{l(x)f(t)}{l(t)(x-t)}\,dt = \sum_j \Res_{x_j} \pf{l(x)f(t)}{l(t)(x-t)} =\sum_j f(x_j)\ell_j(x)=p(x).\]</span></li>
<li>If <span class="math inline">\(\Ga\)</span> encloses all <span class="math inline">\(x_j,x\)</span>, then this adds <span class="math inline">\(\Res_x\pf{l(x)f(t)}{l(t)(x-t)} = -f(x)\)</span>, so <span class="math display">\[p(x) - f(x) = \rc{2\pi i}\int_{\Ga} \fc{l(x)}{l(t)} \fc{f(t)}{x-t}\,dt.\]</span></li>
<li>For the first equation, note <span class="math inline">\(f(x) = \rc{2\pi i}\int_\Ga \fc{l(t)f(t)}{l(t)(t-x)}\)</span>.</li>
</ol>
<h2 id="potential-theory-and-applications">12 Potential theory and applications</h2>
\begin{align}
\ga_n(x,t) &amp;= \prod_{j=0}^n \pf{|t-x_j|}{|x-x_j|}^{\rc{n+1}}\\
\af{l(x)}{l(t)} &amp;= \ga_n(x,t)^{-n-1}\\
\al_n &amp;= \min_{x\in X, t\in \Ga} \ga_n(x,t)\\
\log\ga_n(x,t) &amp;= \ub{\rc{n+1}\sumz jn \log |t-x_j|}{u_n(t)} - \ub{\rc{n+1} \sumz jn \log |x-x_j|}{u_n(s)} \le \al_{n}^{-(n+1)}\\
\log\al_n&amp;= \min_{t\in \G}u_n(t) - \max_{x\in X}u_n(x).
\end{align}
<p>We want to lower bound <span class="math inline">\(\al_n\)</span> to get exponential convergence.</p>
<p>Think of <span class="math inline">\(u\)</span> as a discrete approximation for a continuous potential <span class="math display">\[u(s) = \int_{-1}^1 \ln |s-\tau| \,d\mu(\tau).\]</span> On <span class="math inline">\([-1,1]\)</span>, equally spaced grids converge to <span class="math inline">\(\mu(\tau)=\rc2\)</span> and Chebyshev grids converge to <span class="math inline">\(\mu(\tau) = \rc{\pi \sqrt{1-\tau^2}}\)</span> in weak*.</p>
<p><em>Key property</em>. The Chebyshev measure generates constant potential on <span class="math inline">\([-1,1]\)</span>, and so minimizes <span class="math inline">\(I(\mu)\)</span> (proof omitted).</p>
<p>Some alterntive views.</p>
<ul>
<li>It’s 1 unit of charge in equilibrium, minimal-energy distributoin, where the potential function is logarithmic. The energy is <span class="math display">\[ I(\mu) = -\int_{-1}^1 \int_{-1}^1 \log |s-\tau|\,d\mu(\tau)\,d\mu(s).\]</span></li>
<li><span class="math inline">\(u\)</span> is the functiton in the complex <span class="math inline">\(s\)</span>-plane that is harmonic outside <span class="math inline">\([-1,1]\)</span>, approaches a constant value <span class="math inline">\(s\to [-1,1]\)</span>, and is <span class="math inline">\(\log|s|+O(s^{-1})\)</span> (to make total amount of charge 1). To solve this, find a conformal map mapping the exterior of the interval to the exterior of a disk, <span class="math display">\[\phi(s) = \rc2 (s+i\sqrt{1-s^2}).\]</span> The solution there is <span class="math inline">\(\log|z|\)</span>. Map it back to get <span class="math inline">\(u(s) = \log |s+i\sqrt{1-s^2}|-\log 2\)</span>.
<ul>
<li>From this we wee that the equipotentials are the Bernstein ellipses: <span class="math inline">\(\rh=2e^{u_0}\)</span> gives <span class="math inline">\(u(s)=u_0\)</span>.</li>
</ul></li>
</ul>
<p>The capacity is <span class="math inline">\(\min_\mu I(\mu)\)</span>; for <span class="math inline">\([-1,1]\)</span> it is <span class="math inline">\(\log 2\)</span>. (Each grid point is at an average distance of <span class="math inline">\(\rc2\)</span> from the others in geometric mean.)</p>
<p>How to generalize to other regions (compact sets of <span class="math inline">\(\C\)</span>)?</p>
<ul>
<li>Fekete points: <span class="math inline">\(\prod_{j\ne k} |x_j-x_k|^{\fc{2}{n(n+1)}}\)</span> as large as possible. (This decreases to <span class="math inline">\(c(E)\)</span>.) This is mathematically nice but computationally difficult.</li>
<li>Fejer points: <span class="math inline">\(\phi^{-1}(\{z_j\})\)</span> where <span class="math inline">\(z_j\)</span> are equally spaced around the unit circle, where <span class="math inline">\(\phi\)</span> maps the exterior of <span class="math inline">\(E\)</span> to the exterior of the unit circle.</li>
<li>Leja points: greedy approximations to Fekete poits. Pick <span class="math inline">\(x_i\)</span> successively maximizing <span class="math inline">\(\prod_{j=0}^{n-1}|x_j-x_n|\)</span> at each step.</li>
</ul>
<p><strong>Theorem</strong>. Let <span class="math inline">\(f\in C([-1,1])\)</span>. Let <span class="math inline">\(\rh\)</span> be the parameter of the largest Bernstein ellipse <span class="math inline">\(E_\rh\)</span> where <span class="math inline">\(f\)</span> can be analytically continued, and <span class="math inline">\(p_n\)</span> be the interpolants to any sequence of grids with <span class="math inline">\(\lim_{n\to \iy}(\sup_{x\in [-1,1]}|l(x)|)^{\rc n}\)</span>. Then <span class="math inline">\(\lim_{n\to \iy} \ve{f-p_n}^{\rc n} = \rh^{-1}\)</span>.</p>
<h2 id="equispaced-points-runge-phenomenon">13 Equispaced points, Runge phenomenon</h2>
<p>Interpolation at equally spaced points can diverge exponentially (at the edges). Why? <span class="math inline">\([-1,1]\)</span> is far from being a level curve. The curve <span class="math inline">\(l(x)=l(t)\)</span> is a “football”; <span class="math inline">\(f\)</span> needs to be analytic inside this football for convergence. The football goes to <span class="math inline">\(\approx .526i\)</span>. Ex. <span class="math inline">\(f=\rc{1+25x^2}\)</span> is not analytic inside, so the interpolants diverge.</p>
<p>Even if convergence should take place in theory (<span class="math inline">\(f\)</span> is analytic in the football), rounding errors can be amplified by <span class="math inline">\(O(2^n)\)</span> causing divergence in practice.</p>
<h2 id="discussion-of-high-order-interpolation">14 Discussion of high-order interpolation</h2>
<p>Much of the literature is skeptical of high-order interpolation because people saw it doesn’t work for equispaced point. However, Chebyshev interpolation does work.</p>
<p>2 important issues:</p>
<ol type="1">
<li>Conditioning of the problem: interpolation points must be properly spaced, so the interpolant does not depend sensitively on the data. Ill-conditioning can lead to Runge phenomenon.</li>
<li>Stability of the algorithm: Interpolation algorithm must be stable (e.g. barycentric interpolation formula) even when the problem is well-conditioned. Vandermonde is very poorly conditioned.</li>
</ol>
<h2 id="lebesgue-constants">15 Lebesgue constants</h2>
<p>The <strong>Lebesgue constant</strong> of a set of points <span class="math inline">\(S\sub [-1,1]\)</span> is <span class="math display">\[\La = \sup_f\fc{\ve{p}}{\ve{f}} = \sup_{x\in [-1,1]} \ub{\sumo jn |l_j(x)|}{\la(x)}\]</span> where <span class="math inline">\(p\)</span> is the interpolant of <span class="math inline">\(f\)</span> through <span class="math inline">\(S\)</span>.</p>
<p>The largest possible interpolation error is at least <span class="math inline">\(\La-1\)</span>, and the interpolation error is at most <span class="math inline">\((\La+1) \ve{f-p^*}\)</span>.</p>
<p>More generally, define the Lebesgue constant of any linear projection by <span class="math inline">\(\sup_f \fc{\ve{Lf}}{\ve{f}}\)</span>. Then <span class="math inline">\(\ve{f-p}\le (\La+1)\ve{f-p^*}\)</span>.</p>
<p>No set of interpolation points can lead to convergence for all <span class="math inline">\(C([-1,1])\)</span> (PROOF?) so <span class="math inline">\(\lim_{n\to \iy} \La_n\)</span>.</p>
<p><strong>Theorem</strong>.</p>
<ol type="1">
<li>Lebesgue constants for <span class="math inline">\(|S|=n+1\)</span> satisfy <span class="math inline">\(\La_n \ge \fc{2}{\pi}\log (n+1) + \fc{2}\pi (\ga + \log \pf{4}{\pi})\)</span>.</li>
<li>For Chebyshev points, <span class="math inline">\(\La_n\le \fc{2}{\pi}\log (n+1) +1\)</span>.</li>
<li>For equispaced points, <span class="math inline">\(\La_n&gt;\fc{2^{n-2}}{n^2}\)</span>.</li>
<li>For Chebyshev projection, <span class="math inline">\(\La_n =\rc{2\pi}\int_{-\pi}^{\pi} \af{\sin((n+1/2)t)}{\sin(t/2)}\,dt\sim \fc{4}{\pi^2}\log n\)</span>.</li>
</ol>
<p>Instructive: Plot <span class="math inline">\(\la(x)\)</span> for equispaced and Chebyshev points.</p>
<p>Intuition: The fastest a Lagrange polynomial can decay is <span class="math inline">\(O\prc{x}\)</span>, and adding up these alternating tails with alternating coefficients gives a harmonic series.</p>
<p>Related: For Taylor projection of <span class="math inline">\(f\)</span> analytic on the unit disc, Landau constants <span class="math inline">\(\La_n \sim \rc{\pi}\log n\)</span>.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2016-08-06</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-08-06.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-08-06.html</id>
    <published>2016-08-06T00:00:00Z</published>
    <updated>2016-08-06T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2016-08-06</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-06 
          , Modified: 2016-08-06 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#representation-learning">Representation learning</a></li>
 </ul> </div>

  <div class="blog-main">
    <ul>
<li>Machine learning
<ul>
<li><a href="/posts/tcs/machine_learning/neural_nets/pmi_images.html">PMI images</a>
<ul>
<li>Understood <a href="/posts/tcs/machine_learning/nlp/pmi.html">PMI idea</a> more thoroughly.</li>
<li>Got code from Ben. Having trouble running. See
<ul>
<li>https://www.mathworks.com/matlabcentral/newsreader/view_thread/345876#947272</li>
<li>https://askrc.princeton.edu/question/255/matlab-on-nobel-error-using-mex/</li>
<li>http://stackoverflow.com/questions/38723051/error-using-mex-g-error-no-such-file-or-directory</li>
</ul></li>
<li><a href="/posts/tcs/machine_learning/neural_nets/convnets_ideas.html">Brainstorm on convnets</a>. Skimmed the following. <strong>TODO</strong>: understand more thoroughly.
<ul>
<li>Fastfood - computing Hilbert space expansions in loglinear time
<ul>
<li>Deep-fried convnets</li>
<li>Ailon-Chazelle</li>
</ul></li>
<li><a href="/posts/tcs/machine_learning/nlp/HA16.html">HA16</a> Unsupervised Learning of Word-Sequence Representations from Scratch via Convolutional Tensor Decomposition</li>
<li>[MKHS14] Convolutional Kernel Networks</li>
</ul></li>
</ul></li>
<li>Representation learning problem (see below)
<ul>
<li>Talked with Anand.</li>
<li>Kernel dictionary learning idea. <strong>TODO</strong>: try to make this rigorous, and test it out.</li>
</ul></li>
<li><strong>TODO</strong>: Think about AGD, talk with Cyril.
<ul>
<li><a href="/posts/math/analysis/numerica/chebyshev.html">Chebyshev polynomials</a></li>
</ul></li>
</ul></li>
<li>Probability
<ul>
<li><a href="/posts/math/probability/random_matrices/hi_dim_prob.html">HDP</a> Chapters 2-3</li>
</ul></li>
<li>TT/ATP/PL
<ul>
<li>Reviewed HMTI, read more on type theory. See <a href="/posts/cs/type_theory/types_and_pl.html">Types and PL</a>, <a href="/posts/cs/type_theory/type_theory.html">Type theory</a>.</li>
</ul></li>
</ul>
<h2 id="representation-learning">Representation learning</h2>
<p>In dictionary learning, we assume we have samples <span class="math inline">\(y = Ax + e\)</span> where <span class="math inline">\(x\)</span> comes from a sparse distribution (ex. <span class="math inline">\(x_i\)</span> independent, <span class="math inline">\(x_i\neq 0\)</span> with probability <span class="math inline">\(s/n\)</span> and then is drawn from some distribution not concentrated at 0) and <span class="math inline">\(e\)</span> is error (ex. Gaussian).</p>
<p>The way we stated our problem is that <span class="math inline">\(x\cdot a_i\)</span> is large for only a few <span class="math inline">\(i\)</span>. This is similar to dictionary learning with <span class="math inline">\((A^+)^T\)</span> where the columns of <span class="math inline">\(A\)</span> are the <span class="math inline">\(a_i\)</span>. (I.e. the <span class="math inline">\(x\)</span>’s here are really the <span class="math inline">\(y\)</span>’s in DL.)</p>
<p>I may be wrong but I think that what’s different is that</p>
<ul>
<li>Dictionary learning on <span class="math inline">\((A^+)^T\)</span> would correspond to when <span class="math inline">\((x\cdot a_i)_i\)</span> is sparse + noise. Our assumption is a bit different that <span class="math inline">\(x\cdot a_i\)</span> is large for only a few <span class="math inline">\(i\)</span>, ex. large negative values don’t count against the assumption.</li>
<li>we’re trying to relax the condition on the noise—ex. instead of saying that the noise in the other coordinates is random, we consider worst-case or make an assumption that they’re random-like in some way.</li>
</ul>
<p>(Actually, I think the undercomplete case when the number of <span class="math inline">\(a_i\)</span> is less than the dimension of <span class="math inline">\(n\)</span> doesn’t quite correspond to DL because the map <span class="math inline">\(x\mapsto (x\cdot a_i)_i\)</span> is not invertible…)</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Chebyshev polynomials</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/analysis/numerical/chebyshev.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/analysis/numerical/chebyshev.html</id>
    <published>2016-08-04T00:00:00Z</published>
    <updated>2016-08-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Chebyshev polynomials</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-04 
          , Modified: 2016-08-04 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <p>References:</p>
<ul>
<li><a href="http://blog.mrtz.org/2013/09/07/the-zen-of-gradient-descent.html">The Zen of gradient descent</a></li>
</ul>
Define the Chebyshev polynomials by
\begin{align}
T_0 &amp;=1\\
T_1 &amp;=x\\
T_{n+1} &amp;= 2xT_n - T_{n-1}\\
T_n(\cos x) &amp;= \cos(nx).
\end{align}
<strong>Theorem</strong>. There exists a polynomial <span class="math inline">\(p_k\)</span> such that
\begin{align}
\deg(p_k) &amp;= O\pa{\sqrt{\fc{L}{l}} \ln \prc{\ep}}\\
p_k(0) &amp;= 1\\
p_k(x)&amp;\le \ep, &amp;x\in [l,L]
\end{align}
<p>First attempt: Scale and translate the Chebyshev polynomial to be <span class="math inline">\(\le \ep\)</span> on <span class="math inline">\([l,L]\)</span>. A Chebyshev poly at <span class="math inline">\(x\)</span> is <span class="math inline">\(O(2^{n-1}x^n)\)</span>. Then the value at 0 is <span class="math inline">\(2^{n-1}\)</span> is <span class="math inline">\(2^{n-1}\pf{l}{(L-l)/2}^n\)</span>, it suffices for this to be <span class="math inline">\(\le 1\)</span>. But this only that <span class="math inline">\(n\le \fc{\ln \prc{\ep}}{\ln \pf{4l}{L-l}}\)</span> is sufficient.</p>
<p>This is a bad approximation!</p>
We know
\begin{align}
T_n\pf{e^x+e^{-x}}2 &amp;= \fc{e^{nx}+e^{-nx}}2\\
T_n\pf{u+\rc u}{2} &amp;= \pf{u^n+\rc{u^n}}2.
\end{align}
Solving <span class="math inline">\(u+\rc{u} = x\)</span> gives <span class="math inline">\(u=x \pm\sqrt{x^2-1}\)</span> and <span class="math display">\[u^n +\rc{u^n} = (x+\sqrt{x^2-1})^n + (x-\sqrt{x^2-1})^n.\]</span> Thus it suffices to have (let <span class="math inline">\(k = \fc Ll\)</span>)
\begin{align}
2\ba{\pf{L+l}{L-l} + \sqrt{\pf{L+l}{L-l}^2 - 1}}^n \ep &amp;\le 1\\
\iff n\ln \ba{\pf{L+l}{L-l} + \sqrt{\pf{L+l}{L-l}^2 - 1}} &amp; \le \ln \fc{2}{\ep}\\
\iff n\ln \pf{(\sqrt k+1)^2}{k-1} &amp; \le \ln \pf 2\ep\\
\Leftarrow n\pf{2+2\sqrt k}{k-1} &amp; \le \ln \pf 2\ep\\
\Leftarrow n&amp;\le \fc{k-1}{2(\sqrt k + 1)}\ln \pf 2\ep\\
&amp;=\rc{2}(\sqrt k - 1) \ln \pf 2\ep\\
&amp;=O(\sqrt k \ln \prc{\ep}).
\end{align}
<p>Restatement: There exists a polynomial of degree <span class="math inline">\(O\pa{\sfc{L+l}{L-l}\ln \prc{\ep}}\)</span> such that <span class="math inline">\(p_k(L)=1\)</span> and <span class="math inline">\(p_k(x)=0\)</span> on <span class="math inline">\([-l,l]\)</span>.</p>
Let’s solve <span class="math inline">\(Ax=b\)</span> when <span class="math inline">\(A\)</span> is positive definite. Consider <span class="math inline">\(f=\rc 2 x^TAx - b^Tx\)</span> starting at <span class="math inline">\(x_0=b\)</span>. Gradient descent with step size <span class="math inline">\(t\)</span> gives
\begin{align}
x_{k+1} &amp;= x_k - t (Ax_k-b)\\
&amp;= (I-tA)x_k + tb\\
x_k &amp;= (I+\cdots +(I-tA)^k)tb
\end{align}
Now <span class="math inline">\(p(x) = 1+\cdots + (1-x)^n\)</span> is an approximation of <span class="math inline">\(\rc{x} = \rc{1-(1-x)}\)</span>. Suppose all eigenvalues of <span class="math inline">\(A\)</span> are in <span class="math inline">\([l,L]\)</span>. Then diagonalizing, we get that (take <span class="math inline">\(t=1\)</span>?)
\begin{align}
\ve{(p(tA) - A^{-1})tb}_2 
&amp;= \ve{tb}_2\max_{\la\text{ of }A} (p(\la) - \rc{\la})) \\
&amp;= \ve{tb}_2\max_{x\in [l,L]} (p(x) - \rc x) \\
&amp;= \ve{tb}_2 \ve{p(x) - \rc x}_{L^\iy([l,L])}.
\end{align}
<!-- (We may assume $A$ is diagonalizable with real eigenvalues by multiplying by a unitary.) -->
<p>But <span class="math inline">\(p(x)\)</span>, although it is a power series expansion of <span class="math inline">\(\rc{x}\)</span>, is not the best estimator of <span class="math inline">\(\rc{x}\)</span> in the <span class="math inline">\(L^{\iy}\)</span> norm. Polynomials based on Chebyshev polynomials do better. The Chebyshev polynomials are defined by a recurrence, which corresponds to a different recurrence relation on the <span class="math inline">\(x_k\)</span>.</p>
<p>For the above <span class="math inline">\(p\)</span>, the difference is <span class="math inline">\(\fc{(1-tx)^{n+1}}x\)</span>. Choosing <span class="math inline">\(t=\fc{2}{L+l}\)</span>, we get <span class="math inline">\(\pf{L-l}{L+l}^{n+1} = O\pf{\ka - 1}{\ka+1}^{n}\)</span> convergence.</p>
<p>This motivates the following question.</p>
<p><strong>Question</strong>. What is <span class="math display">\[
\min_{\deg p = n} \ve{p(x) - \rc{x}}_{L^{\iy}([l,L])}?
\]</span></p>
<p>Interpolate at Chebyshev points? Conjugate gradients, etc.</p>
<p>See exercise 8.10 in <a href="ATAP.html">ATAP</a>. First reposition at <span class="math inline">\([-1,1]\)</span> as above. <span class="math inline">\(\rc{x}\)</span> gets sent to <span class="math inline">\(\fc{k+1}{m(k-1)}\rc{x-\pf{k+1}{k-1}}\)</span>. For any <span class="math inline">\(\rh\)</span> such that <span class="math inline">\(\fc{\rh+\rh^{-1}}2\le \fc{k+1}{k-1}\)</span>, i.e., <span class="math inline">\(\rh \le 1+\fc{2}{\sqrt k - 1}\)</span>, we get a bound <span class="math display">\[
\fc{k+1}{m(k-1)} \pa{\pf{k+1}{k-1}-\fc{\rh+\rh^{-1}}{2}} \rh^{-n} = \rc m O_{\rh}(\rh^{-n})
\]</span> This is an upper bound on convergence of conjugate gradient, because conjugate gradient finds the vector in <span class="math inline">\(\spn(\{b, Ab, \ldots, A^{n}b\})\)</span> that is closest to <span class="math inline">\(A^{-1}b\)</span>.</p>
<p>(Thus, conjugate gradient gets <span class="math inline">\(\ep\)</span>-close in <span class="math inline">\(O(\sqrt\ka \ln \prc{\ep})\)</span> steps.)</p>
<p>(We can get a lower bound by computing the best possible <span class="math inline">\(L^2\)</span> of the approximation. It should be on the same order.)</p>
Conjugate gradient:
\begin{align}
v_0&amp;=b\\
w_i&amp;=Av_i\\
v_i&amp;=w_i- \sumz j{i-1} \fc{\an{w_i,v_j}_A}{\an{v_j,v_j}_A}v_j.
\end{align}
<p>We have <span class="math inline">\(\an{w_{i+1},v_j} = \an{v_i,w_{j+1}}_A\)</span> and <span class="math inline">\(w_{i+1}\perp v_{i+2},ldots\)</span> so <span class="math inline">\(v_i\perp_A w_{i+3},\ldots\)</span>.</p>
<!--We try to minimize $\max_\la (\la p(\la)-1)^2)$-->

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Neural net experiments</title>
    <link href="http://holdenlee.github.io/notebook/posts/cs/neural_nets/experiments.html" />
    <id>http://holdenlee.github.io/notebook/posts/cs/neural_nets/experiments.html</id>
    <published>2016-08-03T00:00:00Z</published>
    <updated>2016-08-03T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Neural net experiments</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-03 
          , Modified: 2016-08-03 
	</p>
      
       <p>Tags: <a href="/tags/programming.html">programming</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <h2 id="done">Done</h2>
<ul>
<li>MNIST basics</li>
<li>CIFAR-10 basics</li>
</ul>
<h2 id="todo">Todo</h2>
<p>See <a href="/tcs/machine_learning/neural_nets/convnets_ideas.html">Convnets ideas</a>.</p>
<ul>
<li>LSTM: generate text
<ul>
<li>Character by character</li>
<li>Word by word, using word embeddings</li>
<li>With restrictions</li>
<li>With parse trees</li>
</ul></li>
<li>LSTM for algorithms
<ul>
<li>Thresholding</li>
<li>Neural net Turing machines</li>
</ul></li>
<li>Rotational invariance, etc.</li>
<li>Smoothness regularization in convolution kernels</li>
<li>Get familiar with Tensorboard.</li>
<li>Visualizing neural nets.</li>
<li>Sparsification</li>
<li>Reimplementations
<ul>
<li>Deep-fried convnets</li>
<li>Fooling neural nets</li>
<li>Deep reinforcement learning with Atari games</li>
</ul></li>
<li>“Train twice and put together”: independent ways of doing things (cf. boosting)</li>
<li>Check claims of suspiciously good performance (ex. learning to execute…)</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[HA16] Unsupervised Learning of Word-Sequence Representations from Scratch via Convolutional Tensor Decomposition</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/HA16.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/HA16.html</id>
    <published>2016-08-02T00:00:00Z</published>
    <updated>2016-08-02T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[HA16] Unsupervised Learning of Word-Sequence Representations from Scratch via Convolutional Tensor Decomposition</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-02 
          , Modified: 2016-08-02 
	</p>
      
       <p>Tags: <a href="/tags/nlp.html">nlp</a>, <a href="/tags/word%20embeddings.html">word embeddings</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p>See also [HA15] Convolutional Dictionary Learning through Tensor Factorization.</p>
<ol type="1">
<li>Use PCA to reduce dimensionality of 1-hot encodings to <span class="math inline">\(k\)</span> dimensions. (Wouldn’t it be better to use word embeddings?)</li>
<li>For each dimension and each sentence, obtain a vector <span class="math inline">\(x\)</span>. Train on this set. Minimize <span class="math display">\[\min_{f_i,w_i:\ve{f_i}=1} \ve{x-\sum_{i\in [L]} f_i * w_i}^2.
\]</span> (How to avoid SGD or calculating the sum of these? Some tensor trickery?) <span class="math inline">\(f\)</span>’s are filters, <span class="math inline">\(w\)</span>’s are activation maps.</li>
</ol>
<p>Q: Why are we considering each dimension separately? Wouldn’t it make more sense to consider them together, and try to write <span class="math display">\[ X = \sum_{i\in [L]} F_i*w_i\]</span> where <span class="math inline">\(X\)</span> is a matrix corresponding to a sentence, and <span class="math inline">\(w_i\)</span> is in the row direction. In [HA16]’s way of doing it, you’re learning how a generic subject/theme/atom modulates throughout the space (sentence), not how combinations of subjects modulate. Is this what you want? This does need MUCH fewer atoms though, else you can expect <span class="math inline">\(k\)</span> times more.</p>
<p>(T/F? Tensor algorithms are fragile in the sense that they depend on the model being exactly the way it is. Ex. tensor algorithm for NN—if you change the NN a bit it may fail.)</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Convnets ideas</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/convnets_ideas.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/convnets_ideas.html</id>
    <published>2016-08-02T00:00:00Z</published>
    <updated>2016-08-02T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Convnets ideas</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-02 
          , Modified: 2016-08-02 
	</p>
      
       <p>Tags: <a href="/tags/neural%20nets.html">neural nets</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <ul>
<li>In the Fourier basis translation is diagonalized. So can we train better in the Fourier basis?
<ul>
<li>This seems tempting, but it doesn’t seem to lead anywhere. Local convolution with shared parameters commutes with translation. Here, diagonal maps commute with translation (=multiplication by eigenfunctions). So in terms of computing a linear map, it’s easier.</li>
<li>But then we follow by a nonlinearity: pointwise sigmoid, or local pooling. In the Fourier space, this doesn’t correspond to anything nice! Fourier transform scrambles locality, and locality is important here…</li>
</ul></li>
<li>Adding more invariances:
<ul>
<li>Rotational invariance. (See [CW16] Group Equivariant Convolutional Networks)
<ul>
<li>Make local rotated copies of the image (have to be careful with cropping, interpolating) and stack them. Now do a 3-D convolution in <span class="math inline">\((x,y,\te)\)</span> space.</li>
<li>This is less efficient. We should be able to reduce the number of parameters, but by how much?</li>
<li>Beyond translation symmetry, the dimensionality of symmetries becomes greater than the dimensionality of the image (there is more than one symmetry taking one point to another point). This redundancy causes inefficiency.</li>
</ul></li>
<li>Scaling invariance.
<ul>
<li>Run the same convnet on the picture at different scales.</li>
</ul></li>
<li>Coloration (does this cause problems typically?)</li>
<li>There’s some classical result in theoretical computer vision that identifies the dimensionality of the manifold of image alterations… check Amit Singer’s notes.</li>
</ul></li>
<li>Apply the variant on convolutional dictionary learning <a href="../nlp/HA16.html">HA16</a>, except 2-D.</li>
<li>What if we did global convolutions? (This would add many parameters…)</li>
<li>Add regularization to penalize non-smooth/non-simple kernels. For example, write the kernel in a Fourier or wavelet basis, and penalize larger Fourier or wavelet vectors (ex. term based on Fourier expectation).</li>
<li>I “get” max pooling now: it preserves translation invariance. Pool across close-by transformations.
<ul>
<li>Does overlapping impose consistency?</li>
</ul></li>
<li>Could you compute max-pooling across many local transformations (ex. <span class="math inline">\((x,y,\te)\)</span>) more efficiently? Ex. finding the distance from a manifold of transformations of the image.</li>
<li>How to incorporate common sense? Ex. “A thin object in a person’s hand is likely to be a pencil.”</li>
<li>Can we train on the quotient manifold, quotiented out by symmetries? The problem is that the quotient manifold has cusps. Ex. <span class="math inline">\(\R^2/(x,y)\mapsto (y,x)\)</span> has “cusps” at <span class="math inline">\((x,x)\)</span>. Add regularization/change metric to keep it away? cf. keeping things in the simplex.</li>
<li>(General NN) How much can you sparsify a fully connected layer and still have it work? (cf. dropout)</li>
<li>How much can you reduce parameters in a trained NN and still get similar performance? (+ an explanation of why more parameters that needed helps learning) Can you iterate reducing parameters and training?
<ul>
<li>READ: Deep-fried convnets.</li>
</ul></li>
<li>Fooling neural nets…</li>
<li>Train twice independently and put together?</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>PMI for images</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/pmi_images.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/pmi_images.html</id>
    <published>2016-08-01T00:00:00Z</published>
    <updated>2016-08-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>PMI for images</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-01 
          , Modified: 2016-08-01 
	</p>
      
       <p>Tags: <a href="/tags/PMI.html">PMI</a>, <a href="/tags/neural%20nets.html">neural nets</a>, <a href="/tags/vision.html">vision</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#notes">Notes</a></li>
 <li><a href="#theory">Theory</a></li>
 <li><a href="#thoughts">Thoughts</a></li>
 <li><a href="#todo">Todo</a><ul>
 <li><a href="#coding">Coding</a></li>
 <li><a href="#theory-1">Theory</a></li>
 </ul></li>
 <li><a href="#scratch">Scratch</a></li>
 <li><a href="#steps">Steps</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="notes">Notes</h2>
<p>Experiments:</p>
<ul>
<li><code>mnist_testing.m</code>: Prep data.</li>
<li><code>svd_testing.m</code>: How does reducing the number of features through SVD affect classification accuracy?</li>
<li><code>pairwise_mi.m</code>: Tests related to mutual information</li>
<li><code>mi.m</code></li>
<li><code>MI_GG.m</code></li>
<li><code>rsvd.m</code></li>
</ul>
<pre><code>	         nlayers: 2
             layer: {2x1 cell}
    type_zerolayer: 2
             ndesc: 7200


                         numlayer: 1
                        centering: 0
    median_contrast_normalization: 0
                           npatch: 5
                      subsampling: 2
                             smap: 50
                   type_zerolayer: 2
                            sigma: 0.7459
                                Z: [25x50 double]
                                w: [50x1 double]</code></pre>
<p>In <code>pairwise_mi.m</code> there’s no normalization by scaling (<span class="math inline">\(-2\log\pat{scale}\)</span>).</p>
<p>https://en.wikipedia.org/wiki/Conditional_mutual_information</p>
<h2 id="theory">Theory</h2>
<p>The right PMI measure to use here is <span class="math display">\[
\ln \pf{\an{v,w}}{\an{v,\one}\an{w,\one}}.
\]</span> Because if we assume the activations are like <span class="math inline">\((e^{\chi,v})_\chi\)</span>, then we still get that the expected dot product of these is <span class="math inline">\(\int e^{\an{\chi,v}}e^{\an{\chi,w}}\,d\chi\propto \exp\pa{\fc{\ve{v+w}^2}{2}}\)</span>.</p>
<h2 id="thoughts">Thoughts</h2>
<ul>
<li>Is a picture more like a context or a document? We’re treating it like context (looking at all pairs of features there). But its size makes it seem more like a document. Or better: a sentence, because sentences are big enough to incorporate different features and small enough to still have a vector associated with it.</li>
<li>How to incorporate convolutional ideas? Ex. if features <span class="math inline">\(f_1,f_2\)</span> are in the same relationship (translationally) as <span class="math inline">\(f_1',f_2'\)</span> then we expect PMI to be similar, so we should we really be looking at a <span class="math inline">\(7200\times 7200\)</span> matrix? How about look at PMI of adjacent features? (But they shouldn’t overlap…) Or look at PMI pre-convolution by another layer?</li>
<li>What happens if you apply (convolutional?) DL to the learned features? Then apply SVD to the dimension-reduced vectors?</li>
</ul>
<h2 id="todo">Todo</h2>
<h3 id="coding">Coding</h3>
<ul>
<li>Completed</li>
<li>Compute PMI matrix.</li>
<li>Plot histogram of PMI’s.</li>
<li>Plot histogram of conditional PMI’s. (They don’t look like they have higher values… Try thresholding first? Look at features with largest PMI? Calculate entropies?)</li>
<li>Look at correlations between PMI’s and distances between pairs. (There doesn’t seem to be any correlation.)</li>
<li>Todo</li>
<li>Train on feature vectors.</li>
<li>Do weighted SVD on PMI matrix.</li>
<li>Do dictionary learning on feature vectors/PMI vectors (?).</li>
</ul>
<p>(Compare to purely random.)</p>
<h3 id="theory-1">Theory</h3>
<p>Understand loss function for PMI.</p>
<h2 id="scratch">Scratch</h2>
<p>Qs</p>
<ul>
<li>Why is the normalization <code>nrm=mean(sqrt(sum(compTr.^2)))</code>?</li>
</ul>
<h2 id="steps">Steps</h2>
<p>Run experiments.</p>
<pre><code>nohup nice -19 matlab -nodisplay -nodesktop -nojvm -nosplash &lt; run_experiments2.m &gt; output_2016-08-07.txt 2&gt;&amp;1 &amp;</code></pre>
<div class="sourceCode"><pre class="sourceCode matlab"><code class="sourceCode matlab"><span class="co">%Get psiTr</span>
pmi_experiments(<span class="st">'logs/model_mnist_5_2_0_2_2_0_50_200_0_1.000000e-01_1.000000e-01_1.000000e-01_2.mat'</span>);
<span class="co">%get percentiles for each feature</span>
pc= get_percentiles(psiTr);
save(<span class="st">'pc_1.mat'</span>,<span class="st">'pc'</span>,<span class="st">'-v7.3'</span>);
<span class="co">%calculate PMI after normalizing and deleting zero rows.</span>
[P, inds] = calculate_pmi(psiTr); <span class="co">%inds give the indices that are kept (indices of nonzero rows)</span>
<span class="co">%exploratory analysis</span>
<span class="co">%pmi_experiments(P);</span>
[all_pmi_pairs, positions, dists] = pmi_experiments2(P, inds');
save(<span class="st">'all_pmi_pairs_1.mat'</span>,<span class="st">'all_pmi_pairs'</span>);
save(<span class="st">'positions_1.mat'</span>,<span class="st">'positions'</span>);
save(<span class="st">'dists_1.mat'</span>,<span class="st">'dists'</span>);

<span class="co">%cpmi matrices</span>
Ps = calculate_cpmi(psiTr, Ytr, <span class="st">'1'</span>);
cpmi_experiments(<span class="st">'P_1'</span>);</code></pre></div>
<p><img src="/images/pmi/pmi_histogram_1.jpg"></p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>High-dimensional probability</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/probability/random_matrices/hi_dim_prob.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/probability/random_matrices/hi_dim_prob.html</id>
    <published>2016-08-01T00:00:00Z</published>
    <updated>2016-08-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>High-dimensional probability</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-08-01 
          , Modified: 2016-08-04 
	</p>
      
       <p>Tags: <a href="/tags/random%20matrix.html">random matrix</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#introduction">Introduction</a></li>
 <li><a href="#variance-bounds-and-poincare-inequalities">Variance bounds and Poincare inequalities</a><ul>
 <li><a href="#section">2.1</a></li>
 <li><a href="#markov-semigroups">Markov semigroups</a></li>
 <li><a href="#variance-identities-and-exponential-ergodicity">2.4 Variance identities and exponential ergodicity</a></li>
 <li><a href="#problems">Problems</a></li>
 </ul></li>
 <li><a href="#subgaussian-concentration-and-log-sobolev-inequalities">3 Subgaussian concentration and log-Sobolev inequalities</a><ul>
 <li><a href="#the-martingale-method">3.2 The martingale method</a></li>
 <li><a href="#the-entropy-method">3.3 The entropy method</a></li>
 <li><a href="#log-sobolev-inequalities">3.4 Log-Sobolev inequalities</a></li>
 <li><a href="#problems-1">Problems</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <p>Based on Ramon von Handel’s ORF570 notes.</p>
<h2 id="introduction">Introduction</h2>
<p>Themes:</p>
<ul>
<li>concentration: if <span class="math inline">\(X_{1:n}\)</span> are independent or weakly dependent random variables, and <span class="math inline">\(f\)</span> is not too <em>sensitive</em> to any coordinate, then <span class="math inline">\(f(X_{1:n})\)</span> is <em>close</em> to its mean.</li>
<li>suprema</li>
<li>universality</li>
</ul>
<h2 id="variance-bounds-and-poincare-inequalities">Variance bounds and Poincare inequalities</h2>
<h3 id="section">2.1</h3>
<p>Trivial bound: <span class="math display">\[ \Var[f(X)]\le \rc4 (\sup f - \inf f)^2 \qquad \Var[f(x)] \le \E[(f(X)-\inf f)^2].\]</span></p>
<p>Tensorization gives a bound for functions of independent random variables from bounds for functions of each individual random variable.</p>
<p><strong>Theorem</strong> (Tensorization of variance): <span class="math display">\[\Var[f(X_1,\ldots, X_n)]\le \E\ba{\sumo in \Var_i f(X_1,\ldots, X_n)}\]</span> whenever <span class="math inline">\(X_{1:n}\)</span> are independent.</p>
<p>This is sharp for linear functions.</p>
<em>Proof</em>. Write <span class="math display">\[ f(X_{1:n}) - \E f(X_{1:n}) = \sumo kn \ub{\E[f(X_{1:n}|X_{1:k})] - \E[f(X_{1:n})|X_{1:k-1}]}{\De_k}. \]</span> The <span class="math inline">\(\De_k\)</span> form a martingale. By independence of martingale increments,
\begin{align}
\Var(f) &amp;= \sumo kn \E[\De_k^2] \\
\E[\De_k^2] &amp;= \E[\E[\wt \De_k |X_{1:k}]^2]\\
&amp; \le  \E[(\ub{f - \E[f|X_{1:k-1,k+1:n}]}{\wt \De_k})^2] &amp; \text{Jensen}\\
&amp;= \E\ba{\sumo in \Var_f f(X_1,\ldots, X_n)}.
\end{align}
Define
\begin{align}
D_i f(x) &amp;= (\sup_z-\inf_z)(f(x_{1:i-1},z,x_{i+1:n}))\\
D_i^- f(x) &amp;= f(x) - \inf_z(f(x_{1:i-1},z,x_{i+1:n}))\\
\end{align}
<p><strong>Corollary</strong> (Bounded difference inequality): Tensorization + trivial inequality.</p>
<p><strong>Example</strong>: Consider Bernoulli symmetric matrices. What is the variance of <span class="math inline">\(\la_{\max}(M) = \an{v_{\max}(M), Mv_{\max}(M)}\)</span>? Fix <span class="math inline">\(i,j\)</span>. Let <span class="math display">\[M^- = \amin_{\text{only }M_{ij} \text{ varies}} \la_{\max}(M).\]</span> Then <span class="math display">\[D_{ij}^-\la_{\max}(M) \le \an{v_{\max}(M), (M-M^-) v_{\max}(M)}\le 4|v_{\max}(M)_i||v_{\max}(M)_j|.\]</span> Use the corollary to get <span class="math inline">\(\le 16\)</span>.</p>
<p>This is not sharp. (<span class="math inline">\(\sim n^{-\rc 3}\)</span> is correct.)</p>
<p>Drawbacks to this method:</p>
<ul>
<li>bounds using bounded di↵erence inequalities are typically restricted to situations where the random variables <span class="math inline">\(X_i\)</span> and/or the function <span class="math inline">\(f\)</span> are bounded.</li>
<li>Bounded difference inequalities do not capture any information on the distribution of <span class="math inline">\(X_i\)</span>. In the other direction, the tensorization inequality is too distribution-dependent.</li>
<li>Tensorization depends on independence.</li>
</ul>
<p>Inequalities in this section are roughly of the following form (Poincare inequalities): <span class="math display">\[\Var(f) \le \E[\ve{\nb f}^2].\]</span> “The validity of a Poincar´e inequality for a given distribution is intimately connected the convergence rate of a Markov process that admits that distribution as a stationary measure.”</p>
<h3 id="markov-semigroups">Markov semigroups</h3>
<p>A <strong>Markov process</strong> satisfies: For every bounded measurable <span class="math inline">\(f\)</span> and <span class="math inline">\(s,t\in \R_+\)</span>, here is abounded measurable <span class="math inline">\(P_sf\)</span> such that <span class="math display">\[\E[f(X_{t+s})|\{X_r\}_{r\le t}] = (P_s f)(X_t).\]</span> <span class="math inline">\(\mu\)</span> is <strong>stationary</strong> if <span class="math inline">\(\mu(P_tf)=\mu(f)\)</span> for all <span class="math inline">\(t\in \R_+\)</span>, bounded measurable <span class="math inline">\(f\)</span>.</p>
<p><strong>Lemma 2.7</strong>. <span class="math inline">\(\{P_t\}_{t\in \R_+}\)</span> defines a semigroup of linear operators on <span class="math inline">\(L^p(\mu)\)</span>. It is contractive and conservative (<span class="math inline">\(P_t1=1\)</span> <span class="math inline">\(\mu\)</span>-a.s.).</p>
<p><em>Proof</em>. Jensen.</p>
<p>The semigroup in fact acts on any <span class="math inline">\(f\in L^1(\mu)\)</span>.</p>
<p><strong>Lemma 2.9</strong>. If <span class="math inline">\(\mu\)</span> is stationary, for every <span class="math inline">\(f\)</span>, <span class="math inline">\(\Var_\mu(P_tf)\)</span> is decreasing.</p>
<p><em>Proof</em>. <span class="math inline">\(L^2\)</span> contractivity and semigroup property.</p>
<p>The <strong>generator</strong> is <span class="math display">\[\cal L f = \lim_{t\searrow 0} \fc{P_tf-f}t.\]</span> The set of <span class="math inline">\(f\)</span> where this is defined is the domain; <span class="math inline">\(\cal L:\text{Dom}(\cal L) \to L^2(\mu)\)</span>.</p>
<p>Warning: for Markov processes with continuous sample paths, such as Brownian motion, <span class="math inline">\(Dom(\cL)\sub L^2(\mu)\)</span>. Functional analysis is required for rigor, but results usually extend.</p>
<p><span class="math inline">\(P_t\)</span> is the solution of the Kolmogorov equation <span class="math display">\[ \ddd{t} P_t f = P_t \cL f, \quad P_0f=f.\]</span> The generator and semigroup commute.</p>
<p>A finite-state Markov process with <span class="math display">\[ \Pj[X_{t+\de}=j|X_t=i] = \la_{ij} \de + o(\de), \quad i\ne j\]</span> has generator equal to the transition matrix <span class="math inline">\(\La\)</span>.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<p>Then <span class="math inline">\(P_t=e^{t\La}\)</span>. (In the non-finite case, this makes sense as a power series.)</p>
<p><span class="math inline">\(P_t\)</span> is <strong>reversible</strong> if <span class="math inline">\(P_t\)</span> are self-adjoint on <span class="math inline">\(L^2(\mu)\)</span>: <span class="math display">\[\an{f,P_tg}_\mu = \an{P_tf,g}_\mu.\]</span> Reversibility implies <span class="math display">\[P_tf(x) =\E[f(X_t)|X_0=x] = \E[f(X_0)|X_t=x];\]</span> i.e., the Markov process viewed backwards has the same law. <!-- delta functions? --></p>
<p>For finite state space, this is equivalent to <span class="math display">\[\mu_i \La_{ij} = \mu_j \La_{ji},\]</span> <strong>detailed balance</strong>.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<p><strong>Definition</strong>. A Markov semigroup is <strong>ergodic</strong> if <span class="math inline">\(P_tf \to \mu f\)</span> in <span class="math inline">\(L^2(\mu)\)</span> as <span class="math inline">\(t\to \iy\)</span> for every <span class="math inline">\(f\in L^2(\mu)\)</span>.</p>
<blockquote>
<p>A measure <span class="math inline">\(\mu\)</span> satisfies a Poincare inequality for a certain notion of “gradient” if and only if an ergodic Markov semigroup associated to this “gradient” converges exponentially fast to <span class="math inline">\(\mu\)</span>.</p>
</blockquote>
<p>The <strong>Dirichlet form</strong> is <span class="math display">\[\cal E(f,g) = -\an{f,\cL g}_\mu.\]</span> Note: for complex-valued functions, we take the real part.</p>
<p><strong>Theorem</strong> (Poincare inequality). Let <span class="math inline">\(P_t\)</span> be reversible ergodic Markov semigroup with stationary measure <span class="math inline">\(\mu\)</span>. For <span class="math inline">\(c\ge 0\)</span>, TFAE:</p>
<ol type="1">
<li>(Poincare inequality) <span class="math inline">\(\Var_\mu(f) \le c\cal E(f,f)\)</span></li>
<li><span class="math inline">\(\forall f, t, \ve{P_t f- \mu f}_{L^2(\mu)} \le e^{-\fc tc}\ve{f-\mu f}_{L^2(\mu)}\)</span></li>
<li><span class="math inline">\(\forall f, t, \cal E(P_t f, P_t f) \le e^{-2t/c} \cal E(f,f)\)</span></li>
<li><span class="math inline">\(\forall f \exists \ka(f), \ve{P_t f-\mu f}_{L^2(\mu)} \le \ka(f) e^{-\fc tc}\)</span>.</li>
<li><span class="math inline">\(\forall f \exists \ka(f), \cal E(P_tf,P_tf)\le \ka(f)e^{-2t/c}\)</span>.</li>
</ol>
<p>Note <span class="math inline">\(5\Leftarrow 3\implies 1\Leftrightarrow 2\Rightarrow 4\)</span> doesn’t require reversibility.</p>
<p>Example: Gaussian distribution</p>
<ol type="1">
<li>Define the <strong>Ornstein-Uhlenbeck process</strong> by <span class="math display">\[X_t = e^{-t} X_0 + e^{-t} W_{e^{2t}-1}, \quad X_0\perp W\]</span> where <span class="math inline">\(W_t\)</span> is standard Brownian motion. Note <span class="math inline">\(N(0,1)\)</span> is stationary.</li>
<li>Using Gaussian integration by parts <span class="math inline">\(\E_{N(0,1)} [\xi f(\xi)] = \E_{N(0,1)} [f'(\xi)]\)</span>, show that
<ul>
<li><span class="math inline">\(X_t\)</span> is a Markov process with semigroup <span class="math inline">\(\E[f(e^{-t} x + \sqrt{1-e^{-2t}}\xi)]\)</span>, <span class="math inline">\(\xi\in N(0,1)\)</span>.</li>
<li>The generator is <span class="math inline">\(\cL f(x) = -xf'+f''\)</span>.</li>
<li><span class="math inline">\(\cE (f,g) = \an{f',g'}_\mu\)</span>.</li>
<li>In particular, <span class="math inline">\(\cE(f,f) = \ve{f'}^2_{L^2(\mu)} = \E[f'(\xi)^2]\)</span> is exctly the expected square gradient.</li>
</ul></li>
<li>From the expression for <span class="math inline">\(P_t\)</span> obtain <span class="math inline">\(\ddd{x} P_t f(x) = e^{-t} P_t f'(x)\)</span>. Then <span class="math inline">\(\cE (P_tf,P_tf) \le e^{-2t} \cE(f,f)\)</span>. Hence for <span class="math inline">\(\mu=N(0,1)\)</span>, <span class="math display">\[\Var_\mu(f) \le \ve{f'}_{L^2(\mu)}.\]</span></li>
<li>By tensorization, <span class="math display">\[\Var_\mu(f) \le \E[\ve{\nb f(X_1,\ldots, X_n)}^2].\]</span></li>
</ol>
<p>Note: The O-U process is the solution of the stochastic differential equation <span class="math display">\[dX_t = -X_t \,dt + \sqrt2 \, dB_t.\]</span> Revisit this after I learn stochastic calculus.</p>
<p>Tensorization using Poincare inequality:</p>
<ol type="1">
<li>Construct a random process <span class="math inline">\(X_t=(X_t^1,\ldots, X_t^n)\)</span> by having coordinates re-randomize according to independent Poisson processes.</li>
<li>Then <span class="math display">\[P_tf(x) = \sum_{I\subeq [n]} (1-e^{-t})^{|I|} e^{-t(n-|I|)} \int f(x_1,\ldots, x_n) \prod_{i\in I}\mu_i(dx_i)+o(t).\]</span> (Note the integral is only over the indices in <span class="math inline">\(I\)</span>.) Only the <span class="math inline">\(|I|=1\)</span> terms matter in the limit (makes sense, we’re taking the derivative!),
\begin{align}
\cL f &amp;= -\sumo in \de_i f\\
\de_if(x)&amp;:= f(x) - \int f(x_1,\ldots, x_{i-1}, z, x_{i+1},\ldots, x_n)\,\mu_i(dz).
\end{align}</li>
<li><span class="math inline">\(\int h\de_i g\,d\mu=0\)</span> if <span class="math inline">\(h\)</span> does not depend on <span class="math inline">\(x_i\)</span>. Thus <span class="math inline">\(\cE(f,g) = \sumo in \int \de_i f\de_i g\,d\mu\)</span>. This is symmetric, so the process is reversible.</li>
<li>We have <span class="math inline">\(\cE(f,f) = \sumo in \int \Var_if \,d\mu\)</span>, so the tensorization inequality is exactly <span class="math inline">\(\Var_\mu(f) \le \cE(f,f)\)</span>.</li>
<li>Conclude ergodicity. Conversely, we can prove the tensorization inequality from ergodicity: Note <span class="math display">\[\de_i P_t f=e^{-t} \sum_{I\nin i} (1-e^{-t})^{|I|} e^{-t(n-1-|I|)} \int \de_i f\prod_{i\in I}\mu_i(dx_i)\]</span> so <span class="math inline">\(\cE(P_tf,P_tf) \le \ka(f) e^{-2t}\)</span>.</li>
</ol>
<h3 id="variance-identities-and-exponential-ergodicity">2.4 Variance identities and exponential ergodicity</h3>
<p>We prove the Poincare inequality.</p>
<ol type="1">
<li><strong>Lemma</strong>. <span class="math display">\[\ddd t \Var_\mu(P_t f) = -2\cal E(P_tf,P_tf)\]</span>. <em>Proof</em>. Use <span class="math inline">\(\mu(P_tf)=\mu(f)\)</span>. Both are equal to <span class="math inline">\(\mu(2P_t f\ddd tP_tf)\)</span>.</li>
<li><strong>Corollary</strong>. <span class="math inline">\(\cE(f,f)\ge 0\)</span>.</li>
<li>Integral representation of variance: If the Markov semigroup is ergodic, integrating gives <span class="math inline">\(\Var_\mu (f) = 2\iiy \cE(P_tf,P_tf)\,dt\)</span>.</li>
<li>(<span class="math inline">\(3\implies1\)</span>) Use the integral representation.</li>
<li>(<span class="math inline">\(1\implies2\)</span>) Using <span class="math inline">\(\cal E\propto -\ddd t \Var\)</span>, get a differential inequality that gives exponential decay.</li>
<li>(<span class="math inline">\(2\implies1\)</span>) Write <span class="math inline">\(\cE\)</span> as a limit and apply the inequality to <span class="math inline">\(\Var\)</span>.</li>
<li>If <span class="math inline">\(P_t\)</span> is reversible, then <span class="math inline">\(t\mapsto \log\ve{P_t f}_{L^2(\mu)}^2\)</span>, <span class="math inline">\(\log \cE(P_tf,P_tf)\)</span> are convex. Proof. First derivative is <span class="math inline">\(-\fc{2\an{\cL P_tf, f}}{\ve{P_tf}^2}\)</span>. Differentiate again, use CS.</li>
<li>(<span class="math inline">\(2\implies3\)</span>) The first derivative is increasing. Rearrange to get <span class="math display">\[\fc{\cE(P_tf,P_tf)}{\cE(f,f)}\le \fc{\ve{P_tf}_{L^2(\mu)}^2}{\ve{f}_{L^2(\mu)}^2}\]</span>.</li>
<li>(<span class="math inline">\(4\implies2\)</span>, <span class="math inline">\(5\implies3\)</span>) Use the lemma: if <span class="math inline">\(g\)</span> is convex and <span class="math inline">\(g(t)\le K-\al t\)</span> for <span class="math inline">\(t\ge 0\)</span> then <span class="math inline">\(g(t)\le g(0)-\al t\)</span> for <span class="math inline">\(t\ge 0\)</span>.</li>
</ol>
Intuition: If reversibility holds,
\begin{align}
\cE(f,g) &amp;=\rc2 \sum \mu_i \La_{ij} (f_i-f_j)(g_i-g_j)\\
\cE(f,f) &amp;=\rc2 \sum \mu_i \La_{ij} (f_i-f_j)^2.
\end{align}
<p><a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>In finite dimensions, if <span class="math inline">\(\mu f=0\)</span>, <span class="math display">\[
\cE(f,f) \ge (\ub{\la_1}0-\la_2) \Var_\mu(f).
\]</span> The best constant in the Poincare inequality is the spectral gap. The spectral gap controls the exponential convergence rate. Note it’s essential that <span class="math inline">\(\La\)</span> admits a real spectral decomposition.</p>
<h3 id="problems">Problems</h3>
<ol type="1">
<li>Use <span class="math inline">\(\Var\pa{\ve{\rc n \sumo kn X_k}_B} = \sup_{y\in B^*} \an{\rc n\sumo kn X_k, y}\)</span>. Use the corollary, get <span class="math inline">\(D_k^-\le 2 \sup_{y\in B^*}\an{\rc X_k,y} \le \fc{2C}{n}\)</span>. Now square and sum.</li>
<li>.</li>
<li>.</li>
<li>.</li>
<li>We have <span class="math inline">\((f(x) - f(..., a, ...))^2\le \ve{(b-a)\nb f}^2\)</span>. Now take expectations and sum over different coordinates.</li>
<li>.</li>
<li></li>
<li><ol type="1">
<li><p>Smooth <span class="math inline">\(f\)</span> and use the Gaussian Poincare inequality.</p>
Note we have <span class="math inline">\(\Var[f(x)]\le \E[(f(x)-f(0))^2]\le \E L^2x^2 = L^2\)</span> but this doesn’t help us, because if we sum up derivatives along different coordinates, we overestimate <span class="math inline">\(L\)</span> to <span class="math inline">\(Ln\)</span> instead.</li>
<li>Note <span class="math inline">\((\Si^{\rc Y})_i\)</span> is <span class="math inline">\(\ve{(\Si^{\rc 2})_i}\)</span>-Lipschitz, so <span class="math inline">\(\max((\Si^{\rc 2}Y)_i)\)</span> is <span class="math inline">\(\max \ve{(\Si^{\rc 2})_i}\)</span>-Lipschitz. By (a), <span class="math display">\[ \Var[\max_i X_i] \le \max\ve{(\Si^{\rc 2})_i} = \max \ve{\Si_{ii}} =\max_i \Var(X_i).\]</span></li>
<li>The sum of variances on the LHS is <span class="math inline">\(\sum_j \sum_i F_{x_j}^2 = \ve{\nb F}^2\)</span>. Use the central limit theorem to show that the LHS var approaches the RHS var. There’s a factor of <span class="math inline">\((\max -\min)^2/4=1\)</span>.</li>
</ol></li>
<li></li>
<li><ol type="1">
<li>Let <span class="math inline">\(p_I\)</span> denote the probability of seeing sequence <span class="math inline">\(I\)</span> of jumps in <span class="math inline">\([0,t]\)</span>. (We don’t need to calculate it.) Let <span class="math inline">\(\Pj(x|I)\)</span> be the probability of <span class="math inline">\(x\)</span> after observing jumps in <span class="math inline">\(I\)</span>. We have <span class="math display">\[\E [f(Z_t)] = \sum_I p_I \int \Pj(x|I) f(x)\dx\]</span> which can be written <span class="math inline">\(P_tf\)</span>. Note this converges. (<span class="math inline">\(\sum_I p_I = 1\)</span>.)</li>
<li><p>Only the <span class="math inline">\(|I|=\phi, 1\)</span> terms are significant as <span class="math inline">\(\sum_{|I|=k} p_I = Poisson(n, t, k)\)</span>.</p>
\begin{align}\cL f &amp;= \lim_{t\to 0^+} \pf{e^{-tn} \E f - \E f + \sum (1-e^{-t}) e^{-t (n-1)} \int f\, \mu_i (dx_i|x)}{t}\\
&amp;= \sum_{i=1}^n \ub{\pa{\pa{\int f(x)\mu_i(dx_i|x)} - f(x)}}{=: - \de_if}\\
\cE (f, g) &amp;= -\int f \cL g\,d\mu\\
&amp;= -\int \pa{-\sumo in f\de_i g}\,d\mu\\
&amp;= \sumo in \int \de_if\de_ig\,d\mu
\end{align}
where we used <span class="math inline">\(\int (f-\de_if)\de_ig=0\)</span> because the first term has mean 0 and <span class="math inline">\(\de_ig\)</span> doesn’t depend on <span class="math inline">\(x_i\)</span>.</li>
<li>\begin{align}
\De_i f &amp;= \max_x |f(..., 1_i,...) - f(..., -1_i, ...)|\\
\De_j f \,d\mu_i &amp; = \max_x \ab{\int f(\ldots 1_j\ldots)\,d\mu_i - \int f(\ldots -1_j\ldots)}\\
&amp;\max_x |\Pj(x_i=1|x_{-i, j\leftarrow 1}) f(1_j1_i) - \Pj(x_i=1|x_{-i, j\leftarrow-1}) f(-1_j1_i) + \Pj(x=-1|\cdots)\cdots
\end{align}
The probabilities of <span class="math inline">\(=1|1\)</span> and <span class="math inline">\(=1|-1\)</span> differ by <span class="math inline">\(C_{ij}\)</span> (n.b. typo) so we get
\begin{align}
&amp;\le \max_x |f(x,1_j) - f(x,-1_j)| + C_{ij} \De_i f = \De_j f + \De_i fC_{ij}.
\end{align}</li>
<li>\begin{align}
\De_j\pa{f+\fc tn \cL f} &amp;=
\De_jf + \fc tn \De_j \pa{\pa{\int f(x) \,d\mu_i (dx_i|x)} - f(x)} \\
&amp;\le \pa{1-\fc tn}\De_j f + \fc tn \sumo in \De_i fC_{ij}\\
\De(f+t\cL f/n) &amp;\le \De f(I-t(I-C)/n).
\end{align}</li>
<li>Iterate <span class="math inline">\(n\)</span> times and take <span class="math inline">\(n\to \iy\)</span> to get <span class="math display">\[\De (e^{t\cL}f) = \De P_t f \le \De f e^{-t(I-C)}.\]</span></li>
<li>\begin{align}
\cE(f, f)&amp;=\sumo in \int (f-\int f\,d\mu_i)^2\,d\mu\\
&amp;\le \sumo in |\De_i f|^2\\
\cE(P_tf,P_tf) &amp;\le \sumo in \ve{\De f e^{-2t(I-C)}}^2\\
&amp;\le \ka(f) (\la_{\min}(I-C))^{-1} \\
&amp; = \ka(f) (1-\la_{\max}(C))^{-1}
\end{align}
<p>Use <span class="math inline">\(5\implies 1\)</span> of Poincare.</p></li>
</ol></li>
</ol>
<h2 id="subgaussian-concentration-and-log-sobolev-inequalities">3 Subgaussian concentration and log-Sobolev inequalities</h2>
<strong>Lemma 3.1</strong> (Cheroff bound). Define the <strong>log-moment generating function</strong>
\begin{align}
\psi(\la) :&amp;= \log \E[e^{\la (X-\E X)}]\\
\psi^*(\la)&amp;=\sup_{\la \ge 0} (\la t-\psi(\la)).	
\end{align}
<p>Then <span class="math inline">\(\Pj(X-\E X \ge t) \le e^{-\psi^*(t)}\)</span> for all <span class="math inline">\(t\ge 0\)</span>.</p>
<p><em>Proof</em>. Exponentiate and Markov.</p>
<p>The log-moment generating function is continuous and can be investigated using calculus.</p>
<p><strong>Example</strong>. Gaussian: <span class="math inline">\(\psi(\la) = \fc{\la^2\si^2}2\)</span> and <span class="math inline">\(\psi^*(t) = \fc{t^2}{2\si^2}\)</span> so bound of <span class="math inline">\(e^{-\fc{t^2}{2\si^2}}\)</span>.</p>
<p>A rv is <span class="math inline">\(\si^2\)</span>-<strong>subgaussian</strong> if <span class="math inline">\(\psi(\la)\le \fc{\la^2\si^2}2\)</span>. Then we get tail bounds of <span class="math inline">\(e^{-\fc{t^2}{2\si^2}}\)</span>.</p>
<p><strong>Lemma 3.6</strong> (Hoeffding): If <span class="math inline">\(X\in [a,b]\)</span> a.s., then <span class="math inline">\(X\)</span> is <span class="math inline">\((b-a)^2/4\)</span> subgaussian.</p>
<p><em>Proof</em>. Interpret <span class="math inline">\(\psi''\)</span> as a variance, get <span class="math inline">\(\psi''\le \fc{(b-a)^2}{4}\)</span>, integrate twice.</p>
<h3 id="the-martingale-method">3.2 The martingale method</h3>
<p>We want to show <span class="math inline">\(f\)</span> is subgaussian with variance proxy controlled by a “square gradient” of <span class="math inline">\(f\)</span>.</p>
<p>The subgaussian property does not tensorize.</p>
<p>The proof of subgaussian inequailties can be reduced to a strengthened form of Poincare inequalities, <strong>log-Sobolev</strong> inequalities, that do tensorize.</p>
<p><strong>Lemma</strong> (Azuma): Let <span class="math inline">\(\cF_k\)</span> be a filtration, and 1. (Martingale difference) <span class="math inline">\(\De_k\)</span> is <span class="math inline">\(\cF_k\)</span>-measurable, <span class="math inline">\(\E[\De_k |\cF_{k-1}]=0\)</span>. 2. (Conditional subgaussian) <span class="math inline">\(\E[e^{\la \De_k}|\cF_{k-1}]\le e^{\la^2\si_k^2/2}\)</span>. Then <span class="math inline">\(\sumo kn \De_k\)</span> is subgaussian with variance proxy <span class="math inline">\(\sumo kn \si_k^2\)</span>.</p>
<p><strong>Corollary</strong> (Azuma-Hoeffding): Replace (2) by <span class="math inline">\(A_k\le \De_k\le B_k\)</span> where <span class="math inline">\(A_k,B_k\)</span> are <span class="math inline">\(\cF_{k-1}\)</span>-measurable. The variance proxy is <span class="math inline">\(\rc 4 \sumo kn \ve{B_k-A_k}^2_{\iy}\)</span>. The tail bound is <span class="math inline">\(\exp\pa{-\fc{2t^2}{\sumo kn \ve{B_k-A_k}^2_{\iy}}}\)</span>.</p>
<p><strong>Theorem 3.11</strong> (McDiarmid): For <span class="math inline">\(X_{1:n}\)</span> independent, <span class="math inline">\(f(X)\)</span> is subgaussian with variance proxy <span class="math inline">\(\rc 4\sumo kn \ve{D_kf}_{\iy}^2\)</span> where <span class="math display">\[D_if(x) = (\sup_z-\inf_z)f(x_{1:i-1},z,x_{i+1:n}).\]</span></p>
<p><em>Proof</em>. Use Azuma-Hoeffding on martingale differences <span class="math inline">\(\De_k =\E[f|X_{1:k}] - \E[f|X_{1:k-1}]\)</span>.</p>
<p>This is unsatisfactory because the variance proxy is controlled by a uniform upper bound on square gradient rather than its expectation. Something like <span class="math inline">\(\ve{\sumo kn |D_kf|^2}_{\iy}\)</span> would be better.</p>
<h3 id="the-entropy-method">3.3 The entropy method</h3>
<p>The subgaussian property is equivalent to <span class="math inline">\(\la^{-1}\psi(\la)\precsim \la\)</span>, so it suffices to show <span class="math inline">\(\ddd{\la}(\la^{-1}\psi)\precsim 1\)</span>.</p>
<ul>
<li>Define <span class="math inline">\(\Ent(Z) = \E[Z\ln Z] - (\E Z)(\ln \E Z)\)</span>.</li>
<li>(Entropic formulation of subgaussianity) <span class="math inline">\(\forall \la \ge 0, \Ent(e^{\la X}) \le \fc{\la^2\si^2}{2} \E e^{\la X}\implies \forall \la \ge 0, \psi(\la) \le \fc{\la^2\si^2}2\)</span>.
<ul>
<li><em>Proof</em>. Integrate <span class="math inline">\(\ddd{\la} \fc{\psi(\la)}{\la} = \rc{\la^2} \fc{\Ent(e^{\la x})}{\E (e^{\la x})}\)</span>.</li>
</ul></li>
<li>Variational characterization of entropy: <span class="math inline">\(\Ent(Z) = \sup\set{\E(ZX)}{\E(e^X)=1}\)</span>.
<ul>
<li><em>Proof</em>.
\begin{align}
\Ent(Z) - \E[ZX] &amp;= \Ent_Q (e^{-X}Z)\ge0
\end{align}
with equality when <span class="math inline">\(X=\ln\pf{Z}{\E Z}\)</span>.</li>
</ul></li>
<li>Tensorization: <span class="math inline">\(\Ent(f) \le \E\ba{\sumo in \Ent_i f}\)</span>.
<ul>
<li><em>Proof</em>. Let <span class="math inline">\(Z=f(X)\)</span>.
\begin{align}
U_k :&amp;= \ln \E[Z|X_{1:k}] - \ln \E[Z|X_{1:k-1}]\\
\Ent (Z) &amp;= \sum \E[ZU_k]\\
\E[e^{U_k}|X_{-k}]&amp;=1\implies &amp; \E[ZU_k|X_{-k}]&amp;\le \Ent_k f.
\end{align}</li>
</ul></li>
</ul>
<p>The entropic formulation of subgaussianity and the tensorization inequality tell us that if we prove (for some notion of <span class="math inline">\(\nb\)</span>) <span class="math display">\[ \Ent(e^g) \precsim \E[\ve{\nb g}^2]\]</span> in one dimension, then in any number of dimensions, <span class="math display">\[ \Ent(e^{\la f})\precsim \E[\ve{\nb (\la f)}^2e^{\la f}]\]</span> so <span class="math inline">\(f\)</span> is subgaussian with <span class="math inline">\(\max\ve{\nb f}^2\)</span>.</p>
<ul>
<li>Discrete log-Sobolev: Let <span class="math inline">\(D^-f=f-\inf f\)</span>. Then <span class="math display">\[\Ent[e^f] \le \Cov[f,e^f] \le \E[|D^-f|^2e^f].\]</span>
<ul>
<li><em>Proof</em>. Jensen and convexity.</li>
</ul></li>
<li>On product measure, <span class="math inline">\(f\)</span> is subgaussian with variance proxy <span class="math inline">\(2\ve{\sumo in |D_if|^2}_{\iy}\)</span>. Upper and lower tail bounds with <span class="math inline">\(D_i^-\)</span> and <span class="math inline">\(D_i^+\)</span>.</li>
</ul>
<p><strong>Example</strong> (Random Bernoulli symmetric matrices). Using <span class="math inline">\(D_{ij}^-\la_{\max(M)}\)</span>, get <span class="math display">\[ \Pj(\la_{\max}(M) - \E\la_{\max}(M)\ge t)\le e^{-\fc{t^2}{64}}. \]</span> We can’t use the same technique to look at the lower tail because the bound is in terms of different <span class="math inline">\(M^{(ij)}\)</span>’s.</p>
<h3 id="log-sobolev-inequalities">3.4 Log-Sobolev inequalities</h3>
<p>We have an entropic analogue of just the easy parts of the Poincare inequality equivalence.</p>
<p><strong>Theorem</strong>. 1 and 2 are equivalent. 3 implies 1, 2 if <span class="math inline">\(\Ent_\mu(P_tf)\to 0\)</span> (entropic ergodicity).</p>
<ol type="1">
<li><span class="math inline">\(\Ent_\mu(f)\le c\cE(\ln f, f)\)</span> (log-Sobolev inequality)</li>
<li><span class="math inline">\(\Ent_\mu(P_tf) \le e^{-t/c} \Ent_\mu(f)\)</span> (entropic exponential ergodicity)</li>
<li><span class="math inline">\(\cE(\ln P_tf , P_tf) \le e^{-t/c}\cE(\ln f, f)\)</span>.</li>
</ol>
<p><em>Proof</em>.</p>
<ul>
<li>(<span class="math inline">\(3\implies1\)</span>) Note <span class="math inline">\(\ddd{t} \Ent_\mu(P_tf) = -\cE(\ln P_tf,P_tf)\)</span> using <span class="math inline">\(\mu(\cL P_tf)=0\)</span>. <span class="math inline">\(\Ent_\mu(f) = \lim -\iiy \ddd{\mu} \Ent_\mu(P_tf)\)</span>.</li>
<li>(<span class="math inline">\(1\implies2\)</span>) Inequality for exponential decay</li>
<li>(<span class="math inline">\(2\implies1\)</span>) Take the limit.</li>
</ul>
<strong>Example</strong> (Discrete log-Sobolev inequality). Consider Poisson resampling under <span class="math inline">\(\mu\)</span>. Then
\begin{align}
P_t f&amp;= e^{-t}f + (1-e^{-t}) \mu(f) \\
\cE(f,g)&amp;=\int \de f\de g\,d\mu = \Cov_\mu[f,g]\\
P_tf \ln (P_tf)&amp;\le e^{-t}\ln f + (1-e^{-t}) \mu f\ln \mu f\\
\implies \Ent_\mu[P_t f] &amp;\le e^{-t} \Ent_\mu(f)\\
\implies \Ent_\mu(f) &amp;\le \Cov_\mu(\ln f, f) &amp;(2\implies 1)
\end{align}
<p>The log-Sobolev equivalences cannot reproduce the tensorization inequality for entropy.</p>
<strong>Theorem</strong> (Gaussian log-Sobolev). For independent Gaussian variables,
\begin{align} 
\Ent[f] &amp;\le \rc 2 \E [\nb f \cdot \nb \ln f]&amp; (f\ge 0)\\
\Ent[e^f] &amp; \le \rc 2 \E[\ve{\nb f}^2 e^f].
\end{align}
<p>As a result <span class="math inline">\(f\)</span> is <span class="math inline">\(\si^2 = \ve{\ve{\nb f}^2}_{\iy}\)</span> subgaussian and we get Gaussian concentration, <span class="math display">\[\Pj[f - \E f\ge t] \le e^{-t^2/2\si^2}.\]</span></p>
<em>Proof</em>. Recall <span class="math inline">\(\cE(f,g) = \mu(f'g')\)</span>, <span class="math inline">\((P_tf)' = e^{-t}P_t f'\)</span>. Note <span class="math inline">\(|P_t(fg)|^2 \le P_t(f^2)P_t(g^2)\)</span> by CS (expand out).
\begin{align}
(\ln P_t f)' (P_tf)' &amp;= e^{-2t} \fc{|P_tf|^2}{P_tf}\\
|P_t f'|^2 &amp;\le P_t((\ln f)'f') P_t f&amp;\text{by CS}\\
\implies \cE(\ln (P_tf), P_tf) &amp;\le e^{-2t}\cE(\ln f, f) &amp;\text{by }\int\\
\implies \Ent_\mu(f) &amp;\le \cE(\ln f, f)&amp;(3\implies 1).
\end{align}
Note several different forms of log-Sobolev, equivalent in the Gaussian case (or anytime the chain rule holds for <span class="math inline">\(\cE\)</span>:
\begin{align}
\Ent(f) &amp;\le \rc 2 \E[\nb f \cdot \nb \ln f] = \rc2 \cE (\ln f, f)\\
\Ent(f) &amp;\le \rc 2 \E\pf{\ve{\nb f}^2}{f}\\
\Ent(e^f) &amp;\le \rc 2 \E[\ve{\nb f}^2 e^f]\\
\Ent(f^2) &amp;\le 2\E[\ve{\nb f}^2] = 2\cE(f,f)\\
\E(f^2\ln f)-\E[f^2]\ln \ve{f}_2 &amp;\le c\ve{\nb f}_2^2.
\end{align}
<p>Classical Sobolev inequalities are for <span class="math inline">\(\ved_q\)</span>, <span class="math inline">\(q\ge 2\)</span> and do not tensorize.</p>
<p><strong>Lemma 3.28</strong>: Log-Sobolev <span class="math inline">\(\Ent(f) \le c\cE (\ln f, f)\)</span> implies the Poincare inequality <span class="math inline">\(\Var(f) \le 2c\cE (f,f)\)</span>.</p>
<p><em>Proof</em>. <span class="math display">\[
\ub{\E[\la f e^{\la f}]}{\la^2\cE(f,f) + o(\la^2)} - 
\ub{\E[e^{\la f}] \ln \E[e^{\la f}]}{\la \E f + \la^2(\E[f^2] + \E[f]^2)/2 + o(\la^2)} = 
\ub{\E[\la f e^{\la f}]}{\la \E f + \la^2 \E [f^2] + o(\la^2)}
\]</span></p>
<h3 id="problems-1">Problems</h3>
<p>Equivalent conditions for subgaussianity:</p>
<ol type="1">
<li>The tails are dominated by the tails of the Gaussian: <span class="math display">\[\Pj(|X|\ge t) \le 2\exp(-t^2/K_1^2).\]</span></li>
<li>Moments: For all <span class="math inline">\(p\ge 1\)</span>, <span class="math display">\[
\ve{X}_p = (\E|X|^p)^{\rc p} \le K_2\sqrt p
\]</span></li>
<li>Moment generating function of <span class="math inline">\(X^2\)</span>: <span class="math display">\[\E\exp(X^2/K_3^2)\le 2.\]</span></li>
<li>(If <span class="math inline">\(\E X=0\)</span>,) MGF of <span class="math inline">\(X\)</span>: <span class="math display">\[\E \exp(\la X) \le \exp(\la^2 K_4^2)\]</span> for all <span class="math inline">\(\la\in \R\)</span>.</li>
</ol>
<p>Problems</p>
<ol type="1">
<li><ol type="1">
<li>Expanding <span class="math inline">\(\E(e^{\la (X-\E X)}) \le e^{\la^2\si^2/2}\)</span> gives <span class="math display">\[ 1+\fc{\la^2}2 (X-\E X)^2 \le 1+ \fc{\la^2}2 \si^2.\]</span></li>
<li>Easy.</li>
<li>(<span class="math inline">\(4\implies1\)</span>) This is Chernoff. <span class="math inline">\(\inf_\la \psi(\la)-\la t \le \inf \pa{\fc{\la^2\si^2}2 - \la t} = -\fc{t^2}{2\si^2}\)</span>.</li>
<li>(<span class="math inline">\(1\implies 3\)</span>)
\begin{align}
\E e^{X^2/6\si^2} &amp;= 1+\iiy \fc{t}{3\si^2} e^{\fc{t^2}{6\si^2}} \Pj(|X|\ge t)\,dt\\
&amp; = 1+\iiy \fc{t}{3\si^2} e^{\fc{t^2}{6\si^2}} e^{-\fc{t^2}{2\si^2}}\,dt=2.
\end{align}</li>
<li>? (<span class="math inline">\(3\implies 4\)</span>) <!--Expanding $\E e^{\la X}\le e^{\fc{\la^2\si^2}2 + \la \E X}$ gives--> Weaker: Expanding <span class="math inline">\(\E e^{X^2/6\si^2}\le 2\)</span> gives <span class="math inline">\(\rc{q!}\pf{X^2}{6\si^2}^q\le 1\)</span>, <span class="math inline">\(\E X^{2q}\le (6\si^2)^qq!\)</span>.</li>
<li><span class="math inline">\(\E(e^{X^2/8\si^2}) = \E\pa{\sumz q\iy \rc{q!} \pf{x^2}{8\si^2}^q} \le \E\sumz q\iy\prc{2}^q=2\)</span>.</li>
</ol></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li>Let <span class="math inline">\(Z=\fc{e^{\la X}}{\E e^{\la X}}\)</span>. Then
\begin{align}
\Ent(e^{\la X}) &amp;\le \E(e^{\la X}) \E(Z\ln Z)\\
&amp;\le \E(e^{\la X}) \ln \pf{\E[(e^{\la X})^2]}{(\E e^{\la X})^2}\\
&amp;\le \E(e^{\la X}) (\fc{\la^2\si^2}2 + \ln \pf{e^{2 \la \E X}}{\E(e^{\la X})^2})\\
&amp;\le \E(e^{\la X}) (\fc{\la^2\si^2}2).
\end{align}</li>
<li><ol type="1">
<li>We have
\begin{align}
\Ent Z &amp;= \inf_{t&gt;0} \E[Z\ln Z - Z\ln t - Z+t]\\
\iff (\E Z)(\E \ln Z+1) &amp;=\inf_{t&gt;0} \E(-Z\ln t+t)
\end{align}
Take the derivative; this is minimized at <span class="math inline">\(t=\rc{\E Z}\)</span>.</li>
<li>\begin{align}
\Ent(e^f) &amp;=\inf_{t&gt;0}\E [e^f (f - \ln t - 1) + t]\\
&amp;\le e^f (f-\inf f + e^{\inf f-f} - 1) &amp; t=e^{\inf f}.
\end{align}</li>
<li><span class="math inline">\(\phi(x) = e^{-x}+x-1 \le 1-x+\fc{x^2}2 + x-1 = \fc{x^2}2\)</span>.</li>
<li><span class="math inline">\(\E[\ph(D^-f) e^f] \le \rc{2}\E[|D^-f|^2 e^f]\)</span>. ?? Stuck.</li>
</ol></li>
</ol>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>The kernel is the same as <span class="math inline">\(\La\)</span> except it also records the probbability of staying. <span class="math inline">\(K-I = \La\)</span>.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p><span class="math inline">\(K^*(x,y) = \fc{K(y,x)}{\pi(x)}\pi(y)\)</span>.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>(cf. Laplacian)<a href="#fnref3">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Types and programming languages, Benjamin Pierce</title>
    <link href="http://holdenlee.github.io/notebook/posts/cs/type_theory/types_and_pl.html" />
    <id>http://holdenlee.github.io/notebook/posts/cs/type_theory/types_and_pl.html</id>
    <published>2016-07-31T00:00:00Z</published>
    <updated>2016-07-31T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Types and programming languages, Benjamin Pierce</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-07-31 
          , Modified: 2016-07-31 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#recursive-types">20 Recursive types</a><ul>
 <li><a href="#formalities">20.2 Formalities</a></li>
 <li><a href="#subtyping">20.3 Subtyping</a></li>
 </ul></li>
 <li><a href="#metatheory-of-recursive-types">21 Metatheory of recursive types</a><ul>
 <li><a href="#subtyping-1">21.3 Subtyping</a></li>
 <li><a href="#regular-trees">21.7 Regular trees</a></li>
 <li><a href="#mu-types">21.8 Mu-types</a></li>
 </ul></li>
 <li><a href="#type-reconstruction">22 Type reconstruction</a><ul>
 <li><a href="#let-polymorphism">22.7 Let-polymorphism</a></li>
 </ul></li>
 <li><a href="#universal-types">Universal types</a><ul>
 <li><a href="#system-f">23.3 System F</a></li>
 <li><a href="#section">23.10</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="recursive-types">20 Recursive types</h2>
<p><span class="math inline">\(\mu\)</span> is a recursion operator for types. A definition <span class="math inline">\(T = \mu X. Y\)</span> means: let <span class="math inline">\(T\)</span> be the infinite type satisfying <span class="math inline">\(X=Y\)</span>.</p>
<pre><code>Hungry = \mu A. Nat -&gt; A
Stream = \mu A. Unit -&gt; {Nat, A}
Process = \mu A. Nat -&gt; {Nat, A}
Counter = \mu C. {get: Nat, inc: Unit -&gt; C}</code></pre>
<p>Note: you can’t define Hungry in Haskell because (Then how does printf work? Something with type classes?)</p>
<p>Recursive types well-types the fixed-point combinator. <span class="math display">\[
fix_T = \la f:T\to T.(\la x:(\mu A. A\to T). f (x x)) (\la x:(\mu A. A\to T). f (x x))
\]</span></p>
<p>Every type is inhabited (<span class="math inline">\(\la\_:(). fix_T (\la x:T.x)\)</span>), so systems with recursive types are useless as logics.</p>
<p>[Embed untyped lambda calculus]</p>
<h3 id="formalities">20.2 Formalities</h3>
<p>There are 2 basic approaches to recursive types. What is the relationship between the type and its one-step unfolding?</p>
<ol type="1">
<li>Equi-recursive: They are definitionally equal.</li>
<li>Iso-recursive: They are different but isomorphic. There are functions <code>unfold</code> and <code>fold</code> going both ways. (Ex. Haskell)</li>
</ol>
<p>Note equi-recursive places more demands on the typechecker.</p>
<h3 id="subtyping">20.3 Subtyping</h3>
<h2 id="metatheory-of-recursive-types">21 Metatheory of recursive types</h2>
<p><strong>Theorem</strong> (Knaster-Tarski): Let <span class="math inline">\(X\)</span> be a poset, <span class="math inline">\(f:X\to X\)</span> be order-preserving. Then there exists a fixed point, <span class="math inline">\(\sup\set{x\in X}{x\le f(x)}\)</span>.</p>
<p>Let <span class="math inline">\(\cal U\)</span> be the universal set. Consider <span class="math inline">\((\cal P(\cal U), \subeq)\)</span>. Say <span class="math inline">\(X\)</span> is <span class="math inline">\(F\)</span>-closed/consistent if <span class="math inline">\(F(X)\subeq/\supeq X\)</span>.</p>
<p><em>Corollary</em>. The intersection/union of all <span class="math inline">\(F\)</span>-closed/consistent is the least/greatest fixed point of <span class="math inline">\(F\)</span>, denoted by <span class="math inline">\(\mu F, \nu F\)</span>.</p>
<p>(Principle of induction/coinduction) If <span class="math inline">\(X\)</span> is <span class="math inline">\(F\)</span>-closed, <span class="math inline">\(\mu F\subeq X\)</span>; if <span class="math inline">\(X\)</span> is <span class="math inline">\(F\)</span>-consistent, <span class="math inline">\(X\subeq \nu F\)</span>.</p>
<p>Finite tree types are given by</p>
<pre><code>T = Top | (T, T) | T -&gt; T </code></pre>
<p>Infinite tree types are like this but the tree can be infinite.</p>
<h3 id="subtyping-1">21.3 Subtyping</h3>
<p>Say <span class="math inline">\(T&lt;:Top\)</span>, <span class="math inline">\(S_1&lt;:T_1, S_2&lt;:T_2 \implies (S_1\times S_2)&lt;:(T_1,T_2)\)</span> and similarly for <span class="math inline">\(\to\)</span>. Take the transitive closure to get the subtyping relation.</p>
<h3 id="regular-trees">21.7 Regular trees</h3>
<p>A tree type is regular if subtrees(T) is finite.</p>
<h3 id="mu-types">21.8 Mu-types</h3>
<pre><code>T = X
	| Top
	| T x T
	| T -&gt; T
	| \mu X. T</code></pre>
<p>“Keep substituting” <span class="math inline">\(\mu X. T\)</span> to get the tree type corresponding to the <span class="math inline">\(\mu\)</span>-type, treeof<span class="math inline">\(([X\mapsto \mu X. T]T)(\pi)\)</span>.</p>
<h2 id="type-reconstruction">22 Type reconstruction</h2>
<p>2 questions:</p>
<ol type="1">
<li>Are all substitution instances of t well typed? <span class="math display">\[\forall \si, (\si \Ga \vdash \si t:T)\]</span> Type variables should be held abstract. This leads to <strong>parametric polymorphism</strong>.</li>
<li>Is some substitution instance of <span class="math inline">\(t\)</span> well typed? <span class="math display">\[\exists \si, (\si \Ga \vdash \si t:T)\]</span> Can <span class="math inline">\(t\)</span> be instantiated to a well-typed term by choosing appropriate values? This leads to type reconstruction/inference.</li>
</ol>
<p>Constraint typing: <span class="math inline">\(\Ga \vdash t:T|_{\cal X} C\)</span> means “term <span class="math inline">\(t\)</span> has type <span class="math inline">\(T\)</span> under assumptions <span class="math inline">\(\Ga\)</span> whenever constraints <span class="math inline">\(C\)</span> are satisfied.” <span class="math inline">\(\cal X\)</span> tracks type variables introduced in each subderivation.</p>
<p>(This is a hybrid between the normal deductive system, and the bottom-up constraint generation system.)</p>
<h3 id="let-polymorphism">22.7 Let-polymorphism</h3>
<p>Not allowed: doubleFun:<span class="math inline">\(\forall a . (\forall f : a\to a) \to a \to a\)</span> defined by</p>
<pre class="hs"><code>let doubleFun = \f x -&gt; f (f x)</code></pre>
<p>Reason: a polytype cannot appear inside <code>-&gt;</code>.</p>
<p>T-LetPoly: <span class="math display">\[
\frac{\Ga \vdash [x\mapsto t_1]t_2:T_2 \quad \Ga \vdash t_1:T_1}{\Ga \vdash \text{let }x=t_1\text{ in }t_2:T_2}.
\]</span> Instead of calculating a type for <span class="math inline">\(t_1\)</span>, it substitutes <span class="math inline">\(t_1\)</span> in the body. I.e., perform a step of evaluation before calculating types.</p>
<p>Problem: If the body contains many occurrences, we have to check once for each occurrence. This can take exponential time. See p. 333-4 for solution. Worst-case is still exponential, but in practice it is essentially linear.</p>
<h2 id="universal-types">Universal types</h2>
<p>We need to abstract out a type from a term and instantiate the abstract term with concrete type annotations.</p>
<ul>
<li>Parametric polymorphism: a single piece of code can be typed generically using variables in place of types, and then instantiated. They behave uniformly.
<ul>
<li>Impredicative/first-class</li>
<li>Let-polymorphism (restricted to top-level let-bindings). Functions cannot take polymorphic values as arguments.</li>
</ul></li>
<li>Ad-hoc polymorphism: Exhibit different behaviors when viewed at different types. Overloading: associate single function symbol with many implementations.</li>
<li>Multi-method dispatch</li>
<li>Intensional polymorphism: restricted computation over types at run time.</li>
<li>Subtype polymorphism</li>
</ul>
<h3 id="system-f">23.3 System F</h3>
<p>Equivalent to polymorphic lambda-calculus a.k.a. 2nd-order lambda calculus because it corresponds to 2nd-order intuitionistic logic, which allows quantification over predicates (types) not just terms.</p>
<p>New terms are</p>
<ul>
<li><span class="math inline">\(\la X.t\)</span> (type abstraction)</li>
<li><span class="math inline">\(t [T]\)</span> (type application)</li>
</ul>
<h3 id="section">23.10</h3>
<p>Impredicative: definition involves thing being defined. <span class="math inline">\(T=\forall X.X\to X\)</span> ranges over all types, including <span class="math inline">\(T\)</span> itself.</p>
<p>Predicative/stratified: range is restricted to monotypes.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
