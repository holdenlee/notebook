<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2016-03-08T00:00:00Z</updated>
    <entry>
    <title>Spectral Bounds for Stochastic Diffusion Model in Networks</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/networks/pacm-3-8-16.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/networks/pacm-3-8-16.html</id>
    <published>2016-03-08T00:00:00Z</published>
    <updated>2016-03-08T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Spectral Bounds for Stochastic Diffusion Model in Networks</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-08 
          , Modified: 2016-03-08 
	</p>
      
       <p>Tags: <a href="/tags/pacm.html">pacm</a>, <a href="/tags/GSS.html">GSS</a>, <a href="/tags/networks.html">networks</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p>Eun Jun Lee</p>
<p>This work studies stochastic diffusion model where influence propagates in networks from seed-nodes along edges with independent probabilities. Specifically, we propose spectral bounds for the expected number of nodes that are influenced at the end of propagation. The proposed bounds show significant improvements over the existing bounds in the presence of sensitive edges such as bottlenecks, seed adjacent, and high probability edges.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[CHMAL15] The Loss Surfaces of Multilayer Networks</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/CHMAL15.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/CHMAL15.html</id>
    <published>2016-03-07T00:00:00Z</published>
    <updated>2016-03-07T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[CHMAL15] The Loss Surfaces of Multilayer Networks</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-07 
          , Modified: 2016-03-07 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#initial">Initial</a></li>
 <li><a href="#what">What?</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="initial">Initial</h2>
<ul>
<li>Relate to random matrix theory</li>
<li>Relate to spherical spin-glass model</li>
<li>Global minima lead to overfitting</li>
</ul>
<p>Phenomenon: While multilayer nets do have many local minima, the result of multiple experiments consistently give very similar performance</p>
<p>We rst establish that the loss function of a typical multilayer net with ReLUs can be expressed as a polynomial function of the weights in the network, whose degree is the number of layers, and whose number of monomials is the number of paths from inputs to output.</p>
<p>piecewise, continuous polynomial whose monomials are switched in and out at the boundaries between pieces.</p>
<p>first work to give a “theoretical description of the optimization paradigm with neural networks in the presence of large number of parameters.”</p>
Let <span class="math inline">\(\si(x)=\max(0,x)\)</span>. The random network is
\begin{align}
Y&amp;= q \si(W_H^T\si(\cdots (W_1^TX)\cdots))\\
&amp;=q\sumo i{n_0} \sum_{j=1}^{\ga (=\prod n_i)} X_{ij}\prod_{k=1}^H w_{i,j}^{(k)}.
\end{align}
<p>where <span class="math inline">\(A_{i,j}\)</span> is whether the path is active, <span class="math inline">\(X_{i,j}=X_i\)</span> is starting, and <span class="math inline">\(w_{i,j}^{(k)}\)</span> re weights.</p>
<p>Assumptions</p>
<ul>
<li><span class="math inline">\(X_{i,j}\)</span> independent (so we’re not taking <span class="math inline">\(X_{i,j}=X_i\)</span>?)</li>
<li>Paths: <span class="math inline">\(A_{i,j}\)</span> are Bernoulli. (?)</li>
</ul>
<p><span class="math inline">\((s,\ep)\)</span> reduction image: has only <span class="math inline">\(s\)</span> unique weights but prediction accuracy differs by <span class="math inline">\(\le \ep\)</span>. Some kind of approximation!</p>
<p>ReLU’s.</p>
<h2 id="what">What?</h2>
<ul>
<li>“fully decoupled”</li>
<li>“critical values” of loss function</li>
<li>simulated annealing for neural nets</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[AO15] Linear coupling</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/AO15.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/AO15.html</id>
    <published>2016-03-06T00:00:00Z</published>
    <updated>2016-03-06T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[AO15] Linear coupling</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-06 
          , Modified: 2016-03-06 
	</p>
      
       <p>Tags: <a href="/tags/gradient.html">gradient</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <h2 id="summary">Summary</h2>
<h2 id="page-notes">Page notes</h2>
<ol type="1">
<li></li>
<li></li>
<li></li>
<li></li>
<li>estimation sequence? Thought experiment. Cutoff <span class="math inline">\(K\)</span> for <span class="math inline">\(\ve{\nb f(x)}_2\)</span>. Equate gradient and mirror: <span class="math display">\[\fc{\ep L}{K^2}=\fc{K^2}{\ep^2}.\]</span></li>
<li></li>
<li>Gradient descent guarantee. <span class="math inline">\(f(x_T)-f(x^*) \le O\pf{L\ve{x_0-x^*}_2^2}{T}\)</span>. Distance generating function, Bregman divergence.</li>
</ol>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Mirror descent</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/mirror-descent.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/mirror-descent.html</id>
    <published>2016-03-05T00:00:00Z</published>
    <updated>2016-03-05T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Mirror descent</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-05 
          , Modified: 2016-03-05 
	</p>
      
       <p>Tags: <a href="/tags/gradient.html">gradient</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <h2 id="summary">Summary</h2>
<p>Mirror descent lemma <span class="math display">\[
\al\an{\nb f(z_k), z_k-u}\le \fc{\al^2}{2}\ve{\nb f(z_k)}^2 + \rc2\ve{z_k-u}^2 - \rc2 \ve{z_{k+1}-u}^2.
\]</span></p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Accelerated Gradient descent</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/AGD.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/AGD.html</id>
    <published>2016-03-05T00:00:00Z</published>
    <updated>2016-03-05T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Accelerated Gradient descent</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-05 
          , Modified: 2016-03-05 
	</p>
      
       <p>Tags: <a href="/tags/gradient.html">gradient</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <h2 id="questions">Questions</h2>
<ul>
<li>Lanczos method</li>
<li>Chebyshev approach</li>
<li>Lower bound</li>
<li>Dependence on dimension?</li>
<li>Ellipsoid method</li>
</ul>
<h2 id="references">References</h2>
<p>Keywords: accelerated gradient descent, Chebyshev polynomials, mirror descent</p>
<ul>
<li>http://blog.mrtz.org/2013/09/07/the-zen-of-gradient-descent.html</li>
<li>https://en.wikipedia.org/wiki/Chebyshev_polynomials</li>
<li>http://people.csail.mit.edu/zeyuan/publications.htm</li>
<li>https://blogs.princeton.edu/imabandit/2013/04/01/acceleratedgradientdescent/</li>
<li>https://blogs.princeton.edu/imabandit/2015/06/30/revisiting-nesterovs-acceleration/</li>
<li>https://www.cs.cmu.edu/~ggordon/10725-F12/slides/09-acceleration.pdf</li>
<li>http://statweb.stanford.edu/~candes/math301/Lectures/acc_nesterov.pdf</li>
<li>http://www.asc.tuwien.ac.at/~winfried/teaching/106.079/SS2011/downloads/script-p-046-060.pdf</li>
<li>http://msekce.karlin.mff.cuni.cz/~strakos/download/2012_GerStr.pdf</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Analysis of neural networks</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/analysis.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/analysis.html</id>
    <published>2016-03-05T00:00:00Z</published>
    <updated>2016-03-05T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Analysis of neural networks</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-05 
          , Modified: 2016-03-05 
	</p>
      
       <p>Tags: <a href="/tags/neural%20net.html">neural net</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <h2 id="research-brainstorm">Research brainstorm</h2>
<ul>
<li>Complexity theorety point of view: What can be calculated by depth <span class="math inline">\(n\)</span> threshold circuits? Each gate is a function <span class="math inline">\(\sgn(\sum \al_ix_i)\)</span>.</li>
<li>It doesn’t make sense to assume the ground truth is a neural net. How can we model the ground truth?
<ul>
<li>As a set on the boolean cube <span class="math inline">\(\{0,1\}^n\)</span> or <span class="math inline">\([0,1]^n\)</span>. Model 1: Ising model on the Boolean cube.
<ul>
<li>Understand such a distribution qualitatively. Ex. what is the average noise sensitivity? Fourier spectra? (What’s the boundary look like?) Are these questions answered for random k-SAT? Might other models (ex. DNF’s, CNF’s) be better?</li>
<li>Another layer: There is a probability distribution over inputs as well (you don’t sample uniformly from the whole boolean cube. Makes sense because of next point).</li>
<li>Note that the set of pictures that make sense is probably very small.</li>
<li>What if you want the learned function to be noise stable? What is the noise stability of learned functions? Are threshold circuits noise stable?</li>
</ul></li>
<li>How does this compare to known algorithms for learning boolean functions with low weight? What additional structure do we need to assume? (Low degree algorithm. Lower bounds (min sample complexity, by info theory) for learning b/c of number of functions.)</li>
</ul></li>
<li>“It doesn’t make sense to assume the ground truth is a neural net,” so the approach above is to consider a class of functions and show the class can be learned by neural nets. Is there reason to think of ground truth as a neural net?</li>
<li>Explain me:
<ul>
<li>adversarial phenomena</li>
<li>the distribution of local minima follows laws from statistical physics. Getting to global optima can actually hurt generalization.</li>
</ul></li>
</ul>
<h2 id="people-to-talk-to">People to talk to</h2>
<ul>
<li>ML: Tengyu, Elad, Arora</li>
<li>Zeev: threshold circuits, what’s known. (See Kane’s paper?)</li>
<li>Aizenmann, Abbe: intuition for Ising model over <span class="math inline">\(B^n\)</span>, or other models that make sense (ex. 3SAT).</li>
<li>Fefferman, analysis/geometry students: approximation theory</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>SDP duality</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/sdp-duality.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/sdp-duality.html</id>
    <published>2016-03-04T00:00:00Z</published>
    <updated>2016-03-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>SDP duality</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-04 
          , Modified: 2016-03-04 
	</p>
      
       <p>Tags: <a href="/tags/SDP.html">SDP</a>, <a href="/tags/duality.html">duality</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <p><a href="duality.html">Duality</a></p>
<h2 id="summary">Summary</h2>
<p>The following SDP’s are dual: <span class="math display">\[
\max_{X\succeq 0,\an{A_i,X}=b_i}\an{C,X} \lra \min_{\nu, \sum \nu_iA_i\succeq C} \nu^Tb.
\]</span> (When are they equal? When there is a strictly feasible point.)</p>
<h2 id="derivation">Derivation</h2>
<ol type="1">
<li>First write as a convex program. Replace the condition <span class="math inline">\(X\succeq 0\)</span> by <span class="math display">\[
 \la_{\min}(X)=\min_{\Tr(A)=1, A\succeq 0}\an{A,X}\ge 0.
 \]</span> (Use (2) in <a href="duality.html">duality</a>.)</li>
<li>Simplify, using minimax as a key step.
\begin{align}
 g(\la,\nu) &amp;= \max_X(\an{C,X} + \la \min_{\Tr(A)=1, A\succeq 0}\an{A,X} - \sum \nu_i (\an{A_i,X}-b_i)\\
 &amp;=\max_X\min_{\Tr(A)=1}\an{\ub{C-\sum \nu_iA_i}{Z}+\la A,X} + \nu^Tb\\
 &amp;=\max_X\min_{\Tr(A)=\la}\an{Z + A,X} + \nu^Tb\\
 &amp;= \min_{\Tr(A)=\la}\max_X \an{Z+A,X}\\
 &amp;=\pa{\begin{cases}
 0,&amp;\Tr(Z)=-\la, Z\preceq 0\\
 \iy,&amp;\text{else.}
 \end{cases}}-\nu^Tb
 \end{align}
using minimax, then noting the inside <span class="math inline">\(\max\)</span> forces the condition <span class="math inline">\(Z=-A\)</span>.</li>
<li>Taking <span class="math inline">\(\min\)</span> of this, <span class="math inline">\(Z\preceq 0\)</span> turns into a constraint.</li>
</ol>
<p>See <a href="http://www.eecs.berkeley.edu/~wainwrig/ee227a/SDP_Duality.pdf">notes</a>.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Duality</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/duality.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/duality.html</id>
    <published>2016-03-04T00:00:00Z</published>
    <updated>2016-03-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Duality</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-04 
          , Modified: 2016-03-04 
	</p>
      
       <p>Tags: <a href="/tags/duality.html">duality</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#summary">Summary</a><ul>
 <li><a href="#duality">Duality</a></li>
 <li><a href="#kkt-conditions">KKT conditions</a></li>
 <li><a href="#examples">Examples</a><ul>
 <li><a href="#dual-functions">Dual functions</a></li>
 </ul></li>
 </ul></li>
 <li><a href="#proofs-and-intuition">Proofs and intuition</a><ul>
 <li><a href="#weak-duality">Weak duality</a></li>
 <li><a href="#strong-duality">Strong duality</a></li>
 <li><a href="#intuition">Intuition</a></li>
 </ul></li>
 <li><a href="#questions">Questions</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="summary">Summary</h2>
<h3 id="duality">Duality</h3>
These problems are dual.
\begin{align}
\min_{f_i\le 0, Ax=b} f &amp; \lra \max_{\la \ge 0,\nu} \ub{\min_x \ub{f+\la^T\vec{f} + \nu^T (Ax-b)}{\cal L(x,\la,\nu)}}{g(\la,\nu)}\\
\max_{f_i\ge 0, Ax=b} f &amp; \lra \min_{\la \ge 0,\nu} \ub{\max_x \ub{f+\la^T\vec{f} - \nu^T (Ax-b)}{\cal L(x,\la,\nu)}}{g(\la,\nu)}
\end{align}
<p>In (1) the <span class="math inline">\(f_i\)</span> are convex; in (2) they are concave. In (1), we have dual<span class="math inline">\(\le\)</span>primal: <span class="math display">\[
\min_{f_i\le 0, Ax=b} f \ge \max_{\la \ge 0,\nu} \ub{\min_x \ub{f+\la^T\vec{f} + \nu^T (Ax-b)}{\cal L(x,\la,\nu)}}{g(\la,\nu)}
\]</span> Note: <span class="math inline">\(f^*(y)=\sup y^T - f(x)\)</span> is the conjugate or <strong>Legendre transform</strong>. It is convex, and for convex functions, the double conjugate is the original function.</p>
<p><strong>Slater’s constraints</strong>: Equality holds if the problem is strictly feasible: there exists <span class="math inline">\(x\)</span> such that <span class="math inline">\(f_i(x)&lt;0, Ax=b\)</span>. Linear inequalities are allowed to be non-strict, <span class="math inline">\(f_i(x)\le 0\)</span>.</p>
<h3 id="kkt-conditions">KKT conditions</h3>
<p>The KKT conditions are (here <span class="math inline">\(h=Ax-b\)</span>)</p>
<ol type="1">
<li>(derivative 0) <span class="math inline">\(\nb f+\sum \la_i \nb f_i + \nu^TA=0\)</span>.</li>
<li>(constraints satisfied) <span class="math inline">\(\la \ge 0, f_i\le 0, h_i=0\)</span>.</li>
<li>(complementary slackness) <span class="math inline">\(\la f_i=0\)</span>.</li>
</ol>
<p>Interpretation:</p>
<ul>
<li>If <span class="math inline">\(x,\la,\nu\)</span> satisfy the conditions, then <span class="math inline">\(x\)</span> and <span class="math inline">\((\la,\nu)\)</span> are primal and dual optimal and the optimal values are equal.</li>
<li>if Slater’s condition is satisfied, <span class="math inline">\(x\)</span> is optimal if and only if there exist, <span class="math inline">\(\la,\nu\)</span> that satisfy KKT conditions</li>
</ul>
<h3 id="examples">Examples</h3>
<ol type="1">
<li>Linear programming (equality when there exists a feasible solution)
\begin{align}
\min_{Ax=b,x\ge 0} c^Tx &amp;= \max_{\la\ge 0, A^T\nu\ge -c} -b^T\nu\\
\min_{Ax\le b} c^Tx &amp;= \max_{\la\ge 0, A^T\nu=-c} -b^T\nu.
\end{align}</li>
</ol>
<h4 id="dual-functions">Dual functions</h4>
<p>(Check this.)</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Function</th>
<th style="text-align: left;">Dual</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(u\ln u\)</span></td>
<td style="text-align: left;"><span class="math inline">\(e^{v-1}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\ve{x}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\begin{cases}0,&amp;\ve{y}_*\le 1\\ \iy,&amp;\text{else}\end{cases}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\ln \det(X^{-1})\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\ln \det(-Y)^{-1}-n\)</span></td>
</tr>
</tbody>
</table>
<h2 id="proofs-and-intuition">Proofs and intuition</h2>
<h3 id="weak-duality">Weak duality</h3>
<p>Given a dual feasible <span class="math inline">\(\la,\nu\)</span>, choose <span class="math inline">\(x=x^*\)</span>. Then <span class="math display">\[
g(\la,\nu) = f(x^*) + \sum \la_i \ub{f_i(x^*)}_{\le 0} + \nu^T \ub{(Ax^*-b)}{=0}\le f(x^*).
\]</span></p>
<h3 id="strong-duality">Strong duality</h3>
<h3 id="intuition">Intuition</h3>
<p>Think of the dual as (1) relaxing the strict conditions that <span class="math inline">\(f_i\le 0\)</span> and (2) being a game. If you fail to meet the constraint <span class="math inline">\(f_i\le 0\)</span>, you are penalized by cost <span class="math inline">\(\la_if_i\)</span> (and you gain money from constraints that are met). The adversary first sets “shadow prices” <span class="math inline">\(\la_i,\nu_i\)</span>, the cost for violating the constraints, and then you choose <span class="math inline">\(x^*\)</span>. Duality says you can’t do better in this game. (You can do just as well by choosing your original <span class="math inline">\(x^*\)</span>; this is weak duality.</p>
Let <span class="math inline">\(p^*(u,v)\)</span> be the optima under constraints <span class="math inline">\(f_i\le u_i,h_i=v_i\)</span>. The sensitivity to the constraints are given by the dual optima. We have the following (the derivative equations hold if <span class="math inline">\(p^*\)</span> is differentiable.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>
\begin{align}
p^*(u,v) &amp;\ge p^*(0,0)-\la^{*T} u - \nu^{*T} v\\
\pd{p^*}{v_i} &amp;= -\nu_i\\
\pd{p^*}{u_i} &amp;= -\la_i.
\end{align}
<h2 id="questions">Questions</h2>
<p>If <span class="math inline">\(x\)</span> is a primal optimal, is there necessarily a <span class="math inline">\((\la,\nu)\)</span> that satisfies the equations? I think if <span class="math inline">\(x\)</span> is primal optimal and <span class="math inline">\((\la,\nu)\)</span> is dual optimal, <span class="math inline">\((x,\la,\nu)\)</span> does not necessarily satisfy. Check this.</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Otherwise maybe an inequality? Multiple dual optima are possible.<a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Convex optimization</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/convex_optimization.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/convex_optimization.html</id>
    <published>2016-03-04T00:00:00Z</published>
    <updated>2016-03-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Convex optimization</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-04 
          , Modified: 2016-03-04 
	</p>
      
       <p>Tags: <a href="/tags/convex%20optimization.html">convex optimization</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p>Notes:</p>
<ul>
<li><a href="duality.html">Duality</a></li>
<li></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Gradient descent</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/GD.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/GD.html</id>
    <published>2016-03-04T00:00:00Z</published>
    <updated>2016-03-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Gradient descent</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-04 
          , Modified: 2016-03-04 
	</p>
      
       <p>Tags: <a href="/tags/gradient.html">gradient</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <p>(See 10/15 notebook for detailed notes.)</p>
<h2 id="summary">Summary</h2>
<table style="width:25%;">
<colgroup>
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 5%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Algorithm</th>
<th style="text-align: left;">General</th>
<th style="text-align: left;"><span class="math inline">\(\al\)</span>-strongly convex</th>
<th style="text-align: left;"><span class="math inline">\(\be\)</span>-smooth</th>
<th style="text-align: left;"><span class="math inline">\(\ga\)</span>-convex</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Gradient descent</td>
<td style="text-align: left;"><span class="math inline">\(\rc{\sqrt{T}}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\rc{\al T}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\fc{\be}T\)</span></td>
<td style="text-align: left;"><span class="math inline">\(e^{-\ga T}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Accelerated gradient descent</td>
<td style="text-align: left;"><span class="math inline">\(\fc{d}{T}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\rc{\al T^2}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\fc{\be}{T^2}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(e^{-\sqrt{\ga} T}\)</span></td>
</tr>
</tbody>
</table>
<h3 id="gradient-descent-main-points">Gradient descent main points</h3>
<ul>
<li>What is the general framework?
<ol type="1">
<li>Pick a descent direction <span class="math inline">\(\De x\)</span>.</li>
<li>Choose a step size <span class="math inline">\(\tau&gt;0\)</span>: <span class="math display">\[x^{(t+1)} \leftarrow x+\tau \De x.\]</span></li>
<li>Continue until stop criterion.</li>
</ol></li>
<li>What is vanilla (one shot) gradient descent?
<ul>
<li>Gradient descent lemma: Suppose <span class="math inline">\(f\)</span> is convex and <span class="math inline">\(L\)</span>-smooth, <span class="math inline">\(\ve{\nb f(x)-\nb f(y)}\le L\ve{x-y}\)</span>.
\begin{align}
 x':&amp;=x-\rc L \nb f(x)\\
 \implies f(x')&amp;\le f(x) - \rc{2L}\ve{\nb f(x)}^2.
 \end{align}</li>
<li>There’s no guarantee on smoothness unless we assume <span class="math inline">\(f\)</span> is <span class="math inline">\(l\)</span>-strongly convex, <span class="math inline">\(\ve{\nb f(x)-\nb f(y)}\le l\ve{x-y}\)</span>. Let <span class="math inline">\(\ka=\fc{L}{l}, t = \fc{2}{L+l}\)</span>.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> Then linear convergence holds:
\begin{align}
 \ve{x_{k+1}-x^*} \le \pf{\ka-1}{\ka+1} \ve{x_k-x^*}
 \end{align}</li>
</ul></li>
<li>What is gradient descent with backtracking?
<ul>
<li>Parameters <span class="math inline">\(\al\in (0,0.5)\)</span>, step size, <span class="math inline">\(\be\in (0,1)\)</span> scaling factor.</li>
<li>Choose <span class="math inline">\(\De x=\nb f(x)\)</span>.</li>
<li>Choose <span class="math inline">\(\tau\)</span> by backtracking: Set <span class="math inline">\(t=1\)</span>. While <span class="math inline">\(f(x+t\De x)&gt;f(x)+\al \nb f^T\De x\)</span>, <span class="math display">\[\De x\leftarrow \al \De x.\]</span></li>
<li>Lemma: suppose <span class="math inline">\(mI \preceq \nb^2 f \preceq MI\)</span>. Then <span class="math display">\[\fc{f(x_t)-f(p^*)}{f(x_{t-1})-f(p^*)}\le 1-\min\bc{\fc{2\al \be m}{M}, 2m\al}.\]</span></li>
</ul></li>
<li></li>
</ul>
<h2 id="proofs">Proofs</h2>
<p>Gradient descent lemma: Let <span class="math inline">\(D=\nb f(x)\)</span>. Move to origin. Upper bound is <span class="math display">\[f(x) \le \fc{L}2 x^2 + Dx.\]</span> The minimum is at <span class="math inline">\(-\fc{D}{L}\)</span> and is <span class="math inline">\(\fc{-b^2}{4a} = -\fc{D^2}{2L}.\)</span></p>
<p>For strongly convex: Choose <span class="math inline">\(s\)</span> to maximize the minimum progress in terms of <span class="math inline">\(x\)</span>. <span class="math display">\[\fc{s-\rc{l}}{\rc{l}} = \fc{\rc L-s}{\rc L} \implies s = \fc{2}{L+l}.\]</span></p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Think of this as the harmonic average of how much to move to get to the minima of the upper and lower-bounding quadratics.<a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
