<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2017-05-24T00:00:00Z</updated>
    <entry>
    <title>(TPGB17) The space of transferable adversarial examples</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/TPGB17.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/TPGB17.html</id>
    <published>2017-05-24T00:00:00Z</published>
    <updated>2017-05-24T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>(TPGB17) The space of transferable adversarial examples</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-05-24 
          , Modified: 2017-05-24 
	</p>
      
       <p>Tags: <a href="/tags/neural%20nets.html">neural nets</a>, <a href="/tags/aaml.html">aaml</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#adversarial-subspaces">Adversarial subspaces</a></li>
 <li><a href="#decision-boundaries">Decision boundaries</a><ul>
 <li><a href="#experiments">Experiments</a></li>
 </ul></li>
 <li><a href="#limits-of-transferability">Limits of transferability</a></li>
 </ul> </div>

  <div class="blog-main">
    <!--
See also 

* [intro to problem](adversarial.html).
* [my experiments](adversarial_experiments.html). 
* [confidence](confidence.html).
-->
<h2 id="adversarial-subspaces">Adversarial subspaces</h2>
<ul>
<li>Introduces methods for discovering a subspace of adversarial perturbations.
<ul>
<li>MNIST: 25 dimensions</li>
</ul></li>
<li>Distance traveled before reaching decision boundary is on average larger than distance separating decision boundaries of 2 models in that direction. (This doesn’t seem surprising.)</li>
</ul>
<p>Recall that FGSM is <span class="math display">\[
x^* = x + \ep \nv{\nb_x J(x,y)}.
\]</span></p>
<p>Techniques:</p>
<ul>
<li>Solve optimization problem multiple times, constraining the next direction to be orthogonal to the previous. <strong>GAAS works well, others don’t.</strong>
<ul>
<li>Second-order approximation of loss function <span class="math inline">\(\max_{\ve{r}\le \ep} g^T r + \rc 2 r^T H r\)</span>.</li>
<li>Convex optimization: Write an LP for the region where <span class="math inline">\(f\)</span> is piecewise linear, and throw in orthogonality condition.</li>
<li>GAAS (gradient aligned adversarial subspace): Find orthogonal <span class="math inline">\(r_1,\ldots, r_k\)</span> with <span class="math display">\[\ve{r_i}_2\le \ep, \quad r_i^T \nb_x J(x,y)\ge \ga.\]</span> (Think of a right-angled simplex with vertex at <span class="math inline">\(x\)</span>. We can compute how many there are given desired <span class="math inline">\(\fc{g^Tr_i}{\ve{g}_2}\)</span>: <span class="math inline">\(\min\{\ff{1}{\al^2}, d\}\)</span>.)</li>
<li>JSMA: partition most salient features into <span class="math inline">\(k\)</span> bins; use these <span class="math inline">\(k\)</span> sets to get <span class="math inline">\(k\)</span> orthogonal perturbations.</li>
</ul></li>
</ul>
<p>For DNN, get 44 directions, 25 of which transfer. For CNN, get 15 directions, 2 of which transfer.</p>
<h2 id="decision-boundaries">Decision boundaries</h2>
<p>Adversarial training does not significantly displace decision boundary.</p>
<p>Define unit norm directions <span class="math display">\[
d(f,x) := \fc{x'-x}{\ve{x'-x}}
\]</span> where <span class="math inline">\(x'\)</span> is defined differently in 3 cases:</p>
<ol type="1">
<li>Legitimate direction <span class="math inline">\(d_{leg}\)</span>: <span class="math inline">\(x'\)</span> is closest data point with different class label.</li>
<li>Adversarial example <span class="math inline">\(d_{adv}\)</span>: adversarial example generated from <span class="math inline">\(x\)</span>.</li>
<li><span class="math inline">\(d_{rand}\)</span> random <span class="math inline">\(x'\)</span> in input domain that is classified differently.</li>
</ol>
<p>Define minimum distance <span class="math display">\[
MD_d(f,x) = \amin_{\ep&gt;0} f(x+\ep \cdot d) \ne f(x)
\]</span> and interboundary distance as <span class="math display">\[
ID_d(f_1,f_2,x) = 
|MD_d(f_1,x) - MD_d (f_2,x)|
\]</span></p>
<h3 id="experiments">Experiments</h3>
<p>Transfer from</p>
<ul>
<li>logistic regression (LR)</li>
<li>support vector machine (SVM)</li>
<li>DNN</li>
</ul>
<p>Defenses only prevent white-box attacks by reducing reliability of 1st order approximations (gradient masking).</p>
<h2 id="limits-of-transferability">Limits of transferability</h2>
<p>This hypothesis is false: If 2 models achieve low error while exhibiting low robustness, then adversarial examples transfer between models.</p>
<p>Ex. Adversarial examples on MNIST don’t transfer between linear and quadratic models.</p>
Model-agnostic perturbation: For a fixed feature mapping <span class="math inline">\(\phi\)</span>, define <span class="math inline">\(\de_\phi\)</span> as difference in intra-class means, and the adversarial direction <span class="math inline">\(r_\phi\)</span> for <span class="math inline">\((x,y)\)</span>,
<span class="math display">\[\begin{align}
\de_\phi:&amp;=\rc 2 (\E_{\mu_{+1}} [\phi(x)]
- \E_{\mu_{-1}}[\phi(x)])\\
r_\phi:&amp;= - \ep y \wh \de_\phi.
\end{align}\]</span>
<p>If <span class="math inline">\(f(x) = w^T\phi(x)+b\)</span>, and <span class="math inline">\(\De:=\wh w^T \wh\de_\phi\)</span> is large, and <span class="math inline">\(\phi\)</span> is “pseudo-linear” (<span class="math inline">\(\phi(x+r)-\phi(x)\approx r_\phi\)</span>) then <span class="math inline">\(x+r\)</span> transfers to <span class="math inline">\(f\)</span>.</p>
<p>TLDR: shift points in direction of difference of class means; this transfers well.</p>
<p>Can models with access to same set of input features learn representations that don’t transfer?</p>
<p>There’s a simple (but not very informative…) example where this works: MNIST with XOR artifacts trained on linear and quadratic.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2017-04-29</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2017-04-29.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2017-04-29.html</id>
    <published>2017-04-25T00:00:00Z</published>
    <updated>2017-04-25T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2017-04-29</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-04-25 
          , Modified: 2017-04-25 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#day-by-day">Day by day</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="day-by-day">Day by day</h2>
<ul>
<li><p>Mon. 4-24: If I can bound <span class="math display">\[
\an{\mathcal L g, \an{\nb f, \nb g}} \ll \an{\mathcal L g, \mathcal L g}
\]</span> independent of <span class="math inline">\(f\)</span>, then I can argue that for small <span class="math inline">\(\de\)</span>, eigenvectors for Langevin on <span class="math inline">\((1-\de)f\)</span> are close to eigenvectors for Langevin on <span class="math inline">\(f\)</span>. (One has to be careful with <a href="/posts/math/algebra/linear/matrix_analysis/perturbation.html">which eigenspaces to work with</a>.)</p>
I don’t know how to do this. The best I can do is
<span class="math display">\[\begin{align}
\an{f, \mathcal L_\mu f}_\mu &amp;\le -k \ve{f}_\mu^2\\
\implies \an{f, \mathcal L_{\mu'} f}_{\mu'} &amp;\le -\fc{k}{1+O(\de)}\ve{f}_{\mu'}^2.
\end{align}\]</span>
where <span class="math inline">\(\mu'\)</span> is for <span class="math inline">\((1-\de)f\)</span>, which gives an angle between eigenspaces of <span class="math inline">\(1+O\pf{\la_k}{\la_l}\)</span> where <span class="math inline">\(\la_k,\la_l\)</span> are the threshold values for the eigenspaces. This does NOT go to 1. I need something that goes to 1.</li>
<li>Tue. 4-25:
<ul>
<li>Perceptron writeup.</li>
<li>Experiment with semi-random features.
<ul>
<li>Distribution is normal, square loss. (But maybe square loss is just a proxy?)</li>
<li>Doesn’t seem to work: loss on batch of 100 is on order of 20, even with 50 nodes.</li>
</ul></li>
<li>Can estimate gaussian robustly given samples from something with <span class="math inline">\(L^\iy\)</span> distance (in log space) <span class="math inline">\(h\)</span> with <span class="math inline">\(\wt O (h^2)\)</span> samples. Can we extend this to convex things?</li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Adversarial thoughts</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/adversarial_thoughts.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/adversarial_thoughts.html</id>
    <published>2017-04-16T00:00:00Z</published>
    <updated>2017-04-16T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Adversarial thoughts</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-04-16 
          , Modified: 2017-04-16 
	</p>
      
       <p>Tags: <a href="/tags/neural%20nets.html">neural nets</a>, <a href="/tags/uncertainty.html">uncertainty</a>, <a href="/tags/aaml.html">aaml</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#not-surprising">Not surprising</a></li>
 <li><a href="#a-formalization">A formalization</a></li>
 <li><a href="#a-key-question">A key question</a></li>
 <li><a href="#several-subproblems">Several subproblems</a></li>
 <li><a href="#hypotheses-and-approaches">Hypotheses and approaches</a><ul>
 <li><a href="#glue-approach">Glue approach</a></li>
 <li><a href="#mixture">Mixture</a></li>
 <li><a href="#sampling-from-nns">Sampling from NN’s</a></li>
 <li><a href="#regularization">Regularization</a></li>
 <li><a href="#conditioning">Conditioning</a></li>
 <li><a href="#thresholding-and-quantization">Thresholding and quantization</a></li>
 <li><a href="#is-the-first-layer-the-problem">Is the first layer the problem?</a></li>
 <li><a href="#conservative-concepts-and-detection">Conservative concepts and detection</a></li>
 <li><a href="#more-human-approaches">More human approaches</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <p>See also</p>
<ul>
<li><a href="adversarial.html">intro to problem</a>.</li>
<li><a href="adversarial_experiments.html">my experiments</a>.</li>
<li><a href="confidence.html">confidence</a>.</li>
</ul>
<p>Some explanations for adversarial examples.</p>
<h2 id="not-surprising">Not surprising</h2>
<p>Firstly, I think adversarial examples aren’t very surprising: we shouldn’t expect neural nets to do well against an adversary if we didn’t train against it. Neural nets will do the “laziest” thing, which does not involve the “broader” conceptual class we want them to learn (ex. everything close to a ‘5’ is also a ‘5’); vanilla training doesn’t communicate to them this broader class.</p>
<p>That said, why can’t neural nets do well after adversarial training?</p>
<h2 id="a-formalization">A formalization</h2>
<p>Let <span class="math inline">\(A_a\)</span> be a set of “adversarial” modifications to inputs <span class="math inline">\(x\)</span>. We say that an algorithm adversarially-PAC learns <span class="math inline">\((x,y)\sim D\)</span> if after poly samples and time, it produces <span class="math inline">\(f\in F\)</span> such that <span class="math display">\[
\Pj_{x,y} (\max_a L(f(A_a(x)), y)\ne y) \le
\ep
\]</span> in the realizable case, or <span class="math display">\[
\Pj_{x,y} (\max_a L(f(A_a(x)), y)\ne y) \le
\min_{f\in F} \Pj_{x,y} (\max_a L(f(A_a(x)), y)\ne y) + \ep
\]</span> in the agnostic case, where <span class="math inline">\(L\)</span> is the loss function (ex. 0-1).</p>
<p>We probably also want to assume access to an oracle, which given <span class="math inline">\(f\)</span> and <span class="math inline">\(x\)</span>, produces some <span class="math inline">\(a\)</span> that maximizes <span class="math inline">\(L(f(A_a(x)),y)\)</span>.</p>
<p>Some work has been done here: see Yishay Mansour, Robust learning and inference. Cf. also boosting vs. game theory.</p>
<p>Example: <span class="math inline">\(A_a(x) = x+a\)</span>, restricted to <span class="math inline">\(\ve{a}\le \ep\)</span>.</p>
<p>Now there are settings where the adversarial setting isn’t harder - for example, for SVM, in the above example, if there is still a margin <span class="math inline">\(\ga\)</span> between positive and negative examples even after adversarial perturbation, then we’ll make at most <span class="math inline">\(\rc{\ga^2}\)</span> mistakes by perceptron analysis, which doesn’t care about the distribution.</p>
<p>But this is not the case for neural nets. (The fact that we can get training error to 0 suggests that there may be some “margin” at play (cf. Telgarsky). However, we <em>can’t</em> get to 0 training error if we include adversarial examples!)</p>
<p>Can we study a toy example here like dictionary learning? Generate <span class="math inline">\(Ah = x\)</span>, there is a SVM <span class="math inline">\(\sign(w^Th)=y\)</span>. Make it robust to <span class="math inline">\(+a\)</span>.</p>
<p>(This doesn’t seem to capture what’s going on though in <span class="math inline">\(L^2\)</span> - here the adversary’s best bet is to change in the direction <span class="math inline">\(a_i\)</span> where <span class="math inline">\(i=\amax_i |w_i|\)</span>, which would in fact change the sign of <span class="math inline">\(w^Th\)</span>.)</p>
<h2 id="a-key-question">A key question</h2>
<p>It’s not surprising that adversarial examples exist. A better question is why we can’t “train them away” by normal training methods, and what can we do to fix this.</p>
<p>I think there are 2 possibilities.</p>
<ol type="1">
<li>There exists a neural network, with reasonable parameter sizes (under whatever regularization we are using), that can do well on most adversarial inputs. The issue is that using our optimization methods aren’t finding these parameters.</li>
<li>There is something fundamentally limited about the (typical) neural net architecture that doesn’t capture human concept boundaries.</li>
</ol>
<p>The “margin” analysis above is in favor of (2). (1) could still be true if there are e.g. exponentially more choices of parameters that would do well on normal examples than choices that also do well on the adversarial examples. But this isn’t enough to explain it, because adversarial training stalls eventually.</p>
<h2 id="several-subproblems">Several subproblems</h2>
<p>We can talk about <span class="math inline">\(L^\iy\)</span> or <span class="math inline">\(L^2\)</span> adversarial examples. Also whatever threshold we choose, there shouldn’t be human-adversarial examples below that threshold. (I don’t think this is so much of an issue now.)</p>
<h2 id="hypotheses-and-approaches">Hypotheses and approaches</h2>
<h3 id="glue-approach">Glue approach</h3>
<p>Just take a hodgepodge of things that reduce adversarial examples, including things that detect and reject adversarial examples. For example,</p>
<ul>
<li>check if it has abnormally large coefficients for singular vectors for small singular values.</li>
<li>train a detector for adversarial examples on the internal representation.</li>
</ul>
<p>However, this would be an arms race: likely, for many of these, you can find adversarial examples that pass the test (ex. restrict search to top singular vectors). If this were not the case, that would be <em>very interesting</em>.</p>
<p>Are there things like clipping, normalizing, that you do on input before feeding into the network that could help? (Ex. can damping the smaller singular vectors help?)</p>
<p>These are interesting questions but not as satisfying an approach.</p>
<h3 id="mixture">Mixture</h3>
<p>This doesn’t seem to help.</p>
<p>One question: after doing adversarial training for a while, does the NN still do well against the <em>original</em> adversarial examples? If not, then it’s catastrophically forgetting. If it is forgetting, this should be easy to fix.</p>
<h3 id="sampling-from-nns">Sampling from NN’s</h3>
<p>Train a bunch of NN’s on the real examples and then use majority vote.</p>
<p>Problem: this doesn’t work.</p>
<p>Are we somehow not exploring the space of NN’s which do well on the real examples? Can we use Fisher information, Langevin to sample better?</p>
<p>Probably we can’t do much better. Also the existence of a universal perturbation and the fact that adversarial examples from a “linear” model transfer suggest that most NN’s which do well on real examples suffer from the same adversarial examples. (cf. there being exponentially more NN’s which are weak against these adversarial examples, then good.)</p>
<h3 id="regularization">Regularization</h3>
<p>The objective/regularization is wrong. Ex. we should be encouraging sparsity, using exponentiated gradient/multiplicative weights, etc.</p>
<h3 id="conditioning">Conditioning</h3>
<p>Perhaps the Lipschitz constant is just really large. Can we train a NN to have small Lipschitz constant in the correct norm? (Note this can be challenging because for example the <span class="math inline">\(\iy\to 2\)</span> norm is difficult to compute; even approximating it takes a linear program, which is too computationally intensive to do at every step.)</p>
<p>Also if we can’t find a NN with small Lipschitz constant why not? Does it not exist or is there something wrong with the geometry of the optimization problem?</p>
<p>Path norm, batch normalization seem attractive here.</p>
<h3 id="thresholding-and-quantization">Thresholding and quantization</h3>
<p>ReLUs seem like a very bad idea - the adversary can just keep increasing. But is the problem more for</p>
<ul>
<li>ReLU(x) where <span class="math inline">\(x\)</span> is large, or</li>
<li>ReLU(x) where <span class="math inline">\(x\)</span> is small?</li>
</ul>
<p>The first suggests capping - having <span class="math inline">\(y=ReLU(x)\)</span> grow unlimitedly is a bad idea. (We still have the same problem for sigmoids.) The second suggests that we are getting past thresholds by adding a lot of correlated noise, so we should zero out small ReLU(x)’s.</p>
<p>In the brain we don’t have thresholds exceeded by contributions from hundreds of neurons - there’s some kind of attenuation or normalization (of activations because of limited energy?) that prevents this. What if we just take the top <span class="math inline">\(k\)</span> activations? (During training or testing?)</p>
<p>What about quantization/binarization? For MNIST, binarizing all pixels to be 0 or 1 is fine, and helps against adversarial examples (because small norm changes don’t have much effect - although you should consider other attacks here!). But this is in some sense cheating because MNIST is basically black-white. Can we binarize/quantize intermediate features?</p>
<h3 id="is-the-first-layer-the-problem">Is the first layer the problem?</h3>
<p>I.e. are we just screwed after the first layer because it somehow destroyed the input?</p>
<p>Ex. consider the toy problem of dictionary learning. If the layers of a NN were <em>really</em> doing sparse coding, would this be a problem? I.e. does sparse coding suffer from the same problem? Or are NN really <em>not</em> doing sparse coding? While we sort-of say it is, I don’t think it is at all! For the convolutional NN, the convolution <em>isn’t</em> computing <span class="math inline">\(h\)</span> such that <span class="math inline">\(Ah=x\)</span>. In fact, the overlaps between different patches might make things badly conditioned.</p>
<p>If we actually did DL on the first layer, then solved the sparse recovery problem for every input, would we still have the same problem?</p>
<h3 id="conservative-concepts-and-detection">Conservative concepts and detection</h3>
<p>Have the network be able to “abstain” (like a confidence score). This should be similar in principle to training detection between real and adversarial examples.</p>
<p>What if we use RBF’s on the first layer? Even something like pretraining to find the right RBF’s, and keeping them fixed. The idea is that RBF’s are very conservative. Maybe we’ll need the <span class="math inline">\(L^\iy\)</span> analogue if we’re protecting against that…</p>
<h3 id="more-human-approaches">More human approaches</h3>
<p>A vital thing that’s missing in current NN’s is back-and-forth between higher and lower layers. Humans can reinterpret lower-level data when they see a higher-level pattern.</p>
<p>Also humans have some kind of idea of a “prototype” of a digit - another instance must have its curves match up with that prototype in some fashion. There are these ideas of strokes, etc. Something hierarchical, probabilistic model? cf. Tenenbaum.</p>
<p>It may be that there needs to be something fundamentally new in the architecture - something more principled. Anything from neuroscience?</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Inverse RL</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/inverse_rl.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/inverse_rl.html</id>
    <published>2017-04-03T00:00:00Z</published>
    <updated>2017-04-03T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Inverse RL</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-04-03 
          , Modified: 2017-04-03 
	</p>
      
       <p>Tags: <a href="/tags/reinforcement%20learning.html">reinforcement learning</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#nr00-inverse-reinforcement-learning">[NR00] Inverse reinforcement learning</a></li>
 <li><a href="#hdar16-cooperative-inverse-reinforcement-learning">[HDAR16] Cooperative inverse reinforcement learning</a><ul>
 <li><a href="#formalism">Formalism</a></li>
 <li><a href="#simple-approximation-scheme">Simple approximation scheme</a></li>
 <li><a href="#future-work">Future work</a></li>
 </ul></li>
 <li><a href="#other-papers">Other papers</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="nr00-inverse-reinforcement-learning">[NR00] Inverse reinforcement learning</h2>
<p>Compute constraints that characterize set of reward functions so observed behavior maximizes reward. Use max-margin heuristic.</p>
<h2 id="hdar16-cooperative-inverse-reinforcement-learning">[HDAR16] Cooperative inverse reinforcement learning</h2>
<p>Human knows reward function. Robot does not. Robot payoff is human reward.</p>
<p>IRL: <span class="math inline">\(\Pj(u|\te, x_0) \propto e^{U_\te(x_0,u)}\)</span>.</p>
<ul>
<li>Reduction to POMDP and sufficient statistics</li>
<li>Apprenticeship learning and suboptimality of IRL-like solutions (because H can use a suboptimal action to convey more information to R).</li>
</ul>
<p>Desiderata:</p>
<ol type="1">
<li>Leverage action to improve learning.</li>
<li>Human is not uninterested expert, but cooperative teacher.</li>
</ol>
<h3 id="formalism">Formalism</h3>
<ul>
<li><span class="math inline">\(\Te\)</span>: static reward parameters observed by <span class="math inline">\(H\)</span>.</li>
<li><span class="math inline">\(R: S\times A^H\times A^R\times \Te \to \R\)</span>, reward.</li>
<li><span class="math inline">\(\ga\)</span>: discount</li>
</ul>
<p>At time <span class="math inline">\(t\)</span>, observe <span class="math inline">\(s_t\)</span> and select action <span class="math inline">\(a_t^H, a_t^R\)</span>. Both achieve reward <span class="math inline">\(r_t=R(s_t,a_t^H, a_t^R;\te)\)</span>.</p>
<p>Note: decentralized POMDP - compute optimal joint policy is NEXP-complete.</p>
<p>Here, private info is restricted to <span class="math inline">\(\te\)</span>, so reduction to coordination-POMDP does not blow up state space. (<span class="math inline">\(|S_C|=|S||\Te|\)</span>). (State is tuple or world state, reward parameters, and R’s belief.)</p>
<p>Belief about <span class="math inline">\(\te\)</span> is sufficient statistic for optimal behavior. <span class="math inline">\((\pi^{H*},\pi^{R*})\)</span> depends only on current state and R’s belief.</p>
<p>Apprenticeship learning: imitate demonstrations.</p>
<p>ACIRL: 2 phases, human and robot takes turns; then robot acts independently (deployment).</p>
<p>Ex. With linear dependence on <span class="math inline">\(\te\)</span>, in deployment, optimal policy is to maximize reward induced by mean.</p>
<p>DBE (demonstration by expert): greedily maximizes immediate reward. Best response is to compute posterior over <span class="math inline">\(\te\)</span>.</p>
<p>There exist ACIRL games where <span class="math inline">\(br(br(\pi^E))\ne \pi^E\)</span>.</p>
<!--Human objective $U_\te(x_0,u_R,u_H)$.-->
<ul>
<li>? Seems to require human knowing how robot learns. Unrealistic teaching assumption.</li>
<li>? Is reward observed by robot? No.</li>
</ul>
<h3 id="simple-approximation-scheme">Simple approximation scheme</h3>
<p>Suppose reward is <span class="math inline">\(\phi(s)^T\te\)</span>.</p>
<p><span class="math display">\[\tau^H = \amax_\tau \phi(\tau)^T \te - \eta\ve{\phi_\te-\phi(\tau)}^2.\]</span></p>
<p>Optimal <span class="math inline">\(\pi^R\)</span> under DBE tries to match observed feature counts. (<strong>I don’t get this.</strong>.)</p>
<h3 id="future-work">Future work</h3>
<p>Coordination problem.</p>
<h2 id="other-papers">Other papers</h2>
<ul>
<li>[RA07], [Z…08]: <span class="math inline">\(\pi^H\)</span> is noisy expert. Bayesian approach: prior on rewards, vs. prior on reward functions.</li>
<li>[Nat…10] observe cooperating multiple actors.</li>
<li>[Wau…11], [KS15]: infer payoffs from observed behavior in general.</li>
<li>[Fer…14], hidden-goal MDP, goal unobserved. Human as part of environment</li>
<li>[CL12] Teach learner reward for MDP.</li>
<li>[DS13] Motion best communicating agent’s intention.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Confidence in neural nets</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/confidence.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/confidence.html</id>
    <published>2017-04-03T00:00:00Z</published>
    <updated>2017-04-03T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Confidence in neural nets</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-04-03 
          , Modified: 2017-04-03 
	</p>
      
       <p>Tags: <a href="/tags/neural%20net.html">neural net</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#hg17-baseline-for-detecting-misclassified-and-out-of-distribution-examples">[HG17] Baseline for detecting misclassified and out-of-distribution examples</a><ul>
 <li><a href="#abnormality-detection-with-auxiliary-decoders">Abnormality detection with auxiliary decoders</a></li>
 <li><a href="#discussion">Discussion</a></li>
 </ul></li>
 <li><a href="#hg17-early-methods-for-detecting-adversarial-images">[HG17] EARLY METHODS FOR DETECTING ADVERSARIAL IMAGES</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>Related: <a href="adversarial.html">adversarial examples</a>.</p>
<p>From AAML: RBF/conservative classifier for in vs. out-of-distribution examples. <a href="/posts/cs/ai/control/aaml_workshop.html">AAML workshop notes</a></p>
<p>Q: how go get a neural net to keep a confidence bound?</p>
<h2 id="hg17-baseline-for-detecting-misclassified-and-out-of-distribution-examples">[HG17] Baseline for detecting misclassified and out-of-distribution examples</h2>
<p>High-confidence predictions frequently produced by softmaxes. Ex. random Gaussian noise gives 90+% confidence. (Q: what if you do before softmaxes?)</p>
<p>Prediction probability of incorrect/ood examples are lower.</p>
<p>Give tasks to evaluate.</p>
<p>2 problems</p>
<ol type="1">
<li>error and success prediction: Can we predict whether a classifier will make an error on a held-out test example? (Use this to output <span class="math inline">\(\perp\)</span>.) Tradeoff between false negatives and positives.</li>
<li>In/out-of-distribution detection: Predict whether test example is from different distribution.</li>
</ol>
<p>ROC (receiver operating characteristic) shows <span class="math display">\[
\pa{tpr = \fc{tp}{tp+fn}, fpr = \fc{fp}{fp+tn}}.
\]</span> PR (precision-recall) shows <span class="math display">\[
\pa{\text{precision} = \fc{tp}{tp+fp}, \text{recall} = \fc{tp}{tp+fn}}.
\]</span></p>
<ul>
<li>AUROC is prob that a positive example has greater score than negative example. Not great when different base rates.</li>
<li>AUPR (precision-recall)</li>
</ul>
<h3 id="abnormality-detection-with-auxiliary-decoders">Abnormality detection with auxiliary decoders</h3>
<ul>
<li>Train normal classifier and append auxiliary decoder which reconstructs input. (Blue layers)</li>
<li>Train jointly on in-distribution examples. Freeze blue layers. Train red layers (on top) with clean and noised training examples. (Noised are abnormal.)</li>
</ul>
<p>Improves detection.</p>
<h3 id="discussion">Discussion</h3>
<ul>
<li>Baseline beaten by exploiting representations.</li>
<li>Intra-class variance: if distance from example to another is abnormally high, may be out of distribution.</li>
<li>known-unknown vs. unknown-unknown.</li>
</ul>
<h2 id="hg17-early-methods-for-detecting-adversarial-images">[HG17] EARLY METHODS FOR DETECTING ADVERSARIAL IMAGES</h2>
<p>Adversarial images place abnormal emphasis on lower-ranked principal components from PCA.</p>
<p>(Q: can you do this even independent of PCA - just by looking at e.g. wavelet/Fourier coefficients? Also, what if you adversarially keep PCA components low, incorporate weighted norm into adversarial optimization?)</p>
<p>Use variance of PCA coefficients of whitened images to detect. (What is whitening again?)</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2017-04-08</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2017-04-08.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2017-04-08.html</id>
    <published>2017-04-03T00:00:00Z</published>
    <updated>2017-04-03T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2017-04-08</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-04-03 
          , Modified: 2017-04-03 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#adversarial-training-and-anomaly-detection">Adversarial training and anomaly detection</a></li>
 <li><a href="#langevin-monte-carlo">Langevin Monte Carlo</a></li>
 <li><a href="#nn-and-kernels">NN and kernels</a></li>
 <li><a href="#other-topics">Other topics</a></li>
 <li><a href="#rl">RL</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="adversarial-training-and-anomaly-detection">Adversarial training and anomaly detection</h2>
<ul>
<li><a href="../tcs/machine_learning/neural_nets/adversarial_experiments.html">Experiments</a></li>
<li>OOD detection - papers</li>
<li>ICLR paper</li>
<li>Correlation between gradients?</li>
<li>Look at hidden activations.</li>
</ul>
<h2 id="langevin-monte-carlo">Langevin Monte Carlo</h2>
<ul>
<li>Graph algorithms.</li>
<li>Learn background. (OV14, Villani book, Ledoux book.)</li>
<li>Langevin papers.</li>
<li>Extending [AH16].</li>
</ul>
<h2 id="nn-and-kernels">NN and kernels</h2>
<p>See <a href="../tcs/machine_learning/neural_nets/barron_musings.html">musings</a>.</p>
<ul>
<li>Write up summary of NN and kernels.</li>
<li><span class="citation" data-cites="Jason">@Jason</span></li>
<li><span class="citation" data-cites="Manfred">@Manfred</span></li>
</ul>
<h2 id="other-topics">Other topics</h2>
<ul>
<li>Entropy regularizer for SVM</li>
</ul>
<h2 id="rl">RL</h2>
<p>Come up with project. Concrete problems in AI safety?</p>
<ul>
<li>Read CIRL.</li>
<li>Experiment with mixture. (cf. sleeping in NN)</li>
<li>Impact measure ideas/experiment.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>AAML workshop</title>
    <link href="http://holdenlee.github.io/notebook/posts/cs/ai/control/aaml_workshop.html" />
    <id>http://holdenlee.github.io/notebook/posts/cs/ai/control/aaml_workshop.html</id>
    <published>2017-04-01T00:00:00Z</published>
    <updated>2017-04-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>AAML workshop</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-04-01 
          , Modified: 2017-04-01 
	</p>
      
       <p>Tags: <a href="/tags/ai%20safety.html">ai safety</a>, <a href="/tags/machine%20learning.html">machine learning</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#inductive-ambiguity">Inductive ambiguity</a><ul>
 <li><a href="#formalization">Formalization</a></li>
 </ul></li>
 <li><a href="#environment-goals">Environment goals</a><ul>
 <li><a href="#level-1">Level 1</a></li>
 <li><a href="#level-2">Level 2</a></li>
 <li><a href="#philosophy-view">Philosophy view</a></li>
 </ul></li>
 <li><a href="#impact-measure">Impact measure</a></li>
 <li><a href="#learning-utility-function">Learning utility function</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>See</p>
<ul>
<li><a href="alignment_ml.html">Alignment</a></li>
<li><a href="concrete.html">Concrete problems</a></li>
</ul>
<p><a href="https://workflowy.com/#/4296816d1051">Workflowy notes (private)</a></p>
<h2 id="inductive-ambiguity">Inductive ambiguity</h2>
<p>What’s the problem with KWIK?</p>
<ul>
<li>Too restrictive - many things with small VC dimension can take exponentially many samples with KWIK. (In some sense this is necessary, cf. if only one example is 1.)</li>
<li>Requires realizability.</li>
</ul>
<h3 id="formalization">Formalization</h3>
<p>Dimensions to vary</p>
<ul>
<li>Supervised, then test; vs. online/active</li>
<li>How do you measure supervision? KWIK penalized by 1 for each time it asks user. We can e.g. instead be content with decaying rate of asking.</li>
</ul>
<p>Filter class <span class="math inline">\(F\)</span> and hypothesis class <span class="math inline">\(H\)</span>. Suppose there exists <span class="math inline">\(f,h\)</span> such that <span class="math inline">\(fh(x)=y, 1-y, \perp\)</span> with probability <span class="math inline">\(p, 0, 1-p\)</span>. We want to find <span class="math inline">\(\wh f\wh h\)</span> with <span class="math inline">\(\wh f \wh h(x)=y,1-y, \perp\)</span> with probabilities <span class="math inline">\(p+\ep, \ep', 1-p-\ep-\ep'\)</span> with <span class="math inline">\(\ep'\ll \ep\)</span>.</p>
<p>Neural net anomaly detection:</p>
<ul>
<li>RBF</li>
<li>only predict around points seen</li>
<li>[HG17] A BASELINE FOR DETECTING MISCLASSIFIED AND OUT-OF-DISTRIBUTION EXAMPLES IN NEURAL NETWORKS</li>
</ul>
<h2 id="environment-goals">Environment goals</h2>
<p>2-3 levels of problem.</p>
<ol type="1">
<li>Human can evaluate/give accurate feedback.</li>
<li>Human can evaluate, but agent can prevent human from giving feedback.</li>
<li>Human cannot evaluate.</li>
</ol>
<p>(Lump 2 and 3 together.)</p>
<h3 id="level-1">Level 1</h3>
<p>This is easiest for formalize and tackle as a theory problem. Assume there is a distinguishing function (conservative concept?) that excludes all bad outcomes.</p>
<p>Under human distribution of actions, reward given corresponds to value.</p>
<p>But after gaming (ex. realizing human only checks in 1 place to check room is clean), agent leaves human distribution of actions, and inferred/represented reward stops corresponding to value. Either from maintaining multiple hypothesis of reward function, or human feedback, it realizes this.</p>
<p>Going back and forth, can continuously improve. Can it generalize over human pushback?</p>
<p>Alternative: have separate agent/part of agent that acts as predictor - holds model of world, job is to predict, e.g., whether there’s a strawberry.</p>
<p>Question: can’t you just embed the “plagiarism” problem in here?</p>
<p>Maybe the problem considered here is more concrete: There’s a better notion of what a strawberry/plate is than a good story.</p>
<h3 id="level-2">Level 2</h3>
<p>Note that “putting self in simulation” is a relative term. It means “fooling all its sensors.” If it has a world-model, this means the harder task of fooling the world-model. (Think of world-model itself as a sensor system.) (Maybe the world-model asks for proofs?)</p>
<p>Why can’t you just have a world model or adversarial predictor? Problem if there is no good evaluator.</p>
<p>This contains the conservative concept problem and the reward hacking problem. (I think.) Solving the informed oversight problem is sufficient.</p>
<ul>
<li>Conservative concept: a human non-evaluable task means that every hypothesis class we could train with human-curated data, we could not distinguish between the real and fake thing. There is impossibile without access to the agent’s internal state.</li>
<li>Reward hacking: how not just to keep hitting reward button. Related to shutdown problem—preventing human from changing its reward, for instance.</li>
<li>Informed oversight: give human a human-understandable transcript that would help make a decision about whether value achieved (ex. “I put myself in a simulation.”). This is sufficient.</li>
</ul>
<h3 id="philosophy-view">Philosophy view</h3>
<p>It is impossible to refer to the physical world. Our mapping from physical actions to mental representations is many-to-one; many ways of moving our arm all get translated into a mental story of “picking up the strawberry.” There are many ways to execute this task.</p>
<p>We only live in our own conceptual space. This space is highly bound/coupled to actual physics. (There’s no glitch in the universe that if I move my arm in a specific way, Konami code, I shut down the universe.) Any way I move my arm is roughly the same.</p>
<p>The AI solves tasks within its own conceptual space. We can evaluate that the AI is doing the right thing insofar as it is transparent, we can look at its world model, point at the concept of “strawberry” and see that it’s close enough to our own. We can solve “environmental goals” if the intersection of ontologies is nonempty, and the goal is within that intersection.</p>
<h2 id="impact-measure">Impact measure</h2>
<p>Measure 1: get back to what would have happened under null. <span class="math display">\[
I(s) = \min_{\pi} D_{KL}(P_{t_1}^{\phi}(\bullet | s)|| P_{t_2}^\pi(\bullet | s'))
\]</span></p>
<p>Measure 2: stay similar to trusted region <span class="math inline">\(R\)</span>. Let <span class="math inline">\(f:X\to Y\)</span> be mapping to feature space. <span class="math display">\[
I(s) = d(R, f(s)).
\]</span> For example, <span class="math inline">\(R=\{f(s_0)\}\)</span> and <span class="math inline">\(d=\ved_2\)</span>.</p>
<p>Measure 3: train <span class="math inline">\(I\)</span> on examples, conservatively (ex. RBF) on good examples <span class="math inline">\((s_i,0)\)</span>, and bad examples, <span class="math inline">\((s_j,&gt;0)\)</span>. Also can encode prior information, e.g. about things that are neutral.</p>
<h2 id="learning-utility-function">Learning utility function</h2>
<p>Probability of <span class="math inline">\((x_i,y_i)\)</span> under <span class="math inline">\(p_{\te_1}\)</span> and <span class="math inline">\(p_{\te_2}\)</span>, where <span class="math inline">\(p_i=1\)</span> if <span class="math inline">\((x_i,y_i)\sim D_1\)</span> and 0 if <span class="math inline">\(\sim D_2\)</span>. <span class="math display">\[
\sumo in p_i p_{\te_1}(y_i|x_i) + \sumo in (1-p_i) p_{\te_2}p(y_i|x_i).
\]</span> Max log likelihood. Do EM with <span class="math inline">\(p\)</span>’s and <span class="math inline">\((x,y)\)</span>’s.</p>
<p>Version 2: Use IRL: keep track of best guess nets, or sets of valid hypotheses. Keep track of posterior probabilities of each net. (Update in online fashion.) Update posterior probabilities assuming Markovian switching (cf. DP in HMM, sleeping experts), and gradient descent on parameters.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Neural nets as kernel space</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/nn_kernel.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/nn_kernel.html</id>
    <published>2017-03-23T00:00:00Z</published>
    <updated>2017-03-23T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Neural nets as kernel space</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-03-23 
          , Modified: 2017-03-23 
	</p>
      
       <p>Tags: <a href="/tags/neural%20net.html">neural net</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#things-this-could-explain">Things this could explain</a></li>
 <li><a href="#for-neural-nets">For neural nets</a></li>
 <li><a href="#followup">Followup</a></li>
 <li><a href="#thoughts-33017">Thoughts 3/30/17</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>I think that I can place neural networks with non-steep and well-separated sigmoids as low-norm elements of a RKHS, and that this will explain and unify some learning results.</p>
<p>Precise details to be ironed out. First, choice of RKHS:</p>
<ul>
<li>Sobolev space <span class="math inline">\(H^{\fc n2}(\R^n)\)</span>, or</li>
<li><span class="math inline">\(L^2(\R^n)\)</span> or <span class="math inline">\(H^1(\R^n)\)</span> with bandlimited constraint.</li>
<li>Quotient out by constant functions? Allow a sigmoid <span class="math inline">\(\to 1\)</span> at infinity as kernel?</li>
</ul>
<p>For the bandlimiting, there are 2 technical steps, with parameters to vary.</p>
<ul>
<li>Convolve by Gaussian (or ball indicator) in Fourier space, or multiplying by Gaussian (or FT of ball indicator) in ordinary space.</li>
<li>cutting off Fourier spectrum - how does this influence? I.e. what is Fourier decay? Expect exponential - so get <span class="math inline">\(\ln \prc{\ep}\)</span> in exponent.</li>
</ul>
<p>Expect exponential dependence on steepness.</p>
<h2 id="things-this-could-explain">Things this could explain</h2>
<ul>
<li>Kalai’s result on learning smooth NN.</li>
<li>Learning linear separator with margin.</li>
<li>Exponential dependence on dimension or <span class="math inline">\(\rc{\ep}\)</span> (what exactly?) for agnostically learning halfspace, etc.</li>
</ul>
<h2 id="for-neural-nets">For neural nets</h2>
<ul>
<li>Learn 2-NN with linear output, under some conditions (ex. incoherence)</li>
<li>Maybe can learn 2-NN with sigmoid/majority output, by boosting (cf. majority of majorities).</li>
</ul>
<h2 id="followup">Followup</h2>
<ul>
<li>What is relationship to gradient descent?</li>
</ul>
<h2 id="thoughts-33017">Thoughts 3/30/17</h2>
<ul>
<li>How to get something for convolutional neural nets?
<ul>
<li>As an easier question, think about having filters over the entire image, rather than a grid.</li>
<li>Think of periodic case even.</li>
<li>Then this works by Fourier transform over <span class="math inline">\(\R^\N\)</span> (suitably weighted).</li>
<li>Problem: the simplest convnet is more complicated than this, includes maxpool and then fc. How to deal with maxpool? What if you don’t do maxpool? Sigmoid and then average, or weighted average?</li>
<li>Kernel on Fourier transform space. (See how well FT matches…)</li>
<li>See [ZLW16]</li>
</ul></li>
<li>Overcomplete bases
<ul>
<li>Can define RKHS norm by giving norm when written in terms of basis element.</li>
<li>Can’t define norm with overcomplete set of elements.</li>
<li>Can we define some kind of norm and do something kernel-like with overcomplete basis?
<ul>
<li>Project from larger space?</li>
</ul></li>
<li>Cf. wanting symmetries beyond translation</li>
<li>Cf. wavelets offer a natural overcomplete basis respecting symmetries</li>
<li>Perhaps first thing to do is just try wavelet regularization on MNIST.</li>
<li>If you want to use nonconvolutional kernel method on images, you should first convert to Fourier or wavelet basis. Probably wavelet (except that’s not quite a basis). (Multiply by log size.) (This doesn’t give you translation invariance, just resets the norm.)</li>
</ul></li>
<li>Three kernels
<ul>
<li>Fourier-based.</li>
<li><span class="math inline">\(\rc{2-\an{x,y}}\)</span>.</li>
<li>Arccosine.</li>
</ul></li>
<li>I’m super-confused about why toggling just one parameter <span class="math inline">\(n\)</span> changes the number of layers. <span class="math inline">\(K^{(n)}(x,y)=\fc{n-1}n + \fc{1/n}{(n+1) - n\an{x,y}}\)</span>.</li>
<li>Idea to prove NN separation for Lipschitz layers: Show a function that has exponentially higher norm in terms of <span class="math inline">\(l-1\)</span> norm than <span class="math inline">\(l\)</span>. Problem: norm required to express neural net also increases super-exponentially in dimension.</li>
<li>Improper tensor decomp using same method (use case?) cf. Livni’s poly network</li>
<li>Barron functions form a convex set… The reason why it’s intractable is that it’s infinite-dimensional. Hilbert spaces are infinite-dimensional, but the representor theorem saves you.</li>
<li>Can you cut down representation (after using representor theorem) by sampling? Keep norm, but be cruder? (Pick some elements and rescale.)</li>
<li>Using kernel representation at first level of neural network? Do some kind of AM? How to mix nonparametric and parametric? Power and limit of kernel coming from its nonparametricity.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2017-03-18</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2017-03-18.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2017-03-18.html</id>
    <published>2017-03-18T00:00:00Z</published>
    <updated>2017-03-18T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2017-03-18</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-03-18 
          , Modified: 2017-03-18 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#adversarial-training">Adversarial training</a></li>
 <li><a href="#langevin-monte-carlo">Langevin Monte Carlo</a></li>
 <li><a href="#using-barrons-theorem">Using Barron’s Theorem</a></li>
 <li><a href="#entropy-regularizer-for-svm">Entropy regularizer for SVM</a></li>
 <li><a href="#generalization">Generalization</a></li>
 <li><a href="#rl">RL</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="adversarial-training">Adversarial training</h2>
<ul>
<li><a href="../tcs/machine_learning/neural_nets/adversarial_experiments.html">Experiments</a></li>
</ul>
<h2 id="langevin-monte-carlo">Langevin Monte Carlo</h2>
<ul>
<li>Learn background. (OV14, Villani book, Ledoux book.)</li>
<li>Langevin papers.</li>
<li>Extending [AH16].</li>
</ul>
<h2 id="using-barrons-theorem">Using Barron’s Theorem</h2>
<p>See <a href="../tcs/machine_learning/neural_nets/barron_musings.html">musings</a>.</p>
<h2 id="entropy-regularizer-for-svm">Entropy regularizer for SVM</h2>
<h2 id="generalization">Generalization</h2>
<h2 id="rl">RL</h2>
<p>Come up with project. Concrete problems in AI safety?</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Langevin dynamics</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/probabilistic/langevin.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/probabilistic/langevin.html</id>
    <published>2017-03-15T00:00:00Z</published>
    <updated>2017-03-15T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Langevin dynamics</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2017-03-15 
          , Modified: 2017-03-15 
	</p>
      
       <p>Tags: <a href="/tags/langevin.html">langevin</a>, <a href="/tags/sampling.html">sampling</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#references">References</a><ul>
 <li><a href="#other">Other</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="references">References</h2>
<ul>
<li>Book: Analysis and geometry of Markov diffusion operators</li>
<li>C. Villani. Topics in Optimal Transportation, volume 58 of Graduate Studies in Mathematics</li>
<li>[OV14] GENERALIZATION OF AN INEQUALITY BY TALAGRAND, AND LINKS WITH THE LOGARITHMIC SOBOLEV INEQUALITY</li>
<li>[GM90] Recursive stochastic algorithms for global optimization in R^d</li>
<li>[RT96] Exponential convergence of Langevin distributions and their discrete</li>
<li>[D14] Theoretical guarantees for approximate sampling from smooth and log-concave densities</li>
<li>[ZLC17] A Hitting Time Analysis of Stochastic Gradient Langevin Dynamics</li>
<li>[RRT17] Non-Convex Learning via Stochastic Gradient Langevin Dynamics - A Nonasymptotic Analysis</li>
</ul>
<p>See also: Stochastic calculus, Brownian motion, [AH] on sampling</p>
<h3 id="other">Other</h3>
<ul>
<li>E. Hazan, K. Levi, and S. Shalev-Shwartz. On graduated optimization for stochastic non-convex problems</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
