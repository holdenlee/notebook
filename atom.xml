<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2016-03-15T00:00:00Z</updated>
    <entry>
    <title>[ALLMR16] RAND-WALK: A latent variable model approach to word embeddings</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/randwalk.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/randwalk.html</id>
    <published>2016-03-15T00:00:00Z</published>
    <updated>2016-03-15T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[ALLMR16] RAND-WALK: A latent variable model approach to word embeddings</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-15 
          , Modified: 2016-03-15 
	</p>
      
       <p>Tags: <a href="/tags/nlp.html">nlp</a>, <a href="/tags/word%20embeddings.html">word embeddings</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#task">Task</a></li>
 <li><a href="#previous-work-observed-phenomena">Previous work, observed phenomena</a><ul>
 <li><a href="#mysteries">Mysteries</a></li>
 </ul></li>
 <li><a href="#model">Model</a></li>
 <li><a href="#explanation">Explanation</a></li>
 <li><a href="#algorithm">Algorithm</a></li>
 <li><a href="#followup">Followup</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="task">Task</h2>
<p>Given a corpus (a long sequence of words, e.g. the text of Wikipedia) to learn from, answer analogy questions such as ?:queen::man:woman.</p>
<h2 id="previous-work-observed-phenomena">Previous work, observed phenomena</h2>
<p>The usual approach is to learn “word vectors.”</p>
<p>Define a context of a word <span class="math inline">\(w\)</span> in the corpus to be the two words (say) on either side of <span class="math inline">\(w\)</span>. (Thus, for a context <span class="math inline">\(\chi=(w_{-2},w_{-1},w_1,w_2)\)</span>, <span class="math inline">\(\Pj(\chi|w)\)</span> means the probability of observing the words of <span class="math inline">\(\chi\)</span> in a window of length 2, given that the middle word is <span class="math inline">\(w\)</span>.) (Mikolov)</p>
<p>Pennington et al. and Levy and Goldberg posit the following approach.</p>
<ul>
<li>Model: If <span class="math inline">\(a:b::c:d\)</span> is an analogy, then <span class="math display">\[ \fc{\Pj(\chi|a)}{\Pj(\chi|b)} \approx \fc{\Pj(\chi|a)}{\Pj(\chi|b)}.\]</span></li>
<li>Thus, the solution to the analogy <span class="math inline">\(?:b::c:d\)</span>$ is <span class="math display">\[ \amin_w \sum_\chi \pa{ \ln \pf{\Pj(\chi|w)}{\Pj(\chi|b)} - \ln \pf{\Pj(\chi|c)}{\Pj(\chi|d)} }^2.\]</span></li>
<li>Define probability-mutual-information <span class="math display">\[ PMI(w,\chi) = \ln \pf{\Pj(w,\chi)}{\Pj(w)\Pj(\chi)} = \ln \pf{\Pj(\chi|w)}{\Pj(\chi)}.\]</span> Let <span class="math inline">\(C\)</span> be the set of possible contexts. For a word <span class="math inline">\(w\)</span>, let <span class="math inline">\(v_w\)</span> be the vector indexed by contexts <span class="math inline">\(\chi \in C\)</span>, <!--containing the empirical estimate of PMI from the corpus,
    $$ v_w(\chi)= \wh{PMI}(w,\chi) = \ln \pf{\wh \Pj(w,\chi)}{\wh \Pj(w)\wh \Pj(\chi)} = \ln \pf{\wh \Pj(\chi|w)}{\wh \Pj(\chi)}.$$--> <span class="math display">\[ v_w(\chi)= {PMI}(w,\chi) = \ln \pf{ \Pj(w,\chi)}{\Pj(w) \Pj(\chi)} = \ln \pf{ \Pj(\chi|w)}{\Pj(\chi)}.\]</span> Under this embedding, the summand equals <span class="math inline">\(v_a-v_b-v_c+v_d\)</span>. To find <span class="math inline">\(a\)</span>, solve <span class="math display">\[ \amin_a \ve{v_a-v_b-v_c+v_d}^2.\]</span></li>
<li>Algorithm: GloVe (global vector) method (Pennington) Let <span class="math inline">\(X_{w,w'}\)</span> be the co-occurrence for words <span class="math inline">\(w,w'\)</span>. Find low-dimensional <span class="math inline">\(\wt v_w, \wt v_{w'}, \wt b_w, \wt b_{w'}\)</span> to minimize <span class="math display">\[ \sum_{w,w'} f(X_{w,w'}) (\an{\wt v_w, \wt v_{w'}} - \wt b_w- \wt b_{w'} - \ln X_{w,w'})^2\]</span> for some function <span class="math inline">\(f\)</span>. They choose <span class="math inline">\(f(x) = \min\bc{\pf{x}{x_{\max}}^{.75}, 1}, x_{\max}=100\)</span> from experiments.</li>
</ul>
<p>(How to optimize this?)</p>
<h3 id="mysteries">Mysteries</h3>
<ol type="1">
<li>There is a disconnect between the definition of <span class="math inline">\(v_w\)</span> and the estimate <span class="math inline">\(\wt v_w\)</span>. Namely, the <span class="math inline">\(v_w\)</span> are vectors giving the PMI with all <em>contexts</em> while <span class="math inline">\(\wt v_w\)</span> are the vectors such that <span class="math inline">\(\an{\wt v_w,\wt v_{w'}}\)</span> give the <em>word-word co-occurences</em>. Why do the learned <span class="math inline">\(\wt v_w\)</span> help in solving analogies? Why is the optimization problem in the algorithm a good proxy?</li>
<li>Why is this method stable to noise? <!--Why would we suspect noise is bad?--></li>
</ol>
<h2 id="model">Model</h2>
<p>Let the dimension of the underlying space be <span class="math inline">\(d\)</span>.</p>
<ul>
<li>There are <span class="math inline">\(n\)</span> words <span class="math inline">\(w\)</span> in the dictionary <span class="math inline">\(W\)</span>, and they are associated with vectors <span class="math inline">\(v_w\in \R^d\)</span>.
<ul>
<li>The vectors <span class="math inline">\(v_w\)</span> are iid generated by <span class="math inline">\(v=s\cdot \wh v\)</span> where <!--$\wh v$ is uniform on the sphere and -->
<ul>
<li><span class="math inline">\(\wh v\sim N(0,I_d)\)</span>,</li>
<li><span class="math inline">\(s\)</span> is a random scalar with <span class="math inline">\(\si^2\le d\)</span> and <span class="math inline">\(|s|\le \ka \sqrt d\)</span> for some constant <span class="math inline">\(\ka\)</span>.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></li>
</ul></li>
</ul></li>
<li>At each time <span class="math inline">\(t\in \N\)</span>, there is a context vector <span class="math inline">\(c_t\in \R^d\)</span>.
<ul>
<li>The vectors <span class="math inline">\(c_t\)</span> follow a random walk, satisfying the following:
<ul>
<li>(Uniform stationary distribution) The stationary distribution is <span class="math inline">\([-\rc{\sqrt d},\rc{\sqrt d}]^d\)</span>.</li>
<li>(Small drift) The drift in the context vector is <span class="math inline">\(\ve{c_{t+1}-c_t}_1\le \rc{\ln n}\)</span>. (There are more complicated general conditions under which the theorems work; take this for simplicity.) <!--  * (small drift) $\ve{\De_t}_1\le \rc{\ln n}$.
*--></li>
</ul></li>
</ul></li>
<li>At each time <span class="math inline">\(t\)</span>, word <span class="math inline">\(w\)</span> is emitted with probability
\begin{align}
\Pj[w|c_t] &amp;= \rc{Z_{c_t}} \exp(\an{v_w,c_t})\\
\text{where} Z_{c}:&amp;= \sum_w \exp(\an{v_w,c}).
\end{align}</li>
</ul>
<h2 id="explanation">Explanation</h2>
<p>Let <span class="math inline">\(\Pj(w,w')\)</span> be the probability that <span class="math inline">\(w,w'\)</span> appear consecutively at time <span class="math inline">\(t,t+1\)</span> when <span class="math inline">\(c_t\sim U_{[-\rc{\sqrt d},\rc{\sqrt d}]^d}\)</span> is drawn from the stationary distribution. Then the following hold.</p>
<strong>Theorem 1</strong>: With high probability over choice of <span class="math inline">\(v_w\)</span>’s,
\begin{align}
\forall &amp;w,w', &amp;
\ln \Pj (w,w')&amp;\approx \rc{2d}|v_w+v_w'|^2 - 2\lg Z - o(1)\\
\forall &amp; w,&amp;
\lg \Pj(w) &amp;\approx \rc{2d} |v_w|^2 - \lg Z -o(1)\\
\therefore &amp;&amp; \lg \fc{\Pj[w,w']}{\Pj[w]\Pj[w']} &amp;\approx \rc d \an{v_w,v_w'}\pm o(1).
\end{align}
<p>This is exactly the PMI, so the theorem “explains” Mystery 1.</p>
<p><em>Proof idea</em>:</p>
<p>Let <span class="math inline">\(c\)</span> be the context vector at time <span class="math inline">\(t\)</span> and <span class="math inline">\(c'\)</span> be the vector at time <span class="math inline">\(t+1\)</span>.</p>
<ol type="1">
<li>The main difficulty is that <span class="math inline">\(Z_c\)</span> is intractable to compute. However, because the <span class="math inline">\(v_w\)</span> are random (or isotropic), so <span class="math inline">\(Z_c\)</span> concentrates around its mean, and we can approximate it by a constant <span class="math inline">\(Z\)</span> (Theorem 2 in the paper).</li>
<li>Because drift is small, we can make the approximation <span class="math inline">\(c'\approx c\)</span>. Then
\begin{align}
\Pj(w,w') &amp;= \int_{c,c'} \Pj(w|c)\Pj(w'|c_{t+1})\Pj(c,c')\,dc\,dc'\\
&amp;\sim \rc{Z^2} \EE_{c} \exp(\an{v_w+v_w',c})\\
&amp;\sim \rc{Z^2}\exp\pf{|v_w+v_{w'}|^2}{2}
\end{align}
using some calculus in the last step (exercise). <!--check this--></li>
</ol>
<p>The calculation for <span class="math inline">\(\Pj(w)\)</span> is even simpler.</p>
<h2 id="algorithm">Algorithm</h2>
<p>What algorithm does the theory suggest to estimate the <span class="math inline">\(v_w\)</span>’s?</p>
<p>It suggests minimizing an objective as in GloVe. The weights <span class="math inline">\(f_{w,w'}\)</span> re selected to compensate noise in <span class="math inline">\(X_{w,w'}\)</span>; when <span class="math inline">\(X_{w,w'}\)</span> is on the average larger, it has lower variance, and the weight <span class="math inline">\(f_{w,w'}\)</span> is larger.</p>
<p>(What improvement does it suggest?)</p>
<h2 id="followup">Followup</h2>
<ul>
<li>Polysemy</li>
<li>Weighted SVD (Yuanzhi Li)</li>
</ul>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>The results in the paper are stated as “with high probability over the choice of <span class="math inline">\(v_w\)</span>”. This can probably be relaxed to “For all <span class="math inline">\(v_w\)</span> that are isotropic”, where <strong>isotropic</strong> means <span class="math inline">\(\EE_w [v_wv_w^T]\)</span> has all eigenvalues in <span class="math inline">\([1,1+\de]\)</span>.<a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Thoughts on LDC's</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/coding/3-14-16.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/coding/3-14-16.html</id>
    <published>2016-03-14T00:00:00Z</published>
    <updated>2016-03-14T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Thoughts on LDC's</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-14 
          , Modified: 2016-03-14 
	</p>
      
       <p>Tags: <a href="/tags/LDC.html">LDC</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#section">3-14-16</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="section">3-14-16</h2>
<ul>
<li>Why the lower bound for 3-query LCC’s doesn’t extend to LDC’s.</li>
<li>We aren’t using anything about the form of <span class="math inline">\(\phi\)</span> other than small dual Gowers uniformity…</li>
<li>An interpretation of the AP differences <span class="math inline">\(y\)</span>: it fails to be an LDC if you can’t arbitrarily specify the (relative) density of AP’s in those directions.</li>
<li>A MVC is linear on <span class="math inline">\(\F_p\)</span>, not <span class="math inline">\(\Z/m\)</span>. The coefficients and evaluations are in <span class="math inline">\(\F_p\)</span> even though the vectors aren’t. (Could we do better with codes on <span class="math inline">\(\Z/m\)</span>?) A linear code on <span class="math inline">\(\F_p\)</span> can be converted to a code on <span class="math inline">\(\F_2\)</span> but it loses linearity.
<ul>
<li>However, a MVC is NOT captured by the “query at an AP formalism.” Although the codeword index space is <span class="math inline">\(\F_p^N\)</span>, we’re not using that group structure for querying. Rather, we’re using the group structure of <span class="math inline">\(\F_p^{\times N}\)</span>. So it could still be possible to prove a lower bound querying AP’s.</li>
<li>That is false, it IS captured. The group can be different! There is a group, and there is a field. Two different things!</li>
</ul></li>
<li>Q4.14 in “APs and LDCs”: When <span class="math inline">\(N\)</span> grows large, there doesn’t even exist a degree 1 polynomial for which this holds. This doesn’t seem promising; the only interesting case is for <span class="math inline">\(n\)</span> constant or small.</li>
<li>Variations on the poly method:
<ul>
<li>How to use it for small finite fields? (In Kakeya, <span class="math inline">\(p\to \iy\)</span>.)</li>
<li>How about considering polynomials with multiple outputs, ex. <span class="math inline">\(\F_p^n\to \F_p^{n-2}\)</span> to capture vanishing on a plane?</li>
</ul></li>
<li>To study: Guth’s course. Does the idea for the plane/regulus theorem help in thinking about <span class="math inline">\(\de\)</span>-SG configurations and LDC’s?</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Type and Cotype</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/analysis/metric/type.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/analysis/metric/type.html</id>
    <published>2016-03-14T00:00:00Z</published>
    <updated>2016-03-14T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Type and Cotype</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-14 
          , Modified: 2016-03-14 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <ol type="1">
<li>Say that <span class="math inline">\(X\)</span> has <strong>type</strong> <span class="math inline">\(p\)</span> if there exists <span class="math inline">\(C&gt;0\)</span> such that for every <span class="math inline">\(n, y_1,\ldots, y_n\in X\)</span>, [ _{{1}^n} _XC^{p}. ]
<ul>
<li>This is always true for <span class="math inline">\(p=1\)</span> by the triangle inquality.</li>
<li>The RHS decreases as <span class="math inline">\(p\)</span> increases.</li>
<li>Let <span class="math inline">\(T_p(X)\)</span> be the infimum of valid <span class="math inline">\(T\)</span>.</li>
<li><span class="math inline">\(X\)</span> has <strong>nontrivial type</strong> if it has type <span class="math inline">\(&gt;1\)</span>.</li>
</ul></li>
<li>Say that <span class="math inline">\(X\)</span> has <strong>cotype</strong> <span class="math inline">\(r\)</span> if there exists <span class="math inline">\(C&gt;0\)</span> such that for every <span class="math inline">\(n, x_1,\ldots, x_n\in Y\)</span>, [ _{{1}^n} _YC^{p}. ]
<ul>
<li>This is always true for <span class="math inline">\(p=\iy\)</span> by Jensen.</li>
<li>Let <span class="math inline">\(C_r(X)\)</span> be the infimum of valid <span class="math inline">\(C\)</span>.</li>
<li><span class="math inline">\(X\)</span> has <strong>finite cotype</strong> if it has type <span class="math inline">\(&lt;\iy\)</span>.</li>
</ul></li>
</ol>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Neural nets basics</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/basics.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/basics.html</id>
    <published>2016-03-13T00:00:00Z</published>
    <updated>2016-03-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Neural nets basics</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-13 
          , Modified: 2016-03-13 
	</p>
      
       <p>Tags: <a href="/tags/neural%20net.html">neural net</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <h2 id="types">Types</h2>
<ul>
<li>neural net (vanilla)</li>
<li>convolutional neural net</li>
<li>recurrent neural nets
<ul>
<li>LSTM</li>
</ul></li>
<li>A <strong>Boltzmann machine</strong> has joint distribution of two adjacent layers to be <span class="math inline">\(\exp(x^TAh)\)</span>. If it has only two layers it is reversible, i.e., <span class="math inline">\(\Pj(x|h),\Pj(h|x)\)</span> are both easy to calculate. (A regularized version would be <span class="math inline">\(\exp(x^TAh - hh^T)\)</span>. (?))
<ul>
<li>A <strong>DBM</strong> stacks these units (so that the probability of a configuration is now <span class="math inline">\(\exp\pa{\sum x_i^TA_ix^{i+1}}\)</span>). It loses reversibility.</li>
<li>This is a graphical model. It’s a probability distribution rather than a deterministic function as in a vanilla neural net.</li>
</ul></li>
</ul>
<h2 id="functions">Functions</h2>
<ul>
<li>RELU <span class="math inline">\((x\ge 0) x\)</span>.</li>
</ul>
<h2 id="features">Features</h2>
<ul>
<li>Dropout
<ul>
<li>On nodes: zero out each node in a layer with probability <span class="math inline">\(1-\rh\)</span>.</li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[ALM16] WHY ARE DEEP NETS REVERSIBLE: A SIMPLE THEORY, WITH IMPLICATIONS FOR TRAINING</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/ALM16.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/ALM16.html</id>
    <published>2016-03-13T00:00:00Z</published>
    <updated>2016-03-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[ALM16] WHY ARE DEEP NETS REVERSIBLE: A SIMPLE THEORY, WITH IMPLICATIONS FOR TRAINING</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-13 
          , Modified: 2016-03-13 
	</p>
      
       <p>Tags: <a href="/tags/neural%20net.html">neural net</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#summary">Summary</a></li>
 <li><a href="#model">Model</a><ul>
 <li><a href="#boltzmann-machine">Boltzmann machine</a></li>
 <li><a href="#layer-neural-net">1-layer neural net</a></li>
 <li><a href="#multi-layer-neural-net">Multi-layer neural net</a></li>
 </ul></li>
 <li><a href="#theorems-and-proof-sketches">Theorems and proof sketches</a><ul>
 <li><a href="#baby-theorem">Baby theorem</a></li>
 <li><a href="#reversibility-of-one-layer-nets-with-dropout-and-relu">Reversibility of one-layer nets with dropout and ReLU</a></li>
 <li><a href="#reversibility-for-2-layers">Reversibility for 2+ layers</a></li>
 </ul></li>
 <li><a href="#experiments">Experiments</a></li>
 <li><a href="#questions">Questions</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>Arora, Sanjeev, Yingyu Liang, and Tengyu Ma. “Why are deep nets reversible: A simple theory, with implications for training.” arXiv preprint arXiv:1511.05653 (2015).</p>
<p>http://arxiv.org/pdf/1511.05653</p>
<h2 id="summary">Summary</h2>
<p>Consider a feedforward net with input <span class="math inline">\(x\)</span> and output <span class="math inline">\(h\)</span>. ALM give a model under which a neural net can be said to be predicting the output distribution. This also gives a theoretical explanation of why it’s possible to use a neural net to do the following: given <span class="math inline">\(h\)</span>, generate some <span class="math inline">\(x\)</span> that could have given rise to that <span class="math inline">\(h\)</span> (cf. neural net dreams).</p>
<p>Their theoretical explanation has two important ingredients for theoretical explanations:</p>
<ol type="1">
<li>It specifies a joint distribution of <span class="math inline">\(x,h\)</span>. (Specifying <span class="math inline">\(x|h\)</span> may also be good enough).</li>
<li>It gives a proof that the deep net does compute the most likely <span class="math inline">\(h\)</span> given <span class="math inline">\(x\)</span> (in some sense, up to some error).</li>
</ol>
<h2 id="model">Model</h2>
<p>(See <a href="basics.html">neural net basics</a>.)</p>
<p>We want to model (input, output) by a probability distribution, and prove that a neural net is predicting the output given the input, with respect to that probability distribution.</p>
<h3 id="boltzmann-machine">Boltzmann machine</h3>
<p>A <strong>Boltzmann machine</strong> is a joint distribution of random variables <span class="math inline">\(x,h \in \R^{m\times n}\)</span> given by <span class="math display">\[\Pj(x,h) \approx \exp(-x^TAh + \pat{regularization}).\]</span> It is reversible because <span class="math inline">\(\Pj(x|h),\Pj(h|x)\)</span> are both easy to calculate. (Deep Boltzmann machines are much less nice—they lose reversibility.)</p>
<p>Compare this to a 2-layer neural net. Note a Boltzmann machine is a probabilistic model, not a neural net. A neural net and a Boltzmann machine both model the relationship between an input vector and an output vector, but do it differently: A neural net deterministically computes an output as a function of its input, while a Boltzmann machine gives a probability distribution on (input, output). A Boltzmann machine is trivially reversible; our hope is that a neural net is also reversible. <!-- How are these models related? A Boltzmann machine is one of the most natural generative models for neural nets; one hope is that a neural net is predicting a distribution given by a Boltzmann machine.
Note by this reversibility, there is not much to prove --></p>
<p>Think of <span class="math inline">\(x\)</span> as the observed layer and <span class="math inline">\(h\)</span> as the hidden layer. (For example, think of it as the middle layer of an autoencoder. Because we’re not considering any layers put on top of <span class="math inline">\(h\)</span>, we also think of it as the output.)</p>
<p>We don’t worry about learning (ex. gradient descent) here. We just consider a neural net statically.</p>
<!-- Note we're *not* actually using the Boltzmann machine model.-->
<h3 id="layer-neural-net">1-layer neural net</h3>
<p>The model is the following. Key aspects of the model are modeling weights by random matrices<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>, hypothesizing/enforcing sparsity, and allowing dropout (this is a standard training technique, so we want the theoretical results to hold in this setting).<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<p>Here <span class="math inline">\(x\in \R^n,h\in \R^m\)</span> and <span class="math inline">\(m&lt;n\)</span>. The hidden layer has fewer nodes, so the forward map <span class="math inline">\(x\mapsto h\)</span> is many-to-one. This is necessary if we want to be able to reconstruct <span class="math inline">\(h\)</span> from <span class="math inline">\(x\)</span> with high probability.</p>
<ul>
<li>Generate <span class="math inline">\(h\sim D_h\)</span> where <span class="math inline">\(D_h\)</span> is any distribution on <span class="math inline">\(\R^n_{\ge 0}\)</span> satisfying the following: With <span class="math inline">\(1-\text{neg}(n)\)</span> probability,
<ul>
<li>(Sparsity) <span class="math inline">\(\ve{h}_0\le k\)</span>.</li>
<li>(No coordinate much larger than the average) <span class="math inline">\(\ve{h}_{\iy}\le \be \ve{h}\)</span> where <span class="math inline">\(\be = O\sfc{\ln k}{k}\)</span>.</li>
</ul></li>
<li>Let <span class="math inline">\(W\in \R^{n\times m}\)</span> be a matrix with random <span class="math inline">\(N(0,1)\)</span> entries.</li>
<li>Generate the observed variable by <span class="math display">\[ x \sim r(\al W h)\odot n_{\rh}, \]</span> where
<ul>
<li><span class="math inline">\(n_{\rh}\)</span> is a vector of iid draws from Bernoulli<span class="math inline">\((\rh)\)</span>, and <span class="math inline">\(\odot\)</span> is componentwise multiplication, i.e., entries are zeroed out with probability <span class="math inline">\(1-\rh\)</span>.</li>
<li><span class="math inline">\(\al=\fc{2}{\rh n}\)</span>, so that it normalizes the effect of dropout. (Why factor of 2?)</li>
</ul></li>
</ul>
<p>The feedforward neural net does the following operation. For some <span class="math inline">\(b\)</span>,</p>
<ul>
<li>Given <span class="math inline">\(x\)</span>, compute <span class="math inline">\(\hat h = r(W^Tx+b)\)</span>, where <span class="math inline">\(r(x)=\sgn(x)\cdot (x\ge 0)\)</span><a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> is the <strong>ReLU (rectified linear unit)</strong> function.
<ul>
<li>If we’re using dropout (with parameter <span class="math inline">\(\rc2\)</span>, say), replace <span class="math inline">\(x\)</span> by <span class="math inline">\(x \odot n_{\rc 2}\)</span> before doing this.</li>
</ul></li>
</ul>
<p>The hope is that <span class="math inline">\(\hat h \approx h\)</span>. Why do we take the matrix in the proposed inverse map to be <span class="math inline">\(W^T\)</span>? For random matrices, the transpose is like an inverse, because <span class="math inline">\(\ve{W^TW-I}_2\)</span> will be small.</p>
<h3 id="multi-layer-neural-net">Multi-layer neural net</h3>
<p>Let <span class="math inline">\(W_t,\ldots, W_1\)</span> be random matrices. Basically just iterate the construction <span class="math inline">\(t\)</span> times in both the generative model and the feedforward net (from <span class="math inline">\(h^{(t)}\)</span> get <span class="math inline">\(h^{(t-1)},\ldots, h^{(0)}=x\)</span> and from <span class="math inline">\(x\)</span> generate <span class="math inline">\(\wh h^{(1)}, \ldots , \wh h^{(t)}\)</span>). In the dropout case, do dropout on each layer.</p>
<h2 id="theorems-and-proof-sketches">Theorems and proof sketches</h2>
<h3 id="baby-theorem">Baby theorem</h3>
<p>To get an idea of why reversibility might hold, let’s consider a random one-layer neural net without nonlinearities (or bias), which is just multiplying by a random matrix. In this case <span class="math inline">\(x=\rc nWh\)</span>, <span class="math inline">\(\wh h = W^Tx=\rc n W^TWh\)</span>.</p>
<p><strong>Theorem (Baby version)</strong>: Let <span class="math inline">\(h\in \{0,1\}^m\)</span> be fixed, <span class="math inline">\(m&lt;n\)</span>. Suppose <span class="math inline">\(W\in \R^{n\times m}\)</span> has <span class="math inline">\(N(0,1)\)</span> entries. Then for any polynomial <span class="math inline">\(p(n)\)</span> there is <span class="math inline">\(C\)</span> so that <span class="math display">\[\Pj_{W}\ba{\ve{W^TWh - h}_{\iy}\le C\ln(mn)\sfc{m}{n}}\ge 1-\rc{p(n)}.\]</span></p>
<em>Proof</em>: We have
\begin{align}
(W^TWh)_i &amp;= (w_{i\bullet})^TW h\\
&amp;=\rc n (w_{i\bullet})^T w_{i\bullet}h + \ub{\rc n\sum_{j\ne i} (w_{i\bullet})^T w_{j\bullet} h}{\text{noise}}\\
&amp;=\rc n (w_{i\bullet})^T w_{i\bullet} h +  \rc n\sum_{k=1}^m w_{ik} \sum_{j\ne i} W_{jk} h_k.
\end{align}
<p>By Chernoff, <span class="math inline">\(\rc n w_{i\bullet}^T w_{i\bullet}\)</span>, as a sum of <span class="math inline">\(n\)</span> variables distributed as <span class="math inline">\(\chi_1^2\)</span>, is <span class="math inline">\(\rc{\sqrt{n}}\)</span>-concentrated around 1. There are <span class="math inline">\(&lt;mn\)</span> terms in the noise term, so it is <span class="math inline">\(\fc{\sqrt{mn}}{n}\)</span> concentrated around 0 (a little work is needed here—details left to the reader). Thus we get <span class="math inline">\(\sfc{m}{n}\)</span>-concentration around 0.</p>
<p>If we bound by <span class="math inline">\(\ln(mn)\)</span> times this quantity, then we can use the union bound to finish. <span class="math inline">\(\square\)</span></p>
<p>Before moving on, we note two things.</p>
<ol type="1">
<li>To get a good bound here, we need <span class="math inline">\(n\gg m\)</span>, i.e., the hidden layer needs to be much smaller. Often this is not true in practice.</li>
<li>We get a <span class="math inline">\(L^{\iy}\)</span> bound which naively doesn’t give a good <span class="math inline">\(L^2\)</span> bound.</li>
</ol>
<p>The key next step is to assume that <span class="math inline">\(h\)</span> is sparse. It turns out then having a sigmoid function (ReLU) can be naturally interpreted as picking out the nonzero coordinates, ``recovering&quot; the sparse <span class="math inline">\(h\)</span>. Thus, <em>thresholding has a denoising effect</em>. This allows better recovery (in the <span class="math inline">\(L^2\)</span> norm).</p>
<h3 id="reversibility-of-one-layer-nets-with-dropout-and-relu">Reversibility of one-layer nets with dropout and ReLU</h3>
<p><strong>Theorem 2.3</strong>: (Formulation in appendix A.) Let <span class="math inline">\(W,h,x\)</span> be generated as in the model. Suppose <span class="math inline">\(k\)</span> is such that <span class="math inline">\(k&lt;\rh n&lt;k^2\)</span> (the number of non-dropped entries greater than the minimum sparsity, but also not too much). Then <span class="math display">\[
\ab{\Pj_{x,h,W}\ba{\ve{r(W^T x+b) - h}^2\le \wt O\pf{k}{\rh n}\ve{h}^2}} \ge 1-\rc{\poly(n)}.
\]</span></p>
<p><em>Proof (Theorem 2.3)</em>:</p>
<ol type="1">
<li><p><strong>Lemma 2.5</strong>: For <span class="math inline">\(\de=\wt O\prc{\sqrt m}\)</span>, <span class="math display">\[\Pj_{W,h,x}\ba{\ve{W^Tx - h}_\iy\le \de\ve{h}^2} \ge 1-\rc{\poly(n)}.\]</span></p>
<em>Proof of (2.5)<span class="math inline">\(\implies\)</span>(2.3):</em> If we used the naive <span class="math inline">\(L^\iy-L^2\)</span> bound, we get that w.h.p. the <span class="math inline">\(L^2\)</span> norm is at most <span class="math inline">\(m(\de\ve{h})^2 = \wt O\pf{m\ve{h}^2}{t}\)</span>, which is too large. We need to use sparsity to get a good bound. The idea is to zero out the entries with <span class="math inline">\(h_i=0\)</span> by adding a bias term <span class="math inline">\(b\)</span> and thresholding using <span class="math inline">\(r\)</span>. The <span class="math inline">\(L^{\iy}\)</span> bound in Lemma 2.3 tells us the offset necessary to zero out the entries where <span class="math inline">\(h_i=0\)</span>, <span class="math inline">\(b=-\de \ve{h}_1\)</span>. With this value of <span class="math inline">\(b\)</span>,
\begin{align}
h_i&amp;=0 &amp;\implies ((W^Tx)_i+b_i)&amp;\in [-2\de \ve{h},0]&amp;\implies \hat h_i&amp;=r((W^T)x_i + b_i) &amp;= h_i = 0\\
h_i&amp;\ne 0 &amp; \implies |\wt h_i-h_i| &amp;\le 2\de\ve{h}.
\end{align}
Square and multiply by <span class="math inline">\(k\)</span>.</li>
<li><em>Proof of Lemma 2.5:</em> We have
\begin{align}
x_j &amp;= r\pa{\al \sumo im W_{ji} h_i} (n_\rh)_j\\
\hat h_i &amp;= \sumo jn (W^T)_{ij} x_j\\
&amp;= \sumo jn \ub{W_{ji} r\pa{\al \sumo km W_{jk} h_k}}{=:Z_j} (n_\rh)_j.
\end{align}
As before, think of the sum inside <span class="math inline">\(r\)</span> as a main term plus noise: <span class="math display">\[\sumo km W_{jk} h_k = W_{ji} h_i + \sum_{k\ne i} W_{jk} h_k.\]</span> We want to compute <span class="math inline">\(\E[\hat h_i|h]\)</span>. To do this we need to understand the distribution of expressions that look like <span class="math inline">\(Z_j\)</span>.
<ul>
<li><strong>Lemma (Technical)</strong>: Let <span class="math inline">\(w\sim N(0,1), \xi \sim N(0,\si^2), \si=\Om(1), 0\le h\le \ln \si\)</span>. Then
\begin{align}
\EE_{w,\xi} [w\cdot r(wh+\xi)] &amp; = \fc h2 \pm \wt O\prc{\si^3}\\
\EE_{w,\xi} [w^2\cdot r(wh+\xi)] &amp; \le 3h^2 + \si^2.
\end{align}
<em>Proof:</em> To calculate the expectation, take the integral over <span class="math inline">\(\xi\)</span> first and then <span class="math inline">\(w\)</span>. <span class="math inline">\(w,\xi\)</span> are Gaussians, so we can evaluate the expectation <span class="math display">\[ \E[\E[w\cdot r(wh+\xi)|w]] = \fc{h}2+\E[G(w)]\]</span> for some calculable error <span class="math inline">\(G\)</span>. Estimate <span class="math inline">\(G\)</span> with its Taylor expansion (of degree 4). Now calculate <span class="math inline">\(\E[G(w)]\)</span> by estimating it for values below and above a cutoff. Using the lemma on the terms and noting <span class="math inline">\(\E\ve{n_j}_0=\rh n\)</span>, and the normalizing constant <span class="math inline">\(\al=\fc{2}{\rh n}\)</span>, we have <span class="math display">\[ \E[\wh h_i|h] = h_i \pm \wt O\prc{k^{\fc 32}}.\]</span></li>
<li>Show that <span class="math inline">\(\wh h_i|h_i\)</span> concentrates with high probability. The <span class="math inline">\(Z_j\)</span> are independent so we can use a version of Matrix Bernstein. Technically it seems we have to use a version for subexponential random variables, in terms of Orlicz norms, and check that <span class="math inline">\(Z_j\)</span> hve small norm in expectation. See Theorem D1.</li>
</ul></li>
</ol>
<!--Write
\begin{align}
x_j &= r(W_{ji} h_i + \ub{\sum_{l=1}^m W_{jl} h_l}{=:\eta_j}) (j\in T),\\
h_i &= \sum_{j=1}^n W_{ji}x_j
\end{align}
where $T$ is the set of non-zeroed out coordinates. We have $h_i = $. 
We want to show this is close to $h_j$ with high probability. 
-->
<h3 id="reversibility-for-2-layers">Reversibility for 2+ layers</h3>
<p><strong>Theorem </strong>: Similar theorems hold for 2 and 3 layers, in a weaker sense.</p>
<p>(I haven’t gone through this.)</p>
<h2 id="experiments">Experiments</h2>
<p>The theory gives a way to improve training. Take a labeled data point <span class="math inline">\(x\)</span>, use the current feedforward net to compute the label <span class="math inline">\(h\)</span>, and use the <strong>shadow distribution</strong> <span class="math inline">\(p(x|h)\)</span> to create a synthetic data point <span class="math inline">\(\wt x\)</span>, and use <span class="math inline">\((\wt x,z)\)</span> as a training pair.</p>
<h2 id="questions">Questions</h2>
<ul>
<li>How realistic is the sparsity assumption? How realistic is the model?</li>
<li>Can we use Boltzmann machines as the model instead?</li>
<li>What complications come up for 2+ layers? Is there a proof for any constant number of layers without loss?</li>
</ul>
<!---
Show that feedforward deep nets compute z|x in this model. (You can define this by
MLE, MAP, etc.)
-->
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Because we use properties such as eigenvalue concentration, this suggests that the theorem will still hold for “random-like” matrices, i.e., matrices having these properties.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Dropout encourages all of the nodes to learn useful features, because the neural net will essentially rely on a subset of them to make a prediction. (Think of nodes as taking a “majority vote” over inputs; dropout makes sure this still works even if you only take a subset.)<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>We use the notation <span class="math inline">\((P)=\begin{cases} 1,&amp;P\text{ true}\\ 0,&amp;P\text{ false}. \end{cases}\)</span><a href="#fnref3">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Interests</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/interests.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/interests.html</id>
    <published>2016-03-13T00:00:00Z</published>
    <updated>2016-03-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Interests</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-13 
          , Modified: 2016-03-13 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#standard-topics">“Standard” topics</a><ul>
 <li><a href="#links">Links</a></li>
 </ul></li>
 <li><a href="#other-topics">Other topics</a></li>
 <li><a href="#sanjeev-aroras-group">Sanjeev Arora’s group</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="standard-topics">“Standard” topics</h2>
<ol type="1">
<li>How can we give theoretical guarantees for machine learning? <a href="http://unsupervised.cs.princeton.edu/index.html">Sanjeev Arora’s page</a>.
<ul>
<li>Model the input distribution (e.g. sparsity).</li>
<li>Explain why standard approaches (e.g. neural nets) work (e.g., with respect to the input distribution). <a href="neural_nets/nn_modeling.html">Neural net modeling</a></li>
<li>Use the theoretical understanding to obtain better algorithms.</li>
</ul></li>
<li>Apply math (analysis, geometry, probability) to machine learning problems.
<ul>
<li>For example, understand threshold phenomenon in community detection and other graph problems.</li>
<li>Many problems in convex optimization can be understood in terms of choosing a right kernel, doing random walks, convex geometry, simulated annealing (cf. statistical physics), etc. “Manifold learning,” etc.</li>
<li>Understand the “random” case of objects like graphs and neural nets and analyze using ideas from statistical physics, random matrix theory, etc.</li>
<li>Questions in data science can involve topology, differential geometry, etc.</li>
</ul></li>
</ol>
<h3 id="links">Links</h3>
<ul>
<li><a href="http://www.cs.princeton.edu/~ehazan/">Elad Hazan</a></li>
<li><a href="http://www.scrible.com/contentview/page/64G21C041IKS82NH20O741000I60MGAV:206493171/index.html?utm_source=tb_permalink&amp;utm_medium=permalink&amp;utm_campaign=tb_buttons&amp;_sti=2847545">“Deep stuff about deep learning”</a></li>
</ul>
<h2 id="other-topics">Other topics</h2>
<ol type="1">
<li>How can we obtain theoretical guarantees for behavior of an AI (e.g. on the level of logic)? See <a href="http://cstheory.stackexchange.com/questions/32445/program-reasoning-about-own-source-code">my question</a>. Formulate the question of AI control mathematically and prove theorems. <a href="https://medium.com/ai-control">Paul Christiano</a>.</li>
<li>How can we combine ML/data and logical approaches in domains that require modeling with logic, ex. automated theorem proving, NLP (ex. combine CFG-based approaches with neural net approaches), reasoning about series of events with causality, etc.? What are the structure of these problems that allow tractable inference (ex. sentences in natural language are “mostly unambiguous” if you understand them semantically, unlike worst-case).</li>
<li>Build modular AI systems. Ex. how to make AlphaGo something that anyone can implement a baby version of?</li>
<li>Explore creativity. Ex. make a program that writes poetry or stories, or generates maps for a RPG. cf. Neural net dreams. cf. Hofstadter’s FARG.</li>
<li>How do humans reason? What are the learning problems that humans face, and how are our neural algorithms optimized or not for those problems? How does “bounded computation” come into play? cf. Jacob Steinhardt. Ref: Rationally Speaking #154, Tom Griffiths.</li>
</ol>
<h2 id="sanjeev-aroras-group">Sanjeev Arora’s group</h2>
<ul>
<li><a href="http://www.cs.princeton.edu/~yingyul/">Yingyu Liang</a></li>
<li><a href="http://www.cs.princeton.edu/~tengyu/">Tengyu Ma</a></li>
<li><a href="http://www.cs.princeton.edu/~risteski/">Andrej Risteski</a></li>
</ul>
<p>Summaries/thoughts on the papers.</p>
<!---
Composite of these smaller questions
-->

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Matrix concentration</title>
    <link href="http://holdenlee.github.io/notebook/posts/math/probability/random_matrices/concentration.html" />
    <id>http://holdenlee.github.io/notebook/posts/math/probability/random_matrices/concentration.html</id>
    <published>2016-03-13T00:00:00Z</published>
    <updated>2016-03-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Matrix concentration</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-13 
          , Modified: 2016-03-13 
	</p>
      
       <p>Tags: <a href="/tags/random%20matrix.html">random matrix</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <p>See <a href="https://en.wikipedia.org/wiki/Matrix_Chernoff_bound">wikipedia</a>.</p>
<p><strong>Theorem (Matrix Bernstein)</strong>: Let <span class="math inline">\(\{X_k\}_{k=1}^n\)</span> be a sequence of independent random <span class="math inline">\(d\times d\)</span> matrices with <span class="math display">\[ \E X_k = 0, \quad \la_{\max}(X_k)\le R\text{ a.s.}\]</span> Then for all <span class="math inline">\(t\ge 0\)</span>, <span class="math display">\[ \Pj \ba{\la_{\max}\pa{\sumo kn X_k}} \le de^{-\fc{t^2}{2\si^2+\fc 23 Rt}}\]</span> where <span class="math inline">\(\si^2 = \ve{\sumo kn \E(X_k^2)}\)</span>.</p>
<p>(I don’t understand what a.s. means here. The second inequality is for a finite sum—so we need to quantify the probability that <span class="math inline">\(\la_{\max}(X_i)&gt;R\)</span>. a.s. means with probability 1 so we have to truncate the probability distribution of the <span class="math inline">\(X_i\)</span>? But we can’t do this when there are many <span class="math inline">\(X_i\)</span>.)</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Spectral Bounds for Stochastic Diffusion Model in Networks</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/networks/pacm-3-8-16.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/networks/pacm-3-8-16.html</id>
    <published>2016-03-08T00:00:00Z</published>
    <updated>2016-03-08T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Spectral Bounds for Stochastic Diffusion Model in Networks</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-08 
          , Modified: 2016-03-08 
	</p>
      
       <p>Tags: <a href="/tags/pacm.html">pacm</a>, <a href="/tags/GSS.html">GSS</a>, <a href="/tags/networks.html">networks</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p>Eun Jun Lee</p>
<p>This work studies stochastic diffusion model where influence propagates in networks from seed-nodes along edges with independent probabilities. Specifically, we propose spectral bounds for the expected number of nodes that are influenced at the end of propagation. The proposed bounds show significant improvements over the existing bounds in the presence of sensitive edges such as bottlenecks, seed adjacent, and high probability edges.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[CHMAL15] The Loss Surfaces of Multilayer Networks</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/CHMAL15.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/CHMAL15.html</id>
    <published>2016-03-07T00:00:00Z</published>
    <updated>2016-03-07T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[CHMAL15] The Loss Surfaces of Multilayer Networks</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-07 
          , Modified: 2016-03-07 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#initial">Initial</a></li>
 <li><a href="#what">What?</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="initial">Initial</h2>
<ul>
<li>Relate to random matrix theory</li>
<li>Relate to spherical spin-glass model</li>
<li>Global minima lead to overfitting</li>
</ul>
<p>Phenomenon: While multilayer nets do have many local minima, the result of multiple experiments consistently give very similar performance</p>
<p>We rst establish that the loss function of a typical multilayer net with ReLUs can be expressed as a polynomial function of the weights in the network, whose degree is the number of layers, and whose number of monomials is the number of paths from inputs to output.</p>
<p>piecewise, continuous polynomial whose monomials are switched in and out at the boundaries between pieces.</p>
<p>first work to give a “theoretical description of the optimization paradigm with neural networks in the presence of large number of parameters.”</p>
Let <span class="math inline">\(\si(x)=\max(0,x)\)</span>. The random network is
\begin{align}
Y&amp;= q \si(W_H^T\si(\cdots (W_1^TX)\cdots))\\
&amp;=q\sumo i{n_0} \sum_{j=1}^{\ga (=\prod n_i)} X_{ij}\prod_{k=1}^H w_{i,j}^{(k)}.
\end{align}
<p>where <span class="math inline">\(A_{i,j}\)</span> is whether the path is active, <span class="math inline">\(X_{i,j}=X_i\)</span> is starting, and <span class="math inline">\(w_{i,j}^{(k)}\)</span> re weights.</p>
<p>Assumptions</p>
<ul>
<li><span class="math inline">\(X_{i,j}\)</span> independent (so we’re not taking <span class="math inline">\(X_{i,j}=X_i\)</span>?)</li>
<li>Paths: <span class="math inline">\(A_{i,j}\)</span> are Bernoulli. (?)</li>
</ul>
<p><span class="math inline">\((s,\ep)\)</span> reduction image: has only <span class="math inline">\(s\)</span> unique weights but prediction accuracy differs by <span class="math inline">\(\le \ep\)</span>. Some kind of approximation!</p>
<p>ReLU’s.</p>
<h2 id="what">What?</h2>
<ul>
<li>“fully decoupled”</li>
<li>“critical values” of loss function</li>
<li>simulated annealing for neural nets</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[AO15] Linear coupling</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/AO15.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/optimization/AO15.html</id>
    <published>2016-03-06T00:00:00Z</published>
    <updated>2016-03-06T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[AO15] Linear coupling</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-03-06 
          , Modified: 2016-03-06 
	</p>
      
       <p>Tags: <a href="/tags/gradient.html">gradient</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <h2 id="gradient-descent">Gradient Descent</h2>
<p><strong>Definition</strong>: A convex function is <span class="math inline">\(L\)</span>-smooth if <span class="math display">\[ f(y) \le \blu{f(x)+\an{\nb f(x),y-x}}+\redd{\fc L2 \ve{y-x}^2}\]</span> or equivalently <span class="math display">\[ \ve{\nb f(x)-\nb f(y)}_*\le L\ve{x-y}.\]</span></p>
<p>Assume <span class="math inline">\(f\)</span> is <span class="math inline">\(L\)</span>-smooth.</p>
<ul>
<li>Update:
\begin{align}
x_{k+1} &amp;= \amin_y \ba{\fc{L}{2} \ve{y-x_k}^2 + \an{\nb f(x),y-x_k}}\\
&amp;= x_k - \rc{L}\nb f(x_k)&amp;\text{for $\ell_2$ norm}.
\end{align}</li>
<li><strong>Lemma</strong>: <span class="math display">\[f(x')\le f(x)  - \fc{\ve{\nb f(x)}_*^2}{2L}.\]</span></li>
<li>Guarantee: Let <span class="math inline">\(R=\max_{x:f(x)\le f(x_0)}\ve{x-x^*}_*\)</span>. <span class="math display">\[f(x_T)-f(x^*) \le O\pf{LR^2}{T}.\]</span> Obtain an <span class="math inline">\(\ep\)</span>-approximation in <span class="math inline">\(\fc{LR^2}{\ep}\)</span> steps.</li>
</ul>
<h3 id="proof">Proof</h3>
<p>See also <a href="GD.html">gradient descent</a>.</p>
<ul>
<li>Bound the difference from optimal by the distance to the optimal times the gradient (using convexity and CS).</li>
<li>Bound the gradient by the progress.</li>
<li>Now let <span class="math inline">\(D_k = f(x_k)-f(x^*)\)</span>. Measure progress by <span class="math inline">\(\rc{D_k}\)</span> whose differences telescope.</li>
</ul>
\begin{align}
\ub{f(x_k)-f(x^*)}{D_k}&amp;\le \an{\nb f(x_k),x_k-x^*} \le \ve{\nb f(x_k)} \ve{x_k-x^*}\le \sqrt{2L}\ub{(f(x_k)-f(x_{k+1}))}{D_k-D_{k+1}}R\\
\rc{D_{k+1}}-\rc{D_k} &amp;\ge \rc{2LR^2}\fc{D_k}{D_{k+1}} \ge \rc{2LR^2}\\
\implies \rc{D_T}&amp;\ge T.
\end{align}
<h2 id="mirror-descent">Mirror descent</h2>
<strong>Definition</strong>: <span class="math inline">\(w(x):Q\to \R\)</span> (<span class="math inline">\(Q\)</span> convex) is a <strong>distance generating function</strong> if the following holds. Then define <strong>Bregman divergence</strong> as follows. For all <span class="math inline">\(x\in Q\bs \pl Q, y\in Q\)</span>,
\begin{align}
w(y) &amp;\ge w(x) + \an{\nb w(x),y-x} + \rc2\ve{x-y}^2\\
\end{align}
<p>Note we can replace the gradient by a subgradient below.</p>
<ul>
<li>Update:
\begin{align}
 \wt x &amp;= \text{Mirr}(\al\nb f(x))\\
 \text{where }\text{Mirr}_x(\xi) &amp;= \amin_{y\in Q} V_x(y)+\an{\xi,y-x}.
 \end{align}</li>
<li><strong>Lemma</strong>: For all <span class="math inline">\(u\)</span>,
\begin{align}
\al (f(x_k)-f(u))
&amp;\le \al\an{\nb f(x_k),x_k-u}\\
&amp;\le \fc{\al^2}2\ve{\nb f(x_k)}^2 +V_{x_k}(u) - V_{x_{k+1}}(u)\\
&amp;=  \fc{\al^2}2\ve{\nb f(x_k)}^2 +\rc 2 \ve{z_k-u}^2 -\rc2 \ve{z_{k+1}-u}^2 &amp; \text{for }\ell^2.
\end{align}
Generalization:
\begin{align}
\al (f(x_k)-f(u))
&amp;\le \al\an{\nb f(x_k),x_k-u}\\
&amp;\le \fc{\al^2}2\ve{\nb f(x_k)}^2 +V_{x_k}(u) - V_{x_{k+1}}(u)
\end{align}</li>
<li>Guarantee: When <span class="math inline">\(V_{x_0}(x^*)\le \Te, \ve{\nb f(x)}_*\le \rh\)</span> everywhere, <span class="math display">\[ f(\ol x) - f(x^*) \le \fc{\sqrt{2\Te}\rh}{\sqrt T}.\]</span> Obtain an <span class="math inline">\(\ep\)</span>-approximation in <span class="math inline">\(\fc{2\Te\rh^2}{\ep^2}\)</span> steps.</li>
</ul>
<h3 id="examples">Examples</h3>
\begin{align}
w(y) &amp;= \rc2 \ve{y}_2^2\\
V_x(y) &amp;= \rc2\ve{x-y}_2^2\\
w(y) &amp;= \sum_i y_i \ln y_i &amp;\text{w.r.t. }\ell_1\text{ over }\De\\
V_x(y) &amp;= \sum_{y_i} \ln \pf{y_i}{x_i} \ge \prc{x-y}_1^2.
\end{align}
<h3 id="intuition">Intuition</h3>
<p>Note for <span class="math inline">\(\ved_2\)</span> MD is the same as GD except for a factor <span class="math inline">\(\al\)</span>.</p>
<p>Note 3 formulations of mirror descent.</p>
<ol type="1">
<li>Above.</li>
<li>Set <span class="math display">\[\nb w(x_{k+1}) \leftarrow  \nb w(x_k) - \al \nb f(x_{k}).\]</span> (By this we mean take the value of <span class="math inline">\(x_{k+1}\)</span> that makes this true.)</li>
<li><strong>Regularized follow the leader (RFTL)</strong>: Take <span class="math display">\[x_{k+1} = \amin_y w(y)+ \al \an{y, \sum_{i=0}^k \nb f(x_i)}.\]</span></li>
</ol>
<p>(1)<span class="math inline">\(\iff\)</span>(2): The min is when the gradient is 0.</p>
(2)<span class="math inline">\(\iff\)</span>(3): The min is when the gradient is 0. Write this out for <span class="math inline">\(k,k-1\)</span>and subtract.
\begin{align}
\nb w(x_{k+1}) + \al \sum_{i=0}^k \nb f(x_i) &amp;=0\\
\nb w(x_k) + \al \sum_{i=0}^{k-1} \nb f(x_i) &amp;=0\\
\nb w(x_{k+1})-w(x_k) + \nb f(x_k)&amp;=0
\end{align}
<h2 id="coupling">Coupling</h2>
<h3 id="unconstrained-version-qr">Unconstrained version (Q=R)</h3>
<ul>
<li>Update:
\begin{align}
\label{eq:weighted}
x_{k+1} &amp;= \tau z_k + (1-\tau) y_k\\
y_{k+1} &amp;= \text{Grad}(x_{k+1})\\
z_{k+1} &amp;= \text{Mirr}_{z_k}(\al \nb f(x_{k+1})).
\end{align}</li>
<li><strong>Lemma</strong>: If <span class="math inline">\(\rc{1-\tau}{\tau}=\al L\)</span>, then <span class="math display">\[
\al \an{\nb f(x_{k+1}), x_{k+1}-u} \le \al^2L (f(y_k)-f(y_{k+1}))+V_{z_k}(u) - V_{z_{k+1}}(u).
\]</span></li>
<li>Guarantee: <span class="math display">\[f(\ol x) - f(x^*) \le \fc{\sqrt{2\Te}\rh}{T}.\]</span> Starting at <span class="math inline">\(f(x_0)-f(x^*)\le d\)</span>, <span class="math inline">\(V_{x_0}(x^*)\le \Te)\)</span>, in <span class="math inline">\(T=4\sfc{L\Te}{d}\)</span> steps, obtain <span class="math inline">\(f(\ol x)-f(x^*)\le \fc d2\)</span>. Hence, get an <span class="math inline">\(\ep\)</span>-approximate solution in <span class="math inline">\(O(\sfc{L\Te}{\ep})\)</span> iterations.</li>
</ul>
<h2 id="proof-1">Proof</h2>
<ol type="1">
<li>Why the weird definition for <span class="math inline">\(x_{k+1}\)</span>? If we defined <span class="math inline">\(z_{k+1}=\text{Mirr}_{x_{k+1}}(\al \nb f(x_{k+1}))\)</span>, the expressions involve <span class="math inline">\(x_{k+1},z_{k+1}\)</span> and do not telescope.</li>
<li>Thus, do the mirror descent starting from <span class="math inline">\(z_k\)</span> instead. If we try to bound the regret now,
\begin{align}
\al \an{\nb f(x_{k+1}), x_{k+1}-u} &amp;\le \fc{\al^2}{2} \ve{\nb f(x_k)}^2 + V_{x_{k+1}}(u) - V_{z_{k+1}}(u)\\
&amp;\le \al^2 L (\nb f(x_{k+1}) - \nb f(y_{k+1})) + V_{x_{k+1}}(u) - V_{z_{k+1}}(u).
\end{align}
which still does not telescope.</li>
<li>We want <span class="math inline">\(z_k\)</span> to take the place of <span class="math inline">\(x_{k+1}\)</span>.</li>
<li>Now write the real as the fake regret plus another term.</li>
</ol>
\begin{align}
\al \an{\nb f(x_{k+1}), z_{k}-u} &amp; \le \fc{\al^2}{2} \ve{\nb f(x_k)}^2 + V_{z_k}(u) - V_{z_{k+1}}(u)\\
&amp;\le \al^2 L (\nb f(x_{k+1}) - \nb f(y_{k+1})) + V_{z_k}(u) - V_{z_{k+1}}(u) \label{eq:lem3-2}\\
\al \an{\nb f(x_{k+1}),x_{k+1}-u}
&amp;\le \al \an{\nb f(x_{k+1}), z_{k}-u} + \al \an{\nb f(x_{k+1}), x_{k+1}-z_k}\\
&amp;\le \al \an{\nb f(x_{k+1}), z_{k}-u} + \fc{(1-\tau)\al}{\tau} \an{\nb f(x_{k+1}), y_{k}-x_{k+1}}&amp;\text{by \eqref{eq:weighted}}\\
&amp;\le \al^2 L (\nb f(x_{k+1}) - \nb f(y_{k+1})) + V_{z_k}(u) - V_{z_{k+1}}(u) + \fc{(1-\tau)\al}{\tau}\an{\nb f(x_{k+1}), y_k-x_{k+1}}&amp;\eqref{eq:lem3-2},\text{ convexity}\\
&amp;\le \al^2L(f(y_{k})-f(y_{k+1})) + V_{z_k}(u) - V_{z_{k+1}(u)}.
\end{align}
<p>Telescoping, balancing <span class="math inline">\(\al^2Ld=\Te\implies \al = \sfc{\Te}{Ld}\)</span>, we get error <span class="math inline">\(\fc{2\sqrt{L\Te d}}{T}\)</span>. It takes <span class="math inline">\(T=4\sfc{L\Te}{d}\)</span> steps to halve error from <span class="math inline">\(\fc d2\)</span>, and <span class="math inline">\(O\pa{\sfc{L\Te}{\ep}}\)</span> time to go to <span class="math inline">\(\ep\)</span> error.</p>
<h2 id="index-cards">Index cards:</h2>
<ul>
<li>Gradient lemma</li>
<li>Gradient guarantee</li>
<li>DGF and Bregman divergence</li>
<li>Mirror step</li>
<li>Mirror lemma</li>
<li>Mirror guarantee</li>
<li>Linear coupling</li>
<li>Linear coupling lemma</li>
<li>Linear coupling guarantee</li>
</ul>
<h2 id="page-notes">Page notes</h2>
<ol type="1">
<li></li>
<li></li>
<li></li>
<li></li>
<li>estimation sequence? Thought experiment. Cutoff <span class="math inline">\(K\)</span> for <span class="math inline">\(\ve{\nb f(x)}_2\)</span>. Equate gradient and mirror: <span class="math display">\[\fc{\ep L}{K^2}=\fc{K^2}{\ep^2}.\]</span></li>
<li></li>
<li>Gradient descent guarantee. <span class="math inline">\(f(x_T)-f(x^*) \le O\pf{L\ve{x_0-x^*}_2^2}{T}\)</span>. Distance generating function, Bregman divergence.</li>
</ol>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
