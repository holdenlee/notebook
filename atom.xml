<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2016-07-28T00:00:00Z</updated>
    <entry>
    <title>Representation learning</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/representation.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/representation.html</id>
    <published>2016-07-28T00:00:00Z</published>
    <updated>2016-07-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Representation learning</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-07-28 
          , Modified: 2016-07-28 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#low-dimensional-structure-in-high-dimensional-space">Low-dimensional structure in high-dimensional space</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>Representation learning means you have to first find a good representation—some hidden structure—for the data in order to learn it.</p>
<h2 id="low-dimensional-structure-in-high-dimensional-space">Low-dimensional structure in high-dimensional space</h2>
<p>Consider a distribution on a low-dimensional space. Now suppose this low-dimensional space is embedded in a high-dimensional space, and noise in the complementary (may be orthogonal) subspace is added. Recover the low-dimensional space and the structure on this space.</p>
<ul>
<li>Suppose complementary noise is Gaussian. [TV16] show how to recover given that the fourth moments of the distribution on the low-dim space are bounded away from that of a Gaussian. (Ex. clusters; a product distribution is limited by the dimension closest to Gaussian, etc.)</li>
<li>Can we weaken the assumption on the complementary noise? (Arora, Ge, Ma) Put an assumption on “unimodality” in other directions.</li>
<li>Suppose the subspace is a coordinate subspace, and the structure is a low-rank subspace within that. See <a href="matrices/relevant_coordinates.html">relevant coordinates</a>.</li>
</ul>
<p>Consider a dictionary-learning setting. There are many variations here. What we want to say is the following: given there’s a transformation in a certain class—ex. a neural net—that makes the data structured, can you learn the structure/parameters?</p>
<ul>
<li>Suppose the samples are sparse linear combinations of <span class="math inline">\(a_i\)</span>. This is the setting of dictionary learning. See <a href="matrices/AGM14.html">AGM14</a>, <a href="matrices/AGMM15.html">AGMM15</a>
<ul>
<li>Can we generalize from independent <span class="math inline">\(p\de_0+(1-p)D\)</span> distributions to non-Gaussian distributions?</li>
<li>Suppose that noise is added. The algorithm above works if the dot product between 2 noise vectors is <span class="math inline">\(o(1)\)</span>. Ex. each coordinate is <span class="math inline">\(o\prc{n^{\rc 4}}\)</span>. Then the dot product is summing <span class="math inline">\(n\)</span> numbers <span class="math inline">\(o\prc{n^{\rc 2}}\)</span> which is <span class="math inline">\(o(1)\)</span>. Can we do noise up to <span class="math inline">\(o(\sqrt n)\)</span>? The overlapping communities problem seems hard now. Before we could threshold at <span class="math inline">\(\rc 2\)</span>, but now, noise is up to <span class="math inline">\(\sqrt n\)</span>. cf. in SBM, you can deal with <span class="math inline">\(\rc{\sqrt n}\)</span> difference in probabilities; in SVD, you can add a random matrix of entries <span class="math inline">\(o(\sqrt n)\)</span> because the eigenvalues are much smaller.</li>
<li>In the undercomplete case, if the noise is orthogonal to the subspace spanned by <span class="math inline">\(a_i\)</span>, we’re in the setting of [TV16]. Products of non-Gaussians are OK.</li>
<li>Changing the formulation instead to that <span class="math inline">\((\an{x,a_i})_i\)</span> is sparse is equivalent to learning the dictionary <span class="math inline">\((A^+)^T\)</span>. (Warning: this looks different in the over/undercomplete case.)</li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Complexity of neural networks</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/complexity_of_neural_nets.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/complexity_of_neural_nets.html</id>
    <published>2016-07-25T00:00:00Z</published>
    <updated>2016-07-25T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Complexity of neural networks</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-07-25 
          , Modified: 2016-07-25 
	</p>
      
       <p>Tags: <a href="/tags/neural%20net.html">neural net</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#cs14-simnets---a-generalization-of-convolutional-networks">[CS14] SimNets - A Generalization of Convolutional Networks</a></li>
 <li><a href="#cs16-convolutional-rectifier-networks-as-generalized-tensor-decompositions">[CS16] Convolutional Rectifier Networks as Generalized Tensor Decompositions</a></li>
 <li><a href="#css16-on-the-expressive-power-of-deep-learning---a-tensor-analysis">[CSS16] On the Expressive Power of Deep Learning - A Tensor Analysis</a></li>
 <li><a href="#misc">Misc</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="http://www.cs.huji.ac.il/~cohennadav/index.html">Nadav Cohen’s webpage</a></p>
<p>Papers:</p>
<ul>
<li>[CS14] SimNets - A Generalization of Convolutional Networks</li>
<li>[CS16] Convolutional Rectifier Networks as Generalized Tensor Decompositions</li>
<li>[CS16] Inductive Bias of Deep Convolutional Networks through Pooling Geometry</li>
<li>[CSS16] Deep SimNets</li>
<li>[CSS16] On the Expressive Power of Deep Learning - A Tensor Analysis</li>
</ul>
<h2 id="cs14-simnets---a-generalization-of-convolutional-networks">[CS14] SimNets - A Generalization of Convolutional Networks</h2>
<p>A layer now looks like <span class="math display">\[ h(x) = MEX_\xi [u_l^T \phi(x,z_l) + b_l]_{l=1,\ldots, n} \]</span> where MEX is the exponential mean <span class="math display">\[
\rc{\xi} \ln\pa{\rc n \sumo in \exp(\xi c_i)}
\]</span> and <span class="math inline">\(\phi\)</span> is a kernel function, like <span class="math inline">\((x_iz_i)_i\)</span> or <span class="math inline">\((-|x_i-z_i|^p)_i\)</span>. These give rise to linear and generalized Gaussian kernels when composed with MEX.</p>
<p>MEX interpolates between minimum, average, and maximum pooling.</p>
<!--(This looks to be a generalization of $1\times 1$ convolution? What about larger?)-->
<p>There’s a natural unsupervised initialization scheme for SimNets based on statistical estimation. Assume the data is drawn from a mixture of generalized Gaussians and find max-likelihood parameters.</p>
<p>(cf. initialize the convolution kernels by looking at statistical properties of patches)</p>
<h2 id="cs16-convolutional-rectifier-networks-as-generalized-tensor-decompositions">[CS16] Convolutional Rectifier Networks as Generalized Tensor Decompositions</h2>
<p>ICML16.</p>
<blockquote>
<p>First, we show that convolutional rectifier networks are universal with max pooling but not with average pooling. Second, and more importantly, we show that depth efficiency is weaker with convolutional rectifier networks than it is with convolutional arithmetic circuits. This leads us to believe that developing effective methods for training convolutional arithmetic circuits, thereby fulfilling their expressive potential, may give rise to a deep learning architecture that is provably superior to convolutional rectifier networks but has so far been overlooked by practitioners.</p>
</blockquote>
<p>Generalized tensor decompositions</p>
<p><span class="math display">\[(A\ot_gB)_{d_1,\ldots, d_{P+Q}} = g(A_{d_1,\ldots, d_P}, B_{d_{P+1},\ldots, d_{P+Q}}). \]</span></p>
<p>For arbitrary commutative/associative pooling function, replace <span class="math inline">\(\ot\)</span> with <span class="math inline">\(\ot_g\)</span>.</p>
<h2 id="css16-on-the-expressive-power-of-deep-learning---a-tensor-analysis">[CSS16] On the Expressive Power of Deep Learning - A Tensor Analysis</h2>
<p>COLT2016.</p>
<p><img src="/images/deepnets_tensor_1.png"></p>
<p>Consider a neural network defined on input <span class="math inline">\((x_1,\ldots, x_N)\in (\R^s)^N\)</span>, as follows.</p>
\begin{align}
rep(i,d) &amp;= f_{\te_d}(x_i)\\
conv(i,z) &amp;= \an{a^{z,i}, rep(i,:)} = \sum_d a_d^{z,i} f_{\te_d}(x_i)\\
pool(z) &amp;= \prod_{i=1}^N conv(i,z) = \prod_{i=1}^N \sum_d a_d^{z,i} f_{\te_d}(x_i)\\
out(y) &amp;= \an{a_y, pool(:)} = \sum_{z=1}^Z a_{y,z} \prod_{i=1}^N\sum_d a_d^{z,i} f_{\te_d}(x_i)
\end{align}
<p>Note pool(z) is a 1-D tensor <span class="math display">\[ (a_d^{z,i\ot N *})_{i_1,\ldots, i_N, d} \ot (f_{\te_d}(x_i))^{i\ot N}_{d,i_1,\ldots, i_N} \]</span> so this is a rank <span class="math inline">\(Z\)</span> tensor.</p>
<p>Think of conv as <span class="math inline">\(1\times 1\)</span> convolution. Weight sharing makes the tensor decomposition symmetric.</p>
<p>Deep nets:</p>
<p><img src="/images/deepnets_tensor_2.png"></p>
<p>(<strong>Warning: The indexing of <span class="math inline">\(a\)</span> has been switched around here.</strong></p>
<p>We have a hierarchical decomposition. Ex. first layer:</p>
\begin{align}
conv_1(j,\ga) &amp;= \an{a^{1,j,\ga}, pool_0(j,:)}\\
&amp;=\an{a^{1,j,\ga}, \pa{\prod_{j'\in \{2j-1,2j\}} conv_0(j',k)}_k}\\
&amp;=\an{a^{1,j,\ga}, (\conv_0(2j-1,k)\conv_0(2j,k))_k}\\
&amp;=\sum_k a^{1,j,k} \conv_0(2j-1, k) \conv_0(2j,k).
\end{align}
<p>Iterating this gives the <strong>hierarchical Tucker decomposition</strong></p>
<p><img src="/images/deepnets_ht.png"></p>
<p><strong>Theorem 1</strong>: Consider the space of hierarchical tensors of order <span class="math inline">\(N\)</span> and dimension <span class="math inline">\(M\)</span> in each mode, parametrized by <span class="math inline">\(\{a^{l,j,\ga\}\)</span>. In this space, the tensor has CP-rank <span class="math inline">\(\ge r^{\fc N2}\)</span> a.e.</p>
<p><em>Proof</em>: Because the space of tensors with CP-rank forms an algebraic variety, it suffices to show that there exists a tensor of this rank. Do this by induction on layers and work with the matricization of the tensors (<span class="math inline">\(\ot\)</span> corresponds to Kronecker product). The CP-rank equals the rank of the matricization.</p>
<p>Question: what about convolution with larger windows?</p>
<h2 id="misc">Misc</h2>
<p>What is the motivation behind pooling? How much does it actually help?</p>
<p>Do you want universality?</p>
<p>Arithmetic circuits look much more like complexity-theoretic circuits! Training is harder. Product is AND, like soft version of min on <span class="math inline">\(\{0,1\}^n\)</span>?</p>
<p>(Replacing sigmoids/relus by 0-1?)</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Tensorflow setup</title>
    <link href="http://holdenlee.github.io/notebook/posts/cs/neural_nets/tensorflow.html" />
    <id>http://holdenlee.github.io/notebook/posts/cs/neural_nets/tensorflow.html</id>
    <published>2016-07-22T00:00:00Z</published>
    <updated>2016-07-22T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Tensorflow setup</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-07-22 
          , Modified: 2016-07-22 
	</p>
      
       <p>Tags: <a href="/tags/programming.html">programming</a></p> 
    </div>
    
  </div>
  <!--/div-->

  

  <div class="blog-main">
    <h2 id="setup-on-tiger">Setup on Tiger</h2>
<p>Load tensorflow on startup, e.g. put in <code>.bashrc</code>.</p>
<pre><code>module load python cudatoolkit/7.5 cudann
pip install --user /tigress/plazonic/public_html/tensorflow/rhel6/tensorflow_pkg_gpu/tensorflow-0.8.0-py2-none-any.whl</code></pre>
<p>Sample script</p>
<pre><code>#!/bin/bash

#SBATCH -t 10:00:00
#SBATCH -N 1
#SBATCH --ntasks-per-node=4
#SBATCH --ntasks-per-socket=2
#SBATCH --gres=gpu:2
#SBATCH --mail-type=begin  
#SBATCH --mail-type=end  
#SBATCH --mail-user=holdenl@princeton.edu  

module load python
module load cudatoolkit/7.5
module load cudann
THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python cifar10_multi_gpu_train.py --num_gpus=2 --train_dir='/tigress/holdenl/tmp/cifar10_train1'</code></pre>
<p>Run by <code>sbatch script.cmd</code>.</p>
<h2 id="cifar-setup">CIFAR setup</h2>
<ul>
<li>Train by calling <code>cifar10_multi_gpu_train.py</code> or <code>cifar10_train.py</code>.
<ul>
<li>This calls <code>cifar10.py</code> to build the graph.</li>
<li>It calls <code>cifar10_input.py</code> to download or load the data.
<ul>
<li><code>data_dir</code> defined in <code>cifar10.py</code>. (Changed to <code>/tigress/knv/cifar10_data</code>.)</li>
</ul></li>
</ul></li>
</ul>
<p>Settings: Override flags as above.</p>
<ul>
<li><code>num_gpus=2</code> seems to work best.</li>
<li>Specify training directory, ex. <code>train_dir='/tigress/holdenl/tmp/cifar10_train1'</code>.</li>
</ul>
<h2 id="train">Train</h2>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">train(fs, step_f, output_steps<span class="op">=</span><span class="dv">10</span>, summary_steps<span class="op">=</span><span class="dv">100</span>, save_steps<span class="op">=</span><span class="dv">1000</span>, eval_steps <span class="op">=</span> <span class="dv">1000</span>, max_steps<span class="op">=</span><span class="dv">1000000</span>, train_dir<span class="op">=</span><span class="st">&quot;/&quot;</span>, log_device_placement<span class="op">=</span><span class="va">False</span>, batch_size<span class="op">=</span><span class="dv">128</span>,train_data<span class="op">=</span><span class="va">None</span>,validation_data<span class="op">=</span><span class="va">None</span>, test_data<span class="op">=</span><span class="va">None</span>, train_feed<span class="op">=</span>{}, eval_feed<span class="op">=</span>{}, x_pl<span class="op">=</span><span class="st">&quot;x&quot;</span>, y_pl<span class="op">=</span><span class="st">&quot;y_&quot;</span>, batch_feeder_args<span class="op">=</span>[])</code></pre></div>
<ul>
<li><code>fs</code> is a dictionary containing: inference, loss functions</li>
<li><p><code>step_f</code> is function to execute at each training step, taking arguments <code>fs</code> and <code>global_step</code>. Example</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">lambda</span> fs, global_step: (
  train_step(fs[<span class="st">&quot;loss&quot;</span>], fs[<span class="st">&quot;losses&quot;</span>], global_step, 
             <span class="kw">lambda</span> gs: tf.train.AdamOptimizer(<span class="fl">1e-4</span>)))</code></pre></div></li>
<li></li>
</ul>
<h2 id="misc-notes">Misc notes</h2>
<p>Constants</p>
<ul>
<li><code>NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN=50000</code></li>
<li><code>NUM_EXAMPLES_PER_EPOCH_FOR_EVAL=10000</code></li>
<li><code>NUM_EPOCHS_PER_DECAY</code></li>
<li><code>INITIAL_LEARNING_RATE</code></li>
<li><code>LEARNING_RATE_DECAY_FACTOR</code></li>
<li><code>NUM_CLASSES=10</code></li>
<li><code>MOVING_AVERAGE_DECAY = 0.9999</code></li>
<li><code>NUM_EPOCHS_PER_DECAY = 350.0</code></li>
<li><code>LEARNING_RATE_DECAY_FACTOR = 0.1</code></li>
<li><code>INITIAL_LEARNING_RATE = 0.1</code></li>
</ul>
<h2 id="todo">Todo</h2>
<ul>
<li>Find tiger’s policies on storing files.</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Stanford quals</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/stanford_quals.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/stanford_quals.html</id>
    <published>2016-06-29T00:00:00Z</published>
    <updated>2016-06-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Stanford quals</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-06-29 
          , Modified: 2016-06-29 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#supervised-learning-1">Supervised learning [1]</a></li>
 <li><a href="#unsupervised-learning-1">Unsupervised learning [1]</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>(from Jacob’s notes on Stanford quals)</p>
<h2 id="supervised-learning-1">Supervised learning [1]</h2>
<p><a href="http://cs229.stanford.edu/materials.html">CS229</a></p>
<ul>
<li>Perceptron, logistic regression, SVMs</li>
<li>Kernel methods, Gaussian processes</li>
<li>Boosting (AdaBoost)
<ul>
<li>What is the convergence rate?
<ul>
<li><span class="math inline">\(\exp(-\sum_{t=1}^T \gamma_t^2)\)</span></li>
<li>Key property: <em>adaptive</em></li>
</ul></li>
<li>What problems can it be applied to?
<ul>
<li>binary classification on fixed dataset</li>
</ul></li>
<li><a href="http://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf">notes</a></li>
</ul></li>
<li><a href="cart.html">Decision trees, random forests</a>
<ul>
<li>Bagging (given subset of size <span class="math inline">\(N\)</span>, create many versions of the dataset by subsampling <span class="math inline">\(N\)</span> things with replacement repeatedly)</li>
<li>For each of these versions, also subsample <span class="math inline">\(\sqrt{d})\)</span> of the features to use for the decision tree</li>
</ul></li>
<li>Neural networks</li>
<li>Linear regression</li>
<li>Regularization: L1, L2 and their properties</li>
</ul>
<h2 id="unsupervised-learning-1">Unsupervised learning [1]</h2>
<p><a href="http://cs229.stanford.edu/materials.html">CS229</a></p>
<ul>
<li><a href="matrices/k-means.html">K-means</a></li>
<li><a href="matrices/dimensionality_reduction.html">Linear dimension reduction</a> PCA, CCA, factor analysis, ICA
<ul>
<li>What is PCA / what is it used for?</li>
<li>Given input dataset, assuming it’s elliptical, finds the principle axes of the ellipse
<ul>
<li>In more statistical language, this finds a low-dimensional representation that explains as much of the variance as possible</li>
</ul></li>
<li>Can be computed by just taking SVD of covariance matrix</li>
<li>Typically we mean-center first</li>
<li>Sometimes want to do other scalings but no clear consensus on the best one</li>
<li><a href="matrices/cca.html">CCA</a> What is CCA / what is it used for?</li>
<li>Same intuition as PCA, but wants to find cross-correlations between two sets of variables (X and Y)</li>
<li>Obtained by taking singular vectors of <span class="math inline">\(\Cov(X,X)^{\rc 2}\Cov(X,Y)Cov(Y,Y)^{-\rc2}\)</span>.</li>
<li>?Isn’t this used for semi-supervised learning?
<ul>
<li>?E.g. given two sets of features, use CCA as a regularizer.</li>
</ul></li>
<li><a href="matrices/factor-analysis.html">Factor analysis</a> What is factor analysis / what is it used for?</li>
<li>Basically, this is just matrix factorization</li>
<li>Often allows more domain knowledge to be incorporated</li>
<li><a href="matrices/ica.html">ICA</a> What is ICA / what is it used for?</li>
<li>Blind source separation</li>
<li>Tries to break into independent signals</li>
<li>Often done by maximizing non-gaussianity of signals</li>
</ul></li>
<li>EM</li>
<li>What theoretical property does EM satisfy?
<ul>
<li>Maximizes lower bound <span class="math inline">\(\log p(x) - KL(q(z|x) || p(z|x))\)</span></li>
</ul></li>
<li>What are the general updates?
<ul>
<li>Compute expectation of log-likelihood under current model</li>
<li>Minimize expectation</li>
<li>Sort of like iteratively approximating setting the gradient to zero</li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>CART and random forests</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/cart.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/cart.html</id>
    <published>2016-06-29T00:00:00Z</published>
    <updated>2016-06-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>CART and random forests</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-06-29 
          , Modified: 2016-06-29 
	</p>
      
       <p>Tags: <a href="/tags/CART.html">CART</a>, <a href="/tags/adaptive%20basis%20functions.html">adaptive basis functions</a>, <a href="/tags/random%20forests.html">random forests</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#cart-classification-and-regression-trees">CART (Classification and regression trees)</a></li>
 <li><a href="#random-forests">Random forests</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>See Ch. 16 of Murphy. <a href="http://math.bu.edu/people/mkon/MA751/L18RandomForests.pdf">Presentation</a></p>
<p>An adaptive basis-function model (ABM) is a model of the form <span class="math display">\[ f(x) = w_0+\sumo mM w_m \phi_m(x).\]</span> Typically <span class="math inline">\(\phi_m(x) = \phi(x;v_m)\)</span></p>
<h2 id="cart-classification-and-regression-trees">CART (Classification and regression trees)</h2>
<p>Decision trees recursively partition the input space and define a local model on each region.</p>
<p>For example, if the model is constant on each region, <span class="math inline">\(f(x) = \sumo mM w_m (x\in R_m)\)</span>.</p>
<p>At each node, consider these kinds of splits:</p>
<ul>
<li>Thresholds: <span class="math inline">\(x_i&lt;t\)</span>, <span class="math inline">\(x_i\ge t\)</span>. (Quantitative feature)</li>
<li><span class="math inline">\(x_i=c,x_i\ne c\)</span> (Categorical feature)</li>
</ul>
<p>The cost can be regression or classification cost. Sum the costs for each leaf. Cost:</p>
<ul>
<li>Regression: cost of fitting model on the leaf.</li>
<li>Classification</li>
<li>Misclassification rate of leaf. (Use the most probable class label.)</li>
<li>Entropy: <span class="math inline">\(-\sumo cC \wh\pi_c\lg \wh\pi_c\)</span>. (Recommended)</li>
<li>Gini index <span class="math inline">\(\sumo cC \wh \pi_c(1-\wh \pi_c) = 1-\sumo cC \wh\pi_c^2\)</span>.</li>
</ul>
<p>Algorithm:</p>
<ol type="1">
<li>Start at the root node of a single-node tree, and put all data points at that node.</li>
<li>Find the split at the current node (attribute) that minimizes the cost (maximizes information gain). (If it is deemed not worth splitting, e.g. it doesn’t decrease the cost by much<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>, if it’s reached a specified depth, etc., then move on instead.) Make the split. (The data points are now distributed among the two children.)</li>
<li>Add both child nodes. to the queue.</li>
<li>Continue the algorithm (at 2) with the next node in the queue.</li>
</ol>
<p>Advantages:</p>
<ul>
<li>Easy to interpret</li>
<li>Handle mixed inputs</li>
<li>Insensitive to monotone transformations, robust to outliers.</li>
<li>Automatic variable selection</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>Features are very restricted.</li>
<li>Unstable: small changes in input data can have large changes because of the hierarchical nature of the tree-growing process. (They are high variance estimators.)</li>
</ul>
<h2 id="random-forests">Random forests</h2>
<p><strong>Bagging</strong> (bootstrap aggregating): Train <span class="math inline">\(M\)</span> different trees on independently selected subsets of the data, and compute <span class="math inline">\(f(x)=\sumo mM \rc M f_m(x)\)</span>.</p>
<p>(OR: use boosting instead of taking majority vote.)</p>
<p>BUT this can result in highly correlated predictors.</p>
<p><strong>Random forest</strong>: Decorrelate base learners by learning rees based on a randomly chosen subset of input variables and data points.</p>
<p>(What are the right parameters? <span class="math inline">\(\sqrt d\)</span> features?)</p>
<p><strong>Bayesian adaptive regression trees (BART)</strong>.</p>
<p>? Hierarchical mixtures of experts.</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>This is usually too myopic. Instead use pruning. Grow the full tree, evaluate the cross-validated error on subtrees, and pick a minimal tree whose CV error is within 1 se of the min.<a href="#fnref1">↩</a></p></li>
</ol>
</section>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>k-means clustering</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/k-means.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/k-means.html</id>
    <published>2016-06-28T00:00:00Z</published>
    <updated>2016-06-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>k-means clustering</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-06-28 
          , Modified: 2016-06-28 
	</p>
      
       <p>Tags: <a href="/tags/clustering.html">clustering</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#questions">Questions</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="http://cs229.stanford.edu/notes/cs229-notes7a.pdf">Reference</a></p>
<p>The <span class="math inline">\(k\)</span>-means clustering is the following. (Lloyd’s algorithm) Let data points be <span class="math inline">\(x^{(i)}\)</span>.</p>
<ol type="1">
<li>Initialize <span class="math inline">\(\mu_1,\ldots, \mu_k\)</span>.</li>
<li>Repeat until convergence.
<ol type="1">
<li>Let <span class="math inline">\(c(i) = \amin_j |x^{(i)}-\mu_j|\)</span>.</li>
<li>Let <span class="math inline">\(\mu_j = \EE_{c(i)=j} x^{(i)}\)</span>.</li>
</ol></li>
</ol>
<p>This is alternating minimization on the distortion function <span class="math display">\[J(c,\mu) = \sumo im \ve{x^{(i)}-\mu_{c(i)}}^2\]</span> with respect to <span class="math inline">\(c\)</span> and <span class="math inline">\(\mu\)</span>. This value decreases and so converges (possible to local optimum); in practice, the <span class="math inline">\(\mu\)</span>’s converge.</p>
<h2 id="questions">Questions</h2>
<p>Are there provable guarantees on <span class="math inline">\(k\)</span>-means clustering—if so, what? And/or is there a (worst-case) hardness result?</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>ICA (Independent components analysis)</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/ica.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/ica.html</id>
    <published>2016-06-28T00:00:00Z</published>
    <updated>2016-06-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>ICA (Independent components analysis)</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-06-28 
          , Modified: 2016-06-28 
	</p>
      
       <p>Tags: <a href="/tags/ICA.html">ICA</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#algorithm">Algorithm</a></li>
 <li><a href="#tensor-algorithm">Tensor algorithm</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="http://cs229.stanford.edu/notes/cs229-notes11.pdf">Reference</a></p>
<p>Given that the components of <span class="math inline">\(s\in \R^n\)</span> are independent, and we observe <span class="math inline">\(x=As\)</span>, we want to find <span class="math inline">\(W=A^{-1}\)</span> and recover <span class="math inline">\(s\)</span>. Let <span class="math inline">\(w_i^T\)</span> be the rows of <span class="math inline">\(W\)</span>.</p>
<p>When the <span class="math inline">\(s_i\)</span> are non-gaussian, the solution is unique up to permutation and scaling. (Otherwise, there is rotational invariance.)</p>
<h2 id="algorithm">Algorithm</h2>
<p>Suppose the cdf of the components is logistic. (This is a reasonable default. Mean-center first.) Let <span class="math inline">\(g(s) = \rc{1+e^{-s}}\)</span>.</p>
<p>Change of coordinates gives <span class="math inline">\(p_x(x) = p_s(Wx)\det(W)\)</span>. The log-likelihood is <span class="math display">\[\ell(W) = \sumo im \pa{\sumo jn \ln g'(w_j^T x^{(i)}) + \ln |W|}.\]</span> Use stochastic gradient ascent.</p>
<p>Note: for problems where successive training examples are correlated, when implementing stochastic gradient ascent, it also sometimes helps accelerate convergence if we visit training examples in a randomly permuted order.</p>
<h2 id="tensor-algorithm">Tensor algorithm</h2>
<p>See “new_thread.pdf” page 30.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Factor analysis</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/factor-analysis.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/factor-analysis.html</id>
    <published>2016-06-28T00:00:00Z</published>
    <updated>2016-06-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Factor analysis</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-06-28 
          , Modified: 2016-06-28 
	</p>
      
       <p>Tags: <a href="/tags/factor%20analysis.html">factor analysis</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#solution-1-assume-independence">Solution 1: Assume independence</a></li>
 <li><a href="#solution-2-factor-analysis">Solution 2: Factor analysis</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="http://cs229.stanford.edu/notes/cs229-notes9.pdf">Reference</a></p>
<p>If <span class="math inline">\(n\gg m\)</span>, and we have data points <span class="math inline">\(x^{(i)}\in \R^n\)</span>, <span class="math inline">\(1\le i\le m\)</span>, how can we find Gaussian structure? We don’t have enough data points to even fit a single Gaussian.</p>
<h2 id="solution-1-assume-independence">Solution 1: Assume independence</h2>
<p>If the covariance matrix <span class="math inline">\(\Si\)</span> is diagonal, minimize the negative log likelihood <span class="math display">\[\sum\pa{\pf{\pa{x_j^{(i)}-\mu_j}^2}{2\si_j^2} + \ln \si_j}\]</span> to get <span class="math inline">\(\Si_{jj} = \EE_{i=1}^m (x_j^{(i)}-\mu_j)^2\)</span>. If <span class="math inline">\(\Si=\si I\)</span>, then <span class="math inline">\(\si^2 = \EE_{i,j}(x_j^{(i)}- \mu_j)^2\)</span>.</p>
<h2 id="solution-2-factor-analysis">Solution 2: Factor analysis</h2>
Break the coordinates into 2 parts <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span> and assume
\begin{align}
z&amp;\sim N(0,I)\\
\ep &amp;\sim N(0,\Psi)\\
x &amp;= \mu+ \La z + \ep.
\end{align}
<p>Calculate <span class="math display">\[\coltwo zx \sim N\pa{\coltwo 0\mu, \matt{I}{\La^T}{\La}{\La\La^T+\Psi}}.\]</span> Now do EM on the log likelihood with respect to <span class="math inline">\(z\)</span> and <span class="math inline">\(\La\)</span>. (details…)</p>
<ul>
<li>“This is just matrix factorization.”</li>
<li>“Often allows more domain knowledge to be incorporated.”</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Linear dimensionality reduction ([CG15])</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/dimensionality_reduction.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/dimensionality_reduction.html</id>
    <published>2016-06-28T00:00:00Z</published>
    <updated>2016-06-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Linear dimensionality reduction ([CG15])</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-06-28 
          , Modified: 2016-06-28 
	</p>
      
       <p>Tags: <a href="/tags/dimension%20reduction.html">dimension reduction</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#types-of-reductions">Types of reductions</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>A unified framework for linear dimensionality reduction: given <span class="math inline">\(n\)</span> points <span class="math inline">\([x_1,\ldots, x_n]\in \R^{d\times n}\)</span>, optimize <span class="math inline">\(f_X(\cdot)\)</span> to produce a linear transformation <span class="math inline">\(P\in \R^{r\times d}\)</span> and let <span class="math inline">\(Y=PX\in \R^{r\times n}\)</span> be the low-dimensional transformed data.</p>
<p><img src="/images/dim_red_chart.png"></p>
<h2 id="types-of-reductions">Types of reductions</h2>
<ul>
<li>PCA</li>
<li><a href="ica.html">ICA</a></li>
<li><a href="cca.html">CCA</a></li>
<li><a href="factor-analysis.html">Factor analysis</a></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>CCA (Canonical correlation analysis)</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/cca.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/cca.html</id>
    <published>2016-06-28T00:00:00Z</published>
    <updated>2016-06-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>CCA (Canonical correlation analysis)</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-06-28 
          , Modified: 2016-06-28 
	</p>
      
       <p>Tags: <a href="/tags/CCA.html">CCA</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"></div>

  <div class="blog-main">
    <p><a href="https://en.wikipedia.org/wiki/Canonical_correlation">Wikipedia</a></p>
<p>Goal: Find the linear combination of <span class="math inline">\((X_i)\)</span> and <span class="math inline">\((Y_j)\)</span> with maximum correlation.</p>
<p>Let <span class="math inline">\(\Si_{XY} = \Cov(X,Y)\)</span> (i.e., <span class="math inline">\(XY^T\)</span>).</p>
<p>We want to maximize (let <span class="math inline">\(\ve{v}_M=v^TMv\)</span>) <span class="math display">\[\max_{a,b} \fc{a^T\Si_{XY}b}{\ve{a}_{\Si_{XX}}\ve{b}_{\Si_{YY}}}.\]</span> Let <span class="math inline">\(c=\Si_{XX}^{\rc 2}a\)</span> and <span class="math inline">\(d=\Si_{YY}^{\rc 2}b\)</span>. Then this is <span class="math display">\[\fc{c^T \Si_{XX}^{-\rc2} \Si_{XY} \Si_{YY}^{-\rc2}d}{\ve{c}_2\ve{d}_2}.\]</span> Thus, find the SVD of <span class="math display">\[\Si_{XX}^{-\rc2} \Si_{XY} \Si_{YY}^{-1} \Si_{YX} \Si_{XX}^{-\rc 2}.\]</span> Change coordinates back to find <span class="math inline">\(a,b\)</span>.</p>
<p>More generally, to find the top <span class="math inline">\(k\)</span> dimensions, we want <span class="math display">\[\max_{M_X \in \mathcal{O}^{d_a\times r}, M_Y\in \mathcal{O}^{d_b\times r}} \Tr(M_X^T \Si_{XX}^{-\rc2} \Si_{XY} \Si_{YY}^{-\rc2}M_Y).\]</span> Find the rank <span class="math inline">\(k\)</span> SVD, the matrices consist of the top <span class="math inline">\(k\)</span> SV’s.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
