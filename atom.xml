<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Research Notebook</title>
    <link href="http://holdenlee.github.io/notebook/atom.xml" rel="self" />
    <link href="http://holdenlee.github.io/notebook" />
    <id>http://holdenlee.github.io/notebook/atom.xml</id>
    <author>
        <name>Holden Lee</name>
        <email>oldheneel@gmail.com</email>
    </author>
    <updated>2016-11-01T00:00:00Z</updated>
    <entry>
    <title>Function approximation</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/function_approximation.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/function_approximation.html</id>
    <published>2016-11-01T00:00:00Z</published>
    <updated>2016-11-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Function approximation</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-11-01 
          , Modified: 2016-11-01 
	</p>
      
       <p>Tags: <a href="/tags/reinforcement%20learning.html">reinforcement learning</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#dp-and-rl-in-large-and-continuous-spaces">3 DP and RL in large and continuous spaces</a><ul>
 <li><a href="#nonparametric-appoximation">3.3.2 Nonparametric appoximation</a></li>
 <li><a href="#section">3.3.3</a></li>
 <li><a href="#approximate-value-iteration">3.4 Approximate value iteration</a><ul>
 <li><a href="#convergence">3.4.4 Convergence</a></li>
 <li><a href="#example-approximate-q-iteration-for-a-dc-motor">3.4.5 Example: Approximate Q-iteration for a DC motor</a></li>
 </ul></li>
 <li><a href="#approximate-policy-iteration">3.5 Approximate policy iteration</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <p>See also “Factored MDPs, MDPs with exponential/continuous state space” in <a href="rl_refs.html">refs</a>.</p>
<ul>
<li>[GWTC13]</li>
<li>Reinforcement learning and DP using FA</li>
<li>Bertsekas Ch. 6, ADP</li>
<li><a href="http://site.ebrary.com/lib/princeton/reader.action?docID=10501323">Powell, ADP</a></li>
<li>Sutton, Ch. 8 (v1)</li>
</ul>
<h2 id="dp-and-rl-in-large-and-continuous-spaces">3 DP and RL in large and continuous spaces</h2>
<p><span class="math inline">\(F(\te)(x,u_j) = \phi^T(x,u_j)\te\)</span>, <span class="math inline">\(\phi\)</span> normalized so entries sum to 1.</p>
<h4 id="nonparametric-appoximation">3.3.2 Nonparametric appoximation</h4>
<p>Kernel-based approximator of <span class="math inline">\(Q\)</span> function <span class="math inline">\(\ka: (X\times U)^2\to \R\)</span>.</p>
<p>Form and number of BF’s not defined in advance <span class="math display">\[
\wh Q(x,u) = \sumo{l_s}{n_s} \ka((x,u), (x_{l_s}, u_{l_s}))\te_{l_s}.
\]</span></p>
<p>(I haven’t been exposed to nonparametric methods - what guarantees do nonparametric methods have?) <!-- relies on kernel function that makes sense for the space --></p>
<h4 id="section">3.3.3</h4>
<p>In between: derive small number of good BF’s from data.</p>
<h3 id="approximate-value-iteration">3.4 Approximate value iteration</h3>
<ul>
<li>LSQI: take a bunch of samples, take <span class="math inline">\(Q\)</span> minimizing least squares.</li>
<li>Online: use gradient descent on parameters <span class="math inline">\(\te\)</span>.</li>
</ul>
<p>Approximate Q-learnig requires exploration.</p>
<h4 id="convergence">3.4.4 Convergence</h4>
<p>Proofs for approximate value iteration rely on contraction mapping arguments. Ex. require <span class="math inline">\(F\)</span> and projection <span class="math inline">\(P\)</span> to be nonexpansions.</p>
<p>Suboptimality for convergence point <span class="math inline">\(\te^*\)</span> bounded in terms of min distance between <span class="math inline">\(Q^*\)</span> and fixed point of <span class="math inline">\(F\circ P\)</span>, <span class="math inline">\(\ze_{QI}^*\)</span>.</p>
<p>(Ditto for nonparametric (kernel-based) approximators.)</p>
<h4 id="example-approximate-q-iteration-for-a-dc-motor">3.4.5 Example: Approximate Q-iteration for a DC motor</h4>
<p>Fitted <span class="math inline">\(Q\)</span>-iteration using ensembles of extremely randomized trees.</p>
<h3 id="approximate-policy-iteration">3.5 Approximate policy iteration</h3>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2016-11-05</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-11-05.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-11-05.html</id>
    <published>2016-10-31T00:00:00Z</published>
    <updated>2016-10-31T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2016-11-05</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-31 
          , Modified: 2016-10-31 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#threads">Threads</a><ul>
 <li><a href="#priority">Priority</a></li>
 <li><a href="#other">Other</a></li>
 </ul></li>
 <li><a href="#other-papers">Other papers</a></li>
 <li><a href="#people-to-talk-to">People to talk to</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="2016-10-29.html">Last week</a>. (See also <a href="2016-10-22.html">wonderings</a>.)</p>
<h2 id="threads">Threads</h2>
<h3 id="priority">Priority</h3>
<ul>
<li>Come up with a class of MDPs on exponential space that is interesting and tractable. <a href="/posts/tcs/machine_learning/reinforcement_learning/continuous.html">Thoughts</a>
<ul>
<li>Understand provable guarantees on MDP’s first</li>
</ul></li>
<li>PMI - get some results!
<ul>
<li>Mon. - train CIFAR.</li>
<li>Todos
<ul>
<li>Check MNIST model 1. What are sizes of coefficients?</li>
<li>Run experiments on CIFAR.</li>
</ul></li>
</ul></li>
</ul>
<h3 id="other">Other</h3>
<ul>
<li>SoS <a href="/posts/tcs/complexity/sos.html">summary</a> (Monday: wrote up notes.)</li>
<li>DL experiments <a href="/posts/tcs/machine_learning/matrices/DL_generalization.html">DL generalization</a></li>
<li>On hold
<ul>
<li>(*) NN learns DL. Can write up weak result, worth doing?</li>
</ul></li>
<li>Papers
<ul>
<li>[HM16] on unsupervised learning (finish)</li>
<li>[HMR16] on dynamical system learning</li>
</ul></li>
</ul>
<p>Analyze Arora and Ge’s NMF algorithm in the presence of noise. Exactly how much noise can it tolerate?</p>
<h2 id="other-papers">Other papers</h2>
<ul>
<li>TODO Read this paper: [CFP16] Assessing significance in a Markov chain without mixing</li>
</ul>
<h2 id="people-to-talk-to">People to talk to</h2>
<ul>
<li>Yingyu, on PMI</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>[MVB16] Geometry of Polysemy</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/MVB16.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/nlp/MVB16.html</id>
    <published>2016-10-28T00:00:00Z</published>
    <updated>2016-10-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>[MVB16] Geometry of Polysemy</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-28 
          , Modified: 2016-10-28 
	</p>
      
       <p>Tags: <a href="/tags/nlp.html">nlp</a>, <a href="/tags/word%20vectors.html">word vectors</a>, <a href="/tags/polysemy.html">polysemy</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#abstract">Abstract</a></li>
 <li><a href="#comments">Comments</a></li>
 <li><a href="#summary">Summary</a><ul>
 <li><a href="#algorithm">1 Algorithm</a></li>
 <li><a href="#experiments">2 Experiments</a></li>
 <li><a href="#further-thoughts">Further thoughts</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="abstract">Abstract</h2>
<p>Title: Geometry of Polysemy</p>
<p><a href="https://arxiv.org/pdf/1610.07569v1.pdf">Link</a></p>
<p>Abstract: Vector representations of words have heralded a transformational approach to classical problems in NLP; the most popular example is word2vec. However, a single vector does not suffice to model the polysemous nature of many (frequent) words, i.e., words with multiple meanings. In this paper, we propose a three-fold approach for unsupervised polysemy modeling: (a) context representations, (b) sense induction and disambiguation and (c) lexeme (as a word and sense pair) representations. A key feature of our work is the finding that a sentence containing a target word is well represented by a low rank subspace, instead of a point in a vector space. We then show that the subspaces associated with a particular sense of the target word tend to intersect over a line (one-dimensional subspace), which we use to disambiguate senses using a clustering algorithm that harnesses the Grassmannian geometry of the representations. The disambiguation algorithm, which we call <span class="math inline">\(K\)</span>-Grassmeans leads to a procedure to label the different senses of the target word in the corpus – yielding lexeme vector representations, all in an unsupervised manner starting from a large (Wikipedia) corpus in English. Apart from several prototypical target (word,sense) examples and a host of empirical studies to intuit and justify the various geometric representations, we validate our algorithms on standard sense induction and disambiguation datasets and present new state-of-the-art results.</p>
<h2 id="comments">Comments</h2>
<p>Authors say: Our algorithm (<span class="math inline">\(K\)</span>-Grassmeans) does slightly, but uniformly, better than the precision-recall that you report. But the surprising thing is that a simple baseline we setup using standard word2vec individual vectors also gets very close to the performance of your algorithm! We hypothesize why this could be so in Section 5 where we see that the all the algorithm outputs are highly correlated. They get the easy instance all correct and the ones that they make errors on are actually hard/subtle (or rare in the context of the corpus we use (which is Wikipedia)).</p>
<h2 id="summary">Summary</h2>
<p>(by Yingyu)</p>
<h3 id="algorithm">1 Algorithm</h3>
<p>Their algorithm is quite interesting, building on two key ideas: 1) the context of a word can be represented by a subspace 2) the context subspace of the same sense of a word will intersect.</p>
<p>The algorithm is:</p>
<ol type="1">
<li>for each occurrence of a target word, take say a context window of size 10 around the occurrence. The subspace of the word vectors in the window is the subspace for this context. They take rank-3 PCA. I think this is related to what we did in the linear structure paper.</li>
<li>take all the context subspaces of a target polysemous word, cluster them into K groups. The clustering objective is called K-Grassman. It is similar to k-means, except that the center is a direction and the distance is the distance between a direction and a subspace. (This is word sense induction)</li>
<li>given a new context of the target word, one can build the context subspace, find the closest center obtained in the K-Grassman. (This is word sense disambiguation)</li>
</ol>
<p>Finally, they also talked about lexeme representation. I understand this as given a polysemous word, how to represent its meanings. It’s tempting to represent it as the K cluster centers obtained in the K-Grassman. However, they found that these directions tend to be close; the inner product tends to be as large as 0.9. They leave how to explain this as an open problem, then provide another way to do the lexeme representation: first use the above word sense disambiguation algo to label the words in the corpus, and then train a vector for each sense of each word.</p>
<p><strong>My thoughts</strong>: I think the inner products between these cluster centers are large because of a simple reason: frequent words like “the” pop up in all context, so all context subspace has these components. This problem has been observed in multiple scenarios. For obtaining vectors for sentences, we can avoid this by doing a weighted average of the word vectors. So one way to handle this problem for subspace is: when computing the subspace for a sentence/context, find a subspace minimizing the weighted sum of the distances from the word vectors to the subspace. This should be better than PCA, which is minimizing the unweighted sum.</p>
<p>Their idea also implies a way to do sentence embedding: instead of using vectors, use subspace.</p>
<h3 id="experiments">2 Experiments</h3>
<p>They tested on word similarity task dataset and our police lineup dataset.</p>
<p>On the police lineup, their result is better than ours. But theirs, ours, and simply using word2vec vectors, all lead to quite similar performance. (I observed this for our vectors before.) They mentioned that all methods are OK on easy ones but fail on hard ones in the testset. This is probably because some senses are rare on wiki data.</p>
<h3 id="further-thoughts">Further thoughts</h3>
<p>Since we view a sentence as a subspace, then it makes sense to generalize our random walk model so that the discourse is now a subspace.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>RL references</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/rl_refs.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/rl_refs.html</id>
    <published>2016-10-25T00:00:00Z</published>
    <updated>2016-10-25T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>RL references</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-25 
          , Modified: 2016-11-01 
	</p>
      
       <p>Tags: <a href="/tags/reinforcement%20learning.html">reinforcement learning</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#online">Online</a></li>
 <li><a href="#books">Books</a></li>
 <li><a href="#papers">Papers</a><ul>
 <li><a href="#theoretical-frameworks-and-results">Theoretical frameworks and results</a></li>
 <li><a href="#mdps">MDPs</a><ul>
 <li><a href="#convergence-of-classic-algorithms">Convergence of classic algorithms</a></li>
 <li><a href="#theory-algorithms">Theory algorithms</a></li>
 </ul></li>
 <li><a href="#factored-mdps-mdps-with-exponentialcontinuous-state-space">Factored MDPs, MDPs with exponential/continuous state space</a></li>
 <li><a href="#pomdps">POMDPs</a></li>
 <li><a href="#open-questions">Open questions</a></li>
 <li><a href="#surveys">Surveys</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="online">Online</h2>
<ul>
<li><a href="https://github.com/andrewliao11/Deep-Reinforcement-Learning-Survey/blob/master/Reinforcement-Learning-Papers.md">Deep RL</a></li>
<li>ICML presentation (David Silver)</li>
<li><a href="http://castlelab.princeton.edu/">CASTLE Labs</a>
<ul>
<li><a href="http://optimallearning.princeton.edu/">Optimal learning</a></li>
<li><a href="http://adp.princeton.edu/">Approximate dynamic programming</a>
<ul>
<li><a href="http://adp.princeton.edu/adpIntros.htm">Intros</a></li>
</ul></li>
<li><a href="http://castlelab.princeton.edu/jungle.htm#unifiedframework">Unified framework</a></li>
</ul></li>
</ul>
<h2 id="books">Books</h2>
<p><a href="https://www.quora.com/What-are-the-best-books-about-reinforcement-learning">Quora recommendations</a></p>
<ul>
<li>(*) <a href="https://webdocs.cs.ualberta.ca/~sutton/book/the-book-1st.html">Sutton Barto</a> <a href="rl.html"><strong>Notes</strong></a></li>
<li><a href="https://books.google.com/books?id=VvBjBAAAQBAJ&amp;printsec=frontcover&amp;dq=continuous+markov+decision+processes&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjo3OLywOnPAhVHWD4KHXzgDWUQ6AEIKTAC#v=onepage&amp;q=continuous%20markov%20decision%20processes&amp;f=false">Puterman14</a></li>
<li><a href="http://site.ebrary.com/lib/princeton/reader.action?docID=10501323">Approximate DP, Powell</a></li>
<li><a href="http://site.ebrary.com/lib/princeton/detail.action?docID=10560566">Optimal learning, Powell</a></li>
<li>(*) <a href="http://www.crcnetbase.com/isbn/9781439821091">Function approximators, Lucian Busoniu, Robert Babuska, Bart De Schutter, and Damien Ernst</a></li>
<li><a href="http://web.mit.edu/dimitrib/www/dpchapter.pdf">ADP chapter, Bertsekas</a></li>
<li><a href="https://books.google.com/books?id=-6RiQgAACAAJ&amp;dq=Dynamic+Programming:+Deterministic+and+Stochastic+Models&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjc0pfAyefPAhUGFz4KHaVIDecQ6AEIHjAA">Bertsekas87</a></li>
</ul>
<h2 id="papers">Papers</h2>
<h3 id="theoretical-frameworks-and-results">Theoretical frameworks and results</h3>
<ul>
<li>(*) [KMN02] A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes <a href="http://download.springer.com/static/pdf/530/art%253A10.1023%252FA%253A1017932429737.pdf?originUrl=http%3A%2F%2Flink.springer.com%2Farticle%2F10.1023%2FA%3A1017932429737&amp;token2=exp=1477079019~acl=%2Fstatic%2Fpdf%2F530%2Fart%25253A10.1023%25252FA%25253A1017932429737.pdf%3ForiginUrl%3Dhttp%253A%252F%252Flink.springer.com%252Farticle%252F10.1023%252FA%253A1017932429737*~hmac=6c901205464aff209a8d3ca5ba481b36b72959a0d61fc762dfc512f12c01a38c">paper</a>
<ul>
<li>PAC formulation</li>
</ul></li>
<li>[AAKMR02] A Note on the Representational Incompatibility of Function Approximation and Factored Dynamics.pdf
<ul>
<li>Barrier to solving factored MDP’s is not just computational ([PT87]), it is representational (there is no succinct policy)</li>
<li>DBN-MDP (factored MDP): transition law <span class="math inline">\(\de\)</span> is dynamic Bayes net. The first layer are the variables (and action) at time <span class="math inline">\(t\)</span>, the second layer are the variables at time <span class="math inline">\(t+1\)</span>, the graph is directed, the indegree of each second-layer node is at most constant.</li>
<li>Rewards are linear.</li>
<li>Connection with AM-games: V’s state corresponds to state, P implements policy.</li>
<li>If PSPACE is not contained in P/POLY, then there is a family of DBN-MDPs, such that for any two polynomials <span class="math inline">\(s,a\)</span>, there exist infinitely many <span class="math inline">\(n\)</span> such that no circuit <span class="math inline">\(C\)</span> of size <span class="math inline">\(s(n)\)</span> can compute a policy having expected reward greater than <span class="math inline">\(\rc{a(n)}\)</span> times the optimum.</li>
<li>(This is the policy optimization part. Can you learn Bayes nets? <span class="citation" data-cites="Andrej">@Andrej</span>)</li>
<li>(Note that the “drifting context vector (RANDWALK)” model can be represented by a model with <span class="math inline">\(1\to 1', 2\to 2',\ldots\)</span>.)</li>
<li>What if you only compared to the best policy in a class of policies? (cf. EXP4)</li>
</ul></li>
</ul>
<h3 id="mdps">MDPs</h3>
<h4 id="convergence-of-classic-algorithms">Convergence of classic algorithms</h4>
<ul>
<li>[PB79] On the convergence of policy iteration in stationary dynamic programming.pdf
<ul>
<li>Equivalent to Newton-Kantorovich iteration procedure applied to functional equation of dynamic programming.</li>
<li>Sufficient conditions for superlinear or quadratic convergence. See [Howard].</li>
<li>Note: Does NOT apply to finite state MDPs! (Problem being that “best action” is not continuous in parameters?)</li>
</ul></li>
<li>[SR04] Convergence properties of policy iteration.pdf
<ul>
<li>Compare to method of successive approximations. SA is bad when <span class="math inline">\(\ga\approx 1\)</span>.</li>
</ul></li>
<li>[TVR96] An Analysis of Temporal-Difference Learning with Function Approximation <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.92&amp;rep=rep1&amp;type=pdf">paper</a>
<ul>
<li><span class="math inline">\(TD(\la)\)</span> convergence</li>
<li>What is rate??</li>
</ul></li>
<li>[B95] Residual Algorithms: Reinforcement learning with function approximation <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=akijBQAAQBAJ&amp;oi=fnd&amp;pg=PA30&amp;dq=baird+residual+algorithms+function+approximation&amp;ots=MJ_Hs5vMBs&amp;sig=RTG7KgQgM4GWWIwSvg1wOYRmJDc#v=onepage&amp;q=baird%20residual%20algorithms%20function%20approximation&amp;f=false">paper</a>
<ul>
<li>Q-learning instability</li>
</ul></li>
<li>[WD92] Q-learning
<ul>
<li>Given bounded rewards, learning rates <span class="math inline">\(0\le \al_n&lt;1\)</span>, and <span class="math inline">\(\sumo i{\iy} \al_{n^i(x,a)}=\iy\)</span> (<span class="math inline">\(n^i(x,a)\)</span> is the <span class="math inline">\(i\)</span>th time <span class="math inline">\(a\)</span> is tried in state <span class="math inline">\(x\)</span>) then <span class="math inline">\(Q_n\to Q^*\)</span> wp 1.</li>
<li>Doesn’t address: under what <span class="math inline">\(Q\)</span>? What if updating policy at same time? What’s regret?</li>
</ul></li>
</ul>
<h4 id="theory-algorithms">Theory algorithms</h4>
<ul>
<li>(*) [AO06] Logarithmic online regret bounds for undiscounted reinforcement learning
<ul>
<li>UCRL</li>
<li>Maintain confidence bounds on rewards and transition probabilities.</li>
<li>Only apply to unichain MDP’s: the stationary distribution of any policy does not depend on the start state (this is to make things easier, can do without)</li>
<li>Other work: adversarial reward, index policies (choose action with max return in confidence region)</li>
</ul></li>
<li>[LH12] PAC bounds for discounted MDPs <a href="https://arxiv.org/pdf/1202.3890.pdf">paper</a>
<ul>
<li>UCRL, under assumption of 2 possible next-states for each state/action pair, PAC bound of <span class="math inline">\(\wt O \pa{\fc{|S\times A|}{\ep^2(1-\ga)^3}\ln \prc{\de}}\)</span>.</li>
</ul></li>
<li>[KS02] Near-optimal reinforcement learning in polynomial time</li>
<li>(*) [JOA10] Near-optimal regret bounds for reinforcement learning <a href="http://www.jmlr.org/papers/volume11/jaksch10a/jaksch10a.pdf">paper</a>
<ul>
<li>Improved UCRL (UCRL2)</li>
</ul></li>
<li>[KN09] Near-Bayesian Exploration in Polynomial Time <a href="http://www.zicokolter.com/wp-content/uploads/2015/10/kolter-icml09a-full.pdf">paper</a></li>
<li>[KAL16] Contextual-MDP, which is contextual bandits + RL.
<ul>
<li>Regret wrt policy class.</li>
<li>Poly in parameters, log in number of policies, independent of size of observation space. <span class="math inline">\(\poly(M, K, H, \ep, \ln N, \ln \prc\de)\)</span> <!--what does no dependence on numspace can represent exact-best solution, state transition dynamics are deterministic.--></li>
<li>Unlike POMDP, optimal policy is memoryless. (Definition is just this. For simplicity, consider layered POMDP’s.)
<ul>
<li>Ex. Disjoint contexts. Don’t know which observations correspond to which contexts! (Q: for HMM with “too many” observations what can we do?)</li>
</ul></li>
<li>Warning: inefficient b/c requires enumeration of policy class. (? does this contradict the poly/log time above)</li>
<li>Assumptions
<ul>
<li><span class="math inline">\(Q\)</span> is realizable within the function class. (WHY doesn’t this work in the agnostic case?)</li>
<li>Deterministic transitions</li>
</ul></li>
<li>Algorithm (cMDP-learn)
<ul>
<li>DFS-learn
<ul>
<li>TD-elim: eliminate functions that do not approximate <span class="math inline">\(Q^*\)</span> well (that do significantly worse than the best approximator so far)</li>
<li>Consensus: Compute MC estimates for the observable in the current state (run a lot of episodes, using the same actions up to the current point; because the system is deterministic the state is the same). If they are all close in value, return true, else return false.</li>
</ul></li>
<li>Explore-on-demand
<ul>
<li>Select a surviving policy, estimate <span class="math inline">\(V\)</span> at root; if highly suboptimal value, invoke DFS-learn on paths visited by <span class="math inline">\(\pi_f\)</span>.</li>
</ul></li>
</ul></li>
<li>Idea: if a surviving policy <span class="math inline">\(\pi_f\)</span> visits only states for which TD-Elim has been invoked, it must have near-optimal reward.</li>
<li>Undesirables
<ul>
<li>Deterministic transition</li>
<li>Enumerate class of regression functions</li>
<li>Realizability assumptions</li>
</ul></li>
</ul></li>
<li>[DPWR15] Bayesian Nonparametric Methods for Partially-Observable Reinforcement Learning <a href="http://dspace.mit.edu/handle/1721.1/97034">paper</a>
<ul>
<li>Bayes!</li>
</ul></li>
</ul>
<h3 id="factored-mdps-mdps-with-exponentialcontinuous-state-space">Factored MDPs, MDPs with exponential/continuous state space</h3>
<ul>
<li>[HSMM15] Off-policy Model-based Learning under Unknown Factored Dynamics.pdf
<ul>
<li>Under 3 assumptions, using a greedy approach to finding parents, estimate the transition function (parameters to Bayes net) (compre with prob models literature?)</li>
<li>This is for off-policy evaluation; it doesn’t tell us how to find the optimal policy.</li>
<li>(Is the model learning and policy evaluation coupled or not?)</li>
<li>(It seems to be learning the Bayes net rather than evaluating <span class="math inline">\(\pi\)</span>. Ah, once you learn the Bayes net then you can evaluate just by sampling.)</li>
<li>The difference from simpling learning a Bayes net is that the samples aren’t independent—they were from following a certain policy. Assumptions will ensure that you can still learn the model even if you only have samples from that policy.</li>
</ul></li>
<li>[EGW05] Tree-Based Batch Mode Reinforcement Learning
<ul>
<li>Introduced fitted Q-iteration (see below).</li>
</ul></li>
<li>(*) [AMS08] Fitted Q-iteration in continuous action-space MDPs
<ul>
<li>Fitted Q-iteration: Given a simulator, sample next actions <span class="math inline">\(s'\)</span> given <span class="math inline">\(s,a\)</span>. Given <span class="math inline">\(Q^{n}\)</span>, approximate <span class="math inline">\(Q^{n+1}\)</span> with these samples, then approximate <span class="math inline">\(Q^{n+1}\)</span> as <span class="math inline">\(Q(s,a)=\te^T\phi(s,a)\)</span>. Use least squares: LSFQI. Then pick best <span class="math inline">\(\wh \pi_{n+1}\)</span> using the approximation of <span class="math inline">\(Q^{n+1}\)</span>.</li>
<li>Space of functions can be neural networks, linear combination of selected basis functions, restriction of RKHS (cf. LS-SVM).</li>
<li>Warning: it’s not just the pseudo-dimension (related to VC dimension) of the function class <span class="math inline">\(\mathcal F\)</span> that matters, but that of <span class="math inline">\(\mathcal F_{\max}^{\wedge} = \set{\max_{a\in A} Q(x,a)}{Q\in \mathcal F}\)</span>. (Actually, use the notion of <a href="http://ttic.uchicago.edu/~tewari/lectures/lecture16.pdf">fat shattering functions</a>.)</li>
<li>Also called “fitted actor-critic algorithm”.</li>
<li>Under many assumptions, the error in <span class="math inline">\(V\)</span> can be bounded in terms of the pseudo-dimension of the function class <span class="math inline">\(\mathcal F\)</span>.</li>
</ul></li>
<li>[FDMW04] Dynamic Programming for Structured Continuous Markov Decision Problems <a href="https://arxiv.org/ftp/arxiv/papers/1207/1207.4115.pdf">paper</a>
<ul>
<li>Group together states belonging to the same “plateau” where expected reward is nearly constant.</li>
<li>Use kd-trees to store the rectangular partitions.</li>
</ul></li>
<li>[HK03] Linear Program Approximations for Factored Continuous-State Markov Decision Processes <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2003_CN19.pdf">paper</a>
<ul>
<li>2 settings
<ul>
<li>Factored MDP’s: approximate <span class="math inline">\(V\)</span> within a class of functions <span class="math inline">\(\spn(\{f_i\})\)</span>. (Ex. each <span class="math inline">\(f_i\)</span> depends on a small subset of variables.) Here, minimize <span class="math inline">\(\sum w_i \sum_x f_i(x)\)</span> over all <span class="math inline">\(f\)</span>’s that overestimate the reward: <span class="math inline">\(\sum_i w_i(f_i - \ga \sum_{x_i'} \Pj(x_i'|x_i, a)f_i(x_i'))-R(x,a)\ge 0\)</span> forall <span class="math inline">\(x,a\)</span>.
<ul>
<li>Problem: infinite number of constraints. Insight: only a finite subset are active at any time.</li>
</ul></li>
<li>Continuous MDP’s. Consider conjugate classes of transition models and basis functions that give closed-form expressions.</li>
</ul></li>
</ul></li>
<li>[LL05] Lazy Approximation for Solving Continuous Finite-Horizon MDPs <a href="http://www.aaai.org/Papers/AAAI/2005/AAAI05-186.pdf">paper</a>
<ul>
<li>In value iteration <span class="math display">\[V^{n+1}(x) = \max_{a\in A} \ba{R(x,a) + \int_X T(x'|x,a)V^n(x')\dx'}\]</span> replace <span class="math inline">\(V^n\)</span> with a piecewise constant approximation. (Otherwise it becomes a piecewise higher order polynomial.)</li>
</ul></li>
<li>[MTT] A Fast Analytical Algorithm for MDPs with Continuous State Spaces <a href="http://www.sci.brooklyn.cuny.edu/~parsons/events/gtdt/gtdt06/marecki.pdf">paper</a>
<ul>
<li>Focuses on MDP’s where the transition time (after an action) is governed by a exponential pdf <span class="math inline">\(\la e^{-\la t'}\)</span>.</li>
</ul></li>
<li>[TS06] Probabilistic Inference for Solving Discrete and Continuous State Markov Decision Processes <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2006_ToussaintS06.pdf">paper</a>
<ul>
<li>The problem of solving a MDP (with decay <span class="math inline">\(\ga\)</span>) can be reduced to max likelihood estimation. (It is over a mixture of finite-time MDP’s, weighted geometrically in <span class="math inline">\(\ga\)</span> in the length.)</li>
<li>Now use EM to maximize the likelihood.</li>
</ul></li>
<li><a href="https://www.cs.utah.edu/~piyush/teaching/continuous-mdp.pdf">Lecture notes</a>
<ul>
<li>Discretization</li>
<li>Fitted value iteration: Given a simulator, sample next actions <span class="math inline">\(s'\)</span> given <span class="math inline">\(s,a\)</span>. Given <span class="math inline">\(V^{n}\)</span>, approximate <span class="math inline">\(V^{n+1}\)</span> with these samples, then approximate <span class="math inline">\(V^{n+1}\)</span> as <span class="math inline">\(V(s)=\te^T\phi(s)\)</span> (ex. using least squares).</li>
<li>Can be extended to least-squares policy iteration.</li>
</ul></li>
<li><a href="http://burlap.cs.brown.edu/tutorials/scd/p1.html">tutorial</a></li>
<li><a href="https://scholar.google.com/scholar?hl=en&amp;q=mdp+with+continuous+state+space">google scholar search</a></li>
</ul>
<h3 id="pomdps">POMDPs</h3>
<ul>
<li>(*) [ALA16] Reinforcement Learning of POMDPs using Spectral Methods <a href="http://www.jmlr.org/proceedings/papers/v49/azizzadenesheli16a.pdf">paper</a>
<ul>
<li>Spectral parameter estimation for POMDP’s</li>
<li>Combine with UCRL (exploration-exploitation framework) to get regret bounds (compared to memoryless policies) optimal in dependence on <span class="math inline">\(N\)</span> (<span class="math inline">\(O(\sqrt N)\)</span>)</li>
<li>Challenges
<ul>
<li>Unlike HMM, consecutive observations not conditionally independent</li>
<li>Technical: Concentration inequalities for dependent rv’s. Extend to marix value functions.</li>
</ul></li>
<li>Previous/other work
<ul>
<li>UCRL</li>
<li>model-free algorithms (<span class="math inline">\(Q\)</span>-learning)</li>
<li>policy search methods</li>
<li>separate exploration and exploitation collect examples, then estimate parameters [Guo16]. PAC in RL POMDP?</li>
</ul></li>
<li>Regret bounds optimal in <span class="math inline">\(N\)</span> (<span class="math inline">\(\wt O(\sqrt N)\)</span>). Depends on a natural notion of “diameter” for POMDP’s (different from definition for MDP’s. max mean passage time using best <span class="math inline">\(\pi\)</span>).</li>
<li>Idea: by restricting to memoryless policies, generate conditionally independent views.</li>
<li><span class="math inline">\(\mathcal P\)</span> set of all stochastic memoryless policies that have a non-zero probability to explore all actions. Assume <span class="math inline">\(\pi\in \mathcal P\)</span>.</li>
<li>Method
<ul>
<li>Can’t use spectral method for HMM’s.</li>
<li>But same idea: find 3 conditionally independent views (given <span class="math inline">\(x_t, a_t\)</span>), use a “symmetrization” technique, and find spectral decomposition.</li>
</ul></li>
<li>UCRL integration
<ul>
<li>distribution of the views <span class="math inline">\(v_1, v_2, v_3\)</span> depends on the policy used to generate the samples. As a result, whenever the policy changes, the spectral method should be re-run using only the samples collected by that specific policy.</li>
<li>Construct set of admissible POMDP’s whose T, O, R models are in confidence interval</li>
</ul></li>
<li>Open: analyze UCRL for finite horizon.</li>
<li>Stochastic policies are near-optimal in many domains (?). NP-hard to optimize but under some conditions can approximate</li>
</ul></li>
<li>[ALA16] Open Problem - Approximate Planning of POMDPs in the class of Memoryless Policies (COLT2016) <a href="http://www.jmlr.org/proceedings/papers/v49/azizzadenesheli16b.pdf">paper</a>
<ul>
<li>Find exact or approximate optimal stochastic memoryless policy for POMDP.</li>
<li>What [ALA16] don’t address in other paper: planning. (Complexity considerations? i.e. is this tractable? Kaelbling98)</li>
<li>In their paper they assume access to an optimization oracle that gives best memoryless planning policy at end of each episode. - No algorithm for this right now! <!--SoS? First check if you can reduce from Nash equilibrium, etc.--></li>
</ul></li>
<li>[GDB16] A PAC RL algorithm for episodic POMDPs <a href="http://www.jmlr.org/proceedings/papers/v51/guo16b.pdf">paper</a>
<ul>
<li>PAC: whp, selects near-optial action on all but a number of steps poly in problem paramters (what is the definition?)</li>
<li>PAC learns in time <span class="math inline">\(T(\ep)\)</span> means: achieves an expected episodic reward of <span class="math inline">\(V\ge V^*-\ep\)</span> on all but <span class="math inline">\(T(\ep)\)</span> episodes.</li>
<li>First PAC POMDP RL algorithm for episodic domains</li>
<li>EEPORL
<ul>
<li>Algorithm 1:
<ul>
<li>In each episode, take first four steps randomly (in correlated fashion) to explore. Need to assume that have probability of being anywhere in 2 steps.</li>
<li>Take chosen policy for the rest of the steps.</li>
</ul></li>
<li>Algorithm 2: Update estimates for POMDP parameters.</li>
<li>Algorithm 3: Find best policy for current estimates of parameters.</li>
</ul></li>
</ul></li>
</ul>
<h3 id="open-questions">Open questions</h3>
<ul>
<li>[S99] Open Theoretical Questions in Reinforcement Learning (not sure how open these are anymore!)
<ul>
<li>Control with function approximation
<ul>
<li>TD(<span class="math inline">\(\la\)</span>) understood [TsVR97]</li>
<li>Q-learning unstable [Baird95]</li>
<li>Sarsa ??? (tends to oscillate close to best)</li>
</ul></li>
<li>MC ES convergence (see update in BS?)</li>
<li>Bootstrapping more efficient than MC?</li>
<li>VC dimension over RL
<ul>
<li>Difficulty: different actions lead to different parts of space, so we don’t have a natural “test set” that can be reused to evaluate different policies (Test set seems like it would be drawn from different policies?)</li>
<li>Proposal: trajectory trees: tree of all sample transitions</li>
<li>Extend PAC to this setting.</li>
</ul></li>
</ul></li>
</ul>
<h3 id="surveys">Surveys</h3>
<ul>
<li>[G] Reinforcement learning - a Tutorial Survey and Recent Advances.pdf</li>
<li>[KLM96] Reinforcement Learning - A Survey.pdf</li>
<li>[P14] Clearing the Jungle of Stochastic Optimization
<ul>
<li>4 classes of policies</li>
<li>Dynamic vs. stochastic programs</li>
</ul></li>
<li>[P14] Energy and Uncertainty - models and algorithms for complex energy systems.pdf</li>
<li>(*) [P16] A Unified Framework for Optimization under Uncertainty</li>
<li><a href="http://people.csail.mit.edu/agf/Files/13FTML-RLTutorial.pdf">lin function approximators</a></li>
<li><a href="https://hal.archives-ouvertes.fr/hal-00747575v5/document">optimistic principle</a></li>
<li><a href="http://www.morganclaypool.com/doi/abs/10.2200/S00268ED1V01Y201005AIM009">Algorithms for RL</a></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Reinforcement learning convergence</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/rl_convergence.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/rl_convergence.html</id>
    <published>2016-10-24T00:00:00Z</published>
    <updated>2016-10-24T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Reinforcement learning convergence</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-24 
          , Modified: 2016-10-24 
	</p>
      
       <p>Tags: <a href="/tags/reinforcement%20learning.html">reinforcement learning</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#policy-estimation">Policy estimation</a></li>
 <li><a href="#policyvalue-iteration">Policy/value iteration</a><ul>
 <li><a href="#changing-ga">Changing <span class="math inline">$\ga$</span></a></li>
 <li><a href="#alternating-policy-improvementevaluation">Alternating policy improvement/evaluation</a></li>
 </ul></li>
 </ul> </div>

  <div class="blog-main">
    <p>Here we’re interested in convergence guarantees for algorithms/methods used in practice. (Rather than, e.g., coming up with provable polytime theoretical algorithms that work but are too slow to be used in practice.)</p>
<p>Recall: Mixing time <span class="math inline">\(\tau_\ep = \min\set{t}{\max_{P_0} \ve{P_t - P_{\text{eq}}}\le \ep} \le \fc{\ln \pf{N}{\ep}}{\de}\)</span>, <span class="math inline">\(\de=1-\max_{k\ge 2}|\la_k|\)</span>.</p>
<h2 id="policy-estimation">Policy estimation</h2>
Policy satisfies
\begin{align}
v_\pi  &amp;= d(P_\pi^T R_\pi) + \ga P_\pi v\\
\iff 
v_\pi &amp;= (I-\ga P_\pi)^{-1} d(P_\pi^T R_\pi).
\end{align}
Then
\begin{align}
v_{k+1} &amp;= d(P_\pi^T R_\pi) + \ga P_\pi v_k\\
v_{k+1} - v_\pi &amp;= \ga P_\pi (v_k-v_\pi).
\end{align}
<p>So convergence happens at rate of <span class="math inline">\(\ga \ve{P_\pi}_2\)</span> to <span class="math inline">\(v_\pi\)</span>.</p>
<p>For nondiscounted case, we get <span class="math display">\[
v_t = \rc t[d + P_\pi d+ \cdots + P_\pi^{t-1} d].
\]</span> We find (are stochastic matrices diagonalizable?) <span class="math inline">\(v\)</span> is the projection of <span class="math inline">\(d\)</span> onto space of 1-eigenvectors.</p>
<p>(Notation is easier if the reward only depends on <span class="math inline">\(s,a\)</span>; then we just get <span class="math inline">\(v_\pi = r_\pi + \ga P_\pi v_\pi\)</span>.)</p>
<h2 id="policyvalue-iteration">Policy/value iteration</h2>
<p>Why does it improve? Let</p>
<ul>
<li><span class="math inline">\(P_{\pi, s',s} = \Pj(s'|s,\pi(s))\)</span></li>
<li><span class="math inline">\(R_{\pi, s',s} = r(s,\pi(s),s')\)</span></li>
<li><span class="math inline">\(q_{\pi,\pi'} = q_\pi(s,\pi'(s))\)</span>.</li>
</ul>
Write the Bellman equation as (<span class="math inline">\(d(A)\)</span> is the diagonal of <span class="math inline">\(A\)</span>)
\begin{align}
v_\pi &amp;= d(P_\pi^TR) + \ga P_\pi^T v_\pi\\
q_{\pi,\pi'} &amp;= d(P_{\pi'}^TR_{\pi'}) + \ga P_{\pi'}^T v_\pi\\
v_\pi &amp;=(I-\ga P_\pi^T)^{-1} d(P_{\pi'}^T, R_{\pi'})\\
q_{\pi,\pi'} &amp;= (1-\ga P_{\pi'}^T)^{-1} d(P_{\pi'}^TR_{\pi'})
\end{align}
We have
\begin{align}
q_{\pi,\pi'} &amp; \ge q_{\pi,\pi} = v_\pi\\
\iff 
d(P_{\pi'}^TR_{\pi'}) + \ga P_{\pi'}^T v_\pi &amp; \ge v_\pi \\
\iff
v_{\pi'} = (I-\ga P_{\pi'}^T)^{-1} d(P_{\pi'}^TR) &amp; \ge (I-\ga P_{\pi'}^T)^{-1} (I-\ga P_{\pi'}^T) v_\pi = v_\pi.
\end{align}
<!--\ge d(P_\pi^T R_\pi) + P_\pi^T v_\pi-->
<p>(Note <span class="math inline">\(I-\ga P_{\pi'}^T\)</span> is a geometric series so has positive entries.)</p>
<!-- analysis for $q$-values. How improve when $\ga$ increases to 1?-->
<!--$q$-iteration weirder? $q_a^{t+1} = R_a + \ga P_a^t q_b$. No, don't work with q-->
<ol type="1">
<li><p>Value iteration: <span class="math inline">\(\ve{v^{n+1}-v^n}\le \fc{\ep(1-\la)}{2\la}\implies \ve{v^{n+1}-v_\la^*}&lt;\eph\)</span>. (161)</p>
Proof: Let <span class="math inline">\(v^{n+1}\)</span> be the vale if you follow <span class="math inline">\(\pi^n\)</span> after choosing the best <span class="math inline">\(a\)</span> and <span class="math inline">\(v^{*n+1}\)</span> be the value if you follow <span class="math inline">\(\pi^{n+1}\)</span>. Then
\begin{align}
\ve{v^{*n+1} - v^{n+1}} &amp;\le \ve{Lv^{*n+1} - Lv^{n+1}} + \ve{Lv^{n+1} - v^{n+1}}\\
\implies \ve{v^{*n+1} - v^{n+1}} &amp;\le \fc{\la }{1-\la} \ve{v^{n+1}-v^n}
\end{align}
Geometric series gives <span class="math display">\[\ve{v^{n+1} - v_\la^*} \le \fc{\la}{1-\la} \ve{v^{n+1} - v^n}.\]</span></li>
<li><p>Policy iteration (180)</p></li>
</ol>
<h3 id="changing-ga">Changing <span class="math inline">\(\ga\)</span></h3>
<p>Do a triangle inequality between <span class="math display">\[
v_{*'}^{\ga'} = (I-\ga'P_*')^{-1}r_*, v_{*'}^{\ga}, v_\pi^\ga, v_\pi^{\ga'}.
\]</span> More involved with averaging <span class="math inline">\(\lim_{T\to \iy}\rc T\cdots\)</span>. Choose <span class="math inline">\(\ga\)</span> small enough so that can be approximated with rectangles, etc.</p>
<h3 id="alternating-policy-improvementevaluation">Alternating policy improvement/evaluation</h3>
<p><span class="math display">\[\pi'(s) = \amax_a \sum_{s'} p(s'|s,a) [r(s,a,s') + \ga v_\pi(s')].\]</span></p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2016-10-29</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-10-29.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-10-29.html</id>
    <published>2016-10-24T00:00:00Z</published>
    <updated>2016-10-24T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2016-10-29</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-24 
          , Modified: 2016-10-24 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#threads">Threads</a><ul>
 <li><a href="#priority">Priority</a></li>
 <li><a href="#other">Other</a></li>
 </ul></li>
 <li><a href="#other-papers">Other papers</a></li>
 </ul> </div>

  <div class="blog-main">
    <p><a href="2016-10-22.html">Last week</a>. (See wonderings there.)</p>
<h2 id="threads">Threads</h2>
<h3 id="priority">Priority</h3>
<ul>
<li>Come up with a class of MDPs on exponential space that is interesting and tractable. <a href="/posts/tcs/machine_learning/reinforcement_learning/continuous.html">Thoughts</a>
<ul>
<li>Understand provable guarantees on MDP’s first</li>
</ul></li>
<li>Alexa <a href="https://docs.google.com/document/d/1OtvefjviKSSWH2gzOtYo8T_DVEwPEsI2n0kdrC8WlZI/edit">references</a> (Wed-Fri)</li>
</ul>
<h3 id="other">Other</h3>
<ul>
<li>PMI - get some results!
<ul>
<li>Mon. - train CIFAR.</li>
<li>Todos
<ul>
<li>Check MNIST model 1. What are sizes of coefficients?</li>
<li>Run experiments on CIFAR.</li>
</ul></li>
</ul></li>
<li>SoS - chapters 2–5</li>
<li>DL experiments <a href="/posts/tcs/machine_learning/matrices/DL_generalization.html">DL generalization</a></li>
<li>On hold
<ul>
<li>(*) NN learns DL. Can write up weak result, worth doing?</li>
</ul></li>
<li>Papers
<ul>
<li>[HM16] on unsupervised learning (finish)</li>
<li>[HMR16] on dynamical system learning</li>
</ul></li>
</ul>
<p>Analyze Arora and Ge’s NMF algorithm in the presence of noise. Exactly how much noise can it tolerate?</p>
<h2 id="other-papers">Other papers</h2>
<ul>
<li>TODO Read this paper: [CFP16] Assessing significance in a Markov chain without mixing</li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Reinforcement learning theory</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/rl_theory.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/reinforcement_learning/rl_theory.html</id>
    <published>2016-10-22T00:00:00Z</published>
    <updated>2016-10-22T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Reinforcement learning theory</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-22 
          , Modified: 2016-10-24 
	</p>
      
       <p>Tags: <a href="/tags/reinforcement%20learning.html">reinforcement learning</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#known-model">Known model</a></li>
 <li><a href="#unknown-model">Unknown model</a></li>
 <li><a href="#parametrized-policy">Parametrized policy</a></li>
 <li><a href="#references">References</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>Figure out what’s provably known about RL!</p>
<ul>
<li><a href="dl_refs.html">References</a></li>
<li><a href="rl_convergence.html">Convergence for basic algorithms</a></li>
</ul>
<h2 id="known-model">Known model</h2>
<ul>
<li>LP solves in poly time.</li>
<li>Policy improvement converges to global optimum. Is it poly time? (cf. simplex method is not poly-time, but is under smoothed analysis)
<ul>
<li>This seems unclear - it’s an open problem as of 04.</li>
</ul></li>
<li>Does alternating policy estimation/improvement converge? In poly time? (cf. alternating minimization)</li>
<li>What about attaining the optimal Cesaro sum?</li>
</ul>
<h2 id="unknown-model">Unknown model</h2>
<ul>
<li>Episodic: does MC converge? What is the convergence rate (regret)?
<ul>
<li>Exploring starts, etc. (ability to choose starts? cf. optimal learning)</li>
</ul></li>
<li>TD learning (non-episodic): Convergence (or non-convergence) rate of
<ul>
<li>SARSA (model estimation)</li>
<li>Q-learning.</li>
</ul></li>
</ul>
<h2 id="parametrized-policy">Parametrized policy</h2>
<p>Suppose payout is convex in policy parameters. But why would this ever be the case???</p>
<p>Or: have to decide between several experts.</p>
<h2 id="references">References</h2>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Learning structured, robust, and multimodal deep models</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/salakhutdinov.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/neural_nets/salakhutdinov.html</id>
    <published>2016-10-21T00:00:00Z</published>
    <updated>2016-10-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Learning structured, robust, and multimodal deep models</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-21 
          , Modified: 2016-10-21 
	</p>
      
       <p>Tags: <a href="/tags/neural%20networks.html">neural networks</a>, <a href="/tags/deep%20learning.html">deep learning</a>, <a href="/tags/multimodal.html">multimodal</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#abstract">Abstract</a></li>
 <li><a href="#learning-deep-generative-models">Learning deep generative models</a></li>
 <li><a href="#multi-modal-learning">Multi-modal learning</a></li>
 <li><a href="#open-problems">Open problems</a></li>
 <li><a href="#questions">Questions</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="abstract">Abstract</h2>
<p>Building intelligent systems that are capable of extracting meaningful representations from high-dimensional data lies at the core of solving many Artificial Intelligence tasks, including visual object recognition, information retrieval, speech perception, and language understanding. In this talk I will first introduce a broad class of deep learning models and show that they can learn useful hierarchical representations from large volumes of high-dimensional data with applications in information retrieval, object recognition, and speech perception. I will next introduce deep models that are capable of extracting a unified representation that fuses together multiple data modalities as well as present the Reverse Annealed Importance Sampling Estimator (RAISE) for evaluating these deep generative models. Finally, I will discuss models that can generate natural language descriptions (captions) of images, as well as generate images from captions using attention mechanism. I will show that on several tasks, including modelling images and text, these models significantly improve upon many of the existing techniques.</p>
<ul>
<li>Develop statistical models to mine for structure: Deep learning models support inferences and discover structure at multiple levels. <!-- drug rec-->
<ul>
<li>Ex. understanding images (Nearest neighbor sentence: people taking pictures of a crazy person)</li>
</ul></li>
</ul>
<h2 id="learning-deep-generative-models">Learning deep generative models</h2>
<ul>
<li>RBM: visible <span class="math inline">\(v\in B^D\)</span>, hidden <span class="math inline">\(h\in B^F\)</span>, bipartite connections. <span class="math inline">\(\Pj(v,h) \propto \exp(v^TWh + a^Th + b^Tv)\)</span>.
<ul>
<li>Ex. alphabets</li>
<li>Derivative of LL. Partition function difficult to compute!</li>
<li>Can change to Gaussians (real-valued variables), etc.</li>
<li>Word counts (undirected version of topic models) (bag of words)</li>
<li>Easy to infer states of hidden variables <span class="math inline">\(\Pj(h|v)\)</span>.</li>
<li>“Product of experts”: after marginalizing over hidden variables (Government, corruption, and oil give high probability of Putin). Better for info retrieval than traditional topic models.</li>
</ul></li>
<li>DBM
<ul>
<li>Compose representations.</li>
<li>MRF with hidden variables and specific structure</li>
<li>Hidden variables dependent even conditioned on input.</li>
<li>Both <span class="math inline">\(\E\)</span> now intractable</li>
<li>Use variational inference for <span class="math inline">\(\E_{P_{data}}[vh^{1T}]\)</span> and stochastic approximation (MCMC) for <span class="math inline">\(\E_{P_\te}[vh^{1T}]\)</span>.</li>
<li>Handwritten data: real data more diverse, crisp.</li>
<li>Pattern completion (3-D object recognition) <!-- true bayesian hedges bets--></li>
<li>Model A vs. B: Take training example at random and show, vs. RBM. Compute <span class="math inline">\(P\)</span> on validation set. Need estimate of <span class="math inline">\(Z\)</span>. RBM better than mixture of Bernoullis by 50 nats.</li>
<li>Simple importance sampling. Given easy-to-sample-from and intractable target distribution, reweight and use MC approximation. Can’t just draw from uniform distribution!</li>
<li>Annealed importance sampling, <span class="math inline">\(p_0,\ldots, p_K\)</span>. Geometric average <span class="math inline">\(p_\be(x) = f_\be/Z_\be = f_0^{1-\be}/f_{target}(x)^\be/Z_\be\)</span>. If initial is uniform, <span class="math inline">\(p_\be = f_t^\be/Z_\be\)</span>, <span class="math inline">\(\be\)</span> inverse temperature. (Annealing by averaging moments)
<ul>
<li>AIS gives unbiased estimator of <span class="math inline">\(Z_t\)</span>.</li>
<li>We are interested in estimating <span class="math inline">\(\ln Z_t\)</span>. Jensen: <span class="math inline">\(\E \ln Z_t\le \ln Z_t\)</span>. Underestimate! We get a stochastic lower bound.</li>
<li>Log-probability on test set, overestimate <span class="math inline">\(\ln p = \ln f - \ln Z_t\)</span>. <!--If sloppy, model looks nice!--></li>
</ul></li>
<li>Gibbs sampling. Pretend it’s equilibrium after 1000 steps.
<ul>
<li>Unrolled RBM as deep generative model. As approximation to RBM.</li>
<li><span class="math inline">\(p_{fwd}(x_{0:K}) = p_0(x_0)\prodo kK T_k(x_k|x_{k-1})\)</span>.</li>
<li>Reverse AIS estimator (RAISE). Start at data and melt distribution. Tends to underestimate log-probs.</li>
</ul></li>
<li>Learning hierarchical representations.</li>
</ul></li>
<li>Model evaluation: Good way of evaluating!</li>
</ul>
<p>Learn feature representations! <!--textons, audio features--></p>
<h2 id="multi-modal-learning">Multi-modal learning</h2>
<ul>
<li>Image, text, audio. Joint representations?</li>
<li>Product recommendations</li>
<li><p>Robotic</p></li>
<li>Challenges
<ul>
<li>Images are real-valued, text is sparse.</li>
<li>Noisy and missing data</li>
</ul></li>
<li>Multimodal DBM, go up and then down the other way. Define joint distribution over images and text.</li>
<li>Given text, sample from images
<ul>
<li>MIR-Flickr dataset</li>
</ul></li>
<li>Solve supervised learning tasks. Can do better if use unlabeled data. Learn better features and representations.</li>
<li>Can pre-train image pathway and text pathways. <!-- Q: how much can you decouple? --></li>
<li>Complete descriptions of images.
<ul>
<li>Encoder: CNN to semantic feature space.</li>
<li>Decoder: neural language model.</li>
<li>Learn joint embedding space of images and text. Natural definition of scoring function.</li>
<li>Ex. Fluffy.</li>
<li>Multimodal linguistic regularities: Addition and subtraction. (Cat - bowl + box)
<ul>
<li>Bird and reflection: Two birds are trying to be seen in the water.</li>
<li>Giraffe is standing next to a fence in a field.</li>
<li>Handlebars are trying to ride a bike rack.</li>
</ul></li>
<li>Caption generation with visual attention.</li>
<li>Generate images from captions. (school bus flying in blue skies)</li>
<li>Helmholtz machines/variational autoencoders. Directed counterparts. Generative process goes down. Approximate inference going up. Hinton95 (Science). Now it works, Kingma2014 (NIPS)
<ul>
<li>A toilet seat sits open in the bathroom, grass field</li>
<li>Ask google. <!--worked on toilet project--></li>
</ul></li>
</ul></li>
</ul>
<h2 id="open-problems">Open problems</h2>
<ul>
<li>Unsupevised learning/transfer learning/one-shot learning</li>
<li>Reasoning, attention, memory</li>
<li>Natural language understanding
<ul>
<li>Sequence-to-sequence learning</li>
<li>Skip-thought model
<ul>
<li>Generate previous and forward sentence</li>
<li>Objective: sum of log-probabilities for previous/next sentence conditioned on current.</li>
<li>How similar are 2 sentence are on the scale 1 to 5. (A person is performing a trick on a motorcycle? A person is tricking a man on a motorcycle.)</li>
<li>We use no semantic features. <!-- AdaSent --></li>
</ul></li>
</ul></li>
<li>Deep reinforcement learning</li>
</ul>
<p>Neural storytelling. Take corpus of books (romantic), generate captions about the image.</p>
<p>Kiros2015 NIPS</p>
<p>One-shot learning: humans vs. machines. How can we learn novel concept from few examples (Lake, S, Tenenbaum 2015, Science)</p>
<h2 id="questions">Questions</h2>
<p>CNN better for supervised. We’re trying to build convolutional DBM.</p>
<p>vs. variational autoencoder. Reparametrization trick, backprop through whole model. Optimization better for VA. Both useful.</p>
<p>Learning representation, not with language?</p>
<!-- evaluation
neural image on google $10^5$
-->
<p>Microsoft dataset: 80000 images, 5 captions each. Not big enough, but captions clean!</p>
<p>Topics vs. coherent model of sentences. What do we need? New architectures, training sets? <!--need rep to corresp with reality. have memory, check for consistency with memory --></p>
<!--AlphaGo is more technological. Fast, evaluating -->
<p>Actor-Mimic model.</p>
<p>Transfer learning: learn new games faster by leveraging knowledge about previous games. Ex. star gunner</p>
<p>Continuous state.</p>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>Weekly summary 2016-10-22</title>
    <link href="http://holdenlee.github.io/notebook/posts/summaries/2016-10-22.html" />
    <id>http://holdenlee.github.io/notebook/posts/summaries/2016-10-22.html</id>
    <published>2016-10-19T00:00:00Z</published>
    <updated>2016-10-19T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>Weekly summary 2016-10-22</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-19 
          , Modified: 2016-10-19 
	</p>
      
       <p>Tags: <a href="/tags/none.html">none</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#threads">Threads</a></li>
 <li><a href="#other-papers">Other papers</a></li>
 <li><a href="#talk-with-arora-1019-wed">Talk with Arora 10/19 (Wed)</a></li>
 <li><a href="#wonderings">Wonderings</a></li>
 </ul> </div>

  <div class="blog-main">
    <h2 id="threads">Threads</h2>
<ul>
<li>PMI - get some results!</li>
<li>SoS - chapters 2 and 3</li>
<li>DL: do experiments suggested in <a href="/posts/tcs/machine_learning/matrices/DL_generalization.html">DL generalization</a> (Mon, Tue)
<ul>
<li>(*) NN learns DL. (Mon, Tue) - Wrote up progress so far, where I am stuck.</li>
</ul></li>
<li>Papers
<ul>
<li>[HM16] on unsupervised learning (went through 1st half, Tue)</li>
<li>[HMR16] on dynamical system learning (read <a href="http://www.offconvex.org/2016/10/13/gradient-descent-learns-dynamical-systems/">blog post</a> Tue)</li>
</ul></li>
<li>Come up with a class of MDPs on exponential space that is interesting and tractable. <a href="/posts/tcs/machine_learning/reinforcement_learning/exponential.html">Thoughts</a>
<ul>
<li>Understand provable guarantees on MDP’s first</li>
</ul></li>
<li>Alexa <a href="https://docs.google.com/document/d/1OtvefjviKSSWH2gzOtYo8T_DVEwPEsI2n0kdrC8WlZI/edit">references</a></li>
</ul>
<p>Analyze Arora and Ge’s NMF algorithm in the presence of noise. Exactly how much noise can it tolerate?</p>
<h2 id="other-papers">Other papers</h2>
<ul>
<li>TODO Read this paper: [CFP16] Assessing significance in a Markov chain without mixing</li>
</ul>
<h2 id="talk-with-arora-1019-wed">Talk with Arora 10/19 (Wed)</h2>
<p>Dynamical systems + MDP!</p>
<h2 id="wonderings">Wonderings</h2>
<ul>
<li>Can we generalize the random walk of the context vectors? There’s no reason to think that context vectors just drift on the sphere. (p. 139)
<ul>
<li>Make it a RBM. Say with bounded degree. (There are ways to learn - see the factored MDP paper. We don’t care about MDP here, so it’s easier.)</li>
<li>For example, one node (dimension) could simply control output of common words.</li>
<li>Given the observations, whose probs are <span class="math inline">\(\propto e^{w^TAc}\)</span>, learn the RBM. (Note we can replace <span class="math inline">\(w\)</span> by <span class="math inline">\(A^Tw\)</span>… but if <span class="math inline">\(c\)</span> is in larger space, then it’s not obvious how to learn the <span class="math inline">\(A\)</span>! Can we modify the word embeddings to deal with this? Beware of difficulties… HMMs usually assume full column-rank observations, violated here. Look at the proper hard instance for HMM. - the version I saw with noisy parity wasn’t quite a HMM)</li>
<li>Prereq: given <span class="math inline">\((x,h)\)</span> how to learn RBM or Bayes net? (When <span class="math inline">\(W\)</span>’s entries are small enough, can do via MCMC estimation of partition function and optimization of log-likelihood. Otherwise, is hard worst-case.)
<ul>
<li>I’m confused! There seems to be a line of work on factorial MDP’s. However, where are the basic results about learnability of Bayes nets? Learning the model for FMDP’s is strictly harder—why so much work on this (with too much assumptions, or weak results) without results on learning Bayes nets?</li>
<li>Bresler.</li>
</ul></li>
<li>cf. work on continuous HMM’s. Work on factored HMMs? Any bounds when hidden state has larger dimension? Also, adapt HMM learning to vector observations. (Is the natural generalization a factored prob model rather than a dynamical system? Note probabilistic linear dynamical system IS straightforward generalization of HMM, but the factored prob model is not. Weird generalization though, because only having states <span class="math inline">\(\{e_1,\ldots, e_n\}\)</span> seems decoupled - can couple together any way you want.) <span class="citation" data-cites="Andrej">@Andrej</span> on this.
<ul>
<li>When state has larger dimension, need overcomplete tensor factorization.</li>
</ul></li>
<li>Start with: given an HMM with both transitions and observations being RBMs (say of degree at most 2), observations don’t “lose info” (analogue of full column rank), infer RBM. (Z is over words that exist). Breaks symmetries - the various dimensions are important now? <!-- sparse vectors are meaningful --></li>
</ul></li>
<li>Dictionary learning experiment
<ul>
<li>The kernel DL I want is different from in the literature. There they want <span class="math inline">\(\Phi(Y) \approx \Phi(A)X\)</span>, here we want <span class="math inline">\(\Phi(Y) \approx \Phi(AX)\)</span>. I.e. we want to maximize <span class="math inline">\(K(Y,AX)\)</span> where <span class="math inline">\(X\)</span> is restricted to be sparse. Usual algorithms break down here, but can still consider <span class="math inline">\(K(Y,AX) + \ve{X}_1\)</span>. (137)</li>
<li>Use kernel in <a href="/posts/tcs/machine_learning/neural_nets/PMDH16.html">PMDH16</a>.</li>
</ul></li>
<li>RL questions (135)
<ul>
<li>Given a (continuous) space of policies, converge to a local min in the space of policies.</li>
<li>Find some measure of complexity of a class of policies. Branching is important. (Getting limited info from other policies…) Get a bound independent of number of states, involving this complexity.
<ul>
<li>Example to keep in mind: <span class="math inline">\(2^n\)</span> strategies all branching off into different rewards at end of their paths.</li>
<li>Alternatively, complexity of class of models of environment.</li>
</ul></li>
<li>What is the VC dimension bound for contextual (expert) bandits? Also look at the contextual MDP paper, cf. EXP4.</li>
<li>[ALA16] open question</li>
<li>Scraps
<ul>
<li>Right <span class="math inline">\(\la\)</span>, how do well without learning model?</li>
<li>SoS, minimax, etc.</li>
<li>EXP3:Scrible:EXP4::UCB1:?:? (LinUCB? RUCB?)</li>
</ul></li>
<li>Increasing <span class="math inline">\(\ga\)</span> towards 1 (simulated annealing, temperature schedule…)</li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>
<entry>
    <title>DL experiments</title>
    <link href="http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/dl_experiments.html" />
    <id>http://holdenlee.github.io/notebook/posts/tcs/machine_learning/matrices/dl_experiments.html</id>
    <published>2016-10-17T00:00:00Z</published>
    <updated>2016-10-17T00:00:00Z</updated>
    <summary type="html"><![CDATA[
<div class="container">
  <div id="content">
    <div class="page header">
      <h1>DL experiments</h1>
    </div>
    <div class="info">
      
       
        <p>Posted: 2016-10-17 
          , Modified: 2016-10-17 
	</p>
      
       <p>Tags: <a href="/tags/dictionary%20learning.html">dictionary learning</a></p> 
    </div>
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#experiments">Experiments</a></li>
 <li><a href="#results">Results</a><ul>
 <li><a href="#first-observations">First observations</a></li>
 </ul></li>
 <li><a href="#evaluation">Evaluation</a></li>
 <li><a href="#code">Code</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>What are convergence guarantees for dictionary learning? Consider the settings</p>
<ul>
<li>AGMM15 (Alternating minimization)</li>
<li>2-layer NN
<ul>
<li>With <span class="math inline">\(b^Ty\)</span></li>
<li>With <span class="math inline">\(\sgn(b^Ty)\)</span></li>
</ul></li>
</ul>
<!--
Sanjeev told me you did some experiments, so I wanted to check with you.

Experimentally, does dictionary learning converge to the right dictionary under random initialization? What if we randomly initialize with samples drawn from $x=Ah$? What about for the neural net (backprop) model you showed me last time - does random initialization (with samples) converge to the dictionary? If you have code for experiments, please send it to me.

I've done a lot of the calculations for neural nets learning dictionaries, and am getting stuck on the following: it appears that the gradient of the entire matrix is correlated with the right direction, but individual rows may not be (so a row may get far away until it no longer decodes correctly). Did you encounter something like this? If you have the bandwidth I'd be interested in working with you on this.

-->
<h2 id="experiments">Experiments</h2>
<p>Code is in <code>dl_convergence.py</code>. Run on ionic.</p>
<h2 id="results">Results</h2>
<!--1218589: -->
<ul>
<li>s = 3</li>
<li>m = 50 # hidden vector</li>
<li>n = 25 # observed vector</li>
<li>q = s/m</li>
<li>alpha = .01</li>
<li>batchsize = 1024</li>
<li>vary sigma (how close initialization is) <!-- * Approximate convergence for sigma = .05, .1; not 0.5--></li>
</ul>
<p>Next,</p>
<ul>
<li>add random initialization - check</li>
<li>vary (s,m,n)</li>
<li>check sparsity of learned vectors (do thresholding too) - check</li>
<li>add initialization from samples - check
<ul>
<li>try overcomplete initialization from samples - check</li>
</ul></li>
</ul>
<h3 id="first-observations">First observations</h3>
<p>See <code>am_dl_3_50_25.txt</code> and <code>slurm-1218768.out</code></p>
<ul>
<li>Converges when close enough (as in AGMM15). For this, even 0.5 is close enough. Note it doesn’t converge to <span class="math inline">\(A\)</span> - it converges to something that has columns <span class="math inline">\(\approx 0.1\)</span> away from <span class="math inline">\(A\)</span>, consistant bias. (This makes sense.)</li>
<li>Random initialization does not converge to global optimum. Initialization with samples seems to do slightly better.</li>
</ul>
<h2 id="evaluation">Evaluation</h2>
<p>How to evaluate?</p>
<ul>
<li>Closeness of columns.</li>
<li>Loss: how much sparsity, and how far away. (Reconstruction error)
<ul>
<li>How does reconstruction error compare to SVD? (Make dimensions comparable.)</li>
</ul></li>
<li>Put in random SVM on top. Can it learn the SVM well?</li>
<li>Check framework in [HM16].</li>
</ul>
<h2 id="code">Code</h2>
<ul>
<li>Displaying images
<ul>
<li><a href="http://stackoverflow.com/questions/902761/saving-a-numpy-array-as-an-image">No PIL</a></li>
<li><a href="http://stackoverflow.com/questions/2659312/how-do-i-convert-a-numpy-array-to-and-display-an-image">PIL</a></li>
<li><a href="https://pillow.readthedocs.io/en/3.4.x/reference/index.html">Pillow</a></li>
</ul></li>
</ul>

  </div>

    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class='st_facebook_large' displayText='Facebook'></span>
    <span class='st_twitter_large' displayText='Tweet'></span>
    <span class='st_googleplus_large' displayText='Google +'></span>
    <span class='st_reddit_large' displayText='Reddit'></span>
    <span class='st__large' displayText=''></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
</div>
]]></summary>
</entry>

</feed>
